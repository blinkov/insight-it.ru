<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Insight IT</title><link>https://www.insight-it.ru/</link><description></description><atom:link href="https://www.insight-it.ru/tag/highload/feed/index.xml" rel="self"></atom:link><lastBuildDate>Mon, 28 Nov 2011 01:32:00 +0400</lastBuildDate><item><title>Архитектура Google 2011</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-google-2011/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-google/"&gt;Архитектура Google&lt;/a&gt;
была одной из первых статьей на &lt;strong class="trebuchet"&gt;Insight IT&lt;/strong&gt;. Именно она дала толчок развитию проекта: после её публикации посещаемость блога увеличилась в десятки раз и появились первые сотни подписчиков. Прошли годы, информация устаревает стремительно, так что пришло время взглянуть на Google еще раз, теперь уже с позиции конца 2011 года. Что мы увидим нового в архитектуре интернет-гиганта?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Общее&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Ежедневная аудитория Google составляет около 1 миллиарда
    человек&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;По данным Alexa больше половины аудитории интернета каждый
    день пользуются Google&lt;/li&gt;
&lt;li&gt;По данным IWS аудитория интернета составляет 2.1 миллиарда
    человек&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Используется более 900 тысяч серверов&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;Планируется расширение до 10 миллионов серверов в обозримом
    будущем&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/4cb67b65/" rel="nofollow" target="_blank" title="http://www.google.com/about/datacenters/locations/index.html"&gt;12 основных датацентров в США&lt;/a&gt;,
    присутствие в большом количестве точек по всему миру (более 38)&lt;/li&gt;
&lt;li&gt;Около 32 тысяч сотрудников в 76 офисах по всему миру&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Поиск&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;За последние 14 лет среднее время обработки одного поискового
    запроса уменьшилось с 3 секунд до менее 100 миллисекунд, то есть
    в 30 раз&lt;/li&gt;
&lt;li&gt;Более 40 миллиардов страниц в индексе, если приравнять каждую к
    листу А4 они бы покрыли территорию США в 5 слоев&lt;/li&gt;
&lt;li&gt;Более 1 квинтиллиона уникальных URL (10 в 18 степени); если
    распечатать их в одну строку, её длина составит 51 миллион
    километров, треть расстояния от Земли до Солнца&lt;/li&gt;
&lt;li&gt;В интернете встречается примерно 100 квинтиллионов слов, чтобы
    набрать их на клавиатуре одному человеку потребовалось бы
    примерно 5 миллионов лет&lt;/li&gt;
&lt;li&gt;Проиндексировано более 1.5 миллиардов изображений, чтобы их
    сохранить потребовалось бы 112 миллионов дискет, которые можно
    сложить в стопку высотой 391 километр&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gmail&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Активных пользователей более 170 миллионов&lt;/li&gt;
&lt;li&gt;Второй по популярности почтовый сервис в США, третий в мире (по
    данным comScore)&lt;/li&gt;
&lt;li&gt;При текущем темпе роста аудитории GMail и конкурентов, он станет
    лидером рынка через 2-3 года&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Google+&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Более 40 миллионов пользователей на октябрь 2011, при запуске в
    июне 2011&lt;/li&gt;
&lt;li&gt;25 миллионов пользователей за первый месяц&lt;/li&gt;
&lt;li&gt;70:30 примерное соотношение мужчин и женщин&lt;/li&gt;
&lt;li&gt;Себестоимость разработки больше полумиллиарда долларов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;YouTube&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Загружается более 13 миллионов часов видео в год&lt;/li&gt;
&lt;li&gt;Каждую минуту загружается 48 часов видео, что соответствует
    почти 8 годам контента или 52 тысячам полнометражных фильмов в
    день&lt;/li&gt;
&lt;li&gt;Более 700 миллиардов просмотров видео в год&lt;/li&gt;
&lt;li&gt;Месячная аудитория составляет 800 миллионов уникальных
    посетителей&lt;/li&gt;
&lt;li&gt;Несколько тысяч полнометражных фильмов в &lt;a href="https://www.insight-it.ru/goto/18cd7d94/" rel="nofollow" target="_blank" title="http://www.youtube.com/movies"&gt;YouTube Movies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Более 10% всех видео в формате HD&lt;/li&gt;
&lt;li&gt;13% просмотров (400 миллионов в день) происходит с мобильных
    устройств&lt;/li&gt;
&lt;li&gt;До сих пор работает в убыток, лишь 14% просмотров видео приносят
    выручку с рекламы&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Финансы&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Выручка порядка 36 миллиардов долларов в год&lt;/li&gt;
&lt;li&gt;Прибыль после налогов порядка 10 миллиардов долларов в год&lt;/li&gt;
&lt;li&gt;Капитализация порядка 200 миллиардов долларов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Google&lt;/strong&gt; - огромная интернет-компания, неоспоримый лидер на рынке
поиска в Интернет и владелец большого количества продуктов, многие из
которых также добились определенного успеха в своей нише.&lt;/p&gt;
&lt;p&gt;В отличии от большинства интернет-компаний, которые занимаются лишь
одним продуктом (проектом), архитектура Google не может быть
представлена как единое конкретное техническое решение. Сегодня мы
скорее будем рассматривать общую стратегию технической реализации
интернет-проектов в Google, возможно слегка затрагивая и другие аспекты
ведения бизнеса в Интернет.&lt;/p&gt;
&lt;p&gt;Все продукты Google основываются на постоянно развивающейся программной
платформе, которая спроектирована с учетом работы на миллионах серверов,
находящихся в разных датацентрах по всему миру.&lt;/p&gt;
&lt;h3 id="oborudovanie"&gt;Оборудование&lt;/h3&gt;
&lt;p&gt;Обеспечение работы миллиона серверов и расширение их парка - одна из
ключевых статей расходов Google. Для минимизации этих издержек очень
большое внимание уделяется эффективности используемого серверного,
сетевого и инфраструктурного оборудования.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Использование энергии" class="right" src="https://www.insight-it.ru/images/dc-home-energy-use.png" title="Использование энергии"/&gt;&lt;/p&gt;
&lt;p&gt;В традиционных датацентрах потребление электричества серверами примерно
равно его потреблению остальной инфраструктурой, Google же удалось
снизить процент использования дополнительной электроэнергии до 14%.
Таким образом суммарное энергопотребление датацентром Google сравнимо с
потреблением только серверов в типичном датацентре и вдвое меньше его
общего энергопотребления. Основные концепции, которые используются для
достижения этого результата:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Точное измерение потребления электроэнергии всеми компонентами
    позволяет определить возможности по его уменьшению;&lt;/li&gt;
&lt;li&gt;В датацентрах Google тепло, что позволяет экономить на охлаждении;&lt;/li&gt;
&lt;li&gt;При проектировании датацентра уделяется внимание даже незначительным
    деталям, позволяющим сэкономить даже немного - при таком масштабе
    это окупается;&lt;/li&gt;
&lt;li&gt;Google умеет охлаждать датацентры практически без кондиционеров, с
    использованием воды и её испарения &lt;em&gt;(см. &lt;a href="https://www.insight-it.ru/goto/d1cf5d6b/" rel="nofollow" target="_blank" title="http://www.youtube.com/watch?v=VChOEvKicQQ"&gt;как это реализовано&lt;/a&gt; в Финляндии)&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В Google активно&amp;nbsp;пропагандируют максимальное использование
возобновляемой энергии. Для этого заключаются долгосрочные соглашения с
её поставщиками (на 20 и более лет), что позволяет отрасли активно
развиваться и наращивать мощности. Проекты по генерации возобновляемой
энергии, спонсируемые Google, имеют суммарную мощность более 1.7
гигаватт, что существенно больше, чем используется для работы Google.
Этой мощности хватило бы для обеспечения электричеством 350 тысяч домов.&lt;/p&gt;
&lt;p&gt;Если говорить о жизненном цикле оборудования, то используются следующие
принципы:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Уменьшение транспортировки:&lt;/strong&gt; там, где это возможно, тяжелые
    компоненты (вроде серверных стоек) закупаются у местных поставщиков,
    даже если в других местах аналогичный товар можно было бы купить
    дешевле.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Повторное использование:&lt;/strong&gt; прежде, чем покупать новое оборудование
    и материалы, рассматриваются возможности по использованию уже
    имеющихся. Этот принцип помог избежать покупки более 90 тысяч новых
    серверов.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Утилизация&lt;/strong&gt;: в тех случаях, когда повторное использование
    невозможно, оборудование полностью очищается от данных и продается
    на вторичном рынке. То, что не удается продать, разбирается на
    материалы (медь, сталь, алюминий, пластик и.т.п.) для последующей
    правильной утилизации специализированными компаниями.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Google известны за свои эксперименты и необычные решения в области
серверного оборудования и инфраструктуры. Некоторые запатентованы;
какие-то прижились, какие-то - нет. Подробно останавливаться на них не
буду, лишь вкратце о некоторых:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Резервное питание, интегрированное в блок питания
    &lt;a href="https://www.insight-it.ru/goto/ca5b8b43/" rel="nofollow" target="_blank" title="http://www.youtube.com/watch?v=xgRWURIxgbU"&gt;сервера&lt;/a&gt;, обеспеченное
    стандартными 12V батарейками;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/c7f3ff41/" rel="nofollow" target="_blank" title="http://www.datacenterknowledge.com/closer-look-googles-server-sandwich-design/"&gt;"Серверный сендвич"&lt;/a&gt;,
    где материнские платы с двух сторон окружают водяную систему
    теплоотвода в центре стойки;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8dc33e81/" rel="nofollow" target="_blank" title="http://www.youtube.com/watch?v=zRwPSFpLX8I"&gt;Датацентр из контейнеров&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В заключении этого раздела хотелось бы взглянуть правде в глаза:
&lt;strong&gt;идеального оборудования не бывает&lt;/strong&gt;. У любого современного устройства,
будь то сервер, коммутатор или маршрутизатор, есть шанс прийти в
негодность из-за производственного брака, случайного стечения
обстоятельств или других внешних факторов. Если умножить этот, казалось
бы, небольшой шанс на количество оборудования, которое используется в
Google, то окажется, что чуть ли не каждую минуту из строя выходит одно,
или несколько, устройств в системе. На оборудование полагаться нельзя,
по-этому вопрос отказоустойчивости переносится на плечи программной
платформы, которую мы сейчас и рассмотрим.&lt;/p&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;p&gt;В Google очень рано столкнулись с проблемами ненадежности оборудования и
работы с огромными массивами данных. Программная платформа,
спроектированная для работы на многих недорогих серверах, позволила им
абстрагироваться от сбоев и ограничений одного сервера.&lt;/p&gt;
&lt;p&gt;Основными задачами в ранние годы была минимизация точек отказа и
обработка больших объемов слабоструктурированных данных. Решением этих
задач стали три основных слоя платформы Google, работающие один поверх
другого:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/479dfa95/" rel="nofollow" target="_blank" title="http://research.google.com/archive/gfs.html"&gt;Google File System&lt;/a&gt;:&lt;/strong&gt;
    распределенная файловая система, состоящая из сервера с метаданными
    и теоретически неограниченного количества серверов, хранящих
    произвольные данные в блоках фиксированного размера.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/f56e059a/" rel="nofollow" target="_blank" title="http://research.google.com/archive/bigtable.html"&gt;BigTable&lt;/a&gt;:&lt;/strong&gt;
    распределенная база данных, использующая для доступа к данным две
    произвольных байтовых строки-ключа (обозначающие строку и столбец) и
    дату/время (обеспечивающие версионность).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/dbd8fea6/" rel="nofollow" target="_blank" title="http://research.google.com/archive/mapreduce.html"&gt;MapReduce&lt;/a&gt;:&lt;/strong&gt;
    механизм распределенной обработки больших объемов данных,
    оперирующий парами ключ-значение для получения требуемой информации.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Такая комбинация, дополненная другими технологиями, довольно долгое
время позволяла справляться с индексацией Интернета, пока... скорость
появления информации в Интернете не начала расти огромными темпами из-за
"бума социальных сетей". Информация, добавленная в индекс даже через
полчаса, уже зачастую становилась устаревшей.&amp;nbsp;В дополнение к этому в
рамках самого Google стало появляться все больше продуктов,
предназначенных для работы в реальном времени.&lt;/p&gt;
&lt;p&gt;Спроектированные с учетом совершенно других требований Интернета
пятилетней давности компоненты, составляющие ядро платформы Google,
потребовали фундаментальной смены архитектуры индексации и поиска,
который около года назад был представлен публике&amp;nbsp;под кодовым названием
&lt;strong&gt;Google Caffeine&lt;/strong&gt;. Новые, переработанные, версии старых "слоев" также
окрестили броскими именами, но резонанса у технической публики они
вызвали намного меньше, чем новый поисковый алгоритм в SEO-индустрии.&lt;/p&gt;
&lt;h4&gt;Google Colossus&lt;/h4&gt;
&lt;p&gt;Новая архитектура GFS была спроектирована для минимизации задержек при
доступе к данным (что критично для приложений вроде GMail и YouTube), не
в ущерб основным свойствам старой версии: отказоустойчивости и
прозрачной масштабируемости.&lt;/p&gt;
&lt;p&gt;В оригинальной же реализации упор был сделан на повышение общей
пропускной способности: операции объединялись в очереди и выполнялись
разом, при таком подходе можно было прождать пару секунд еще до того,
как первая операция в очереди начнет выполняться. Помимо этого в старой
версии было большое слабое место в виде единственно мастер-сервера с
метаданными, сбой в котором грозил недоступностью всей файловой системы
в течении небольшого промежутка времени (пока другой сервер не подхватит
его функции, изначально это занимало около 5 минут, в последних версиях
порядка 10 секунд) - это также было вполне допустимо при отсутствии
требования работы в реальном времени, но для приложений, напрямую
взаимодействующих с пользователями, это было неприемлемо с точки зрения
возможных задержек.&lt;/p&gt;
&lt;p&gt;Основным нововведением в Colossus стали распределенные мастер-сервера,
что позволило избавиться не только от единственной точки отказа, но и
существенно уменьшить размер одного блока с данными (с 64 до 1
мегабайта), что в целом очень положительно сказалось на работе с
небольшими объемами данных. В качестве бонуса исчез теоретический предел
количества файлов в одной системе.&lt;/p&gt;
&lt;p&gt;Детали распределения ответственности между мастер-серверами, сценариев
реакции на сбои, а также сравнение по задержкам и пропускной
способности обоих версий, к сожалению, по-прежнему конфиденциальны. Могу
предположить, что используется вариация на тему хэш-кольца с репликацией
метаданных на ~3 мастер-серверах, с созданием дополнительной копии на
следующем по кругу сервере в случае в случае сбоев, но это лишь догадки.
Если у кого-то есть относительно официальная информация на этот счет -
буду рад увидеть в комментариях.&lt;/p&gt;
&lt;p&gt;По прогнозам Google текущий вариант реализации распределенной файловой
системы "уйдет на пенсию" в 2014 году из-за популяризации твердотельных
накопителей и существенного скачка в области вычислительных технологий
(процессоров).&lt;/p&gt;
&lt;h4&gt;Google Percolator&lt;/h4&gt;
&lt;p&gt;MapReduce отлично справлялся с задачей полной перестройки поискового
индекса, но не предусматривал небольшие изменения, затрагивающие лишь
часть страниц. Из-за потоковой, последовательной природы MapReduce для
внесения изменений в небольшую часть документов все равно пришлось бы
обновлять весь индекс, так как новые страницы непременно будут каким-то
образом связаны со старыми. Таким образом задержка между появлением
страницы в Интернете и в поисковом индексе при использовании MapReduce
была пропорциональна общему размеру индекса (а значит и Интернета,
который постоянно растет), а не размеру набора измененных документов.&lt;/p&gt;
&lt;p&gt;Ключевые архитектурные решения, лежащие в основе MapReduce, не позволяли
повлиять на эту особенность и в итоге система индексации была построена
заново с нуля, а MapReduce продолжает использоваться в других проектах
Google для аналитики и прочих задач, по прежнему не связанных с реальным
временем.&lt;/p&gt;
&lt;p&gt;Новая система получила довольно своеобразное название &lt;strong&gt;Percolator&lt;/strong&gt;,
попытки узнать что оно значит приводит к различным устройствам по
фильтрации дыма, кофеваркам и непойми чему еще. Но наиболее адекватное
объяснение мне пришло в голову, когда я прочитал его по слогам: per
col - &lt;em&gt;по колонкам&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Percolator представляет собой надстройку над BigTable, позволяющую
выполнять комплексные вычисления на основе имеющихся данных,
затрагивающие много строк и даже таблиц одновременно (в стандартном API
BigTable это не предусмотрено).&lt;/p&gt;
&lt;p&gt;Веб-документы или любые другие данные изменяются/добавляются в систему
посредством модифицированного API BigTable, а дальнейшие изменения в
остальной базе осуществляются посредством механизма&amp;nbsp;"обозревателей".
Если говорить в терминах реляционных СУБД, то обозреватели - что-то
среднее между триггерами и хранимыми процедурами. Обозреватели
представляют собой подключаемый к базе данных код (на &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;),
который исполняется в случае возникновении изменений в определенных
&lt;em&gt;колонках&lt;/em&gt; BigTable (откуда, видимо, и пошло название). Все используемые
системой метаданные также хранятся в специальных колонках BigTable. При
использовании Percolator все изменения происходят в транзакциях,
удовлетворяющих принципу ACID, каждая из которых затрагивает именно те
сервера в кластере, на которых необходимо внести изменения. Механизм
транзакций на основе BigTable разрабатывался в рамках отдельного проекта
под названием &lt;a href="https://www.insight-it.ru/storage/2011/google-megastore/"&gt;Google Megastore&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Таким образом, при добавлении нового документа (или его версии) в
поисковый индекс, вызывается цепная реакция изменений в старых
документах, скорее всего ограниченная по своей рекурсивности. Эта
система при осуществлении случайного доступа поддерживает индекс в
актуальном состоянии.&lt;/p&gt;
&lt;p&gt;В качестве бонуса в этой схеме удалось избежать еще двух недостатков
MapReduce:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Проблемы "отстающих":&lt;/strong&gt; когда один из серверов (или одна из
    конкретных подзадач) оказывался существенно медленнее остальных, что
    также значительно задерживало общее время завершения работы
    кластера.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Пиковая нагрузка:&lt;/strong&gt; MapReduce не является непрерывным процессом, а
    разделяется на работы с ограниченной целью и временем исполнения.
    Таким образом помимо необходимости ручной настройки работ и их
    типов, кластер имеет очевидные периоды простоя и пиковой нагрузки,
    что ведет к неэффективному использованию вычислительных ресурсов.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Но все это оказалось не бесплатно: при переходе на новую систему удалось
достичь той же скорости индексации, но при этом использовалось &lt;em&gt;вдвое&lt;/em&gt;
больше вычислительных ресурсов. Производительность Percolator находится
где-то между производительностью MapReduce и производительностью
традиционных СУБД. Так как Percolator является распределенной системой,
для обработки фиксированного небольшого количества данных ей приходится
использовать существенно больше ресурсов, чем традиционной СУБД; такова
цена масштабируемости. По сравнению с MapReduce также пришлось платить
дополнительными потребляемыми вычислительными ресурсами за возможность
случайного доступа с низкой задержкой.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Google Percolator Benchmark" class="left" src="https://www.insight-it.ru/images/google-percolator-benchmark.png" title="Google Percolator Benchmark"/&gt;&lt;/p&gt;
&lt;p&gt;Тем не менее, при выбранной архитектуре Google удалось достичь
практически линейного масштабирования при увеличении вычислительных
мощностей на много порядков &lt;em&gt;(см. график, основан на тесте TPC-E)&lt;/em&gt;.
Дополнительные накладные расходы, связанные с распределенной природой
решения, в некоторых случаях до 30 раз превосходят аналогичный
показатель традиционных СУБД, но у данной системы есть солидный простор
для оптимизации в этом направлении, чем Google активно и занимается.&lt;/p&gt;
&lt;h4&gt;Google Spanner&lt;/h4&gt;
&lt;p&gt;Spanner представляет собой единую систему автоматического управления
ресурсами &lt;strong&gt;всего парка серверов&lt;/strong&gt; Google.&lt;/p&gt;
&lt;p&gt;Основные особенности:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Единое пространство имен:&lt;ul&gt;
&lt;li&gt;Иерархия каталогов&lt;/li&gt;
&lt;li&gt;Независимость от физического расположения данных&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Поддержка слабой и сильной целостности данных между датацентрами&lt;/li&gt;
&lt;li&gt;Автоматизация:&lt;ul&gt;
&lt;li&gt;Перемещение и добавление реплик данных&lt;/li&gt;
&lt;li&gt;Выполнение вычислений с учетом ограничений и способов
    использования&lt;/li&gt;
&lt;li&gt;Выделение ресурсов на всех доступных серверах&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Зоны полу-автономного управления&lt;/li&gt;
&lt;li&gt;Восстановление целостности после потерь соединения между
    датацентрами&lt;/li&gt;
&lt;li&gt;Возможность указания пользователями высокоуровневых требований,
    например:&lt;ul&gt;
&lt;li&gt;99% задержек при доступе к этим данным должны быть до 50 мс&lt;/li&gt;
&lt;li&gt;Расположи эти данные на как минимум 2 жестких дисках в Европе, 2
    в США и 1 в Азии&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Интеграция не только с серверами, но и с сетевым оборудованием, а
    также системами охлаждения в датацентрах&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Проектировалась из расчета на:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1-10 миллионов серверов&lt;/li&gt;
&lt;li&gt;~10 триллионов директорий&lt;/li&gt;
&lt;li&gt;~1000 петабайт данных&lt;/li&gt;
&lt;li&gt;100-1000 датацентров по всему миру&lt;/li&gt;
&lt;li&gt;~1 миллиард клиентских машин&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Об этом проекте Google известно очень мало, официально он был
представлен публике лишь однажды в 2009 году, с тех пор лишь местами
упоминался сотрудниками без особой конкретики. Точно не известно
развернута ли эта система на сегодняшний день и если да, то в какой
части датацентров, а также каков статус реализации заявленного
функционала.&lt;/p&gt;
&lt;h4&gt;Прочие компоненты платформы&lt;/h4&gt;
&lt;p&gt;Платформа Google в конечном итоге сводится к набору сетевых сервисов и
библиотек для доступа к ним из различных языков программирования (в
основном используются&amp;nbsp;&lt;a href="/tag/c/"&gt;C/C++&lt;/a&gt;,&amp;nbsp;&lt;a href="/tag/java/"&gt;Java&lt;/a&gt;,&amp;nbsp;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt; и&amp;nbsp;&lt;a href="/tag/perl/"&gt;Perl&lt;/a&gt;). Каждый продукт, разрабатываемый Google, в большинстве случаев использует эти библиотеки для осуществления доступа к данным, выполнения комплексных вычислений и других задач, вместо стандартных механизмов, предоставляемых операционной системой, языком программирования или opensource библиотеками.&lt;/p&gt;
&lt;p&gt;Вышеизложенные проекты составляют лишь основу платформы Google, хотя она
включает в себя куда больше готовых решений и библиотек, несколько
примеров из публично доступных проектов:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/f5686a9/" rel="nofollow" target="_blank" title="http://code.google.com/webtoolkit/"&gt;GWT&lt;/a&gt; для реализации
    пользовательских интерфейсов на &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7f9cb797/" rel="nofollow" target="_blank" title="http://code.google.com/closure/"&gt;Closure&lt;/a&gt; - набор инструментов для
    работы с &lt;a href="/tag/javascript/"&gt;JavaScript&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/41604156/" rel="nofollow" target="_blank" title="http://code.google.com/apis/protocolbuffers/"&gt;Protocol Buffers&lt;/a&gt; -
    не зависящий от языка программирования и платформы формат бинарной
    сериализации структурированных данных, используется при
    взаимодействии большинства компонентов системы внутри Google;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/bb496d06/" rel="nofollow" target="_blank" title="http://code.google.com/p/leveldb/"&gt;LevelDB&lt;/a&gt; -
    высокопроизводительная встраиваемая &lt;a href="/tag/subd/"&gt;СУБД&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/44c46d8/" rel="nofollow" target="_blank" title="http://code.google.com/p/snappy/"&gt;Snappy&lt;/a&gt; - быстрая компрессия
    данных, используется при хранении данных в GFS.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi_1"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Стабильные, проработанные и повторно используемые базовые
    компоненты проекта&lt;/strong&gt; &lt;em&gt;- залог её стремительного развития, а также
    создания новых проектов на той же кодовой базе&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Если задачи и обстоятельства, с учетом которых проектировалась
    система, существенно изменились&amp;nbsp;&lt;em&gt;- не бойтесь вернуться на стадию проектирования и реализовать новое решение&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Используйте инструменты, подходящие для решения каждой конкретной
    задачи&lt;/em&gt;, а не те, которые навязывает мода или привычки участников
    команды.&lt;/li&gt;
&lt;li&gt;Даже, казалось бы, незначительные недоработки и допущения на большом
    масштабе могут вылиться в огромные потери &lt;em&gt;- уделяйте максимум
    внимания деталям при реализации проекта.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Нельзя полагаться даже на очень дорогое оборудование &lt;em&gt;- все ключевые
    сервисы должны работать минимум на двух серверах, в том числе и базы
    данных.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Распределенная платформа, общая для всех проектов, позволит новым
    разработчикам легко вливаться в работу над конкретными продуктами, с
    минимумом представления о внутренней архитектуре компонентов
    платформы.&lt;/li&gt;
&lt;li&gt;Прозрачная работа приложений в нескольких датацентрах - одна из
    самых тяжелых задач, с которыми сталкиваются интернет-компании.
    Сегодня каждая из них решает её по-своему и держит подробности в
    секрете, что сильно замедляет развитие opensource решений.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;p&gt;Не гарантирую достоверность всех нижеизложенных источников информации,
ставших основой для данной статьи, но ввиду конфиденциальности подобной
информации на большее рассчитывать не приходится.&lt;/p&gt;
&lt;p&gt;Поправки и уточнения приветствуются :)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/4431029a/" rel="nofollow" target="_blank" title="http://www.google.com/about/datacenters/index.html"&gt;Official Google Data Centers
    Site&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7437fe8a/" rel="nofollow" target="_blank" title="http://research.google.com/people/jeff/WSDM09-keynote.pdf"&gt;Challenges in Building Large-Scale Information Retrieval
    Systems&lt;/a&gt;
    (Jeff Dean, WCDMA '09)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7f1ddbff/" rel="nofollow" target="_blank" title="http://www.odbms.org/download/dean-keynote-ladis2009.pdf"&gt;Designs, Lessons and Advice from Building Large Distributed
    Systems&lt;/a&gt;
    (Jeff Dean, Ladis '09)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/6ec4bc06/" rel="nofollow" target="_blank" title="http://research.google.com/pubs/pub36726.html"&gt;Google Percolator official
    paper&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/3473b129/" rel="nofollow" target="_blank" title="http://research.google.com/pubs/pub36971.html"&gt;&lt;em&gt;Google Megastore official
    paper&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/dd8bec64/" rel="nofollow" target="_blank" title="http://www.theregister.co.uk/2010/09/24/google_percolator/"&gt;Google
    Percolator&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/82efffd8/" rel="nofollow" target="_blank" title="http://www.theregister.co.uk/2010/09/09/google_caffeine_explained/"&gt;Google Caffeine
    Explained&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/63b6ec67/" rel="nofollow" target="_blank" title="http://www.theregister.co.uk/2009/10/23/google_spanner/"&gt;Google
    Spanner&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/46520ede/" rel="nofollow" target="_blank" title="http://www.theregister.co.uk/2011/06/08/google_software_infrastructure_dubbed_obsolete_by_ex_employee/"&gt;Google Software Infrastructure Dubbed Obsolete by
    ex-Employee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/49879386/" rel="nofollow" target="_blank" title="http://www.theregister.co.uk/2011/06/23/google_moves_off_the_google_file_system/"&gt;Google Moves Off the Google File
    System&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/745a779b/" rel="nofollow" target="_blank" title="http://www.google.co.uk/intl/en/landing/internetstats/"&gt;Google Internet
    Stats&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/d11024ba/" rel="nofollow" target="_blank" title="http://www.businessblogshub.com/2010/10/google-statistics-yes-they-are-very-big/"&gt;Google
    Statistics&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/bb33b2dd/" rel="nofollow" target="_blank" title="http://www.splashnology.com/article/google-plus-killer-facts-and-statistics-inforgaphics/2689/"&gt;Google Plus - Killer Facts and
    Statistics&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/d20404a7/" rel="nofollow" target="_blank" title="http://www.youtube.com/t/press_statistics"&gt;YouTube statistics&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/9909f6c8/" rel="nofollow" target="_blank" title="http://www.alexa.com/siteinfo/google.com"&gt;&lt;em&gt;Alexa on Google&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/b088e740/" rel="nofollow" target="_blank" title="http://www.internetworldstats.com/stats.htm"&gt;&lt;em&gt;Internet World
    Stats&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/86a009d0/" rel="nofollow" target="_blank" title="http://www.google.com/finance?fstype=bi&amp;amp;cid=694653"&gt;Google Inc.
    financials&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/2515e26d/" rel="nofollow" target="_blank" title="http://www.geekwire.com/2011/stats-hotmail-top-worldwide-gmail-posts-big-gains"&gt;Hotmail still on top worldwide; Gmail gets
    bigger&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/2428f635/" rel="nofollow" target="_blank" title="http://www.datacenterknowledge.com/archives/2011/08/01/report-google-uses-about-900000-servers/"&gt;&lt;em&gt;Google Server
    Count&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/b6d09313/" rel="nofollow" target="_blank" title="http://www.datacenterknowledge.com/archives/2009/10/20/google-envisions-10-million-servers/"&gt;Google Envisions 10 Million
    Servers&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/bb3d77a4/" rel="nofollow" target="_blank" title="http://www.datacenterknowledge.com/archives/2008/03/27/google-data-center-faq/"&gt;&lt;em&gt;Google Data Center
    FAQ&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="bonus-tipichnyi-pervyi-god-klastera-v-google"&gt;Бонус: типичный первый год кластера в Google&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;~1/2 перегрева (большинство серверов выключаются в течении 5 минут,
    1-2 дня на восстановление)&lt;/li&gt;
&lt;li&gt;~1 отказ распределителя питания (~500-1000 резко пропадают, ~6
    часов на восстановление)&lt;/li&gt;
&lt;li&gt;~1 передвижение стойки (много передвижений, 500-100 машин, 6 часов)&lt;/li&gt;
&lt;li&gt;~1 перепрокладка сети (последовательной отключение ~5% серверов на
    протяжении 2 дней)&lt;/li&gt;
&lt;li&gt;~20 отказов стоек (40-80 машин мгновенно исчезают, 1-6 часов на
    восстановление)&lt;/li&gt;
&lt;li&gt;~5 стоек становится нестабильными (40-80 машин сталкиваются с 50%
    потерей пакетов)&lt;/li&gt;
&lt;li&gt;~8 запланированных технических работ с сетью (при четырех могут
    случаться случайные получасовые потери соединения)&lt;/li&gt;
&lt;li&gt;~12 перезагрузок маршрутизаторов (потеря DNS и внешних виртуальных
    IP на несколько минут)&lt;/li&gt;
&lt;li&gt;~3 сбоя маршрутизаторов (восстановление в течении часа)&lt;/li&gt;
&lt;li&gt;Десятки небольших 30-секундных пропаданий DNS&lt;/li&gt;
&lt;li&gt;~1000 сбоев конкретных серверов (~3 в день)&lt;/li&gt;
&lt;li&gt;Много тысяч сбоев жестких дисков, проблем с памятью, ошибок
    конфигурации и т.п.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 28 Nov 2011 01:32:00 +0400</pubDate><guid>tag:www.insight-it.ru,2011-11-28:highload/2011/arkhitektura-google-2011/</guid><category>BigTable</category><category>Caffeine</category><category>Closure</category><category>GFS</category><category>Google</category><category>GWT</category><category>highload</category><category>Percolator</category><category>Protocol Buffers</category><category>Snappy</category><category>Spanner</category><category>архитектура Google</category><category>датацентры</category><category>Масштабируемость</category><category>энергоэффективность</category></item><item><title>Есть вопросы?</title><link>https://www.insight-it.ru//misc/2011/est-voprosy/</link><description>&lt;p&gt;Недавно несколько человек довольно независимо друг от друга подтолкнули
меня к новой странице-рубрике на Insight IT. Как не трудно догадаться по
заголовку, это &lt;strong&gt;&lt;a href="https://www.insight-it.ru/highload/faq/"&gt;F.A.Q. по высоконагруженным проектам&lt;/a&gt;&lt;/strong&gt; и связанным темам.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Я не считаю себя истиной в последней инстанции, так что публикую этот
анонс, чтобы попросить Вас, лояльных читателей, помочь мне в составлении
данного несомненно полезного для общества материала. Очень хотелось бы
увидеть дополнения, поправки и комментарии к моим ответам, а также
предложения по поводу новых вопросов и под-тем, которые стоило бы
осветить.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Текущая версия состоит из тех вопросов, которые мне задавали в последнее
время. В дальнейшем я постараюсь дополнить её более старыми вопросами от
читателей и клиентов, но в большей степени я все же надеюсь на вашу
поддержку :)&lt;/p&gt;
&lt;div class="card blue lighten-4"&gt;
&lt;p&gt;&lt;div class="card-content"&gt;
Еще раз ссылка на основной материал: &lt;strong&gt;&lt;a href="https://www.insight-it.ru/highload/faq/"&gt;Вопросы и ответы&lt;/a&gt;&lt;/strong&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Комментарии здесь закрываю, всё обсуждение этой темы по ссылке.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 26 Nov 2011 01:54:00 +0400</pubDate><guid>tag:www.insight-it.ru,2011-11-26:misc/2011/est-voprosy/</guid><category>FAQ</category><category>highload</category><category>Вопросы и ответы</category><category>Масштабируемость</category></item><item><title>HighLoad++ 2011</title><link>https://www.insight-it.ru//event/2011/highload-2011/</link><description>&lt;p&gt;В этом году я почти до последнего момента не знал попаду ли на
&lt;a href="https://www.insight-it.ru/goto/727c9436/" rel="nofollow" target="_blank" title="http://www.highload.ru"&gt;HighLoad++&lt;/a&gt;, но благодаря поговорке "не имей
сто рублей, а имей сто друзей" все же попал в список участников
(огромное спасибо &lt;a href="https://www.insight-it.ru/goto/2185bfa9/" rel="nofollow" target="_blank" title="http://www.facebook.com/pruttskova"&gt;Аксане&lt;/a&gt; и &lt;a href="https://www.insight-it.ru/goto/d0552db0/" rel="nofollow" target="_blank" title="http://hsbi.ru"&gt;ВШБИ&lt;/a&gt;). По ходу мероприятия написать традиционный отчет
не получилось, но лучше поздно, чем никогда :)
&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Начну, как обычно, с организационных моментов и прочих вещей не по делу.
Регистрация заняла ровно 30 секунд, никаких очередей. Удивило, что
поленились хоть капельку поменять раздаточные материалы и их дизайн с
прошлого года - выдали абсолютно идентичные прошлогодним пакетик,
блокнот и ручку. На входе стоял стенд с ходящей по беговой дорожке
топлесс девушкой с бодиартом, которую потом все кому не лень выкладывали
в твиттер с соответствующим хэштегом. На том же стенде разыгрывали Mac
Pro между теми, кто оставит контакты. К слову, владельцы стенда
оказались разработчиками ужасно работающего под Android 3.1 покерного
сервиса, ничего более примечательного о них сказать не могу. Стендов,
как обычно, было по пальцам пересчитать: пару хостинг-провайдеров и
Битрикс ничем особым не выделались, а в Google тоже разыгрывали технику,
один из последних смартфонов Samsung, а еще за прохождение теста в
Google Docs дарили футболки с надписью &lt;em&gt;"I'm&amp;nbsp;feeling&amp;nbsp;lucky"&lt;/em&gt; на спине -
очень актуально на экзаменах и в казино :)&lt;/p&gt;
&lt;p&gt;С WiFi традиционные проблемы почти весь первый день, планшет с 3G спасал
ситуацию. Кормили вполне приемлемо, правда ассортимент скудный - я,
наверное, за весь год меньше жареной рыбы съел, чем за эти два дня.
Первые три ряда мест в залах были "только для VIP", которые, как я
понял, включали в себя докладчиков и возможно личных знакомых
организаторов, причем таких человек было на вскидку меньше 5% участников
и в итоге 95% мест пустовали до тех пор, пока все не забили на эту
схему. Определенно не самый удачный ход, если уж и бронировать места, то
не в таком количестве. Трансляция одного из залов в коридоре - спорное
решение, с одной стороны можно послушать пока ешь, с другой - общаться
мешает капитально, особенно когда собираются "тусовки" вокруг
докладчика, например во второй день нужно было сильно напрягаться, чтобы
услышать Олега Илларионова в кулуарах.&lt;/p&gt;
&lt;p&gt;Перейдем, собственно, к основной тематике конференции, то есть к
докладам. В первый день один зал практически полностью был выделен для
англоязычных спикеров, что в целом хорошая тенденция. Если бы такое было
возможно, то вообще всю конференцию стоило бы перевести на английский -
терминология в родном её варианте звучит намного приятнее и лаконичнее.
Оба русскоговорящих докладчика, выступавших в "англоязычном" зале, были
тихим ужасом - даже комментировать их не хочу, просто двум не
заслуживающим этого людям дали покрасоваться перед публикой. Английские
же доклады были в основном на достаточно высоком уровне.&lt;/p&gt;
&lt;p&gt;Швейцарец &lt;strong&gt;Alvaro Videla&lt;/strong&gt; делился своим опытом работы с RabbitMQ, я,
признаюсь, не особо следил за этим проектом, но подал он его достаточно
вкусно и судя по обозначенным векторам развития внимания RabbitMQ все же
стоит.&lt;/p&gt;
&lt;p&gt;Второй докладчик, &lt;strong&gt;Buddy Brewer&lt;/strong&gt;, повествовал о тестировании скорости
отрисовки страниц в браузерах как искусственно (посредством диаграммы
загрузки компонентов страницы в виде водопада), так и вживую на
пользователях (посредством window.performance.timing, о котором я
почему-то раньше не слышал, и различных Javascript-библиотек).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Robert Treat&lt;/strong&gt; из OmniTI выступил намного хуже, чем в прошлом году,
довольно скучно рассказывал про PostgreSQL и мало кому нужные ньюансы
работы с ним, хотя доклад назывался совсем по-другому.&lt;/p&gt;
&lt;p&gt;После обеда шел доклад &lt;strong&gt;Domas Mituzas&lt;/strong&gt;, простого DBA из Facebook, хотя
в прошлом году выступал более высокопоставленный представитель. Речь шла
на этот раз про MySQL, в частности про основные проблемы и доработки,
сделанные в форке от Facebook, также затрагивался вопрос грядущего в
следующем году MySQL 5.6.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Josh Berkus" class="left" src="https://www.insight-it.ru/images/berkus.jpg" title="Josh Berkus"/&gt;
&lt;strong&gt;Josh Berkus&lt;/strong&gt; в этом году выступил более удачно, не зря ему дали
выступить дважды. Первый был довольно специфичным, но от этого не менее
интересным, основной темой был довольно узкий класс приложений, который
он называл firehose applications (пожарный шланг). Они представляют
собой сборщики данных в реальном времени для последующей аналитики, было
два примера: обработчик отчетов о критических сбоях в Mozilla Firefox и
анализ работы ферм ветряных мельниц. Забавно оказалось их сравнение с
традиционными сайтами:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;в сайтах главное, чтобы данные из базы всегда можно было хотя бы
    прочитать, и в случае сбоев или плановых работ её делают доступной
    только для чтения;&lt;/li&gt;
&lt;li&gt;в обсуждаемых же приложениех главное записать все поступающие
    данные, а пользовательский интерфейс в целом можно запланированно и
    не очень отключать - кому какое дело, если интерфейс для
    аналитических отчетов не работал 8 часов в ночь с субботы на
    воскресенье?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В его решениях обычно использовался та или иная форма NoSQL хранилищ,
для сбора текущих данных и PostgreSQL для хранения уже обработанных
отчетов и предоставления их пользователям. Второй доклад Josh'а был
менее формальным и был посвящен вопросам SQL vs NoSQL, что по сути
свелось к тому, что наличие SQL-интерфейса - дело десятое при выборе
СУБД для конкретного проекта, важнее понимать другие особенности каждого
конкретного хранилища данных.&lt;/p&gt;
&lt;p&gt;Mi***ft в этом году, как и Facebook, сменил докладчика, но это не
уберегло их от очередного провала. Хоть и после доклада появилось-таки
несколько вопросов от аудитории, но разработчиков по-прежнему в целом
мало интересует тематика проприетарного ПО, даже при поддержке PHP и
Hyper-V. Продавать простым смертным у них получается намного лучше, чем
продавать специалистам.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Robert Virding" class="left" src="https://www.insight-it.ru/images/virding.jpg" title="Robert Virding"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Robert Virding&lt;/strong&gt; был, пожалуй, одним из лучших докладчиков первого
дня. Хоть я и неплохо знаком с Erlang, но послушать о нем из уст одного
из создателей - невероятно увлекательно, как-будто переживаешь на своей
шкуре историю целого языка программирования. Robert очень подробно
объяснил чуть ли не каждое ключевое решение, принятое при проектировании
и разработке Beam, основной реализации виртуальной машины Erlang, а
также освятил несколько альтернативных воплощений этого языка в жизнь.&lt;/p&gt;
&lt;p&gt;После чего были как раз два русских доклада, о которых я решил не
рассказывать и второй доклад Josh'а, о котором я уже написал чуть выше.
Первый день завершал фуршет от одного из спонсоров конференции: пиво я
принципиально не пью, а на виски настроения особо не было, так что
слегка перекусил и даже не стал дожидаться выступления приглашенной
группы.&lt;/p&gt;
&lt;p&gt;Второй день был полностью "на русском". Первый зал на 100% состоял из
размусоливаний различных систем хранения данных - не знаю хорошо это или
плоха, вопрос актуальный, но то что очень однообразно и об одном и том
же - факт. Второй зал был по-разнообразнее, но я там был всего на двух с
половиной докладах.&lt;/p&gt;
&lt;p&gt;Я немного опоздал на первый доклад от &lt;strong&gt;mail.ru&lt;/strong&gt;, но, судя по всему, не
много потерял - они снова пропагандировали свое детище, Tarantool, на
этот раз в сравнении с Redis. Штука довольно интересная, но каких-то
особых преимуществ перед последним я так и не увидел или, может быть, не
хотел увидеть.&lt;/p&gt;
&lt;p&gt;Представители &lt;strong&gt;Skype&lt;/strong&gt; рассказывали о своем опыте работы с Redis и, в
частности, о конкретном кейсе с решардингом "на живую" - довольно
увлекательно, красивое решение с фильтрующим replication proxy, которое
правда скорее всего скоро потеряет актуальность при текущем направлении
развития проекта Redis. Сам активно им пользуюсь, но что-то подобное
проделывать пока не приходилось.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Владимир Климонтович&lt;/strong&gt; рассказал довольно подробно о проекте Apache
Cassandra, правда ничего нового я не услышал - все что было сказано
легко можно найти на официальном сайте проекта, что я в свое время и
сделал.&lt;/p&gt;
&lt;p&gt;На докладе &lt;strong&gt;OpenStat&lt;/strong&gt; я тупо сидел в Интернете, особо не вслушиваясь -
после их же доклада в прошлом году слушать еще раз примерно о тех же
заморочках с Apache Hadoop было не особо интересно, особенно если
учесть, что я в 2008-м с ним очень плотно работал.&lt;/p&gt;
&lt;p&gt;После обеда произошел мой "трансфер" во второй зал. &lt;strong&gt;Комсомольская
правда&lt;/strong&gt; прямо-таки ужаснула совковостью своей реализации и служила
скорее анти-примером, а упоминавшиеся проблемы и их решения были очень
спорными.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Олег Илларионов" class="left" src="https://www.insight-it.ru/images/illarionov.jpg" title="Олег Илларионов"/&gt;
Выступление &lt;strong&gt;Вконтакте&lt;/strong&gt; стало снова лучшим на конференции, не смотря на отсутствие Павла Дурова. &lt;strong&gt;Олег Илларионов&lt;/strong&gt;, хоть и является, как я понял, просто разработчиком, но хорошо ориентируется в технической реализации проекта
и очень доходчиво рассказывает - представлять компанию на HL++ его
отправляют совершенно оправданно. Основная тема доклада звучала как
&lt;strong&gt;AJAX Layout&lt;/strong&gt; и по сути отражала основное техническое изменение на
Вконтакте за последний год: навигация через # плюс обновления и чат в
реальном времени. Опять же, не могу похвастаться реализацией чего-то
подобного на своей практике, как-то не приходилось, но давно хотелось
выделить на это время, так что информация оказалась невероятно
актуальной. Про серверную часть было сказано очень мало: чистый C и
epoll, мол при их нагрузках по-другому никак. Хотя мне кажется Erlang
справился бы лучше, правда не факт, что у них есть соответствующие
разработчики. Основной ад данной задачи находится на клиентской стороне:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;кроссбраузерное сохранение работы кнопок вперед/назад/перезагрузить
    без полной перезагрузки страниц,&lt;/li&gt;
&lt;li&gt;кроссбраузерное же постоянное соединение браузера с сервером
    (используют long polling, websocket по очевидным причинам пока
    практически не применим),&lt;/li&gt;
&lt;li&gt;сохранение корректной работы с выключенным JavaScript и в
    "необычных" браузерах,&lt;/li&gt;
&lt;li&gt;выдача нового контента частями через скрытый iframe для улучшения
    визуальной скорости загрузки страницы,&lt;/li&gt;
&lt;li&gt;эмуляция процесса загрузки страницы через динамический favicon,&lt;/li&gt;
&lt;li&gt;очистка памяти при переходах между страницами,&lt;/li&gt;
&lt;li&gt;использование одного постоянного соединения для нескольких вкладок
    посредством локального хранилища из HTML5&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;...и многие другие проблемы и задачи - все это довольно подробно Олег
разложил по полочкам. Помимо этого он рассказал как реализован поиск по
друзьям внутри браузера через выгрузку списка друзей в JSON, а также
целая куча вопросов о том о сем, например про почту и все ту же
"волшебную" базу данных на C. Новую &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-vkontakte/"&gt;статью про Вконтакте&lt;/a&gt; я в этом году не планировал, так что особо не записывал, хотя возможно и зря. Выступление Олега слегка продлили, так как следующий докладчик куда-то пропал, но по факту когда он нашелся выступление про новомодный Node.js оказалось не очень и большая часть зала перекочевала "в кулуары" дальше слушать Олега и задавать ему вопросы.&lt;/p&gt;
&lt;p&gt;После кофе-брейка я вернулся в первый зал, где продолжалась эпопея с
хранением данных. &lt;strong&gt;Одноклассники&lt;/strong&gt; наконец-то поняли, что их старинное
решение на BerkleyDB + Java RMI - не панацея от всех болезней, и перешли
на другое решение для одной из задач, хранения изображений, не менее
самопальное. Задумка была в основном в замене BerkleyDB на традиционные
блобы фиксированного размера, аналогично решению в Facebook, при этом
остальная схема работы изменилась не сильно - доступ к системе через
HTTP посредствам Tomcat, внутри правда отказались от RMI в пользу
сокетов и собственного протокола, для хранения конфигурации - Apache
Zookeper, типичный для Java-мира наборчик. Из интересных решений -
использование JBOD с созданием уникального идентификатора винчестера и
возможностью переткнуть любой из них в другой сервер. В остальном ничего
особенного: лог транзакций, репликация, индекс в памяти посредством
собственной реализации HashMap и т.п.&lt;/p&gt;
&lt;p&gt;Последний доклад, который я послушал, был слегка похож на предыдущий, с
той лишь разницей, что хостинг-провайдер &lt;strong&gt;Clodo&lt;/strong&gt; не стал изобретать
велосипед и воспользовался "полуфабрикатом" под названием Openstack
Swift - хранилище бинарных объектов с HTTP интерфейсом, которыми вполне
могут быть и изображения. Доклад был о том как они его допиливали, в
основном посредством использования nginx и его модулей. К слову,
посмотрел их цены (возможно скоро &lt;strong class="trebuchet"&gt;Insight IT&lt;/strong&gt; придется снова переезжать, приглядываюсь к вариантам) и был разочарован, очень существенно выше рынка не смотря на неизвестность компании, в такой ситуации наоборот логичнее было бы демпинговать...&lt;/p&gt;
&lt;p&gt;В целом впечатления от конференции положительные, процент стоящих
докладов близок к 20-30%, что для технических конференций очень
прилично. Организация и контингент тоже на уровне. Но нельзя не сказать
про ценник (21к за живое участие и 11к за онлайн-трансляцию) - он явно
рассчитан на оплату работодателями и посещение конференции за
собственный счет определенно того не стоит. Альтернативные варианты
попасть на мероприятие хоть и есть, но довольно ограничены - каждый раз
приходится изворачиваться, чтобы попасть бесплатно при отсутствии
работодателя. Хотя может быть надо было просто заморочиться и попасть в
список "информационной поддержки"...&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Кстати, в этом году в первый раз ко мне начали подходить незнакомые
люди и благодарить за блог - очень приятно :)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;До встречи на GDD через пару дней!&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 20 Oct 2011 15:16:00 +0400</pubDate><guid>tag:www.insight-it.ru,2011-10-20:event/2011/highload-2011/</guid><category>highload</category><category>конференции</category><category>мероприятия</category></item><item><title>Архитектура высоконагруженных интернет-проектов, 29 марта</title><link>https://www.insight-it.ru//event/2011/arkhitektura-vysokonagruzhennykh-internet-proektov-29-marta/</link><description>&lt;p&gt;Как и обещал выкладываю видео &lt;a href="https://www.insight-it.ru/event/2011/otkrytaya-lekciya-o-tom-kak-rabotayut-krupnye-internet-proekty/"&gt;недавней лекции&lt;/a&gt;.
Качество звука оставляет желать лучшего, так что выкладываю
перекодированный в flv вариант и оригинал для тех, кому в онлайн плеере
совсем не слышно. Презентация доступна онлайн по адресу
&lt;a href="https://www.insight-it.ru/goto/eb8f8266/" rel="nofollow" target="_blank" title="http://bit.ly/highload"&gt;bit.ly/highload&lt;/a&gt;, рекомендую смотреть на слайды
параллельно с просмотром, так как на проекторе видно тоже так себе.&lt;/p&gt;
&lt;div class="video-container"&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="480" src="//www.youtube.com/embed/VHsyYcojAvc?rel=0" width="853"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Приятного просмотра!&lt;/strong&gt; Если есть какие вопросы или замечания - с
удовольствием готов пообщаться в комментариях к посту :)&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 05 Apr 2011 18:12:00 +0400</pubDate><guid>tag:www.insight-it.ru,2011-04-05:event/2011/arkhitektura-vysokonagruzhennykh-internet-proektov-29-marta/</guid><category>highload</category><category>Архитектура высоконагруженных интернет-проектов</category><category>видео</category><category>лекция</category><category>МФТИ</category></item><item><title>HighLoad++ 2010</title><link>https://www.insight-it.ru//event/2010/highload-2010/</link><description>&lt;p&gt;&lt;img alt="HighLoad++ Logo" class="left" src="https://www.insight-it.ru/images/highload-conference-logo.png" title="HighLoad++ Logo"/&gt;
25-26 октября прошла конференция &lt;a href="https://www.insight-it.ru/goto/727c9436/" rel="nofollow" target="_blank" title="http://www.highload.ru"&gt;HighLoad++ 2010&lt;/a&gt;,
посвященная разработке высоконагруженных систем. После конференции у
меня сразу родились планы на два поста: типичный отчет и описание
архитектуры Вконтакте. С порядком написания я, видимо, не прогадал -
получился один из самых успешных постов на &lt;strong&gt;Insight IT&lt;/strong&gt;. Остальные
доклады на мероприятии были, пожалуй, существенно менее животрепещущими
для общественности, но все же не менее интересными.
Приступим.&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="organizatsionnye-momenty"&gt;Организационные моменты&lt;/h2&gt;
&lt;p&gt;Прежде чем переходить собственно к рассказу о докладах, хочется сразу
высказаться по организационным вопросам, чтобы далее не отвлекаться.
Возможно организаторы учтут при проведении последующих мероприятий.&lt;/p&gt;
&lt;p&gt;Во-первых, участие в конференции: цены конечно не самые высокие для
двухдневных конференций, но все равно слегка зашкаливают - лично я бы не
пошел на данное мероприятие за такие деньги, даже не смотря на то что
тематика полностью совпадает со сферой моих профессиональных интересов.
За кого-то заплатил работодатель, а мне вот пришлось доставать
бесплатное участие через знакомых знакомых... &lt;em&gt;(спасибо добрым людям,
если вдруг читают :) )&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Во-вторых, удивила ситуация со связью со внешним миром: интернет был на
очень хорошем для конференций уровне - тупил местами, но в целом
стабильно работал, а вот мобильная связь не работала практически
совсем - уезжал домой с почти севшим телефоном.&lt;/p&gt;
&lt;p&gt;Политика организовывать не ставить два потенциально интересных доклада
параллельно меня очень порадовал - послушал в живую все, что хотел. А
небольшая давка в первом зале в начале первого дня мне кажется была
очень даже справедливой платой за отсутствие необходимости разрываться
на части.&lt;/p&gt;
&lt;p&gt;С едой все было в порядке, очереди конечно великоваты не смотря на два
обеда в разное время, но всегда можно было обойти данное неудобство
(перейти в другой "раздаточный пункт" или залезть на сцену, хоть и не
разрешали).&lt;/p&gt;
&lt;p&gt;Еще очень порадовало, что презентации первого дня конференции были уже
доступны участникам еще за пару часов до окончания первого дня. Но вот с
оставшимися презентациями и видео с мероприятия видимо произошла
какая-то заминка и я так и не получил ссылку на них до сих пор, судя по
всему они так и не доступны.&lt;/p&gt;
&lt;h2 id="den-pervyi"&gt;День первый&lt;/h2&gt;
&lt;p&gt;Основной особенностью первого дня было выделение целого зала под
англоязычные доклады зарубежных коллег. Как я уже писал, желающих
послушать иностранцев, было очень много - и в первой половине дня люди
толпились чуть ли не в коридоре, но ближе к вечеру ситуация
стабилизировалась.&lt;/p&gt;
&lt;p&gt;После приветственного слова Олега Бунина (одного из основных
организаторов конференции) слово взял &lt;a href="https://www.insight-it.ru/goto/c9ed5ba0/" rel="nofollow" target="_blank" title="http://www.timetobleed.com"&gt;Joe
Damato&lt;/a&gt;, которого позиционировали как
известного хакера, активно работающего над развитием &lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt;.
Темой выступления был обзор различных инструментов и приемов для анализа
ситуации в серверном &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;-окружении. Некоторые моменты
были мне известны и ранее, но в целом больше половины доклада было для
меня очень интересно и ново. Перечислять упомянутые им приемы я, честно
говоря, не вижу смысла - будет просто дублирование презентации. Если
ранжировать доклады первого дня по интересу лично для меня, то это
выступление заняло бы, пожалуй, второе место.&lt;/p&gt;
&lt;p&gt;Вторым докладчиком был также приверженец секты Рубистов, &lt;a href="https://www.insight-it.ru/goto/6a62c9ef/" rel="nofollow" target="_blank" title="http://jamesgolick.com/"&gt;James
Golick&lt;/a&gt;, один из основателей социальной сети
для фетишистов (простите за отсутствие ссылки). Основной фишкой доклада
было "разоблачение мифов", в частности об облачных вычислениях и NoSQL.
Количество пользователей этой социальной сети, но они очень активны и
генерируют достаточно много контента (особенно по Российским меркам).
Проект изначально располагался в компании, которая предоставляла услуги
managed hosting (хостинг на арендуемых серверах + за тебя
администрируют), но они посчитали, что слишком много переплачивают за
этот самый "managed", и решили поддаться тренду и переехать в облако
(&lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; &lt;a href="/tag/ec2/"&gt;EC2&lt;/a&gt;). По деньгам получилось не сильно
дешевле, но больше всего из расстроила производительность виртуальных
машин (кажется, был слайд со скоростью доступа к дисковой подсистеме,
выставляющий облако не в лучшем свете). Второй эпопеей в их проекте были
попытки оптимизировать подсистему хранения данных путем перенесения ее
части в NoSQL хранилище: пробовали &lt;a href="/tag/mongodb/"&gt;MongoDB&lt;/a&gt; (выкинули
из-за блокировок на операциях удаления) и &lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt;
(выкинули из-за медленного случайного чтения). Финальным решением стал
&lt;a href="/tag/redis/"&gt;Redis&lt;/a&gt; + &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;, просто и со вкусом - их
всецело устраивает на данный момент, как я понял.&lt;/p&gt;
&lt;p&gt;Третьим выступал &lt;a href="https://www.insight-it.ru/goto/aab04203/" rel="nofollow" target="_blank" title="http://www.facebook.com/robert"&gt;Robert Johnson&lt;/a&gt; из
&lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt;, доклад был практически таким же, как и в
ГУ-ВШЭ за несколько дней до этого - о нем &lt;a href="https://www.insight-it.ru/event/2010/facebook-how-we-scaled-to-500-000-000-users-by-robert-johnson/"&gt;я уже
писал&lt;/a&gt;,
так что подробно останавливаться не буду. Основным отличием были
дополнительные технические детали, но подавляющее большинство из них и
так уже были описаны в статье &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-facebook/"&gt;"Архитектура
Facebook"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;После обеда выступал Patrice Pelland из &lt;a href="/tag/microsoft/"&gt;Microsoft&lt;/a&gt;,
доклад&amp;nbsp; был о том, как работают их облачные сервисы (видимо live,
skydrive и прочие). Естественно все на их же продуктах, большинство
названий я даже не слышал. Единственное, что запомнил из выступления - у
мелкомягких есть даже клон &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;, но с какими-то
дополнительными плюшками. Это был единственный доклад, после которого
никто не захотел задать даже одного вопроса, что в целом наглядно
продемонстрировало незаинтересованность аудитории в платных решениях. В
твиттере после этого выступления проскользнула обиженная фраза
докладчика, что-то в духе: "До них просто не дошло, о чем я говорил".&lt;/p&gt;
&lt;p&gt;После этого недоразумения от MS началась длинная серия докладов от
людей, причастных к созданию &lt;a href="/tag/postgresql/"&gt;PostgreSQL&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simon Riggs из &lt;a href="https://www.insight-it.ru/goto/8078c405/" rel="nofollow" target="_blank" title="http://www.2ndquadrant.com/"&gt;2nd Quadrant&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Robert Treat из &lt;a href="https://www.insight-it.ru/goto/556109e4/" rel="nofollow" target="_blank" title="http://www.omniti.com"&gt;Omni TI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bruce Momjian из &lt;a href="https://www.insight-it.ru/goto/37e2eb7f/" rel="nofollow" target="_blank" title="http://www.enterprisedb.com/"&gt;EnterpriseDB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Было 5 выступлений о PostgreSQL подряд:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Повышение производительности&lt;/li&gt;
&lt;li&gt;Управление репликацией&lt;/li&gt;
&lt;li&gt;Масштабирование&lt;/li&gt;
&lt;li&gt;Быстрая смена версии средствами pg_update&lt;/li&gt;
&lt;li&gt;Потоковая репликация&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В целом очень актуальные доклады, если Вы плотно работаете с PostgreSQL
в своем проекте или на своей работе. Я вообще тоже когда стоит выбор
между доступными реляционными СУБД чаще всего склоняюсь к PostgreSQL, но
доклады были детализированными не там, где нужно, и было скучновато. В
этой секции порадовали три вещи:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;очень качественный английский у докладчиков&lt;/li&gt;
&lt;li&gt;забавная манера выступления Роберта, особенно про красные кроки
    (что-то типа галош)&lt;/li&gt;
&lt;li&gt;активная реклама новых вкусностей PostgreSQL 9.0, релиз которой я по
    каким-то причинам проворонил - надо будет обязательно попробовать ее
    в деле&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;После кофе-брейка я пошел на больше всего понравившийся мне доклад (за
первый день) - выступал &lt;a href="https://www.insight-it.ru/goto/217e612/" rel="nofollow" target="_blank" title="http://www.phpied.com/"&gt;Stoyan Stefanov&lt;/a&gt; из
Yahoo! Темой доклада была заявлена неочевидная формулировка "Progressive
Downloads and Rendering", хотя на самом деле все свелось к грамотно
построенному докладу о клиентской оптимизации: несколько вводных
картинок, один слайд с базовыми приемами и много-много примеров
очевидных и не очень случаев, когда с точки зрения пользователя сайт
начинает тупить, даже если серверная часть проекта написано грамотно и
работает достаточно быстро. По некоторым аспектам, в частности про
кроссбраузерному использованию data:URL+MHTML, он ссылался на русские
источники, а также очень позитивно отзывался о &lt;a href="https://www.insight-it.ru/goto/209c3c57/" rel="nofollow" target="_blank" title="http://sunnybear.moikrug.ru/"&gt;Николае
Мациевском&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Последним, что я посетил в первый день, была "открытая встреча" c James
Golick и Joe Damato про сам &lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt;. Ожидал большего: в итоге
Joe вообще не выступил, а большая часть времени ушла на разжёвывание
базовых возможностей языка и несколько мелких холиваров.&lt;/p&gt;
&lt;h2 id="den-vtoroi"&gt;День второй&lt;/h2&gt;
&lt;p&gt;На второй день я немного проспал и приехал ближе к концу первого
доклада: оказалось, что я не один такой - людей было раза в три меньше,
чем за день до этого. Выбор потока куда пойти был легок: в первом зале
все утро было посвящено &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;, с которым я последнее
время довольно плотненько работал.&lt;/p&gt;
&lt;p&gt;После докладов на английском "отечественные" выступления смотрелись
совсем блекло. Конец выступления Андрея Смирнова про
&lt;a href="/tag/twisted/"&gt;Twisted&lt;/a&gt; не принес мне хоть какой-либо полезной
информации, тем более мне все равно больше по душе
&lt;a href="/tag/tornado/"&gt;Tornado&lt;/a&gt;. Вопрос про их сравнение от одного из слушателей
вызвал у докладчика рассказать историю о том, как будущий автор Tornado
тусовался в сообществе Twisted, а потом взял и сделал свой продукт.&lt;/p&gt;
&lt;p&gt;Следующий доклад был про профилирование памяти в &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;
от Антона Грицая - начал он историю очень издалека, с того что такое
утечки памяти, какие бывают "сборщики мусора", какие есть варианты
искать утечки в &lt;a href="/tag/python/"&gt;Python&lt;/a&gt; и чем они плохи, собственно до
"дела" он дошел лишь к концу доклада. Было предложено пользоваться
продуктом под названием &lt;a href="https://www.insight-it.ru/goto/f077b18a/" rel="nofollow" target="_blank" title="http://guppy-pe.sourceforge.net/"&gt;heapy&lt;/a&gt;,
который обладает широким спектром возможности, но при этом документация
сильно хромает.&lt;/p&gt;
&lt;p&gt;Последним докладом в секции про &lt;a href="/tag/python/"&gt;Python&lt;/a&gt; было выступление
трех бравых ребят из HeadHunter, которые рассказывали про их внутренний
продукт под названием Frontik, представляющий собой надстройку над
&lt;a href="/tag/tornado/"&gt;Tornado&lt;/a&gt;, аггрегирующую данные с нескольких
HTTP-сервисов. В целом идея мне понравилась, но ввиду исторических
причин реализация у них накручена очень муторно:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;основной формат передачи данных - &lt;a href="/tag/html/"&gt;XML&lt;/a&gt; по HTTP&lt;/li&gt;
&lt;li&gt;генерация HTML посредством &lt;a href="/tag/xslt/"&gt;XSLT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;регулярные выражения где надо и где не надо (для повышения
    производительности)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Основным событием оставшейся части второго дня, как Вы уже наверное
поняли, был аншлаг с участием Вконтакте и лично Павлом Дуровым в главной
роли. Результаты подробно расписаны в статье &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-vkontakte/"&gt;"Архитектура
Вконтакте"&lt;/a&gt;,
повторяться не буду, с Вашего позволения.&lt;/p&gt;
&lt;p&gt;Остальные доклады я застал лишь частями, так как блуждал по залам без
особого энтузиазма, да и в толпе вокруг Павла чуток потусовался.
Расскажу вкратце запомнившиеся моменты:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Юрий Востриков из Mail.ru рассказывал про
    &lt;a href="https://www.insight-it.ru/goto/4cece2af/" rel="nofollow" target="_blank" title="http://opensource.mail.ru"&gt;Tarantool/Silverbox&lt;/a&gt; - еще после их
    технологического форума подумываю попробовать этот продукт в деле,
    но после этого выступления понял, что пока рановато: не известно ни
    об одном успешном применении вне компании-разработчика, да и
    библиотеки с реализацией полноценного протокола есть далеко не под
    все языки программирования.&lt;/li&gt;
&lt;li&gt;На доклад про реализацию одного из топовых приложений Вконтакте на
    &lt;a href="/tag/rails/"&gt;Rails&lt;/a&gt; я попал почти к самой сессии вопросов-ответов,
    запомнился только тот факт, что после того как компания, в которой
    работал докладчик, передала приложение заказчику - они почти сразу
    же переписали его на &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;. Заставляет задуматься.&lt;/li&gt;
&lt;li&gt;В третьем, дополнительном, зале во второй половине дня расположился
    тренинг Start in Garage для людей, планирующих сделать свой стартап;
    ребята рассказывали весело и непринужденно, но по сути все было
    очень примитивно - ушел минут через 20 после начала на аншлаг
    вконтакте.&lt;/li&gt;
&lt;li&gt;Про &lt;a href="https://www.insight-it.ru/goto/e72a7ec5/" rel="nofollow" target="_blank" title="http://www.scalaxy.ru"&gt;Scalaxy&lt;/a&gt; было бы интересно написать
    отдельную статью, больно часто они всплывают на конференциях и в
    онлайн-сообществах. На этот раз рассказывали о том, как они выделяют
    избыточные дисковые массивы для виртуальных машин (которые они
    собственно в аренду сдают). Технология называется Vast Sky, родом
    откуда-то из Азии, позволяет легко выделять заданное количество
    блоковых устройств на разных дисковых системах и подключать их к
    виртуальной машине в виде софтверного RAID. В сочетании с их QDR
    Infiniband от Voltaire работает очень даже шустро (по крайней мере
    если верить их бенчмаркам по сравнению с альтернативными
    технологиями).&lt;/li&gt;
&lt;li&gt;Scalaxy же запускает сервис ddosme, предназначенный для нагрузочного
    тестирования интернет-проектов. Попал опять только на
    вопросы-ответы, из них понял, что они предлагают через прокси
    походить по своему ресурсу, затем на основе логов составляются
    маршруты движения ботов по сайту и тестирование запускается на
    нужных мощностях. Сколько стоит не понял.&lt;/li&gt;
&lt;li&gt;Последним докладом, который я застал краем глаза, было обсуждение
    основных косяков, мешающих 1С-Битрикс обслуживать пристойное
    количество пользователей - для меня совершенно не актуальный вопрос,
    так что после этого я начал собираться в сторону выхода и отправился
    смотреть &lt;a href="https://www.insight-it.ru/event/2010/socialnaya-set/"&gt;"Социальная сеть"&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="zakliuchenie"&gt;Заключение&lt;/h2&gt;
&lt;p&gt;Впечатления от конференции очень положительные: большинство докладов
хотя бы немного полезны, рекламы вообще минимум, организация на уровне.
По-прежнему не знаю стоит ли она своих денег, но потраченного времени
точно стоит.&amp;nbsp; Надеюсь в следующем году будет по-проще попасть.&lt;/p&gt;
&lt;p&gt;Хотелось бы видеть больше докладов не о конкретных инструментах и
технологиях, а о их применении в рамках построения общей архитектуры
проекта или решения конкретных нетривиальных задач. Доклады в духе "у
нас вот такая классная штука есть, но стоит денег" и "приходите к нам
работать, чтобы попробовать в деле эту технологию" как обычно скучны, но
вроде их было довольно мало (надеюсь докладчики хотябы платят
организатором за возможность порекламироваться?). Приглашенные
иностранные гости - ход очень классный, мне кажется основной ключ успеха
прошедшего мероприятия, в этом направлении определенно стоит двигаться -
хотелось бы увидеть представителей известных проектов (Google, Ebay,
Amazon, Flickr, Twitter, Baidu, QQ и.т.д.) и людей, решающих реально
нетривиальные задачи, вроде Joe Damato.&lt;/p&gt;
&lt;p&gt;В любом случае спасибо организаторам за два с толком проведенных дня :)&lt;/p&gt;
&lt;p&gt;Да, думаю Вы уже заметили, что блог Insight IT снова потихоньку
возвращается к жизни, так что &lt;a href="/feed/"&gt;подписываться на RSS никогда не поздно&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 31 Oct 2010 23:24:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-10-31:event/2010/highload-2010/</guid><category>highload</category><category>высоконагруженные системы</category><category>конференции</category><category>Масштабируемость</category><category>мероприятия</category><category>разработка</category></item><item><title>Архитектура Вконтакте</title><link>https://www.insight-it.ru//highload/2010/arkhitektura-vkontakte/</link><description>&lt;p&gt;&lt;img alt="Логотип Вконтакте" class="left" src="https://www.insight-it.ru/images/vkontakte-logo.png" title="Логотип Вконтакте"/&gt;
Самая популярная социальная сеть в рунете пролила немного света на то,
как же она работает. Представители проекта в лице Павла Дурова и Олега
Илларионова на конференции &lt;a href="https://www.insight-it.ru/event/2010/highload-2010/"&gt;HighLoad++&lt;/a&gt; ответили на шквал вопросов по совершенно разным аспектам работы
&lt;a href="https://www.insight-it.ru/goto/a14c17dd/" rel="nofollow" target="_blank" title="http://www.vkontakte.ru"&gt;Вконтакте&lt;/a&gt;, в том числе и техническим. Спешу
поделиться своим взглядом на архитектуру проекта по результатам данного
выступления.&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/debian/"&gt;Debian&lt;/a&gt; &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; - основная операционная
    система&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/nginx/"&gt;nginx&lt;/a&gt; - балансировка нагрузки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; + &lt;a href="/tag/xcache/"&gt;XCache&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; + &lt;a href="/tag/mod_php/"&gt;mod_php&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Собственная СУБД на &lt;a href="/tag/c/"&gt;C&lt;/a&gt;, созданная "лучшими умами" России&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/node-js/"&gt;node.js&lt;/a&gt; - прослойка для реализации XMPP, живет за
    &lt;a href="/tag/haproxy/"&gt;HAProxy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Изображения отдаются просто с файловой системы &lt;a href="/tag/xfs/"&gt;xfs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/ffmpeg/"&gt;ffmpeg&lt;/a&gt; - конвертирование видео&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;95 миллионов учетных записей&lt;/li&gt;
&lt;li&gt;40 миллионов активных пользователей во всем мире (сопоставимо с
    аудиторией интернета в России)&lt;/li&gt;
&lt;li&gt;11 миллиардов запросов в день&lt;/li&gt;
&lt;li&gt;200 миллионов личных сообщений в день&lt;/li&gt;
&lt;li&gt;Видеопоток достигает 160Гбит/с&lt;/li&gt;
&lt;li&gt;Более 10 тысяч серверов, из которых только 32 - фронтенды на
    &lt;a href="/tag/nginx/"&gt;nginx&lt;/a&gt; (количество серверов с
    &lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; неизвестно)&lt;/li&gt;
&lt;li&gt;30-40 разработчиков, 2 дизайнера, 5 системных администраторов, много
    людей в датацентрах&lt;/li&gt;
&lt;li&gt;Каждый день выходит из строя около 10 жестких дисков&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;h3 id="obshchie-printsipy"&gt;Общие принципы&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cервера многофункциональны и используются одновременно в нескольких
    ролях:&lt;ul&gt;
&lt;li&gt;Перебрасывание полуавтоматическое&lt;/li&gt;
&lt;li&gt;Требуется перезапускать daemon'ы&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Генерация страниц с новостями (микроблоги) происходит очень похожим
    образом с &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt; (см. &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-facebook/"&gt;Архитектура&amp;nbsp;Facebook&lt;/a&gt;), основное
    отличие - использование собственной СУБД вместо MySQL&lt;/li&gt;
&lt;li&gt;При балансировке нагрузки используются:&lt;ul&gt;
&lt;li&gt;Взвешенный round robin внутри системы&lt;/li&gt;
&lt;li&gt;Разные сервера для разных типов запросов&lt;/li&gt;
&lt;li&gt;Балансировка на уровне ДНС на 32 IP-адреса&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Большая часть внутреннего софта написано самостоятельно, в том
    числе:&lt;ul&gt;
&lt;li&gt;Собственная СУБД (см. ниже)&lt;/li&gt;
&lt;li&gt;Мониторинг с уведомлением по СМС (Павел сам помогал верстать
    интерфейс :) )&lt;/li&gt;
&lt;li&gt;Автоматическая система тестирования кода&lt;/li&gt;
&lt;li&gt;Анализаторы статистики и логов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Мощные сервера:&lt;ul&gt;
&lt;li&gt;8-ядерные процессоры Intel (по два на сервер, видимо)&lt;/li&gt;
&lt;li&gt;64Гб оперативной памяти&lt;/li&gt;
&lt;li&gt;8 жестких дисков (соответственно скорее всего корпуса 2-3U)&lt;/li&gt;
&lt;li&gt;RAID не используется&lt;/li&gt;
&lt;li&gt;Не брендированные&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Вычислительные мощности серверов используются менее, чем на 20%&lt;/li&gt;
&lt;li&gt;Сейчас проект расположен в 4 датацентрах в Санкт-Петербурге и
    Москве, причем:&lt;ul&gt;
&lt;li&gt;Вся основная база данных располагается в одном датацентре в
    Санкт-Петербурге&lt;/li&gt;
&lt;li&gt;В Московских датацентрах только аудио и видео&lt;/li&gt;
&lt;li&gt;В планах сделать репликацию базы данных в другой датацентр в
    ленинградской области&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/cdn/"&gt;CDN&lt;/a&gt; на данный момент не используется, но в планах есть&lt;/li&gt;
&lt;li&gt;Резервное копирование данных происходит ежедневно и инкрементально&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="volshebnaia-baza-dannykh-na-c"&gt;Волшебная база данных на &lt;a href="/tag/c/"&gt;C&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Этому продукту, пожалуй, уделялось максимум внимания аудитории, но при
этом почти никаких подробностей о том, что он собственно говоря собой
представляет, так и не было обнародовано. Известно, что:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Разработана "лучшими умами" России, победителями олимпиад и
    конкурсов топкодер; озвучили даже имена этих "героев" Вконтакте
    (писал на слух и возможно не всех успел, так что извиняйте):&lt;ul&gt;
&lt;li&gt;Андрей Лопатин&lt;/li&gt;
&lt;li&gt;Николай Дуров&lt;/li&gt;
&lt;li&gt;Арсений Смирнов&lt;/li&gt;
&lt;li&gt;Алексей Левин&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Используется в огромном количестве сервисов:&lt;ul&gt;
&lt;li&gt;Личные сообщения&lt;/li&gt;
&lt;li&gt;Сообщения на стенах&lt;/li&gt;
&lt;li&gt;Статусы&lt;/li&gt;
&lt;li&gt;Поиск&lt;/li&gt;
&lt;li&gt;Приватность&lt;/li&gt;
&lt;li&gt;Списки друзей&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Нереляционная модель данных&lt;/li&gt;
&lt;li&gt;Большинство операций осуществляется в оперативной памяти&lt;/li&gt;
&lt;li&gt;Интерфейс доступа представляет собой расширенный протокол
    &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;, специальным образом составленные ключи
    возвращают результаты сложных запросов (чаще всего специфичных для
    конкретного сервиса)&lt;/li&gt;
&lt;li&gt;Хотели бы сделать из данной системы универсальную СУБД и
    опубликовать под GPL, но пока не получается из-за высокой степени
    интеграции с остальными сервисами&lt;/li&gt;
&lt;li&gt;Кластеризация осуществляется легко&lt;/li&gt;
&lt;li&gt;Есть репликация&lt;/li&gt;
&lt;li&gt;Если честно, я так и не понял зачем им &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt; с такой
    штукой - возможно просто как legacy живет со старых времен&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="audio-i-video"&gt;Аудио и видео&lt;/h3&gt;
&lt;p&gt;Эти подпроекты являются побочными для социальной сети, на них особо не
фокусируются. В основном это связанно с тем, что они редко коррелируют с
основной целью использования социальной сети - &lt;em&gt;общением&lt;/em&gt;, а также
создают большое количество проблем: видео траффик - основная статья
расходов проекта, плюс всем известные проблемы с нелегальным контентом и
претензиями правообладателей. Медиа-файлы банятся по хэшу при удалении
по просьбе правообладателей, но это неэффективно и планируется
усовершенствовать этот механизм.&lt;/p&gt;
&lt;p&gt;1000-1500 серверов используется для перекодирования видео, на них же оно
и хранится.&lt;/p&gt;
&lt;h3 id="xmpp"&gt;XMPP&lt;/h3&gt;
&lt;p&gt;Как известно, некоторое время назад появилась возможность общаться на
Вконтакте через протокол Jabber (он же XMPP). Протокол совершенно
открытый и существует масса opensource реализаций.&lt;/p&gt;
&lt;p&gt;По ряду причин, среди которых проблемы с интеграцией с остальными
сервисами, было решено за месяц создать собственный сервер,
представляющий собой прослойку между внутренними сервисами Вконтакте и
реализацией XMPP протокола. Основные особенности этого сервиса:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Реализован на &lt;a href="/tag/node-js/"&gt;node.js&lt;/a&gt; (выбор обусловлен тем, что
    JavaScript знают практически все разработчики проекта, а также
    хороший набор инструментов для реализации задачи)&lt;/li&gt;
&lt;li&gt;Работа с большими контакт-листами - у многих пользователей
    количество друзей на Вконтакте измеряется сотнями и тысячами&lt;/li&gt;
&lt;li&gt;Высокая активность смены статусов - люди появляются и исчезают из
    онлайна чаще, чем в других аналогичных ситуациях&lt;/li&gt;
&lt;li&gt;Аватарки передаются в &lt;code&gt;base64&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Тесная интеграция с внутренней системой обмена личными сообщениями
    Вконтакте&lt;/li&gt;
&lt;li&gt;60-80 тысяч человек онлайн, в пике - 150 тысяч&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/haproxy/"&gt;HAProxy&lt;/a&gt; обрабатывает входящие соединения и
    используется для балансировки нагрузки и развертывания новых версий&lt;/li&gt;
&lt;li&gt;Данные хранятся в &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt; (думали о MongoDB, но
    передумали)&lt;/li&gt;
&lt;li&gt;Сервис работает на 5 серверах разной конфигурации, на каждом из них
    работает код на&lt;a href="/tag/node-js/"&gt;node.js&lt;/a&gt; (по 4 процесса на сервер), а
    на трех самых мощных - еще и &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;В &lt;a href="/tag/node-js/"&gt;node.js&lt;/a&gt; большие проблемы с использованием
    &lt;a href="/tag/openssl/"&gt;OpenSSL&lt;/a&gt;, а также течет память&lt;/li&gt;
&lt;li&gt;Группы друзей в XMPP не связаны с группами друзей на сайте - сделано
    по просьбе пользователей, которые не хотели чтобы их друзья из-за
    плеча видели в какой группе они находятся&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="integratsiia-so-vneshnimi-resursami"&gt;Интеграция со внешними ресурсами&lt;/h3&gt;
&lt;p&gt;Во Вконтакте считают данное направление очень перспективным и
осуществляют массу связанной с этим работы. Основные предпринятые шаги:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Максимальная кроссбраузерность для виджетов на основе библиотек
    easyXDM и fastXDM&lt;/li&gt;
&lt;li&gt;Кросс-постинг статусов в &lt;a href="/tag/twitter/"&gt;Twitter&lt;/a&gt;, реализованный с
    помощью очередей запросов&lt;/li&gt;
&lt;li&gt;Кнопка "поделиться с друзьями", поддерживающая openGraph теги и
    автоматически подбирающая подходящую иллюстрацию (путем сравнивание
    содержимых тега &lt;code&gt;&amp;lt;title&amp;gt;&lt;/code&gt; и атрибутов alt у изображений, чуть ли не
    побуквенно)&lt;/li&gt;
&lt;li&gt;Возможность загрузки видео через сторонние видео-хостинги (YouTube,
    RuTube, Vimeo, и.т.д.), открыты к интеграции с другими&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="interesnye-fakty-ne-po-teme_1"&gt;Интересные факты не по теме&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Процесс разработки близок к Agile, с недельными итерациями&lt;/li&gt;
&lt;li&gt;Ядро операционной системы модифицированно (на предмет работы с
    памятью), есть своя пакетная база для &lt;a href="/tag/debian/"&gt;Debian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Фотографии загружаются на два жестких диска одного сервера
    одновременно, после чего создается резервная копия на другом сервере&lt;/li&gt;
&lt;li&gt;Есть много доработок над &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;, в.т.ч. для
    более стабильного и длительного размещения объектов в памяти; есть
    даже persistent версия&lt;/li&gt;
&lt;li&gt;Фотографии не удаляются для минимизации фрагментации&lt;/li&gt;
&lt;li&gt;Решения о развитии проекта принимают Павел Дуров и Андрей Рогозов,
    ответственность за сервисы - на них и на реализовавшем его
    разработчике&lt;/li&gt;
&lt;li&gt;Павел Дуров откладывал деньги на хостинг с 1 курса :)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;p&gt;В целом Вконтакте развивается в сторону увеличения скорости
распространения информацию внутри сети. Приоритеты поменялись в этом
направлении достаточно недавно, этим обусловлено, например, перенос
выхода почтового сервиса Вконтакте, о котором очень активно говорили
когда появилась возможность забивать себе текстовые URL вроде
&lt;code&gt;vkontakte.ru/ivan.blinkov&lt;/code&gt;. Сейчас этот подпроект имеет низкий приоритет
и ждет своего часа, когда они смогут предложить что-то более удобное и
быстрое, чем Gmail.&lt;/p&gt;
&lt;p&gt;Завеса тайны насчет технической реализации Вконтакте была немного
развеяна, но много моментов все же остались секретом. Возможно в будущем
появится более детальная информация о собственной СУБД Вконтакте,
которая как оказалось является ключом к решению всех самых сложных
моментов в масштабируемости системы.&lt;/p&gt;
&lt;p&gt;Как я уже упоминал этот пост написан почти на память, на основе
небольшого конспекта "круглого стола Вконтакте", так что хочется сразу
извиниться за возможные неточности и недопонимания. Я лишь
структурировал хаотичную кучу ответов на вопросы. Буду рад уточнениям и
дополнениям.&lt;/p&gt;
&lt;p&gt;Если хотите быть в курсе новых веяний в сфере масштабируемости
высоконагруженных интернет-проектов - по традиции рекомендую
&lt;a href="/feed/"&gt;подписаться на RSS&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 28 Oct 2010 21:12:00 +0400</pubDate><guid>tag:www.insight-it.ru,2010-10-28:highload/2010/arkhitektura-vkontakte/</guid><category>Apache</category><category>C++</category><category>Debian</category><category>featured</category><category>ffmpeg</category><category>HAProxy</category><category>highload</category><category>Linux</category><category>Memcached</category><category>mod_php</category><category>MySQL</category><category>nginx</category><category>node.js</category><category>openssl</category><category>PHP</category><category>XCache</category><category>xfs</category><category>Архитектура Вконтакте</category><category>Вконтакте</category></item><item><title>Архитектура Stack Overflow</title><link>https://www.insight-it.ru//highload/2010/arkhitektura-stack-overflow/</link><description>&lt;p&gt;&lt;img alt="Stack Overflow" class="right" src="https://www.insight-it.ru/images/stack-overflow-logo.png" title="Stack Overflow"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/33fc61d9/" rel="nofollow" target="_blank" title="https://stackoverflow.com/"&gt;Stack Overflow&lt;/a&gt; является любимым многими
программистами сайтом, где можно задать профессиональный вопрос и
получить ответы от коллег. Этот проект был написан двумя никому не
известными парнями, о которых никто никогда раньше не слышал. Хорошо, не
совсем так. Stack Overflow был создан топовыми программистами и звездами
блогосферы:&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/374a081/" rel="nofollow" target="_blank" title="http://www.codinghorror.com/blog/"&gt;Jeff Atwood&lt;/a&gt; и&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/31657700/" rel="nofollow" target="_blank" title="http://www.joelonsoftware.com/"&gt;Joel Spolsky&lt;/a&gt;. В этом отношении Stack
Overflow похож на ресторан, владельцами которого являются знаменитости.
По оценкам Joel'а около 1/3 программистов всего мира использовали этот
интернет-ресурс, так что должно быть он представляет собой что-то
достаточно полезное и интересное.&lt;/p&gt;
&lt;p&gt;Одним из ключевых моментов в истории Stack Overflow является
использование вертикального масштабирования, как достаточно
работоспособного решения достаточного большого класса проблем. Не смотря
на то, что публика на сегодняшний день больше склоняется к подходу с
использованием горизонтальным масштабирования и&amp;nbsp;не-SQL баз данных.&lt;/p&gt;
&lt;p&gt;Если Вы стремитесь к масштабу Google, у Вас нет другого выхода, как
двигаться в направлении не-SQL. Но Stack Overflow - это не Google, ровно
как и подавляющее большинство других сайтов. Когда Вы задумываетесь о
возможных вариантов дизайна Вашего проекта, попробуйте учесть и историю
Stack Overflow, она тоже имеет право на жизнь. В этот век многоядерных
машин с большим объемом оперативной памяти и невероятными темпами
развития методов параллельного программирования, вертикальное
масштабирование все еще является жизнеспособной стратегией и не должна
сразу же отбрасываться в сторону просто так как это теперь больше не
модно. Возможно в один прекрасный день мы получим лучшее из обоих миров,
но на сегодняшний момент перед нами лежит большой болезненный выбор
стратегии масштабирования, от которого определенно зависит судьба Вашего
проекта.&lt;/p&gt;
&lt;p&gt;Joel любит похвастаться тем, что они достигли производительности,
сравнимой с другими сайтами аналогичных размеров, используя в 10 раз
меньше оборудования. Он удивляется, работали над этими сайтами
по-настоящему хорошие программисты. Давайте взглянем на то, как им это
удалось, и дадим Вам возможность побыть судьей.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;&lt;em&gt;Перевод &lt;a href="https://www.insight-it.ru/goto/98b904e0/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2009/8/5/stack-overflow-architecture.html"&gt;статьи&lt;/a&gt;, автор оригинала - Todd Hoff. Возможно будет еще один пост с менее формальной информацией на ту же тему.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;16 миллионов просмотров страниц в месяц&lt;/li&gt;
&lt;li&gt;3 миллионов уникальных пользователей в месяц (для сравнения: Facebook
насчитывает около 77 миллионов уникальных пользователей в месяц)&lt;/li&gt;
&lt;li&gt;6 миллионов посещений в месяц&lt;/li&gt;
&lt;li&gt;86% трафика приходит с Google&lt;/li&gt;
&lt;li&gt;9 миллионов активных программистов во всем мире и 30% пользуются Stack
Overflow&lt;/li&gt;
&lt;li&gt;Более дешевые лицензии были получены через программу
Microsoft&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/205088ae/" rel="nofollow" target="_blank" title="http://www.microsoft.com/BizSpark"&gt;BizSpark&lt;/a&gt;. Скорее всего
они заплатили около 11000\$ за лицензии на ОС и MSSQL.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Стратегия монетизации: ненавязчивая реклама, вакансии, конференции
DevDays, достижения других смежных ниш (Server Fault, Super User),
разработка&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/631e88c2/" rel="nofollow" target="_blank" title="https://stackexchange.com/"&gt;StackExchange&lt;/a&gt; и возможно
каких-то других систем рейтингов для программистов.&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Microsoft ASP.NET MVC&lt;/li&gt;
&lt;li&gt;SQL Server 2008&lt;/li&gt;
&lt;li&gt;C#&lt;/li&gt;
&lt;li&gt;Visual Studio 2008 Team Suite&lt;/li&gt;
&lt;li&gt;jQuery&lt;/li&gt;
&lt;li&gt;LINQ to SQL&lt;/li&gt;
&lt;li&gt;Subversion&lt;/li&gt;
&lt;li&gt;Beyond Compare 3&lt;/li&gt;
&lt;li&gt;VisualSVN 1.5&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Веб уровень:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 x Lenovo ThinkServer RS110 1U&lt;/li&gt;
&lt;li&gt;4 ядра, 2.83 Ghz, 12 MB L2 cache&lt;/li&gt;
&lt;li&gt;500 GB жесткие диски, зеркалирование RAID1&lt;/li&gt;
&lt;li&gt;8 GB RAM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Уровень базы данных:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 x Lenovo ThinkServer RD120 2U&lt;/li&gt;
&lt;li&gt;8 ядер, 2.5 Ghz, 24 MB L2 cache&lt;/li&gt;
&lt;li&gt;48 GB RAM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Четвертый сервер был добавлен для запуска
&lt;a href="https://www.insight-it.ru/goto/d3829019/" rel="nofollow" target="_blank" title="https://superuser.com/"&gt;superuser.com&lt;/a&gt;. Все сервера вместе обеспечивают
работу &lt;a href="https://www.insight-it.ru/goto/33fc61d9/" rel="nofollow" target="_blank" title="https://stackoverflow.com/"&gt;Stack Overflow&lt;/a&gt;,&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/9b0a2d16/" rel="nofollow" target="_blank" title="https://serverfault.com/"&gt;Server
Fault&lt;/a&gt;, и&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/d3829019/" rel="nofollow" target="_blank" title="https://superuser.com/"&gt;Super User&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;QNAP TS-409U NAS для резервного копирования данных. Было принято решение
не использовать "облачные" решения, так как вызванные ими дополнительные
5GB трафика ежедневно были бы накладными.&lt;/li&gt;
&lt;li&gt;Сервера располагаются у&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/4f3f2e08/" rel="nofollow" target="_blank" title="http://www.peakinternet.com/"&gt;Peak Internet&lt;/a&gt;. В
основном из-за впечатляющей детализации технических ответов и разумных
расценок.&lt;/li&gt;
&lt;li&gt;Полнотекстный поиск в SQL Server активно используется для реализации
поиска по сайту и выявления повторных вопросов. Lucene .NET
рассматривается как достаточно заманчивая альтернатива.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;p&gt;Данный список является сборником уроков от Jeff и Joel, а также из
комментариев к их записям:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Если Вы комфортно себя чувствуете в деле управления серверами - не
бойтесь покупать их. Две основных проблемы с издержками аренды
оборудования:&lt;ol&gt;
&lt;li&gt;невероятные цены на дополнительную оперативную память и
жесткие диски;&lt;/li&gt;
&lt;li&gt;хостинг-провайдеры на самом деле не могут управлять
чем-либо за Вас.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Делайте одноразовые более крупные инвестиции в оборудование, чтобы
избежать быстро растущих ежемесячных издержек по аренде, которые
окажутся более высокими в долгосрочном периоде.&lt;/li&gt;
&lt;li&gt;Обновляйте сетевые драйвера. Производительность запросто может
удвоиться.&lt;/li&gt;
&lt;li&gt;Использование 48GB RAM требует обновления до MS Enterprise edition.&lt;/li&gt;
&lt;li&gt;Оперативная память невероятно дешевая. Используйте возможности по её
расширению по максимуму для получения практически бесплатной
производительности. У Dell, например, переход от 4GB памяти до 128GB
стоит всего 4378\$.&lt;/li&gt;
&lt;li&gt;Stack Overflow скопировали ключевую часть структуры базы данных у
Wikipedia. Это обернулось огромной ошибкой, для исправления которой
потребуется большой и болезненный рефакторинг базы данных. Основным
направлением изменений будет избавление от излишних операций по
объединению данных в большом количестве ключевых запросов. Это ключевой
урок, который стоит усвоить у гигантских много-терабайтных схем (вроде
Google BigTable), которые полностью избавлены от операций объединения
данных. Этот вопрос был достаточно важен для Stack Overflow, так как их
база данных практически полностью располагается в оперативной памяти и
операции join по прежнему требуют относительно много вычислительных
ресурсов.&lt;/li&gt;
&lt;li&gt;Производительность CPU оказывается на удивление важным фактором для
серверов баз данных. Переход от 1.86 GHz, к 2.5 GHz, и к 3.5 GHz
процессорам дает практически линейный прирост к времени выполнения
типичных запросов. Исключение: запросы, которые затрагивают не только
оперативную память.&lt;/li&gt;
&lt;li&gt;Когда оборудование арендуется, обычно никто не платит за дополнительную
оперативную память, если только вы не на помесячном контракте.&lt;/li&gt;
&lt;li&gt;В 90% случаев наиболее узким местом является база данных.&lt;/li&gt;
&lt;li&gt;При небольшом количестве серверов, &amp;nbsp;ключевым компонентом издержек
становится не место в стойках, электроэнергия, интернет-канал, сервера
или программное обеспечение, а СЕТЕВОЕ ОБОРУДОВАНИЕ. Вам потребуется как
минимум гигабитное соединение между уровнями веб-серверов и баз данных.
Между интернетом и веб-серверами потребуется firewall, маршрутизатор и
VPN. К моменту добавления второго веб-сервера понадобится решение для
балансировки нагрузки. Суммарная стоимость такого оборудования может
запросто вдвое превосходить стоимость пяти серверов.&lt;/li&gt;
&lt;li&gt;EC2 предназначен для горизонтального масштабирования, для того чтобы
нагрузка могла быть распределена между большим количеством машин
(достаточно хорошая идея, если Вы планируете расширяться). Еще больше
смысла в таком подходе появляется, если вы планируете масштабироваться
по необходимости (то есть добавлять и убирать машины в зависимости от
уровня нагрузки).&lt;/li&gt;
&lt;li&gt;Горизонтальное масштабирование может проходить относительно
безболезненно только при использовании open source программного
обеспечения. В противном случае вертикальное масштабирование значит
сокращение издержек, связанных с лицензиями, в ущерб стоимости
оборудования, а горизонтальное масштабирование - наоборот: экономия на
оборудовании, но требуется существенно больше лицензий на программное
обеспечение.&lt;/li&gt;
&lt;li&gt;RAID-10 отлично работает для баз данных с высокой нагрузкой операций
чтения и записи.&lt;/li&gt;
&lt;li&gt;Разделяйте работу приложений и баз данных таким образом, чтобы они могли
масштабироваться независимо друг от друга. Например, базы данных могут
масштабироваться вертикально, а сервера приложений - горизонтально.&lt;/li&gt;
&lt;li&gt;Приложения должны хранить все информацию о своем состоянии в базе данных
для обеспечения возможности роста путем простого добавления серверов
приложений в кластер.&lt;/li&gt;
&lt;li&gt;Одна из основных проблем со стратегией вертикального масштабирования -
недостаток избыточности. Кластеризация добавляет надежности, но когда
стоимость каждого сервера высока - это не так просто реализовать.&lt;/li&gt;
&lt;li&gt;Некоторые приложения могут масштабироваться линейно относительно числа
процессоров. Но зачастую будут использоваться механизмы блокировки, что
приведет к сериализации вычислений и в итоге к существенному уменьшению
эффективности приложения.&lt;/li&gt;
&lt;li&gt;С более крупными серверами, занимающими от 7U в стойке, электроэнергия и охлаждение становятся критичными вопросами. Возможно использование
чего-то среднего между 1U и 7U может облегчить Ваши взаимоотношения с
датацентром.&lt;/li&gt;
&lt;li&gt;С добавлением все новых и новых серверов баз данных издержки на лицензии SQL Server могут стать очень существенными. Если Вы начнете с
вертикального масштабирования и постепенно начнете переходить к
горизонтальному с использованием не open source продуктов, возможно это
сильно ударит по Вашему финансовому состоянию. Это справедливо, что в
этой заметке речь идет не совсем об архитектуре проекта. Мы знаем об их
серверах, об используемом наборе инструментов, об их двухуровневой
схеме, где база данных используется напрямую из кода веб-серверов. Но мы
не знаем практически ничего о самой реализации, например таких мелочей
как теги. Если Вам интересен этот вопрос, возможно Вам удастся получить
интересующую Вас информацию из &lt;a href="https://www.insight-it.ru/goto/61237733/" rel="nofollow" target="_blank" title="http://sqlserverpedia.com/wiki/Understanding_the_StackOverflow_Database_Schema"&gt;описания их схемы базы данных&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 08 Jan 2010 00:31:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-01-08:highload/2010/arkhitektura-stack-overflow/</guid><category>ASP</category><category>ASP .NET</category><category>Beyond Compare 3</category><category>C++</category><category>highload</category><category>JQuery</category><category>Lenovo</category><category>Lenovo ThinkServer</category><category>LINQ</category><category>Microsoft</category><category>MSSQL</category><category>MVC</category><category>Server Fault</category><category>SQL Server 2008</category><category>Stack Overflow</category><category>Subversion</category><category>Super User</category><category>Visual Studio 2008 Team Suite</category><category>VisualSVN</category><category>архитектура</category><category>архитектура Stack Overflow</category><category>архитектура высоконагруженных сайтов</category><category>Масштабируемость</category></item><item><title>Архитектура MySpace</title><link>https://www.insight-it.ru//highload/2009/arkhitektura-myspace/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/f989d588/" rel="nofollow" target="_blank" title="http://www.myspace.com"&gt;MySpace.com&lt;/a&gt; является одним из наиболее быстро
набирающих популярность сайтов в Интернете с 65 миллионами пользователей
и 260000 регистрациями в день. Этот сайт часто подвергается критике
из-за не достаточной производительности, хотя на самом деле MySpace
удалось избежать ряда проблем с масштабируемостью, с которыми
большинство других сайтов неизбежно сталкивались. Как же им это
удалось?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Данная статья является переводом статьи &lt;a href="https://www.insight-it.ru/goto/e9f0b809/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2009/2/12/myspace-architecture.html"&gt;MySpace
Architecture&lt;/a&gt;,
автором которой является Todd Hoff. Когда-то давно один из читателей
этого блога просил меня осветить и эту тему, тогда я так и не решился
из-за отсутствия моего личного интереса, но сейчас снова случайно
наткнулся на эту статью и подумал: а почему бы и нет?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/45549701/" rel="nofollow" target="_blank" title="http://www.infoq.com/news/2009/02/MySpace-Dan-Farino"&gt;Презентация: за сценой MySpace.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8d5c1d4d/" rel="nofollow" target="_blank" title="http://www.baselinemag.com/c/a/Projects-Networks-and-Storage/Inside-MySpacecom/"&gt;Внутри MySpace.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ASP .NET 2.0&lt;/li&gt;
&lt;li&gt;Windows&lt;/li&gt;
&lt;li&gt;IIS&lt;/li&gt;
&lt;li&gt;MSSQL Server&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="chto-vnutri"&gt;Что внутри?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;300 миллионов пользователей.&lt;/li&gt;
&lt;li&gt;Отдает 100Gbps в Интернет. 10Gbps из них является HTML контентом.&lt;/li&gt;
&lt;li&gt;4,500+ веб серверов со связкой: Windows 2003 / IIS 6.0 / ASP .NET.&lt;/li&gt;
&lt;li&gt;1,200+ кэширующих серверов, работающих на 64-bit Windows 2003. На
    каждом 16GB объектов находятся в кэше в оперативной памяти.&lt;/li&gt;
&lt;li&gt;500+ серверов баз данных, работающих на 64-bit Windows и SQL Server
    2005.&lt;/li&gt;
&lt;li&gt;MySpace обрабатывает 1.5 миллиарда просмотров страниц в день, а
    также 2.3 миллионов одновременно работающих пользователей в течении
    дня.&lt;/li&gt;
&lt;li&gt;Вехи по количеству пользователей:&lt;ul&gt;
&lt;li&gt;500 тысяч пользователей: простая архитектура перестает
справляться&lt;/li&gt;
&lt;li&gt;1 миллион пользователей: вертикальное партиционирование временно
спасает от основных болезненных вопросов с масштабированием&lt;/li&gt;
&lt;li&gt;3 миллиона пользователей: горизонтальное масштабирование
побеждает над вертикальным&lt;/li&gt;
&lt;li&gt;9 миллионов пользователей: сайт мигрирует на ASP.NET, создается
виртуализированная система хранения данных (SAN)&lt;/li&gt;
&lt;li&gt;26 миллионов пользователей: MySpace переходит на 64-битную
технологию.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;500 тысяч учетных записей было многовато для двух веб-серверов и
    одного сервера баз данных.&lt;/li&gt;
&lt;li&gt;На 1-2 миллионах учетных записей:&lt;ul&gt;
&lt;li&gt;Они использовали архитектуру базы данных, построенную на
концепции вертикального партиционирования, с отдельными базами
данных для разных частей сайта, которые использовались для
выполнения различных функций, таких как экран авторизации, профили
пользователей и блоги.&lt;/li&gt;
&lt;li&gt;Схема с вертикальным партиционированием помогала разделить
нагрузку как для операций чтения, так и для операций записи, а если
пользователям в друг оказывалась нужна новая функциональная
возможность - достаточно было просто добавить еще один сервер баз
данных для её обслуживания.&lt;/li&gt;
&lt;li&gt;MySpace переходит от использования систем хранения, подключенных
к серверам баз данных напрямую, к сетям хранения данных (SAN), при
таком подходе целый массив систем хранения объединяется вместе
специализированной сетью с высокой пропускной способностью, и
сервера баз данных также получают доступ к хранилищам через эту
сеть. Переход к SAN оказал положительное влияние как на
производительность, так и на доступность и надежность системы.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;На 3 миллионах учетных записей:&lt;ul&gt;
&lt;li&gt;Решение с вертикальным партиционированием не протянуло долго, так
как им приходилось реплицировать какую-то часть информации (например
информацию об учетных записях) по всем вертикальным частям базы
данных. С таким большим количеством операций репликации данных один
узел даже при незначительном сбое мог существенно замедлить
обновление информации во всей системе.&lt;/li&gt;
&lt;li&gt;Индивидуальные приложения вроде блогов на под-секциях сайта
достаточно быстро стали слишком большими для нормальной работы с
единственным сервером базы данных&lt;/li&gt;
&lt;li&gt;Произведена реорганизация всех ключевых данных для более логичной
организации в единственную базу данных&lt;/li&gt;
&lt;li&gt;Пользователи были разбиты на группы по миллиону в каждой и каждая
такая группа была перемещена на отдельный SQL Server&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;9&amp;ndash;17 миллионов учетных записей:&lt;ul&gt;
&lt;li&gt;Переход на ASP .NET, который требовал меньше ресурсов по
сравнению с их предыдущим вариантом архитектуры. 150 серверов,
использовавших новый код могли обработать нагрузку, для которой
раньше требовалось 246 серверов.&lt;/li&gt;
&lt;li&gt;Снова пришлось столкнуться с узким местом в системе хранения
данных. Реализация SAN решило какую-то часть старых проблем с
производительностью, но на тот момент потребности сайта начали
периодически превосходить возможности SAN по пропускной способности
операций ввода-вывода - той скорости, с которой она может читать и
писать данные на дисковые массивы.&lt;/li&gt;
&lt;li&gt;Столкнулись с лимитом производительности при размещении миллиона
учетных записей на одном сервере, ресурсы некоторых серверов начали
исчерпываться.&lt;/li&gt;
&lt;li&gt;Переход к виртуальному хранилищу, где весь SAN рассматривается
как одно большое общее место для хранения данных, без необходимости
назначать конкретные диски для хранения данных определенной части
приложения. MySpace на данный момент работает со стандартизированным
оборудованием от достаточно нового вендора SAN - 3PARdata&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Был добавлен кэширующий уровень &amp;mdash; прослойка из специализированных
    серверов, расположенных между веб-серверами и серверами данных, чья
    единственная задача была захватывать копии часто запрашиваемых
    объектов с данными в памяти и отдавать их веб-серверам для
    минимизации количества поиска данных в СУБД.&lt;/li&gt;
&lt;li&gt;26 миллионов учетных записей:&lt;ul&gt;
&lt;li&gt;Переход на 64-битные сервера с SQL Server на правах решения
проблемы с недостатком оперативной памяти. С тех пор их стандартный
сервер баз данных оснащен 64 GB RAM.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Горизонтальная федерация баз данных&lt;/strong&gt;. Базы данных
    партиционируются в зависимости от своего назначения. У них есть базы
    данных с профилями, электронными сообщениями и так далее. Каждая
    партиция основана на диапазоне пользователей. По миллиону в каждой
    базе данных. Таким образом, у них есть Profile1, Profile2 и все
    остальные базы данных вплоть до Profile300, если считать, что у них
    на данный момент зарегистрировано 300 миллионов учетных записей.&lt;/li&gt;
&lt;li&gt;Кэш ASP не используется, так как он не обеспечивает достаточного
    процента попаданий на веб серверах. Кэш, организованный как
    промежуточный слой, имеет существенно более высокое значение данного
    показателя.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Изоляция сбоев&lt;/strong&gt;. Внутри веб-сервера запросы сегментируются по
    базам данным. Разрешено использование только 7 потоков для работы с
    каждой базой данных. Таким образом, если база данных по каким-то
    причинам начинает работать медленно, только эти потоки замедлятся, в
    то время как остальные потоки будут успешно продолжать обрабатывать
    поток трафика.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="rabota-saita"&gt;Работа сайта&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Коллектор данных о производительности&lt;/strong&gt;. Централизованная система
    сбора информации о производительности через UDP. Такой подход более
    надежен, чем стандартный механизм Windows, а также позволяет любому
    клиенту подключиться и увидеть статистику.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Веб-система по просмотру дампов стеков процессов&lt;/strong&gt;. Можно просто
    сделать клик правой кнопкой мыши на проблемном сервере и увидеть
    дамп стека процессов, управляемых .NET. И это после привычки каждой
    раз удаленно подключаться к серверу, включать дебаггер и через
    полчаса получать свой ответ о том что же все таки происходит.
    Медленно, немасштабируемо и утомительно. Эта же система позволяет
    увидеть не просто стек процесса, но и предоставляет большое
    количество информации о контексте, в котором он работает.
    Обнаружение проблем намного проще при таком подходе, например можно
    легко увидеть, что база не отвечает, так как 90 ее потоков
    заблокировано.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Веб-система создания дампа heap-памяти&lt;/strong&gt;. Создает дамп всей
    выделенной памяти. Очень удобно и полезно для разработчиков.
    Сэкономьте часы на выполнение этой работы вручную.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Профайлер&lt;/strong&gt;. Прослеживает запрос от начала до конца и выводит
    подробный отчет. В нем можно увидеть URL, методы, статус, а также
    все, что поможет идентифицировать медленный запрос и его причины.
    Обнаруживает проблемы с блокировкой потоков, непредвиденными
    исключениями, другими словами все, что может оказаться интересным. В
    то же время остается очень легковесным решением. Работает на одной
    машине из каждой VIP (группа из 100 серверов) в production-среде.
    Опрашивает 1 поток каждые 10 секунд. Постоянно следит за системой в
    фоновом режиме.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Powershell&lt;/strong&gt;. Новая программная оболочка от Microsoft, которая
    работает в процессе и передаем объекты между командами вместо работы
    с текстовыми данными. MySpace разрабатывает множество так называемых
    commandlets'ов для поддержки различных операций.&lt;/li&gt;
&lt;li&gt;Разработана собственная технология асинхронной коммуникации для
    того, чтобы обойти проблемы с сетевыми проблемами Windows и работать
    с серверами как с группой. Например, она позволяет доставить файл
    .cs, скомпилировать его, запустить, и доставить результат обратно.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Развертывание&lt;/strong&gt;. Обновление кодовой базы происходит с помощью
    упомянутой выше собственной технологии. Ранее происходило до 5 таких
    обновлений в день, сейчас же они происходят лишь раз в неделю.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;С помощью стека Microsoft тоже можно делать большие веб-сайты.&lt;/li&gt;
&lt;li&gt;Стоит использовать кэширование с самого начала.&lt;/li&gt;
&lt;li&gt;Кэш является более подходящим местом для хранения временных данных,
    не требующих персистентности, например информации о пользовательских
    сессиях.&lt;/li&gt;
&lt;li&gt;Встроенные в операционные систему возможности, например по
    обнаружению DDoS-атака, могут приводить к необъяснимым сбоям.&lt;/li&gt;
&lt;li&gt;Храните свои данные в географически удаленных датацентрах для
    минимизации проблем, связанных со сбоями в электросети.&lt;/li&gt;
&lt;li&gt;Рассматривайте возможности использования виртуализированных систем
    хранения данных или кластерных файловых систем с самого начала. Это
    позволит существенно параллелизировать операции ввода-вывода, а
    также увеличивать дисковое пространство без необходимости какой-либо
    реорганизации.&lt;/li&gt;
&lt;li&gt;Разрабатывайте утилиты для работы с production окружением.
    Невозможно смоделировать все ситуации в тестовой среде.
    Масштабируемость и все различные варианты использования API не могут
    быть симулированы в процессе тестирования качества программного
    обеспечения. Обычные пользователи и хакеры обязательно найдут такие
    способы использования вашего продукта, о которых вы даже никогда и
    не подумаете в процессе тестирования, хотя конечно большая часть все
    же обнаружима в процессе QA тестирования.&lt;/li&gt;
&lt;li&gt;Когда это возможно - лучше просто использовать дополнительное
    оборудование для решения проблем. Это намного проще, чем изменять
    поведение программного обеспечения для того чтобы решать задачи
    как-то по-другому. Примером может служить добавление нового сервера
    на каждый миллион пользователей. Возможно было бы более эффективным
    изменить подход к самой работе с СУБД, но на практике все же проще и
    дешевле добавлять все новые и новые сервера. По крайней мере на
    данный момент.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 21 Dec 2009 16:15:00 +0300</pubDate><guid>tag:www.insight-it.ru,2009-12-21:highload/2009/arkhitektura-myspace/</guid><category>ASP</category><category>ASP .NET</category><category>highload</category><category>IIS</category><category>Microsoft</category><category>MSSQL</category><category>MySpace</category><category>myspace.com</category><category>online</category><category>Windows</category><category>Windows Server</category><category>архитектура</category><category>архитектура MySpace</category><category>Масштабируемость</category></item><item><title>РИТ: Высокие нагрузки</title><link>https://www.insight-it.ru//event/2008/rit-vysokie-nagruzki/</link><description>&lt;p&gt;Если кто не в курсе, 22 и 23 сентября в Москве проходила конференция для
разработчиков высоконагруженных систем. Не знаю могу ли я себя
полноценно отнести к этой категории людей, но тем не менее данной
мероприятие я сегодня посетил, пост будет опубликован скорее всего
несколько позже, но начинаю писать прямо с ходу в первый же день
конференции. Здесь будут лишь мои личные впечатления вперемешку с кратким пересказом происходившего. Что ж, приступим.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="predistoriia"&gt;Предистория&lt;/h3&gt;
&lt;p&gt;Попал я на данное мероприятие достаточно тривиально: благодаря акции для
студентов, предоставляющей бесплатное участие по результатам
online-тестирования (не знаю заплатил ли бы я недельную зарплату за
участие, если бы ее не было, хотя впринципе со скидками тоже не так уж и
много получилось бы), а также стратежно добавленной в RSS-агрегатор
нужной ленте новостей.&lt;/p&gt;
&lt;p&gt;Само же тестирование меня несколько позабавило. Не знаю кто его
составлял, но суть его была примерно в следующем: за полтора часа
предлагплось ответить на 10 вопросов, семь из которых формулировалось
примерно как "какой результат будет получен после выполнения программы
&amp;lt;код на перле&amp;gt;". Основной прикол был в том, что этот код написан на Perl
там сказано не было, если честно с первого вопроса я его даже не
признал, приняв за какой-то псевдо-код. На втором вопросе до меня
дошла-таки истина, благодаря не столь отдаленному во времени посещению
&lt;a href="https://www.insight-it.ru/event/2008/yapcrussia-2008-may-perl/"&gt;YAPC::Russia 2008 "May Perl"&lt;/a&gt; и, в
частности, проходившему там мастер-классу &lt;em&gt;Ивана Сережкина&lt;/em&gt;. Дальнейшее
тестированме проходило с открытым &lt;code&gt;vim&lt;/code&gt;'ом и &lt;code&gt;/bin/perl&lt;/code&gt; под рукой,
лишь один или два вопроса пришлось неторопливо погуглить...&lt;/p&gt;
&lt;p&gt;За пару дней до конференции на почту пришло-таки поздравление с успешным
прохождением тестирования и приглашением. В планы на эти два дня входило
как минимум посещение ВУЗа и работы, но особого труда их освободить не
составило. Так я и оказался сегодня утром в "Инфопространстве".&lt;/p&gt;
&lt;h3 id="den-pervyi"&gt;День первый&lt;/h3&gt;
&lt;p&gt;Добравшись-таки до точки назначения на 20 минут позже запланированного
открытия регистрация я как раз успел к ее началу. В нагрузку к бэйджику
девушки выдали пакетик с программкой и кучей макулатуры, а также адский
круглый девайс, на проверку оказавшийся шлепанцами.&lt;/p&gt;
&lt;p&gt;Оставшееся до открытия время пришлось коротать по большей части изучая
программку и здороваясь с тем очень скромным количеством лично знакомых
мне людей, которые тоже посетили данное мероприятие. На удивление с ходу
увидел много каким-то непонятным образом виртуально-знакомых лиц, с
которыми лично определенно никогда не общался. Конференция проходила в
два потока, так что в процессе ознакомления с программкой пришлось и
определяться с более полезными для себя докладами; на практике дело
оказалось непростым, так как даже прилагавшаяся книжечка с тезисами не
позволяла заранее оценить качество доклада. В итоге в этом нелегком деле
даже пришлось припомнить пару когда-то давно прочитанных статей по
эвристике - очень помогло.&lt;/p&gt;
&lt;p&gt;После непродолжительно-формального открытия начался первый доклад от
Анатолия Орлова из Яндекс. Доклад был, как ни странно, вводно-обзорным,
и посвящен он был по большей части взгляду на потенциальную нагрузку на
веб-сервера с точки зрения оборудования и операционной системы. Никаких
деталей, достаточно тривиально, но напомнить никогда не помешает, да и
со своей задачей - задать соответствующий настрой в аудитории он
определенно справился.&lt;/p&gt;
&lt;p&gt;После непродолжительного перерыва на кофе слово перешло еще одному
представителю компании Яндекс - Филиппу Дельгядо. На этот раз речь пошла
о веб-проектах средних размеров, которым необходимо справляться с
серьезной нагрузкой (наравне с прочими нефункциональными требованиями)
но с несколько другой точки зрения, программиста. Выступление также
носило обзорный характер и содержало в себе повествования о достаточно
большом количестве граблей, на которые могут натыкаться те или иные
проекты. На самом деле по затрагиваемым темам мне в свое время довелось
прочитать достаточно много материалов, пару раз споткнуться на практике
и даже написать пару постов, так что лично я рассматривал этот доклад
как закрепление знакомых тем. Разве что затронутый вопрос
горизонтального масштабирования хранения данных меня несколько смутил:
не знаю, может быть я несколько не в теме или еще что, но суть была
примерно такова, что под partitioning'ом рассматривались только
псевдо-универсальные решения, входящие в комплект некоторых СУБД, в то
время как sharding'ом было предложено понимать все остальные,
"самопальные", решения, специфичные для конкретного проекта. Я же
почему-то всегда рассматривал такой подход как нечто
концептуально-общее, так как конкретика реализации не важна, а важна
сама идея данного подхода, и считаю эти два слова достаточно близкими
синонимами (ровно как и многочисленные их русские аналоги), хотя может
быть я и не прав...&lt;/p&gt;
&lt;p&gt;Следующее выступление было, пожалуй, лучшим из увиденных мной в первый
день. В программке оно значилось как: "Как писать высокопроизводительные
веб-сервера", Антон Самохвалов (Яндекс). Были рассмотрен ряд возможных
подходов к написанию веб-серверов: начиная банальными неработоспособными
однопоточными вариантами и заканчивая противопоставлению вполне рабочих
вариантов: threads, &lt;abbr title="Finite State Machine"&gt;FSM&lt;/abbr&gt; и co-routine. Докладчик оказался убежденным сторонником подхода с использованием co-routine или на худой конец thread'ов, а также
использующего схожую идеологию веб-сервера от ASF. Презентация сыграла
лишь роль детонатора для последующей бурной дискуссии с участием многих
слушателей, а также ведущего этой секции - Игоря Сысоева, по
совместительству разработчика известного в "узких" кругах проекта nginx.
Ничего удивительного, что обсуждение плавно перетекло в Holy War:
Threads vs &lt;abbr title="Finite State Machine"&gt;FSM&lt;/abbr&gt;, но в отличии от знаменитых войн анонимусов на
linux.org.ru, шоу выдалось захватывающим и информационно-полезным.
Основными аргументами были даже не цифры о производительности,
полученные ребятами из Яндекса в полу-синтетических тестах (Антон
активно упоминал тот факт, что на примере базового поиска Яндекса
использованние thread'ы давало примерно 10% прирост производительности
по сравнению с &lt;abbr title="Finite State Machine"&gt;FSM&lt;/abbr&gt;), а скорее споры о maintenability кода и его
простоте, а также теоретических пределах использования этих подходов в
разных условиях (речь даже дошла до гипотетических ситуаций с
использованием более чем четырехядерных процессоров). Довелось выслушать
мнения профессионалов по обе стороны "баррикад" и самостоятельно сделать
все требуемые выводы, самый банальный и немаловажный из которых
заключается просто в том факте, что оба подхода имеют право на жизнь в
реальных проектах и какой из них использовать очень во многом зависит от
личных предпочтений разработчиков и специфики проекта.&lt;/p&gt;
&lt;p&gt;Далее по расписанию был запланировал обеденный перерыв, на котором я
решил заглянуть в аудиторию параллельного потока (где весь день
рассказывали о различных системах хранения данных). В это время там как
раз проходил доклад Павла Уварова из Рамблер о их собственной разработке
для хранения и передачи данных под названием HCS (читается почему-то как
"хикс"). Самое начало доклада я, к сожалению, пропустил, но почти с
первых же увиденных мной слайдов проект меня заинтересовал. Суть данной
технологии примерно характеризуется ее названием: Hierarchically
Compressed Stream. С одной стороны это формат хранения иерархически
структурированных данных, с другой же - данные в нем представляют
обычный файл, который легко обернуть в произвольный протокол передачи
потоковых данных (т.е. например просто раздавать через любой
HTTP-сервер). По представленным в презентации benchmark'ам запись и
линейный доступ к данным в этом формате происходит в разы, а то и
порядки быстрее по сравнению с РСУБД, а для ускорения случайного
доступа, на сколько я понял, имеется возможность построения индекса по
верхнему уровню иерархии. Помимо всего прочего, Павел обрадовал
аудиторию, что данная технология в ближайшем времени будет опубликована
в opensource, обязательно надо будет попробовать ее в деле как только
появится возможность.&lt;/p&gt;
&lt;p&gt;Вернувшись после обеда в аудиторию первого потока я сверился с
программкой и обнаружил, что сейчас будут рассказывать про архитектуру
сервиса SpyLog. Выступление Сергея Скворцова было очень общим и не
углублялось в какие-либо детали, если априори представлять себе как
работают сервисы веб-аналитики в целом, то становилось и вовсе скучно.
Единственной конкретикой было разве что упоминание о том, что для сбора
первичных данных используется nginx (модифицированный, если я правильно
понял)&lt;/p&gt;
&lt;p&gt;После этого доклада можно было стать свидетелем повествования работников
mail.ru сначала об их собственной разработке под названием Imagine
Framework, которая тоже вполне соответствует своему названию: ее можно
разве что представить - ни одного примера кода, закрыт для использования
только в рамках mail.ru, много слов ни о чем и общий стиль изложения в
духе "вот какую клевую штуку мы придумали, но никому не дадим и не
покажем". В общем я даже, если честно, не понял для какого он языка
программирования предназначен и чем он лучше (или хотябы просто
отличается) от opensource аналогов (упоминались собственные системы
кэширования, мониторинга и бинарный протокол). В итоге я до конца так и
не дослушал и ушел снова в соседний поток слушать про хранение
слабосвязанных данных.&lt;/p&gt;
&lt;p&gt;Илья Космодемьянцев из SUP Fabrik вел рассказ о том, как могут
развиваться события, если тот или иной проект сталкивается с хранением
данных, плохо вписывающихся в модель РСУБД. Разработчикам приходят мысли
о создании собственного специфического формата хранения данных, о том с
какими проблемами могут столкнуться в процессе. Возможные варианты
решения проблем также рассматривались, например в виде написания не
формата хранения данных, а собственного хранилища для, например, MySQL -
это может помочь избежать части проблем. Помимо этого выдвигалось очень
спорное предложение о создании некого "UseCase API", которое могло бы
позволить инкапсулировать для бэкэнда все особенности хранения данных в
случае их нетрадиционности. Хотя на практике подобные методики
применяются достаточно давно, особенно в объектно-ориентированных языках
программирования в виде data-access objects.&lt;/p&gt;
&lt;p&gt;После еще одного кофе-брейка оставалось лишь два заключительных доклада
первого дня, из которых я решил посетить нечто под названием "Масштабный
Jabber-кластер, IM и не только" от Андрея Федоровского из РБК. Начало
доклада было достаточно примитивным - что такое Jabber и с чем его едят,
всем это и без того было известно, так что было достаточно скучно. Даже
обзор доступных расширений не открыл ничего нового - все те же давно
знакомые вещи, разве что некий интерес вызвал процесс отправки бинарных
данных (т.е. произвольных файлов): было заявлено что отправка файла
временно обрывает поток XML-данных и отправляет вместо него бинарный
поток, но не заставил себя ждать очевидный вопрос: а как же тогда
происходит общение в этот момент. Вокруг этого образовалась небольшая
дискуссия, но к консенсусу прийти так и не удалось. Далее пошел рассказ
собственно о том как на сегодняшний день можно построить
Jabber-кластер - подробностей повествованию явно не хватало, но суть
была примерно такова: единственным более-менее сносным Jabber-сервером
является ejabberd, написанный на до сих пор экзотическом языке Erlang
(что впоследствии подтвердили сотрудники Яндекс: в своем проекте
Я.Онлайн они используют тоже его). По-умолчанию он использует нативное
для Erlang хранилище данных под названием Mnesia, но в том проекте РБК,
на основе которого и строился весь доклад (правда я так и не понял как
он называется), почему-то побоялись его использовать (видимо из-за
нежелания разбираться в том как оно функционирует) и использовали
имеющуюся возможность интеграции с более традиционной СУБД - PostgreSQL.
В дискуссии после доклада речь шла скорее о самом ejabberd, Erlang'е,
острой нехватке программистов на нем и тому подобных вещах, чем о самом
докладе.&lt;/p&gt;
&lt;p&gt;Подошедший к концу первый день конференции оставил массу положительных
впечатлений и тем для размышления. Практическая полезность конференции
не вызывает никаких сомнений, чего-то подобное невозможно увидеть и
услышать в интернете или где-либо еще. Были конечно же и негативные
моменты в виде абсолютно бестолковых докладов, но обычно это удавалось с
ходу обнаружить и вовремя ретироваться в соседний поток.&lt;/p&gt;
&lt;h3 id="den-vtoroi"&gt;День второй&lt;/h3&gt;
&lt;p&gt;Вот уже почти как неделю никак не могу найти времени дописать этот пост,
в связи с жесткой нехваткой свободного времени, благодаря работе и
учебе, так что позволю себе опубликовать пока в недописанном варианте.
Как дойдут руки - обязательно допишу, благо во второй день тоже было
несколько интересных выступлений, из которых особенно запомнилось
выступление Тимура Хайруллина из Яндекс о методиках нагрузочного
тестирования.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 02 Oct 2008 01:49:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-10-02:event/2008/rit-vysokie-nagruzki/</guid><category>highload</category><category>Life</category><category>ProfyClub</category><category>высокие нагрузки</category><category>конференция</category><category>Масштабируемость</category><category>РИТ</category></item></channel></rss>