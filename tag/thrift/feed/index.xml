<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Insight IT</title><link>https://www.insight-it.ru/</link><description></description><atom:link href="https://www.insight-it.ru/tag/thrift/feed/index.xml" rel="self"></atom:link><lastBuildDate>Mon, 04 Jun 2012 05:38:00 +0400</lastBuildDate><item><title>Серверная часть интерактивного сайта и потоки сообщений</title><link>https://www.insight-it.ru//interactive/2012/servernaya-chast-interaktivnogo-sajjta-i-potoki-soobshhenijj/</link><description>&lt;p&gt;Вернемся к теме &lt;a href="https://www.insight-it.ru/interactive/"&gt;интерактивных сайтов&lt;/a&gt; с обратной стороны, серверной. В ней есть огромный простор для творчества, так как
в отличии от клиентской части отсутствуют ограничения, накладываемыми
браузерами. С "простором" же приходит и неоднозначность/неопределенность, вариантов как реализовать одно и то же множество, так что возможно приводимые мной примеры Вам окажутся не по душе &amp;nbsp;- и это нормально, правильный путь не единственный, их много :)&lt;/p&gt;
&lt;p&gt;Приступим!&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="vnutrennie-servisy"&gt;Внутренние сервисы&lt;/h2&gt;
&lt;p&gt;Напомню, что обычно на внутренние сервисы ложится реализация всей или
большей части бизнес-логики приложения. Они получают пользовательские
запросы в стандартизированном виде через прослойки в виде внешних
интерфейсов и, при необходимости взаимодействуя друг с другом и
остальными компонентами системы, определяют какой ответ необходимо
отправить и какие другие действия предпринять.&lt;/p&gt;
&lt;p&gt;Я не буду здесь особо вдаваться в возможные детали реализации самой
бизнес-логики - она практически всегда уникальна, скорее заслуживает
внимания её "обертка" - сам процесс, принимающий и создающий внутренние
запросы.&lt;/p&gt;
&lt;p&gt;Вообще создание внутренних сервисов очень хорошо ложится на так
называемую &lt;a href="https://www.insight-it.ru/goto/7e699ecd/" rel="nofollow" target="_blank" title="http://ru.wikipedia.org/wiki/%D0%9C%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C_%D0%B0%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2"&gt;модель "акторов"&lt;/a&gt;,
система разбивается на некие логические примитивы, общающиеся между
собой исключительно передачей сообщений. По сути процессы с
определенными разработчиками наборами входящих и исходящих сообщений и
алгоритмом преобразования одних в другие. При таком подходе группа
одинаково функционирующих акторов (вероятно распределенная по нескольким
серверам для отказоустойчивости и возможности масштабирования) и
образует внутренний сервис.&lt;/p&gt;
&lt;p&gt;На практике есть масса способов воплотить эту модель в жизнь,
перечислю&amp;nbsp;с пояснениями наиболее заслуживающие внимания&amp;nbsp;на мой взгляд:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Функциональные языки программирования,&lt;/strong&gt;&amp;nbsp;в &lt;a href="/tag/erlang/"&gt;Erlang&lt;/a&gt;
    и &lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt; модель акторов является практически "сердцем"
    всего языка и связанной платформы; у обоих есть библиотеки для
    реализации надежных, высокопроизводительных и масштабируемых акторов
    (&lt;strong&gt;OTP&lt;/strong&gt; и &lt;strong&gt;Akka&lt;/strong&gt;, соответственно). Если не боитесь кардинально
    отличающейся от нынче модного ООП парадигмы разработки, этот вариант
    наиболее жизнеспособный, &lt;em&gt;рекомендую&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Асинхронный HTTP-сервер&lt;/strong&gt;, в частности &lt;a href="/tag/tornado/"&gt;Tornado&lt;/a&gt; и
    &lt;a href="/tag/node-js/"&gt;node.js&lt;/a&gt;&amp;nbsp;- они основаны на &lt;a href="https://www.insight-it.ru/linux/2012/kak-rabotaet-epoll/"&gt;epoll&lt;/a&gt; и помимо эффективной обработки HTTP-запросов умеют и эффективно их отправлять посредством идущего в комплекте асинхронного же клиента.
    При таком подходе по сути получается несколько "уровней"
    HTTP-серверов, первый из которых публично доступен для общения с
    внешним миром и в ответ на каждый входящий запрос обращается сразу к
    нескольким внутренним HTTP-сервисам (вероятно параллельно) и на их
    основе составляет ответ пользователю. Этот подход одно время активно
    пропагандировали на конференциях ребята из одного крупного
    отечественного сайта с вакансиями. Особенным бонусом этого варианта
    является возможность использовать в роли внутреннего сервиса
    какую-то старую, доставшуюся по наследству &lt;em&gt;(legacy)&lt;/em&gt;, систему,
    которая с одной стороны по-прежнему нужна, а с другой - человек,
    который в ней&amp;nbsp;разбирался уже давно уволился.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/c/"&gt;С++&lt;/a&gt; и &lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt;&lt;/strong&gt; - хоть одного из
    участников этой пары можно легко заменить на альтернативу, вместе
    они смотрятся наиболее органично: потенциально
    высокопроизводительная реализация бизнес-логики на С++ плюс
    проверенная в деле многими крупными и очень крупными проектами
    обертка для создания серверов и клиентов, легко общающихся из разных
    языков программирования (речь о Thrift, если не очевидно). Если в
    команде проекта есть гуру C++ - этот вариант Ваш, в противном случае
    не рекомендую, т.к. &lt;em&gt;очень&lt;/em&gt; легко накосячить.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Иногда внутренние сервисы возможно сделать совсем изолированными, то
есть без взаимодействия с другими компонентами системы. Но в большинстве
случаев это не так, зачастую для принятия решения им необходимы внешние
данные.&lt;/p&gt;
&lt;h2 id="baza-dannykh-i-keshirovanie"&gt;База данных и кэширование&lt;/h2&gt;
&lt;p&gt;По большому счету интерактивные сайты не особо сильно отличаются от
статичных с точки зрения организации хранения данных.&lt;/p&gt;
&lt;p&gt;Из особенностей хочу отметить более-менее четкое разграничение
&lt;strong&gt;стабильной&lt;/strong&gt; информации и &lt;strong&gt;свежей&lt;/strong&gt;, актуальной лишь короткое время.
Для социальной сети это могут быть, например, профили пользователей
(стабильная) и сообщения (свежая).&lt;/p&gt;
&lt;p&gt;В соответствии с этим стоит выбирать хранилище данных и политику
кэширования:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Стабильная информация, которая редко обновляется и в тысячи раз чаще
    читается, прекрасно поддается кэшированию и возможно даже прекрасно
    будет себя чувствовать в реляционной СУБД.&lt;/li&gt;
&lt;li&gt;Свежую информацию вероятно вообще важнее доставить в кратчайшие
    сроки получателю, а сохранять в персистентном виде можно вообще
    постфактум для архива, на маловероятный случай когда она повторно
    понадобится. Про кэширование лучше вообще забыть. Для этого самого
    "архива" часто используют нереляционные распределенные базы данных
    вроде &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt;, &lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt; или
    &lt;a href="/tag/riak/"&gt;Riak&lt;/a&gt;. А про оперативную доставку получателю поговорим
    в следующем разделе.&lt;/li&gt;
&lt;li&gt;Хранилища данных в памяти вроде &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; или
    &lt;a href="/tag/redis/"&gt;Redis&lt;/a&gt; с отключенной персистентностью можно
    использовать независимо для временного хранения каких-то побочных
    данных (восстановимых производных данных или просто чего-то не особо
    важного, вроде счетчиков пользователей онлайн).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="potoki-soobshchenii"&gt;Потоки сообщений&lt;/h2&gt;
&lt;p&gt;Одной из ключевых задач интерактивного сайта является доставка сообщений
пользователем в реальном времени, причем их источник может быть как
внешний, так и внутренний, зачастую это просто другие пользователи.&lt;/p&gt;
&lt;p&gt;Часть системы, отвечающую за маршрутизацию таких сообщений, обычно
назвают &lt;strong&gt;брокером сообщений&lt;/strong&gt;&amp;ensp;&lt;em&gt;(message broker)&lt;/em&gt;. Для доставки
сообщений в браузер чаще всего используют &lt;strong&gt;интерфейс сериализованных
данных&lt;/strong&gt;, подробно обсуждавшийся в &lt;a href="https://www.insight-it.ru/interactive/2012/postoyannoe-soedinenie-mezhdu-brauzerom-i-serverom/"&gt;одной из предыдущих статей серии&lt;/a&gt;. Когда пользователь устанавливает соединение с этим интерфейсом, он, в
свою очередь, напрямую или через внутренний сервис регистрируется в
брокере сообщений для оперативного получения сообщений, предназначенных
соответствующему пользователю.&lt;/p&gt;
&lt;p&gt;Предлагаю рассмотреть типичные сценарии маршрутизации сообщений, они
довольно просты:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Конкретный получатель&lt;/strong&gt;, к сообщению (которое обычно никак не
    анализируется брокером) прикрепляется метка-идентификатор,
    обозначающий кому именно оно предназначено. Такое сообщение получит
    только процесс, зарегистрировавшийся с аналогичным идентификатором.
    Типичный пример использования - личные сообщения от пользователя к
    пользователю.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Группа получателей&lt;/strong&gt;, актуально для проектов, где пользователи
    взаимодействуют не на глобальном пространстве, а разбиты на части по
    какому-то признаку. Скажем это может быть какой-то B2B сервис и
    сообщения ходят только между сотрудниками одной компании-клиента.
    Обычно используется такие же метки, как и при конкретном получателе,
    только с одной из сторон (обычно принимающей) вместо конкретного
    идентификатора указывается какой-то паттерн, вроде &lt;code&gt;CompanyA.*&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Публичные сообщения&lt;/strong&gt; - получают все пользователи, метки не
    используются. Обычно это уведомления о глобальных для сайта событиях
    или публикации каких-то материалов.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Реализаций брокеров сообщений есть много разных, общий принцип работы у
всех примерно одинаковый и соответствует трем изложенным выше пунктам.
Для интернет-проектов очень рекомендую &lt;a href="/tag/rabbitmq/"&gt;RabbitMQ&lt;/a&gt;, в нем
эти стратегии маршрутизации называются &lt;em&gt;direct&lt;/em&gt;, &lt;em&gt;topic&lt;/em&gt; и &lt;em&gt;fanout&lt;/em&gt;
exchange, соответственно.&lt;/p&gt;
&lt;p&gt;Отправлять сообщения через брокер в большинстве случаев будут различные
внутренние сервисы в случае возникновения определенных событий &lt;em&gt;(читай:
получения ими определенных входящих сообщений и попадания в определенную
ветвь алгоритма их обработки)&lt;/em&gt;. Какую стратегию маршрутизации
использовать - тоже на их совести.&lt;/p&gt;
&lt;p&gt;К слову, внутренние сервисы также могут подписываться на получение части
сообщений из брокера, например для асинхронного создания "архива"
событий, отправки почтовых уведомлений или выполнения ресурсоемких задач
вроде конвертации медиа-файлов.&lt;/p&gt;
&lt;p&gt;При получении сообщения клиентская часть меняет соответствующим образом
текущую версию открытой страницы. От открытия дополнительного
всплывающего окна до просто смены цифры в количестве чего-нибудь.&lt;/p&gt;
&lt;p&gt;Будьте аккуратны с публичными сообщениями - их количество в единицу
времени может рости очень быстро с увеличением размеров аудитории.
Горизонтально масштабируемый брокер сообщений очень важен, если в Вашем
проекте в основном используются именно публичные сообщения.&lt;/p&gt;
&lt;h2 id="zakliuchenie"&gt;Заключение&lt;/h2&gt;
&lt;p&gt;Таким образом наша цепь замыкается - между браузерами любых
пользователей можно в "мягком" реальном времени пересылать любые
сообщения, пропуская их через бизнес-логику для регулирования данного
процесса, и, при необходимости, использовать постоянные и временные
хранилища данных.&lt;/p&gt;
&lt;p&gt;Как я уже упоминал&amp;nbsp;&lt;a href="https://www.insight-it.ru/interactive/2012/arkhitektura-interaktivnykh-sajjtov/"&gt;в первой статье серии&lt;/a&gt;, серверная часть у интерактивного сайта не так уж и кардинально отличается от любого другого - примерно те же компоненты, примерно так же работают и взаимодействуют. Разница в деталях.&lt;/p&gt;
&lt;p&gt;В следующей, заключительной, статье серии мы по второму кругу пройдемся
по ключевым моментам и попробуем рассмотреть наиболее перспективные
моменты для улучшений и оптимизации, хотя, как говорится, заранее
оптимизировать - плохая примета :)&lt;/p&gt;
&lt;div class="card green"&gt;
&lt;p&gt;&lt;div class="card-content white-text"&gt;
Эта статья - пятая в &lt;a class="green-text text-lighten-4" href="https://www.insight-it.ru/interactive/"&gt;серии про Интерактивные сайты&lt;/a&gt;, автор - &lt;a class="green-text text-lighten-4" href="https://www.insight-it.ru/goto/b03d9116/" rel="nofollow" target="_blank" title="http://blinkov.ru"&gt;Иван&amp;nbsp;Блинков&lt;/a&gt;, основано на личном опыте.
До встречи &lt;a class="green-text text-lighten-4" href="/feed/"&gt;на страницах Insight IT&lt;/a&gt;!
&lt;/div&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 04 Jun 2012 05:38:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-06-04:interactive/2012/servernaya-chast-interaktivnogo-sajjta-i-potoki-soobshhenijj/</guid><category>Akka</category><category>C++</category><category>Cassandra</category><category>Erlang</category><category>HBase</category><category>Memcached</category><category>OTP</category><category>RabbitMQ</category><category>Redis</category><category>Riak</category><category>Scala</category><category>Thrift</category></item><item><title>Twitter Storm</title><link>https://www.insight-it.ru//highload/2012/twitter-storm/</link><description>&lt;p&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/964382fd/" rel="nofollow" target="_blank" title="https://github.com/nathanmarz/storm/"&gt;Storm&lt;/a&gt; является распределенной
системой для выполнения вычислений в реальном времени.&lt;/em&gt; Она родилась в
рамках проекта Backtype, который специализировался на аналитике твитов и
который в июле 2011 был приобретен &lt;a href="/tag/twitter/"&gt;Twitter&lt;/a&gt;. Так же как
&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; предоставляет набор
базовых абстракций, инструментов и механизмов для пакетной обработки
данных, &lt;strong&gt;Twitter Storm&lt;/strong&gt; делает это для задачи обработки данных &lt;em&gt;в
режиме реального времени&lt;/em&gt;. Хотите узнать в чем их отличие?&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="otlichie"&gt;Отличие&lt;/h2&gt;
&lt;p&gt;Не смотря на то, что &lt;em&gt;Storm&lt;/em&gt; изначально появился на свет в процессе
неудачных попыток приспособить &lt;em&gt;Hadoop&lt;/em&gt; к задаче обработки данных в
реальном времени, сравнивать их некорректно. Никакой хак или патч не
сможет заставить Hadoop работать по-настоящему в режиме реального
времени, так как в его основе лежит фундаментально другая концепция и
набор принципов, которые актуальны лишь в контексте задачи пакетной
обработки данных. &lt;strong&gt;Storm&lt;/strong&gt; можно представить как &lt;strong&gt;"Hadoop для
вычислений в реальном времени"&lt;/strong&gt;, но по факту между ними нет практически
ничего общего, кроме изначально-распределенной природы, слегка похожей
архитектуры, работы внутри JVM и публичной доступности. Для понимания
задачи, которая стоит перед Storm, лучше взглянуть на то, как она обычно
решается.&lt;/p&gt;
&lt;p&gt;Традиционно, если перед проектом или бизнесом вставала задача обработки
какой-то информации в реальном времени, то она в итоге сводилась к
цепочке преобразований данных и распределялась по серверам, которые их
выполняют и передают результаты друг другу посредством сообщений и
очередей-посредников. При таком подходе &lt;em&gt;существенная&lt;/em&gt; часть времени
уходила на маршрутизацию сообщений, настройку и развертывание новых
промежуточных очередей и обработчиков, обеспечение отказоустойчивости и
надежности. По сути &lt;strong&gt;Storm&lt;/strong&gt; берет все вышеперечисленное на себя,
позволяя разработчикам сосредоточиться на реализации логики обработки
сообщений.&lt;/p&gt;
&lt;h2 id="osobennosti"&gt;Особенности&lt;/h2&gt;
&lt;p&gt;Итак, основные особенности Storm, вытекающие из требований к подобным
системам:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Три основных варианта использования, но ими он не ограничивается:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Обработка потоков сообщений&lt;/strong&gt; &lt;em&gt;(stream processing)&lt;/em&gt; в реальном
времени, с возможностью внесения изменений во внешние базы данных;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Постоянные вычисления&lt;/strong&gt;&amp;nbsp;&lt;em&gt;(continuous computation)&lt;/em&gt; на основе
источников данных с публикацией результатов произвольным клиентам в
реальном времени;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Распределенные удаленные вызовы&lt;/strong&gt; &lt;em&gt;(distributed RPC)&lt;/em&gt; с
выполнением комплексных вычислений параллельно во время запроса.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Масштабируемость:&lt;/strong&gt; Storm может обрабатывать огромное количество
сообщений в секунду. Для масштабирование необходимо лишь добавить
сервера в кластер и увеличить параллельность в настройках топологии. В
одном из первых приложений для Storm обрабатывался 1 миллион сообщений в
секунду на кластере из 10 серверов, при этом выполнялось несколько сотен
запросов в секунду к внешней базе данных.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Гарантия отсутствия потерь данных:&lt;/strong&gt;&amp;nbsp;в отличии от других систем
обработки сообщений в реальном времени (например
&lt;a href="https://www.insight-it.ru/goto/9cb3bfa0/" rel="nofollow" target="_blank" title="http://incubator.apache.org/s4"&gt;S4&lt;/a&gt; от Yahoo!) это свойство изначально
является частью архитектуры Storm.&amp;nbsp;Для этого используется механизм
подтверждения&amp;nbsp;&lt;em&gt;(acknowledgement)&lt;/em&gt;&amp;nbsp;успешной обработки каждого конкретного
сообщения.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Стабильность:&lt;/strong&gt;&amp;nbsp;в то время как &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; позволительны
простои по несколько часов, так как он априори не является системой
реального времени, одной из основных целей Storm является стабильная
бесперебойная работа кластера, с максимально безболезненным его
управлением.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Защита от сбоев:&lt;/strong&gt;&amp;nbsp;если что-то пошло не так во время выполнения
вычисления, Storm переназначит задачи и попробует снова. В его задачи
входит обеспечение бесконечной работы вычислений (или до момента
запланированной или ручной остановки).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Независимость от языка программирования:&lt;/strong&gt; в то время как большая
часть системы написана на &lt;a href="/tag/clojure/"&gt;Clojure&lt;/a&gt; и работает в
&lt;a href="/tag/jvm/"&gt;JVM&lt;/a&gt;, сами компоненты системы могут быть реализованы на
любом языке, что удобно для проектов, использующих в основном другие
технологии.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;У Вас уже могло сложиться общее представление, о том что собой
представляет &lt;strong&gt;Twitter Storm&lt;/strong&gt; и насколько он актуален лично для Вас или
Вашего проекта. Если интерес все еще не погас, предлагаю перейти к
концепции, предлагаемой Storm для разработки приложений под эту
платформу.&lt;/p&gt;
&lt;h2 id="kontseptsiia"&gt;Концепция&lt;/h2&gt;
&lt;p&gt;Для начала пройдемся по основным абстракциям, которые используются в
Storm:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Поток&lt;/strong&gt; &lt;em&gt;(Stream)&lt;/em&gt;: неограниченный поток сообщений, представленных в
виде кортежей (произвольных именованный список значений). При этом все
кортежи в одном потоке должны иметь одинаковую схему: элемент на каждой
позиции должен иметь один и тот же тип данных и значение.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Струя воды из крана&lt;/strong&gt;&amp;nbsp;&lt;em&gt;(Spout)&lt;/em&gt;: источник потоков, который берет их из какой-то внешней системы.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cтруя состояния&lt;/strong&gt;&amp;nbsp;&lt;em&gt;(state spout)&lt;/em&gt;: предоставляет распределенный доступ к некому общему состоянию, которое кэшируется в памяти на исполнителях и синхронно обновляется при внешних изменениях. Таким образом возможно избежать обращений к внешней базе данных при обработке каждого сообщения. В случае с Twitter этим общим состоянием является сам
социальный граф.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Молния&lt;/strong&gt; &lt;em&gt;(Bolt)&lt;/em&gt;: обрабатывает входящие потоки и создает исходящие
потоки, производя какую-либо обработку данных (по сути здесь реализуется
основная бизнес-логика). Помимо этого никто не запрещает использовать
при обработке какие угодно внешние сервисы вроде &lt;a href="/tag/subd/"&gt;СУБД&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Топология&lt;/strong&gt; &lt;em&gt;(Topology)&lt;/em&gt;: произвольная связанная сеть из "молний" и
"струй". При создании топологии можно указать:&lt;ul&gt;
&lt;li&gt;&lt;em&gt;уровень параллелизма&lt;/em&gt; для каждого компонента, что создаст
необходимое количество его потоков исполнения в кластере.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;группировку потоков&lt;/em&gt;, то есть как именно сообщения будут
распределяться между созданными потоками исполнения каждого
компонента, есть четыре основных варианта - случайно &lt;em&gt;(shuffle)&lt;/em&gt;,
каждый получит по копии &lt;em&gt;(all)&lt;/em&gt;, хэш по определенным полям сообщения
&lt;em&gt;(fields)&lt;/em&gt;, один поток получает все сообщения &amp;nbsp;&lt;em&gt;(global)&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Таким образом, для создания приложения для обработки данных в реальном
времени с использованием &lt;strong&gt;Storm&lt;/strong&gt;, необходимо:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Определить схему(ы) потока(ов) сообщений.&lt;/li&gt;
&lt;li&gt;Реализовать источник(и) сообщений, основанные на парсинге каких-то
    внешних данных (для Backtype это был Twitter firehose, поток всех
    твитов) или реакции на события (допустим действия пользователей в
    виде HTTP-запросов).&lt;/li&gt;
&lt;li&gt;Реализовать обработчик(и) сообщений, которые преобразуют входящие
    сообщения и либо создают новые потоки сообщений, либо как-то влияют
    на внешний мир, например изменяя что-то в базе данных (они
    используют &lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt; для этого).&lt;/li&gt;
&lt;li&gt;Объединить реализованные компоненты в топологию и запустить её на
    кластере.&lt;/li&gt;
&lt;li&gt;При необходимости оптимизировать систему, включив общее состояние в
    топологию.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;С точки зрения разработчика приложения большего знать и не нужно, но
самое интересное происходит как раз дальше. Что собой представляет
Storm-кластер и как с его помощью исполняется реализованное описанным
выше способом приложение?&lt;/p&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;Проект очень сильно завязан на &lt;a href="/tag/zookeeper/"&gt;Zookeeper&lt;/a&gt; для
координации работы кластера, с чем он очень неплохо справляется. Все
остальные компоненты системы системы не содержат в себе состояния, что
обеспечивает их быстрый запуск, даже после &lt;code&gt;kill -9&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Storm Cluster" class="responsive-img" src="https://www.insight-it.ru/images/storm-cluster.png" title="Storm Cluster"/&gt;&lt;/p&gt;
&lt;p&gt;В остальном все достаточно просто:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Мастер-сервер &lt;em&gt;(Nimbus)&lt;/em&gt; отвечает за распространение кода,
    распределение задач и мониторинг сбоев.&lt;/li&gt;
&lt;li&gt;На каждом сервере в кластере запускается процесс-надсмотрщик
    &lt;em&gt;(Supervisor)&lt;/em&gt;, который запускает локально потоки исполнения,
    отвечающие за выполнение назначенных ему компонентов топологий.&lt;/li&gt;
&lt;li&gt;Передача сообщений между компонентами топологий осуществляется
    напрямую, посредством &lt;a href="/tag/zeromq/"&gt;ZeroMQ&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Топологии являются &lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt;-структурами, а
    мастер-сервер - &lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt;-сервисом, что позволяет
    осуществлять регистрацию топологий и другие операции программно из
    любого языка программирования.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Присутствующий в единственном экземпляре мастер-сервер является
единственной точкой отказа лишь на первый взгляд. По факту он
используется лишь для внесение изменений в кластер и топологии, так что
его непродолжительное отсутствие не повлияет на функционирование
запущенных вычислений. А так как состояние кластера хранится в
&lt;em&gt;Zookeeper&lt;/em&gt;, то запуск мастера на другой машине в случае аппаратного
сбоя - вопрос лишь грамотно настроенного мониторинга и максимум одной
минуты.&lt;/p&gt;
&lt;p&gt;Используемый механизм подтверждений успешной обработки сообщения
&lt;em&gt;(acknowledgement)&lt;/em&gt; гарантирует, что все сообщения, попавшие в систему,
рано или поздно будут обработаны, даже при локальных сбоях оборудования.
Хотя более глобальные катаклизмы вроде "потери" стойки все же могут
нарушить функционирование системы, про работу в нескольких датацентрах
речь также не идет.&lt;/p&gt;
&lt;h2 id="plany-na-budushchee"&gt;Планы на будущее&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Использование &lt;a href="https://www.insight-it.ru/goto/6984b642/" rel="nofollow" target="_blank" title="http://incubator.apache.org/mesos"&gt;Mesos&lt;/a&gt; для
    распределения и изоляции вычислительных ресурсов.&lt;/li&gt;
&lt;li&gt;Изменение кода "на лету", сейчас для этого нужно остановить старую
    топологию и запустить новую, что может означать простой в пару
    минут.&lt;/li&gt;
&lt;li&gt;Автоматическое определения необходимого уровня параллельности и
    адаптация под изменения в интенсивности входящего потока сообщений.&lt;/li&gt;
&lt;li&gt;Еще более высокоуровневые абстракции.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;На самом деле подход, лежащий в основе Storm, не является чем-то
кардинально-новым. Помимо упоминавшегося выше S4 можно найти еще
несколько альтернатив, пускай и менее близких по идеологии. Подробнее
про эту тему можно узнать погуглив &lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/c81ea66c/" rel="nofollow" target="_blank" title="http://www.google.com/search?q=complex+event+processing"&gt;complex event processing&lt;/a&gt;&lt;/strong&gt;
или &lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/14ea9c79/" rel="nofollow" target="_blank" title="http://www.google.com/search?q=real-time+stream+processing"&gt;real-time stream processing&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storm&lt;/strong&gt; выделяет из их числа простота, гибкость, масштабируемость и
отказоустойчивость в одном флаконе. Обеспечивает это в первую очередь
простая и понятная архитектура, основанная на (уже) проверенном временем
и многими проектами распределенном координаторе в виде &lt;strong&gt;Zookeeper&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Хоть за проектом и стоит крупный интернет-проект в лице
&lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-twitter-dva-goda-spustya/"&gt;Twitter&lt;/a&gt;,
он достаточно молод и нужно быть морально готовым к возможным сбоям и
неудачным моментам. Плюс не забывайте, что существенная часть написана
на &lt;strong&gt;Clojure&lt;/strong&gt; - для, пожалуй, большинства разработчиков изучение
исходников проекта будет капитальным "выносом мозга". Мое первое
знакомство с &lt;strong&gt;Lisp&lt;/strong&gt; &lt;em&gt;(Clojure - его диалект, работающий в
JVM)&lt;/em&gt;&amp;nbsp;надолго засело в памяти из-за обилия скобочек за каждым углом :)&lt;/li&gt;
&lt;li&gt;В любом случае из доступных &lt;a href="/tag/opensource/"&gt;opensource&lt;/a&gt; реализаций
систем для распределенных вычислений в реальном времени &lt;strong&gt;Storm&lt;/strong&gt;&amp;nbsp;на мой
взгляд является наиболее перспективным для применения в
интернет-проектах.&lt;/li&gt;
&lt;li&gt;Если Вашему проекту нужна лишь одна-две топологии и особо большого
кластера не планируется, то подобную схему достаточно не сложно
реализовать и просто посредством &lt;strong&gt;Zookeeper&lt;/strong&gt; + &lt;strong&gt;ZeroMQ&lt;/strong&gt; или
альтернативных технологий. Это избавит проект от возможных заморочек с
Clojure и другими "особенностями" Storm, ценой вероятно существенно
большей собственной кодовой базы, которую придется самостоятельно
тестировать и поддерживать. Какой путь ближе - команда каждого проекта
решает для себя сама.&lt;/li&gt;
&lt;li&gt;Помимо различных вариаций&amp;nbsp;&lt;strong&gt;веб-аналитики&lt;/strong&gt; заманчивыми применениями
подобной системы в Интернете может стать:&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;построение индекса для поисковых систем&lt;/strong&gt;, на сколько я знаю от
&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt; здесь отказался только
&lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-google-2011/"&gt;Google&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;поведенческий таргетинг для рекламы&lt;/strong&gt; - собираем действия
пользователей и делаем на их основе выводы в реальном времени;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ведение рейтингов чего-либо в реальном времени&lt;/strong&gt; - в зависимости
от специфики проекта можно определять и показывать лучшие, самые
просматриваемые или самые комментируемые
статьи/фото/видео/музыку/товары/комментарии/что-нибудь-еще;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;предлагаем свои варианты в комментариях&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Удачи в построении приложений для вычислений в реальном времени и &lt;a href="/feed/"&gt;до встречи на страницах &lt;strong&gt;Insight IT&lt;/strong&gt;&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/6f56bb97/" rel="nofollow" target="_blank" title="http://www.infoq.com/presentations/Storm"&gt;Видео презентации проекта&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/fbfa7e01/" rel="nofollow" target="_blank" title="http://www.slideshare.net/nathanmarz/storm-distributed-and-faulttolerant-realtime-computation"&gt;Слайды с презентации проекта&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a027c9d7/" rel="nofollow" target="_blank" title="https://github.com/nathanmarz/storm"&gt;Репозиторий проекта&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 31 Mar 2012 00:08:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-03-31:highload/2012/twitter-storm/</guid><category>Clojure</category><category>JVM</category><category>opensource</category><category>real time</category><category>Storm</category><category>Thrift</category><category>Twitter</category><category>Twitter Storm</category><category>ZeroMQ</category><category>ZooKeeper</category></item><item><title>Архитектура Tumblr</title><link>https://www.insight-it.ru//highload/2012/arkhitektura-tumblr/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/e90ae6ea/" rel="nofollow" target="_blank" title="http://www.tumblr.com"&gt;&lt;strong&gt;Tumblr&lt;/strong&gt;&lt;/a&gt; - одна из самых популярных в мире
платформ для блоггинга, которая делает ставку на привлекательный внешний
вид, юзабилити и дружелюбное сообщество. Хоть проект и не особо на слуху
в России, цифры говорят сами за себя: 24й по посещаемости сайт в США
с&amp;nbsp;15 миллиардами просмотров страниц в месяц. Хотите познакомиться с
историей этого проекта, выросшего из простого стартапа?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="vvedenie"&gt;Введение&lt;/h2&gt;
&lt;p&gt;Как и всем успешным стартапам, Tumblr удалось преодолеть опасную пропать
между начинающим проектом и широко известной компанией. Поиск правильных
людей, эволюция инфраструктуры, поддержка старых решений, паника по
поводу значительного роста посещаемости от месяца к месяцу, при этом в
команде только 4 технических специалиста - все это заставляло
руководство Tumblr принимать тяжелые решения о том над чем стоит
работать, а над чем - нет. Сейчас же технический персонал расширился до
20 человек и у них достаточно энергии для преодоления всех текущих
проблем и разработки новых интересных технических решений.&lt;/p&gt;
&lt;p&gt;Поначалу Tumblr был вполне типичным большим &lt;a href="/tag/lamp/"&gt;LAMP&lt;/a&gt;
приложением. Сейчас же они двигаются в направлении модели распределенных
сервисов, построенных вокруг существенно менее распространенных
технологий. Основные усилия сейчас вкладываются в постепенный уход от
&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; в пользу более "правильных" и "современных" решений,
оформленных в виде сервисов. Параллельно с переходом к новым технологиям
идут изменения и в команде проекта: от небольшой группы энтузиастов к
полноценной команде разработчиков, имеющей четкую структуру и сферы
ответственности, но тем не менее жаждущей реализовывать новый функционал
и обустраивать совершенно новую инфраструктуру проекта.&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/centos/"&gt;CentOS&lt;/a&gt; на серверах, &lt;a href="/tag/mac-os-x/"&gt;Mac OS X&lt;/a&gt; для
    разработки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; - основной веб-сервер&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;, &lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt;, &lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt; - языки
    программирования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/finagle/"&gt;Finagle&lt;/a&gt;&amp;nbsp;- асинхронный RPC сервер и клиент&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;, &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt;&amp;nbsp;- СУБД&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;,&amp;nbsp;&lt;a href="/tag/redis/"&gt;Redis&lt;/a&gt;&amp;nbsp;- кэширование&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/varnish/"&gt;Varnish&lt;/a&gt;, &lt;a href="/tag/nginx/"&gt;nginx&lt;/a&gt; - отдача статики&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/haproxy/"&gt;HAProxy&lt;/a&gt; - балансировка нагрузки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/kestrel/"&gt;kestrel&lt;/a&gt;, &lt;a href="/tag/gearman/"&gt;gearman&lt;/a&gt; - очередь задач&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt; - сериализация&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/kafka/"&gt;Kafka&lt;/a&gt; - распределенная шина сообщений&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; - обработка статистики&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/zookeeper/"&gt;ZooKeeper&lt;/a&gt; - хранение конфигурации и состояний
    системы&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/git/"&gt;git&lt;/a&gt; - система контроля версий&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/jenkins/"&gt;Jenkins&lt;/a&gt; - непрерывное тестирование&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Около 500 миллионов просмотров страниц в день&lt;/li&gt;
&lt;li&gt;Более 15 миллиардов просмотров страниц в месяц&lt;/li&gt;
&lt;li&gt;Посещаемость растет примерно на 30% в месяц&lt;/li&gt;
&lt;li&gt;Пиковые нагрузки порядка 40 тысяч запросов в секунду&lt;/li&gt;
&lt;li&gt;Около 20 технических специалистов в команде&lt;/li&gt;
&lt;li&gt;Каждый день создается около 50Гб новых постов и 2.7Тб обновлений списков
последователей&lt;/li&gt;
&lt;li&gt;Более 1Тб статистики обрабатывается в &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; ежедневно&lt;/li&gt;
&lt;li&gt;Используется порядка 1000 серверов:&lt;ul&gt;
&lt;li&gt;500 веб-серверов c Apache и PHP-приложением&lt;/li&gt;
&lt;li&gt;200 серверов баз данных (существенная их часть - резервные)&lt;ul&gt;
&lt;li&gt;47 пулов&lt;/li&gt;
&lt;li&gt;30 партиций (шардов)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;30 серверов &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;25 серверов Redis&lt;/li&gt;
&lt;li&gt;15 серверов Varnish&lt;/li&gt;
&lt;li&gt;25 серверов HAProxy&lt;/li&gt;
&lt;li&gt;8 серверов nginx&lt;/li&gt;
&lt;li&gt;14 серверов для очередей задач&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="tipichnoe-ispolzovanie"&gt;Типичное использование&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tumblr&lt;/strong&gt; используется несколько по-другому, чем другие социальные
сети:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;При более чем 50 миллионах постов в день, каждый из них попадает в
среднем к нескольким сотням читателей. Это и не несколько
пользователей с миллионами читателей (например, популярные личности
в Twitter) и не миллиарды личных сообщений.&lt;/li&gt;
&lt;li&gt;Ориентированность на длинные публичные сообщения, полные интересной
информацией и картинками/видео, заставляет пользователей проводить
долгие часы каждый день за чтением Tumblr.&lt;/li&gt;
&lt;li&gt;Большинство активных пользователей подписывается на сотни других
блоггеров, что практически гарантирует много страниц нового контента
при каждом заходе на сайт. В других социальных сетях поток новых
сообщений переполнен ненужным контентом и толком не читается.&lt;/li&gt;
&lt;li&gt;Как следствие, при сложившемся количестве пользователей, средней
аудиторией каждого и высокой активностью написания постов, системе
приходится обрабатывать и доставлять огромное количество информации.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Публичные блоги называют Tumblelog'ами, они не так динамичны и легко
кэшируются.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Сложнее всего масштабировать Dashboard, страницу, где пользователи в
реальном времени читают что нового у блоггеров, на которых они
подписаны:&lt;ul&gt;
&lt;li&gt;Кэширование практически бесполезно, так как для активных
пользователей запросы редко повторяются.&lt;/li&gt;
&lt;li&gt;Информация должна отображаться в реальном времени, быть целостной и
не "задерживаться".&lt;/li&gt;
&lt;li&gt;Около 70% просмотров страниц приходится именно на Dashboard, почти
все пользователи им пользуются.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="staraia-arkhitektura"&gt;Старая архитектура&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Когда проект только начинался, Tumblr размещался в Rackspace и последние выдавали каждому блогу с собственным доменом A-запись. Когда они переросли Rackspace, они не смогли полноценно мигрировать в новый
датацентр, в том числе из-за количества пользователей. Это было в 2007
году, но у них по-прежнему часть доменов ведут на Rackspace и
перенаправляются в новый датацентр с помощью HAProxy и Varnish. Подобных
"унаследованных" проблем у проекта очень много.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;С технической точки зрения проект прошел по пути типичной эволюции
&lt;strong&gt;LAMP&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Исторически разработан на &lt;strong&gt;PHP&lt;/strong&gt;, все началось с веб-сервера,
сервера баз данных и начало потихоньку развиваться.&lt;/li&gt;
&lt;li&gt;Чтобы справляться с нагрузкой они начали использовать memcache,
затем добавили кэширование целых страниц и статических файлов, потом
поставили HAProxy перед кэшами, после чего сделали партиционирование
на уровне &lt;strong&gt;MySQL&lt;/strong&gt;, что сильно облегчило им жизнь.&lt;/li&gt;
&lt;li&gt;Они делали все, чтобы выжать максимум из каждого сервера.&lt;/li&gt;
&lt;li&gt;Было разработано два сервиса на C: генератор уникальных
идентификаторов на основе HTTP и libevent, а также
&lt;a href="https://www.insight-it.ru/goto/533d5aae/" rel="nofollow" target="_blank" title="http://engineering.tumblr.com/post/7819252942/staircar-redis-powered-notifications"&gt;Staircar&lt;/a&gt;,
использующий Redis для обеспечения уведомлений в реальном времени на
Dashboard.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dashboard использует подход
&lt;a href="https://www.insight-it.ru/goto/f4d9020c/" rel="nofollow" target="_blank" title="http://www2.parc.com/istl/projects/ia/papers/sg-sigir92/sigir92.html"&gt;"разбрасывать-собирать"&lt;/a&gt;,
так как из-за отсортировонности данных по времени традиционные схемы
партиционирования работали не очень хорошо. По их прогнозам текущая
реализация позволит им рости еще в течении полугода.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="novaia-arkhitektura"&gt;Новая архитектура&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Приоритетным направлением стали технологии, основанные на JVM, по
причине более быстрой разработки и доступности квалифицированных
кадров. Мотивация несколько спорная, особенно если учесть, что речь
идет в первую очередь о &lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt;, а не
о&amp;nbsp;&lt;a href="/tag/scala/"&gt;Java&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Основная цель - вынести все из PHP приложения в отдельные сервисы, что
сделает его лишь тонким клиентом к внутреннему API.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Почему выбор пал именно на &lt;strong&gt;Scala&lt;/strong&gt; и &lt;strong&gt;Finagle&lt;/strong&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Многие разработчики имели опыт с Ruby и PHP, так что Scala был
привлекательным (цитата, логики мало)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/d7e3c54e/" rel="nofollow" target="_blank" title="https://github.com/twitter/finagle"&gt;Finagle&lt;/a&gt; был одним из основных
факторов в пользу JVM: это библиотека, разработанная в Twitter,
которая решает большинство распределенных задач вроде маршрутизации
запросов и обнаружение/регистрацию сервисов - не пришлось
реализовывать это все с нуля.&lt;/li&gt;
&lt;li&gt;В Scala не принято использовать общие состояния, что избавляет
разработчиков от забот с потоками выполнения и блокировками.&lt;/li&gt;
&lt;li&gt;Им очень нравится Thrift в роли программного интерфейса из-за его
высокой производительности (он кроссплатформенный и к JVM никак не
относится)&lt;/li&gt;
&lt;li&gt;Нравится &lt;a href="/tag/netty/"&gt;Netty&lt;/a&gt;, но не хочется связываться с Java, еще
один аргумент в пользу Scala.&lt;/li&gt;
&lt;li&gt;Рассматривали &lt;a href="/tag/node-js/"&gt;Node.js&lt;/a&gt;, но отказались так как под
JVM проще найти разработчиков, а также из-за отсутствия стандартов,
"лучших практик" и большого количества качественно протестированного
кода.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Старые внутренние сервисы также переписываются с C + libevent на Scala + Fingle.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Был создан общий каркас для построения внутренних сервисов:&lt;ul&gt;
&lt;li&gt;Много усилий было приложено для автоматизации управления
распределенной системой.&lt;/li&gt;
&lt;li&gt;Создан аналог скаффолдинга - используется некий шаблон для создания
каждого нового сервиса.&lt;/li&gt;
&lt;li&gt;Все сервисы выглядят одинаково с точки зрения системного
администратора: получение статистики, мониторинг, запуск и остановка
реализованы одинаково для всех сервисов.&lt;/li&gt;
&lt;li&gt;Созданы простые инструменты для сборки сервисов без вникания в
детали используемых стандартных решений.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Используется 6 внутренних сервисов, над которыми работает отдельная
команд. На запуск сервиса с нуля уходит около 2-3 недель.&lt;/li&gt;
&lt;li&gt;Новые, нереляционные СУБД, такие как HBase и Redis, вводятся в
эксплуатацию, но основным хранилищем по-прежнему остается сильно
партиционированный MySQL.&lt;/li&gt;
&lt;li&gt;HBase используется для сервиса сокращенных ссылок для постов, а также
всех исторических данных и аналитики. HBase хорошо справляется с
ситуациями, где необходимы миллионы операций записи в секунду, но он не
достаточно стабилен, чтобы полностью заменить проверенное временем
решение на MySQL в критичных для бизнеса задачах.&lt;/li&gt;
&lt;li&gt;Партиционированный MySQL плохо справляется с отсортированными по времени данными, так как один из серверов всегда оказывается существенно более "горячим", чем остальными. Также сталкивались с значительными задержками в репликации из-за большого количества параллельных операций добавления данных.&lt;/li&gt;
&lt;li&gt;Используется 25 серверов Redis с 8-32 процессами на каждом, что означает
порядка 300-400 экземпляров Redis в сумме.&lt;ul&gt;
&lt;li&gt;Используется для уведомлений в реальном времени на Dashboard (о
событиях вроде "кому-то понравился Ваш пост").&lt;/li&gt;
&lt;li&gt;Высокое соотношений операций записи к операциям чтения сделало MySQL
не очень подходящим кандидатом.&lt;/li&gt;
&lt;li&gt;Уведомления не так критичны, их потеря допустима, что позволило
отключить персистентность Redis.&lt;/li&gt;
&lt;li&gt;Был создан интерфейс между Redis и отложенными задачами в Finagle.&lt;/li&gt;
&lt;li&gt;Сервис коротких ссылок также использует Redis как кэш, а HBase для
постоянного хранения.&lt;/li&gt;
&lt;li&gt;Вторичный индекс Dashboard также построен вокруг Redis.&lt;/li&gt;
&lt;li&gt;Redis также используется для хранения задач Gearman, для чего был
написан memcache proxy на основе Finale.&lt;/li&gt;
&lt;li&gt;Постепенно отказываются от memcached в пользу Redis в роли основного
кэша. Производительность у них сопоставима.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Внутренним сервисам необходим доступ к потоку всех событий в системе
(создание, редактирование и удаление постов, нравится или не нравится и
т.п.), для чего была созданна внутренняя шина сообщений &lt;em&gt;(англ.
firehose, пожарный шланг)&lt;/em&gt;:&lt;ul&gt;
&lt;li&gt;Пробовали использовать в этой роли Scribe, но так как оно по сути
свелось к пропусканию логов через grep в реальном времени - нагрузки
оно не выдержало.&lt;/li&gt;
&lt;li&gt;Текущая реализация основана на Kafka, решению аналогичной задачи от
LinkedIn на Scala.&lt;/li&gt;
&lt;li&gt;MySQL также не рассматривался из-за большой доли операций записи.&lt;/li&gt;
&lt;li&gt;Внутри сервисы используют HTTP потоки для чтения данных, хотя Thrift
интерфейс также используется.&lt;/li&gt;
&lt;li&gt;Поток сообщений хранит события за последнюю неделю с возможностью
указать момент времени с которого считывать данные при открытии
соединения.&lt;/li&gt;
&lt;li&gt;Поддерживается абстракция "группы потребителей", которая позволяет
группе клиентов вместе обрабатывать один поток данных вместе и
независимо, то есть одно и то же сообщение не попадет дважды к
клиентам из одной группы.&lt;/li&gt;
&lt;li&gt;ZooKeeper используется для периодического сохранения текущей позиции
каждого клиента в потоке.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Новая архитектура Dashboard основана на принципе ячеек или ящиков
входящих сообщений:&lt;ul&gt;
&lt;li&gt;Каждая "ячейка" отвечает за группу пользователей и читает новые события
с шины сообщений, если один из её пользователей-подопечных подписан на
автора только что опубликованного поста, то пост добавляется в "почтовый
ящик" подписанного пользователя.&lt;/li&gt;
&lt;li&gt;Когда пользователь заходит в Dashboard его запрос попадает в его ячейку,
которая возвращает ему нужную часть непрочитанных постов.&lt;/li&gt;
&lt;li&gt;Каждая ячейка состоит из трех групп серверов:&lt;ul&gt;
&lt;li&gt;HBase для постоянного хранения копий постов и почтовых ящиков;&lt;/li&gt;
&lt;li&gt;Redis для кэширование свежих данных;&lt;/li&gt;
&lt;li&gt;Сервис, читающий данные из шины и предоставляющий доступ к ящикам
посредством Thrift.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;В HBase используется две таблицы:&lt;ul&gt;
&lt;li&gt;Отсортированный &lt;strong&gt;список идентификаторов постов&lt;/strong&gt; для каждого
пользователя в ячейке, именно в том виде, как они будут отображены в
итоге.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Копии всех постов&lt;/strong&gt; по идентификаторам, что позволяет выдать все
данные для отрисовки Dashboard без обращений к серверам вне одной
ячейки.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ячейки представляют собой независимые единицы, что позволяет легко
масштабировать систему при росте числа пользователей.&lt;/li&gt;
&lt;li&gt;Платой за относительно безболезненность масштабирования является
чрезвычайная избыточность данных: при том что ежедневно создается лишь
50Гб постов, суммарный объем данных в ячейках растет на 2.7Тб в день.&lt;/li&gt;
&lt;li&gt;Альтернативой было бы использование общего кластера со всеми постами, но
тогда он бы стал единственной точкой отказа и потребовалось бы делать
дополнительные удаленные запросы. Помимо этого выигрыш по объему был бы
не велик - списки идентификаторов занимают значительно больше места, чем
сами посты.&lt;/li&gt;
&lt;li&gt;Пользователи, которые подписаны или на которых подписаны миллионы других
пользователей, обрабатываются отдельно - страницы с их постами
генерируются не заранее (как описывалось выше), а при поступлении
запроса - это позволяет не тратить впустую много ресурсов (этот подход
называется выборочная материализация,
&lt;a href="https://www.insight-it.ru/goto/ab0d48a1/" rel="nofollow" target="_blank" title="http://research.yahoo.com/pub/3203"&gt;подробнее&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Количество пользователей в одной ячейке позволяет управлять балансом
между уровнем надежности и стоимостью содержания этой подсистемы.&lt;/li&gt;
&lt;li&gt;Параллельное чтение их шины сообщений оказывает серьезную нагрузку на
сеть, в дальнейшем из ячеек можно будет составить иерархию: только часть
будет читать напрямую из шины сообщений, а остальным сообщения будут
ретранслироваться.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tumblr географически по-прежнему находится в одном датацентре (если не
считать незначительное присутствие в Rackspace), распределение по
нескольким лишь в планах.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="razvertyvanie"&gt;Развертывание&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Начиналось как несколько rsync-скриптов для распространения
    PHP-приложения. Как только машин стало больше 200 такой подход стал
    занимать слишком много времени.&lt;/li&gt;
&lt;li&gt;Следующий вариант был основан на &lt;a href="/tag/capistrano/"&gt;Capistrano&lt;/a&gt;:
    были созданы три стадии процесса развертывания (разработка,
    тестирование, боевой). Неплохо справлялся с десятками серверов, но
    на сотнях также был слишком медленным, так как основывался на SSH.&lt;/li&gt;
&lt;li&gt;Итоговый вариант основан на &lt;strong&gt;Func&lt;/strong&gt;, решении от
    &lt;a href="/tag/redhat/"&gt;RedHat&lt;/a&gt;, позволившим заменить &lt;a href="/tag/ssh/"&gt;SSH&lt;/a&gt; на
    более легковесный протокол.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="razrabotka"&gt;Разработка&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Поначалу философия была такова, что каждый мог использовать любые
технологии, которые считал уместным. Но довольно скоро пришлось
стандартизировать стек технологий, чтобы было легче нанимать и вводить в
работу новых сотрудников, а также для более оперативного решения
технических проблем.&lt;/li&gt;
&lt;li&gt;Каждый разработчик имеет одинаковую заранее настроенную рабочую станцию,
которая обновляется посредством &lt;a href="/tag/puppet/"&gt;Puppet&lt;/a&gt;:&lt;ul&gt;
&lt;li&gt;Настроена публикация изменений, тестирование и развертывание новых
версий.&lt;/li&gt;
&lt;li&gt;Разработчики используют vim и Textmate.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Новый PHP код систематически инспектируется другими разработчиками.&lt;/li&gt;
&lt;li&gt;Внутренние сервисы подвергаются непрерывному тестированию посредством
Jenkins.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="struktura-komand"&gt;Структура команд&lt;/h2&gt;
&lt;p&gt;Проект разбит на 6 команд:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Инфраструктура:&lt;/strong&gt;&amp;nbsp;все, что ниже 5 уровня по модели OSI -
    маршрутизация, TCP/IP, DNS, оборудование и.т.п.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Платформа:&lt;/strong&gt;&amp;nbsp;разработка основного приложения, партиционирование
    SQL, взаимодействие сервисов.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Надежность (SRE):&lt;/strong&gt;&amp;nbsp;сфокусирована на текущие потребности с точки
    зрения надежности и масштабируемости.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Сервисы:&lt;/strong&gt;&amp;nbsp;занимается более стратегической разработкой того, что
    понадобится через один-два месяца.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Эксплуатация:&lt;/strong&gt;&amp;nbsp;отвечает за обнаружение и реагирование на
    проблемы, плюс тонкая настройка.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="naim"&gt;Найм&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;На интервью они обычно избегают математики и головоломок, основной
    упор идет в основном именно на те вещи, которым придется заниматься
    кандидату.&lt;/li&gt;
&lt;li&gt;Основной вопрос: будет ли он успешно решать поставленные задачи?
    Цель в том, чтобы найти отличных людей, а не в том, чтобы никого не
    брать.&lt;/li&gt;
&lt;li&gt;Разработчиков обязательно просят привести пример своего кода, даже
    во время телефонных интервью.&lt;/li&gt;
&lt;li&gt;Во время интервью кандидатов не ограничивают в наборе инструментов,
    можно даже гуглить.&lt;/li&gt;
&lt;li&gt;Поиск людей с опытом в крупных проектах достаточно сложен, так как
    всего нескольких компаниях по всему миру решают подобные проблемы.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Автоматизация - ключ к успеху крупного проекта.&lt;/li&gt;
&lt;li&gt;При партиционировании MySQL может масштабироваться, но лишь при
    преобладании операций чтения.&lt;/li&gt;
&lt;li&gt;Redis с отключенной персистентностью легко может заменить memcached.&lt;/li&gt;
&lt;li&gt;Scala достойно себя проявляет в роли языка программирования для
    внутренних сервисов, во многом благодаря обширной Java-экосистеме.&lt;/li&gt;
&lt;li&gt;Внедряйте новые технологии постепенно, поначалу работать с HBase и
    Redis было очень болезненно, они были включены в основной стек
    технологий только после испытаний в некритичных сервисах и
    подпроектах, где цена ошибки не так велика.&lt;/li&gt;
&lt;li&gt;Проект должен строиться вокруг навыков его команды, а не наоборот.&lt;/li&gt;
&lt;li&gt;Нужно нанимать людей только если они вписываются в команду и в
    состоянии довести работу до результата.&lt;/li&gt;
&lt;li&gt;При выборе технологического стека одну из ключевых ролей играет
    доступность соответствующих специалистов на кадровом рынке.&lt;/li&gt;
&lt;li&gt;Читайте публикации и статьи в блогах. Ключевые аспекты архитектуры,
    включая "ячейки" и частичную материализацию были позаимствованы из
    внешних источников.&lt;/li&gt;
&lt;li&gt;Поспрашивайте своих коллег, кто-то из них мог общаться с
    специалистами из
    &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-facebook/"&gt;Facebook&lt;/a&gt;,
    &lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-twitter-dva-goda-spustya/"&gt;Twitter&lt;/a&gt;,
    &lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-google-2011/"&gt;Google&lt;/a&gt;
    или
    &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-linkedin/"&gt;LinkedIn&lt;/a&gt; -
    если нет прямого доступа, всегда можно получить нужную информацию
    через одно-два "рукопожатия".&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Статья написана на основе &lt;a href="https://www.insight-it.ru/goto/1444dc9b/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2012/2/13/tumblr-architecture-15-billion-page-views-a-month-and-harder.html"&gt;интервью&lt;/a&gt;&amp;ensp;&lt;a href="https://www.insight-it.ru/goto/d7daa0cd/" rel="nofollow" target="_blank" title="http://www.linkedin.com/in/bmatheny"&gt;Blake Matheny&lt;/a&gt;, директора по разработке платформы Tumblr.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 21 Feb 2012 16:29:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-02-21:highload/2012/arkhitektura-tumblr/</guid><category>Apache</category><category>Capistrano</category><category>CentOS</category><category>Finagle</category><category>Func</category><category>gearman</category><category>Git</category><category>Hadoop</category><category>HAProxy</category><category>HBase</category><category>jenkins</category><category>kafka</category><category>Kestrel</category><category>LAMP</category><category>Mac OS X</category><category>Memcached</category><category>MySQL</category><category>nginx</category><category>PHP</category><category>puppet</category><category>Redis</category><category>Ruby</category><category>Scala</category><category>Thrift</category><category>Tumblr</category><category>Varnish</category><category>ZooKeeper</category></item><item><title>Архитектура Facebook</title><link>https://www.insight-it.ru//highload/2010/arkhitektura-facebook/</link><description>&lt;p&gt;&lt;img alt="Facebook Logo" class="left" src="https://www.insight-it.ru/images/facebook_logo.jpg" title="Facebook Logo"/&gt;
На сегодняшний день &lt;a href="https://www.insight-it.ru/goto/fbae133c/" rel="nofollow" target="_blank" title="https://www.facebook.com"&gt;Facebook&lt;/a&gt; является пожалуй
самым обсуждаемым интернет-проектом во всем мире. Не смотря на довольно
низкий уровень проникновения Facebook в России, темпы захвата аудитории
этим проектом мягко говоря поражают. Как же им удается управляться с
таким огромным социальным графом и удовлетворять потребности в общении
невероятно большого количества людей по всему миру?&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; - операционная система&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; с &lt;a href="/tag/hiphop/"&gt;HipHop&lt;/a&gt; - код на PHP компилируется в
    C++&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; - агрессивное кэширование объектов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt; - используется как хранилище пар ключ-значение,
    никаких join'ов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt; - интерфейс взаимодействия между сервисами,
    написанными на разных языках программирования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/scribe/"&gt;Scribe&lt;/a&gt; - универсальная система сбора и агрегации
    данных с рабочих серверов&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Более 500 миллионов активных пользователей (месячная аудитория)&lt;/li&gt;
&lt;li&gt;Более миллиарда социальных связей&lt;/li&gt;
&lt;li&gt;Более 200 миллиардов просмотров страниц в месяц&lt;/li&gt;
&lt;li&gt;Более 4 триллионов действий попадает в новостные ленты каждый день&lt;/li&gt;
&lt;li&gt;Более 150 миллионов обращений к кэшу в секунду; 2 триллиона объектов
    в кэше&lt;/li&gt;
&lt;li&gt;Более 8 миллиардов минут провели пользователи на Facebook'е
    ежедневно&lt;/li&gt;
&lt;li&gt;Более 3 миллиардов фотографий загружается каждый месяц, до 1.2
    миллиона фотографий в секунду&lt;/li&gt;
&lt;li&gt;20 миллиардов фотографий в 4 разрешениях = 80 миллиардов фотографий,
    их бы хватило чтобы покрыть поверхность земли в 10 слоев; это
    больше, чем на всех других фото-ресурсах в месте взятых&lt;/li&gt;
&lt;li&gt;О более чем 5 миллиардах единиц контента рассказывается друзьям
    еженедельно&lt;/li&gt;
&lt;li&gt;Более миллиарда сообщений в чате каждый день&lt;/li&gt;
&lt;li&gt;Более ста миллионов поисковых запросов в день&lt;/li&gt;
&lt;li&gt;Более 250 приложений и 80 тысяч сторонних ресурсов на платформе
    Facebook Connect&lt;/li&gt;
&lt;li&gt;Более 400 тысяч разработчиков сторонних приложений&lt;/li&gt;
&lt;li&gt;Менее 500 разработчиков и системных администраторов в штате&lt;/li&gt;
&lt;li&gt;Более миллиона активных пользователей на одного инженера&lt;/li&gt;
&lt;li&gt;Десятки тысяч серверов, десятки гигабит трафика&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;h3 id="obshchie-printsipy"&gt;Общие принципы&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Балансировщик нагрузки выбирает веб-сервер для обработки запроса&lt;/li&gt;
&lt;li&gt;PHP-код в веб-сервере подготавливает HTML, пользуясь данными из
    различных источников:&lt;ul&gt;
&lt;li&gt;MySQL&lt;/li&gt;
&lt;li&gt;memcached&lt;/li&gt;
&lt;li&gt;Специализированные сервисы&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Если взглянуть с другой стороны, то получим трехуровневую
    архитектуру:&lt;ul&gt;
&lt;li&gt;Вер-приложение&lt;/li&gt;
&lt;li&gt;Распределенный индекс&lt;/li&gt;
&lt;li&gt;Постоянное хранилище&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Использование открытых технологий там, где это возможно&lt;/li&gt;
&lt;li&gt;Поиск возможностей оптимизации используемых продуктов&lt;/li&gt;
&lt;li&gt;Философия Unix:&lt;ul&gt;
&lt;li&gt;Старайтесь делать каждый компонент системы простым и
    производительным&lt;/li&gt;
&lt;li&gt;Комбинируйте компоненты для решения задач&lt;/li&gt;
&lt;li&gt;Концентрируйте внимание на хорошо обозначенных точках
    взаимодействия&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Все усилия направлены на масштабируемость&lt;/li&gt;
&lt;li&gt;Попытки минимизации количества точек отказа&lt;/li&gt;
&lt;li&gt;Простота, простота, простота!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="php"&gt;PHP&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Почему PHP?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Во многом "так исторически сложилось"&lt;/li&gt;
&lt;li&gt;Хорошо подходит для веб-разработки&lt;/li&gt;
&lt;li&gt;Легок в изучении: небольшой набор выражений и языковых конструкций&lt;/li&gt;
&lt;li&gt;Легок в написании: нестрогая типизация и универсальный "массив"&lt;/li&gt;
&lt;li&gt;Легок в чтении: синтаксис похож на C++ и Java&lt;/li&gt;
&lt;li&gt;Прост в дебаггинге: нет необходимости в перекомпиляции&lt;/li&gt;
&lt;li&gt;Большой ассортимент библиотек, актуальных для веб-проектов&lt;/li&gt;
&lt;li&gt;Подходит для процесса разработки с короткими итерациями&lt;/li&gt;
&lt;li&gt;Активное сообщество разработчиков по всему миру&lt;/li&gt;
&lt;li&gt;Динамическая типизация, интерпретируемый язык для скриптов&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Как оказалось на самом деле?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Высокий расход оперативной памяти и вычислительных ресурсов&lt;/li&gt;
&lt;li&gt;Сложно работать, когда объем исходного кода очень велик: слабая
    типизация и ограниченные возможности для статичного анализа и
    оптимизации кода&lt;/li&gt;
&lt;li&gt;Не особо оптимизирован для использования в крупных проектах&lt;/li&gt;
&lt;li&gt;Линейный рост издержек при подключении файлов с исходным кодом&lt;/li&gt;
&lt;li&gt;Механизм разработки расширений не очень удобен&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Доработки:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Оптимизация байт-кода&lt;/li&gt;
&lt;li&gt;Улучшения в APC (ленивая загрузка, оптимизация блокировок,
    "подогрев" кэша)&lt;/li&gt;
&lt;li&gt;Свои расширения (клиент memcache, формат сериализации, логи,
    статистика, мониторинг, механизм асинхронной обработки событий)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/169aa287/" rel="nofollow" target="_blank" title="http://github.com/facebook/hiphop-php"&gt;HipHop&lt;/a&gt;&lt;/strong&gt; - трансформатор
    исходных кодов:&lt;ul&gt;
&lt;li&gt;Разработчики пишут на PHP, который конвертируется в
    оптимизированный C++&lt;/li&gt;
&lt;li&gt;Статический анализ, определение типов данных, генерация кода,
    и.т.д.&lt;/li&gt;
&lt;li&gt;Облегчает разработку расширений&lt;/li&gt;
&lt;li&gt;Существенно сокращает расходы оперативной памяти и
    вычислительных ресурсов&lt;/li&gt;
&lt;li&gt;У команды из трех программистов ушло полтора года на разработку,
    переписаны большая часть интерпретатора и многие расширения
    языка&lt;/li&gt;
&lt;li&gt;Опубликован под opensource лицензией в начале года, нет
    необходимости проходить этот же путь с нуля&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="mysql"&gt;MySQL&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Как используется MySQL?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Используется как хранилище пар ключ-значение&lt;/li&gt;
&lt;li&gt;Большое количество логических узлов распределено между физическими
    машинами&lt;/li&gt;
&lt;li&gt;Балансировка нагрузке на уровне физических серверов&lt;/li&gt;
&lt;li&gt;Репликация для распределения операций чтения &lt;strong&gt;не&lt;/strong&gt; используется&lt;/li&gt;
&lt;li&gt;Большинство запросов касаются самой свежей информации: оптимизация
    таблиц для доступа к новым данным, архивация старых записей&lt;/li&gt;
&lt;li&gt;В целом быстро и надежно&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Как оказалось на самом деле?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Логическая миграция данных &lt;em&gt;очень&lt;/em&gt; сложна&lt;/li&gt;
&lt;li&gt;Создавать большое количество логических баз данных и
    перераспределять их между физическими узлами, балансируя таким
    образом нагрузку, намного удобнее&lt;/li&gt;
&lt;li&gt;Никаких join'ов на рабочих серверах баз данных&lt;/li&gt;
&lt;li&gt;Намного проще наращивать вычислительные мощности на веб-серверах,
    чем на серверах баз данных&lt;/li&gt;
&lt;li&gt;Схемы, основанные на структуре данных, делают программистов
    счастливыми и создают большую головную боль администраторам&lt;/li&gt;
&lt;li&gt;Никогда не храните не-статичные данные в централизованное базе
    данных&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Доработки:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Практически никаких модификаций исходного кода MySQL&lt;/li&gt;
&lt;li&gt;Своя схема партиционирования с глобально-уникальными
    идентификаторами&lt;/li&gt;
&lt;li&gt;Своя схема архивирования, основанная на частоте доступа к данным
    относительно каждого пользователя&lt;/li&gt;
&lt;li&gt;Расширенный движок запросов для репликации между датацентрами и
    поддержания консистенции кеша&lt;/li&gt;
&lt;li&gt;Библиотеки для доступа к данным на основе графа:&lt;ul&gt;
&lt;li&gt;Объекты (вершины графа) с ограниченными типами данных (целое
    число, строка ограниченно длины, текст)&lt;/li&gt;
&lt;li&gt;Реплицированные связи (ребра графа)&lt;/li&gt;
&lt;li&gt;Аналоги распределенных внешних ключей (foreign keys)&lt;/li&gt;
&lt;li&gt;Большинство данных распределено случайно&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="memcache"&gt;Memcache&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Как используется memcached?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Высокопроизводительная распределенная хэш-таблица&lt;/li&gt;
&lt;li&gt;Содержит "горячие" данные из MySQL&lt;/li&gt;
&lt;li&gt;Снижает нагрузку на уровень баз данных&lt;/li&gt;
&lt;li&gt;Основная форма кэширования&lt;/li&gt;
&lt;li&gt;Используется более 25TB памяти на нескольких тысячах серверов&lt;/li&gt;
&lt;li&gt;Среднее время отклика менее 250 микро-секунд&lt;/li&gt;
&lt;li&gt;Кэшируются сериализованные структуры данных PHP&lt;/li&gt;
&lt;li&gt;Отсутствие автоматического механизма проверки консистенции данных
    между memcached и MySQL - приходится делать это на уровне
    программного кода&lt;/li&gt;
&lt;li&gt;Множество multi-get запросов для получения данных на другом конце
    ребер графа&lt;/li&gt;
&lt;li&gt;Ограниченная модель данных, неэффективен для маленьких объектов&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Доработки:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Порт на 64-битную архитектуру&lt;/li&gt;
&lt;li&gt;Более эффективная сериализация&lt;/li&gt;
&lt;li&gt;Многопоточность&lt;/li&gt;
&lt;li&gt;Улучшенный протокол&lt;/li&gt;
&lt;li&gt;Компрессия&lt;/li&gt;
&lt;li&gt;Проксирование запросов&lt;/li&gt;
&lt;li&gt;Доступ к memcache через UDP:&lt;ul&gt;
&lt;li&gt;уменьшает расход памяти благодаря отсутствию тысяч буферов TCP
    соединений&lt;/li&gt;
&lt;li&gt;управление ходом исполнения приложение (оптимизация для
    multi-get)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Статистика о работе потоков по запросу - уменьшает блокировки&lt;/li&gt;
&lt;li&gt;Ряд изменений в ядре Linux для оптимизации работы memcache:&lt;ul&gt;
&lt;li&gt;распределение управления сетевыми прерывания по всем ядрам&lt;/li&gt;
&lt;li&gt;оппортунистический опрос сетевых интерфейсов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;После вышеперечисленных модификаций memcached способен выполнять до
    250 тысяч операций в секунду, по сравнению со стандартными 30-40
    тысячами без данных изменений&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="thrift"&gt;Thrift&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Что это?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Легкий механизм построения приложений с использованием нескольких
    языков программирования&lt;/li&gt;
&lt;li&gt;Высокая цель: предоставить механизм прозрачного взаимодействия между
    языками программирования.&lt;/li&gt;
&lt;li&gt;Предоставляет язык описания интерфейсов, статический генератор кода&lt;/li&gt;
&lt;li&gt;Поддерживаемые языки: &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;, &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;,
    &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;, &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, &lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt;,
    &lt;a href="/tag/erlang/"&gt;Erlang&lt;/a&gt;, &lt;a href="/tag/perl/"&gt;Perl&lt;/a&gt;, &lt;a href="/tag/haskell/"&gt;Haskell&lt;/a&gt; и
    многие другие&lt;/li&gt;
&lt;li&gt;Транспорты: простой интерфейс для ввода-вывода (сокеты, файлы,
    буферы в памяти)&lt;/li&gt;
&lt;li&gt;Протоколы: стандарты сериализации (бинарный, JSON)&lt;/li&gt;
&lt;li&gt;Серверы: неблокирующие, асинхронные, как однопоточные, так и
    многопоточные&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Почему именно Thrift?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Альтернативные технологии: SOAP, CORBA, COM, Pillar, Protocol
    Buffers - но у всех есть свои существенные недостатки, что вынудило
    Facebook создать свою технологию&lt;/li&gt;
&lt;li&gt;Он быстрый, очень быстрый&lt;/li&gt;
&lt;li&gt;Меньше рабочего времени тратится каждым разработчиком на сетевые
    интерфейсы и протоколы&lt;/li&gt;
&lt;li&gt;Разделение труда: работа над высокопроизводительными серверами
    ведется отдельно от работы над приложениями&lt;/li&gt;
&lt;li&gt;Общий инструментарий, знакомый всем разработчикам&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="scribe"&gt;Scribe&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Что это?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Масштабированный распределенный механизм ведения логов&lt;/li&gt;
&lt;li&gt;Перемещает данные с серверов в центральный репозиторий&lt;/li&gt;
&lt;li&gt;Широкая сфера применения:&lt;ul&gt;
&lt;li&gt;Логи поисковых запросов&lt;/li&gt;
&lt;li&gt;Публикации в новостных лентах&lt;/li&gt;
&lt;li&gt;Данные по A/B тестированиям&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Более надежен, чем традиционные системы логгирования, но
    недостаточно надежен для транзакций баз данных&lt;/li&gt;
&lt;li&gt;Простая модель данных&lt;/li&gt;
&lt;li&gt;Построен на основе Thrift&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="khranenie-fotografii"&gt;Хранение фотографий&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Сначала сделали это просто:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Загрузка на сервер: приложение принимает изображение, создает
    миниатюры в нужных разрешениях, сохраняет в NFS&lt;/li&gt;
&lt;li&gt;Загрузка с сервера: изображения отдаются из NFS через HTTP&lt;/li&gt;
&lt;li&gt;NFS построена на коммерческих продуктах&lt;/li&gt;
&lt;li&gt;Это было необходимо, чтобы сначала проверить, что продукт
    востребован пользователями и они правда будут активно загружать
    фотографии&lt;/li&gt;
&lt;li&gt;На самом деле оказалось, что:&lt;ul&gt;
&lt;li&gt;Файловые системы непригодны для работы с большим количеством
    небольших файлов&lt;/li&gt;
&lt;li&gt;Метаданные не помещаются в оперативную память, что приводит к
    дополнительным обращениям к дисковой подсистеме&lt;/li&gt;
&lt;li&gt;Ограничивающим фактором является ввод-вывод, а не плотность
    хранения&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Потом начали оптимизировать:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Кэширование более часто используемых миниатюр изображений в памяти
    на оригинальных серверах для масштабируемости, надежности и
    производительности&lt;/li&gt;
&lt;li&gt;Распределение их по &lt;a href="/tag/cdn/"&gt;CDN&lt;/a&gt; для уменьшения сетевых задержек&lt;/li&gt;
&lt;li&gt;Возможно сделать еще лучше:&lt;ul&gt;
&lt;li&gt;Хранение изображений в больших бинарных файлах (blob)&lt;/li&gt;
&lt;li&gt;Сервис, отвечающий за фотографии имеет информацию о том, в каком
    файле и с каким отступом от начала расположена каждая фотография
    (по ее идентификатору)&lt;/li&gt;
&lt;li&gt;Этот сервис в Facebook называется Haystack и он оказался в 10
    раз эффективнее "простого" подхода и в 3 раза эффективнее
    "оптимизированного"&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="drugie-servisy"&gt;Другие сервисы&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SMC&lt;/strong&gt;: консоль управления сервисами - централизованная
    конфигурация, определение на какой физической машине работает
    логический сервис&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ODS&lt;/strong&gt;:&amp;nbsp;инструмент для визуализации изменений любых статистических
    данных, имеющихся в системе; удобен для мониторинга и оповещений&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gatekeeper:&lt;/strong&gt; разделение развертывания и запуска, A/B
    тестирования, таргетированный запуск, постепенный запуск&lt;/li&gt;
&lt;li&gt;И еще около 50 других сервисов...&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="kak-eto-rabotaet-vse-vmeste_1"&gt;Как это работает все вместе?&lt;/h2&gt;
&lt;h3 id="novye-albomy-druzei"&gt;Новые альбомы друзей&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Facebook Screenshot" class="responsive-img" src="https://www.insight-it.ru/images/facebook_screenshot.jpg" title="Facebook"/&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Получаем профиль по идентификатору пользователя (скорее всего из
    кэша, но потенциально возможно обращение к базе данных)&lt;/li&gt;
&lt;li&gt;Получаем список друзей (опять же на основе идентификатора
    пользователя из кэша или из базы данных в случае промаха)&lt;/li&gt;
&lt;li&gt;Параллельно запрашиваем идентификаторы последних 10 альбомов для
    каждого из друзей (multi-get, каждый промах мимо кэша индивидуально
    вытаскивается из MySQL)&lt;/li&gt;
&lt;li&gt;Параллельно получаем данные о всех альбомах (на основе
    идентификаторов альбомов из предыдущего шага)&lt;/li&gt;
&lt;li&gt;Все данные получены, выполняем логику отрисовки конкретной страницы
    на PHP&lt;/li&gt;
&lt;li&gt;Отправляем HTML в браузер, пользователь счастлив :)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="novostnaia-lenta"&gt;Новостная лента&lt;/h3&gt;
&lt;p&gt;&lt;img alt="News Feed Screenshot" class="responsive-img" src="https://www.insight-it.ru/images/facebook_screenshot_2.jpg" title="News Feed"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="News Feed Scheme" class="responsive-img" src="https://www.insight-it.ru/images/facebook_screenshot_3.jpg" title="News Feed"/&gt;&lt;/p&gt;
&lt;h3 id="poisk"&gt;Поиск&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Search Screenshot" class="responsive-img" src="https://www.insight-it.ru/images/facebook_screenshot_4.jpg" title="Search"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Search Scheme" class="responsive-img" src="https://www.insight-it.ru/images/facebook_screenshot_5.jpg" title="Search"/&gt;&lt;/p&gt;
&lt;h2 id="podvodim-itogi_1"&gt;Подводим итоги&lt;/h2&gt;
&lt;p&gt;LAMP не идеален&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PHP+MySQL+Memcache решает большинство задач, но не может решить
    совсем все:&lt;ul&gt;
&lt;li&gt;PHP не может хранить состояния&lt;/li&gt;
&lt;li&gt;PHP не самый производительный язык&lt;/li&gt;
&lt;li&gt;Все данные находятся удаленно&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Facebook разрабатывает собственные внутренние сервисы, чтобы:&lt;ul&gt;
&lt;li&gt;Располагать исполняемый код ближе к данным&lt;/li&gt;
&lt;li&gt;Скомпилированное окружение более эффективно&lt;/li&gt;
&lt;li&gt;Некоторая функциональность присутствует только в других языках
    программирования&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Философия сервисов:&lt;ul&gt;
&lt;li&gt;Создание сервисов только при необходимости (минимизация издержек
    по развертке, поддержке и ведению отдельной кодовой базы;
    потенциальная дополнительная точка сбоя)&lt;/li&gt;
&lt;li&gt;Создание общего набора инструментов для создания сервисов
    (Thrift, Scribe, ODS, средства мониторинга и уведомлений)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Использование правильных языка программирования, библиотек и
    инструментов для решения задачи&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Возвращение инноваций общественности - важный аспект разработки в
    Facebook:&lt;ul&gt;
&lt;li&gt;Опубликованные свои проекты:&lt;ul&gt;
&lt;li&gt;Thrift&lt;/li&gt;
&lt;li&gt;Scribe&lt;/li&gt;
&lt;li&gt;Tornado&lt;/li&gt;
&lt;li&gt;Cassandra&lt;/li&gt;
&lt;li&gt;Varnish&lt;/li&gt;
&lt;li&gt;Hive&lt;/li&gt;
&lt;li&gt;xhprof&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Доработки популярных решений:&lt;ul&gt;
&lt;li&gt;PHP&lt;/li&gt;
&lt;li&gt;MySQL&lt;/li&gt;
&lt;li&gt;memcached&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Информация о взаимодействии Facebook с opensource-сообществом,
    этих и других проектах расположена на &lt;a href="https://www.insight-it.ru/goto/535d8e6b/" rel="nofollow" target="_blank" title="http://developers.facebook.com/opensource/"&gt;странице, посвященной
    opensource&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ключевые моменты культуры разработки в Facebook:&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Двигайся быстро&lt;/strong&gt; и не бойся ломать некоторые вещи&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Большое влияние&lt;/strong&gt; маленьких команд&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Будь откровенным&lt;/strong&gt; и инновационным&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;p&gt;Данная статья не является переводом готовой статьи, в качестве
источников информации послужили записи выступлений сотрудников Facebook
на конференциях:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/4c672c94/" rel="nofollow" target="_blank" title="http://www.infoq.com/presentations/Facebook-Software-Stack"&gt;Facebook Architecture: Science and the Social Graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/2ccd1899/" rel="nofollow" target="_blank" title="http://www.infoq.com/presentations/Facebook-Moving-Fast-at-Scale"&gt;Facebook: Moving Fast at Scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/3867625a/" rel="nofollow" target="_blank" title="http://www.infoq.com/presentations/Scale-at-Facebook"&gt;Scale at Facebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Очень рекомендую посмотреть материалы в оригинале, так как естественно я
осветил в статье далеко не все, да и неточности какие-либо неисключены.
Помимо этого возможно многим будет интересно мероприятие &lt;a href="https://www.insight-it.ru/goto/ff11ad2b/" rel="nofollow" target="_blank" title="http://styleru.timepad.ru/event/3571"&gt;"Facebook: how we scaled to 500 000 000 users "&lt;/a&gt;,
где Robert Johnson выступает 22 октября в Москве. Еще он числится в
списке докладчиков &lt;a href="https://www.insight-it.ru/goto/727c9436/" rel="nofollow" target="_blank" title="http://www.highload.ru"&gt;Highload++&lt;/a&gt; с аналогичным
выступлением. Дополнительную информацию можно почерпнуть в &lt;a href="https://www.insight-it.ru/goto/f38bc794/" rel="nofollow" target="_blank" title="http://facebook.com/eblog"&gt;блоге инженеров Facebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPD:&lt;/strong&gt; Обновил некоторые моменты после посещения вышеупомянутого
выступления Роберта.&lt;/p&gt;
&lt;p&gt;И по традиции напоминаю, что так как я пишу довольно редко - читать мой
блог намного удобнее по &lt;a href="/feed/"&gt;RSS&lt;/a&gt;. Спасибо за внимание :)&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 20 Oct 2010 13:02:00 +0400</pubDate><guid>tag:www.insight-it.ru,2010-10-20:highload/2010/arkhitektura-facebook/</guid><category>CDN</category><category>Facebook</category><category>featured</category><category>HipHop</category><category>Linux</category><category>Memcached</category><category>MySQL</category><category>ODS</category><category>PHP</category><category>Scribe</category><category>Thrift</category><category>Архитектура Facebook</category></item></channel></rss>