<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Insight IT</title><link>https://www.insight-it.ru/</link><description></description><atom:link href="https://www.insight-it.ru/tag/arkhitektura/feed/index.xml" rel="self"></atom:link><lastBuildDate>Wed, 04 Apr 2012 01:05:00 +0400</lastBuildDate><item><title>Архитектура интерактивных сайтов</title><link>https://www.insight-it.ru//interactive/2012/arkhitektura-interaktivnykh-sajjtov/</link><description>&lt;p&gt;В &lt;a href="https://www.insight-it.ru/interactive/2012/anons-serii-statejj-interaktivnye-sajjty/"&gt;анонсе&lt;/a&gt;&amp;nbsp;серии статей &lt;a href="https://www.insight-it.ru/interactive/"&gt;"Интерактивные сайты"&lt;/a&gt; я постарался максимально доходчиво изложить свою мотивацию к ей созданию, да и актуальность самой темы, так что сразу к делу!
&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Итак, мы хотим сделать так, чтобы с точки зрения &lt;em&gt;пользователя&lt;/em&gt; наш сайт
выглядел &lt;strong&gt;интерактивным&lt;/strong&gt;. То есть воспринимался не как набор отдельно
загружающихся страниц, а скорее как обычное приложение для компьютера.
Ему, в целом, не важно как именно мы этого добьемся, главное чтобы при
этом браузер вел себя как обычно и визуально все реагировало почти
мгновенно.&lt;/p&gt;
&lt;p&gt;Сразу хочу предупредить, что далеко не всем сайтам такая
глобальная&amp;nbsp;&lt;em&gt;интерактивность&lt;/em&gt; пойдет на пользу. Если пользователи сайта
редко что-либо делают и большую часть времени читают длинные тексты, то,
вероятно, этот проект как раз из этой категории: быстрота реакции на
клик не так радует, когда большую часть времени приходится работать
скроллом. Если Ваш интернет-проект является сайтом с длинным контентом в
формате блога, новостей или вики - подумайте лишний раз, прежде чем
переделывать весь сайт, вероятно будет достаточно интерактивных
комментариев и голосований на &lt;a href="/tag/ajax/"&gt;AJAX&lt;/a&gt;. Для социальных сетей,
сайтов знакомств, интернет-магазинов, поисковых систем, корпоративных
сайтов и многих других визуально заметная&amp;nbsp;&lt;em&gt;интерактивность&lt;/em&gt;&amp;nbsp;определенно
станет одним из ключевых &lt;strong&gt;конкурентных преимуществ&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;В этом посте я постараюсь нарисовать крупными мазками картину того, как
этого можно достичь. Визуально её я представил следующим образом:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Архитектура интерактивного сайта" class="responsive-img" src="https://www.insight-it.ru/images/interactive-site-architecture.jpeg" title="Архитектура интерактивного сайта"/&gt;&lt;/p&gt;
&lt;h2 id="obshchie-zamechaniia"&gt;Общие замечания&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Здесь и далее я буду описывать "среднестатистический" интернет-проект, в
зависимости от специфики могут появляться дополнительные компоненты или
убираться за ненадобностью упомянутые. Например, если известно, что
пользоваться сайтом будут только сотрудники какой-то компании, то помимо
замены Интернета на Интранет, можно запросто избавиться и от
&lt;strong&gt;HTML-интерфейса&lt;/strong&gt; и всего, что с ним связано - никаким роботам доступ
к нему не нужен.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Наверное стоит прямым текстом сказать, что:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Голубые элементы - компоненты проекта, а серые - внешние.&lt;/li&gt;
&lt;li&gt;Связи в виде прямых линий означают&amp;nbsp;двусторонний&amp;nbsp;обмен данными,
транспорт и формат пока особо не важны.&lt;/li&gt;
&lt;li&gt;Схема логическая, так что вопросы балансировки нагрузки,
отказоустойчивости, репликации и т.п. остались в стороне; за каждым
блоком может стоять произвольное количество серверов.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Я решил не загромождать схему мониторингом, резервным копированием,
почтой, сервисом доменов и прочими хоть и важными, но не связанными
напрямую с темой компонентами системы.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;В этом посте не будет никаких конкретных примеров реализации и
технологий, оставим это на &lt;a href="https://www.insight-it.ru/interactive/"&gt;следующие статьи&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="klientskaia-chast"&gt;Клиентская часть&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Начнем наше путешествие. Пользователь вводит в адресной строке браузера
адрес нашего сайта и жмет Enter, инициируя тем самым сначала определение
IP-адреса через DNS, а затем и HTTP-запрос к нашему &lt;strong&gt;HTML-интерфейсу&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Получив в ответ &lt;strong&gt;страницу&lt;/strong&gt; в формате &lt;a href="/tag/html/"&gt;HTML&lt;/a&gt; браузер
начинает загружать указанные в нем внешние ресурсы, в том числе и
&lt;strong&gt;Javascript-клиент&lt;/strong&gt;, которому и передается слово. Сама страница
параллельно отрисовывается как и в статичном сайте.&lt;/li&gt;
&lt;li&gt;Как уже упоминалось выше, некоторые проекты решают не тратить силы на
поддержку двух интерфейсов к сайту, жертвуя тем самым доступом
большинства роботов и браузеров без поддержки
&lt;a href="/tag/javascript/"&gt;JavaScript&lt;/a&gt;. Стартовый HTML-документ в этом случае
превращается просто в практически пустой статичный файл, который служит
лишь для загрузки клиента.&lt;/li&gt;
&lt;li&gt;Так как наша цель стоит в интерактивном взаимодействии пользователем,
повторять эти действия при каждом переходе на другую страницу -
&lt;em&gt;непозволительная роскошь&lt;/em&gt;. Кстати, можно начинать думать и общаться не
в терминах страниц, а в терминах &lt;strong&gt;экранов&lt;/strong&gt;, которые видит
пользователь.&lt;/li&gt;
&lt;li&gt;Для обеспечения этого&amp;nbsp;&lt;strong&gt;JavaScript-клиент&lt;/strong&gt; должен переопределить
стандартные обработчики событий перехода по внутренним ссылкам сайта и
отправки форм. Ему нужно отменить стандартный механизм перехода на
другую страницу и вместо него отправить запрос через &lt;strong&gt;интерфейс
сериализованных данных&lt;/strong&gt;. На основе полученного в ответ сообщения он
меняет какую-то часть загруженного ранее HTML-документа, чтобы он
соответствовал тому &lt;em&gt;экрану&lt;/em&gt;, который ожидал увидеть пользователь. В
итоге пользователь видит в браузере ровно ту же картинку, как если бы он
ввел тот адрес, на который вела нажатая ссылка, или просто нажал на нее
с отключенным JavaScript.&lt;/li&gt;
&lt;li&gt;Важно не сломать при этом поведение браузера: кнопка "назад" должна
работать как обычно, а в адресной строке должны меняться ссылки (это
актуально, например, когда пользователь отправляет содержимое адресной
строки кому-то по почте или через мессенджер).&lt;/li&gt;
&lt;li&gt;При ожидании ответа от сервера стоит эмулировать курсор и иконку
загрузки страницы, чтобы пользователь &lt;em&gt;не паниковал&lt;/em&gt; в случае (пускай и
редком) визуально заметных задержек.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;На резонный вопрос &lt;em&gt;"Почему, собственно, этот трюк обеспечит
интерактивность?"&lt;/em&gt;, ответ хоть и не всегда однозначен, но он все же
есть:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Сериализованные &lt;strong&gt;изменения&lt;/strong&gt; страницы занимают меньший объем, чем
&lt;strong&gt;полный&lt;/strong&gt; HTML со всеми связанными ресурсами, заголовками, версткой
и прочим - &lt;em&gt;значительно меньше данных нужно передавать по сети&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Как правило, есть возможность держать &lt;strong&gt;постоянное&lt;/strong&gt; соединение
между браузером и интерфейсом сериализованных данных, что позволяет
&lt;em&gt;не делать лишние HTTP-запросы&lt;/em&gt;. Обратная сторона медали - это самое
соединение постоянно же использует часть серверных ресурсов, но есть
способы минимизировать эти издержки.&lt;/li&gt;
&lt;li&gt;Для некоторых действий изменения HTML не требуют ответа сервера и
могут быть сделаны параллельно с отправкой запроса (например
&amp;nbsp;различные вариации на тему +1 или написание комментария, текст
которого можно взять из формы).&lt;/li&gt;
&lt;li&gt;Как правило, можно предсказать наиболее вероятные переходы по
экранам и &lt;em&gt;загрузить необходимые изменения заранее&lt;/em&gt;. Хотя этот
вопрос скорее из области оптимизации.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Таким образом, в большинстве случаев есть техническая возможность
снизить время отклика &amp;nbsp;на действие пользователя с 500-2000 мс в
случае неплохо сделанного статического сайта до 20-200 мс., что
вполне сопоставимо с откликом десктопного приложения.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Как все это сделать на практике - тема следующей статьи из серии.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="servernaia-chast"&gt;Серверная часть&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;С серверной точки зрения основным отличием является четкое разделение
двух входных точек:&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;HTML-интерфейс&lt;/strong&gt;&amp;nbsp;отдает готовые документы в ответ на HTTP-запросы.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Интерфейс сериализованных данных&lt;/strong&gt;&amp;nbsp;использует какое-то постоянное
соединение, хотя в некоторых случаях целесообразно ограничиться
просто асинхронными HTTP-запросами.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Если для статичных сайтов полное &lt;em&gt;выделение бизнес-логики в отдельные
сервисы&lt;/em&gt; - просто хорошее архитектурное решение, то для интерактивных
сайтов - это практически &lt;strong&gt;необхохимо&lt;/strong&gt;. Иначе придется реализовывать и
поддерживать две копии кода для каждого интерфейса и надеяться, что они
постоянно будут оставаться совместимыми и выдавать одинаковый результат.&lt;/li&gt;
&lt;li&gt;Хорошей практикой является использование какого-то одного протокола
общения между компонентами системы, в частности пользовательских
интерфейсов с сервисами бизнес-логики и последних друг с другом.
Желательно использовать что-то бинарное и с поддержкой разных языков
программирования, хотя если весь проект на одной платформе и не
планирует это менять - можно использовать и стандартный для этой
платформы протокол.&lt;/li&gt;
&lt;li&gt;Чтобы не включать элементы верстки при передаче через интерфейс
сериализованных данных, рекомендую использовать кроссплатформенный
формат HTML-шаблонов. Об этом будет отдельная статья.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Интерфейс сериализованных данных&lt;/strong&gt; при необходимости легко может быть
адаптирован для использования в роли &lt;em&gt;API для сторонних сервисов или
собственных приложений&lt;/em&gt; для мобильных платформ или настольных
компьютеров.&lt;/li&gt;
&lt;li&gt;В целом внутренние сервисы общаются с остальными располагающимися на
серверной части компонентами системы вполне обычным образом.&lt;/li&gt;
&lt;li&gt;В статье про серверную часть подробно будет рассматриваться
использование брокера сообщений для уведомлений пользователей в реальном
времени.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Глобальная интерактивность&lt;/em&gt; сайта требует использования достаточно
    сложного и комплексного JavaScript-клиента и создания
    дополнительного более легковесного внешнего инетрфейса на серверной
    части.&lt;/li&gt;
&lt;li&gt;По-настоящему &lt;em&gt;мгновенной&lt;/em&gt; реакцией сайта смогут насладиться лишь
    пользователи с &lt;strong&gt;современным браузером&lt;/strong&gt; и относительно &lt;strong&gt;широким
    интернет-каналом&lt;/strong&gt;. Из-за возможных сетевых задержек или
    особенностей устаревших браузеров эффект мгновенного перехода все же
    может теряться, но при должном тестировании реально добиться
    нормального поведения сайта и в таких ситуациях. Хотя зачастую
    "проваливание" до обычного режима статичных страниц в подобных
    ситуациях - вполне резонное решение.&lt;/li&gt;
&lt;li&gt;Архитектура серверной части проекта в большинстве случаев не требует
    существенных изменений. Хотя если в ней все было хаотично и не
    продумано, то создание интерактивного клиента может послужить
    неплохим поводом пересмотреть и привести в порядок и её.&lt;/li&gt;
&lt;li&gt;Кроме очевидной потребности в использовании JavaScript для клиента,
    особых ограничений на используемые технологии и языки
    программирования, обсуждаемая схема не накладывает.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="card green"&gt;
&lt;p&gt;&lt;div class="card-content white-text"&gt;
Эта статья - первая в &lt;a class="green-text text-lighten-4" href="https://www.insight-it.ru/interactive/"&gt;серии про Интерактивные сайты&lt;/a&gt;, автор - &lt;a class="green-text text-lighten-4" href="https://www.insight-it.ru/goto/b03d9116/" rel="nofollow" target="_blank" title="http://blinkov.ru"&gt;Иван&amp;nbsp;Блинков&lt;/a&gt;, основано на личном опыте.
До встречи &lt;a class="green-text text-lighten-4" href="/feed/"&gt;на страницах Insight IT&lt;/a&gt;!
&lt;/div&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 04 Apr 2012 01:05:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-04-04:interactive/2012/arkhitektura-interaktivnykh-sajjtov/</guid><category>архитектура</category><category>Интерактивные сайты</category><category>клиентская часть</category><category>Масштабируемость</category><category>серверная часть</category><category>схема</category></item><item><title>Анонс серии статей: интерактивные сайты</title><link>https://www.insight-it.ru//interactive/2012/anons-serii-statejj-interaktivnye-sajjty/</link><description>&lt;p&gt;Интернет развивается огромными темпами. В борьбе за аудиторию крупные
интернет-компании поднимают стандарты качества веб-приложений на все
более и более высокий уровень. Одним из важнейших качеств современных
сайтов является &lt;strong&gt;интерактивность&lt;/strong&gt;, если раньше все они поголовно
представляли собой коллекцию статичных страниц, где можно что-то
почитать или посмотреть, то сегодня они - почти живой организм.&lt;/p&gt;
&lt;p&gt;Пользователи все больше привыкают узнавать о событиях и видеть реакцию
на свои действия мгновенно, не дожидаясь загрузок страниц и прочих
задержек. Раньше это было возможно только для обычных приложений, но с
сегодняшним уровнем технологий общаться с пользователем в реальном
времени можно и &lt;strong&gt;посредством браузера&lt;/strong&gt;, причем доступно это не только
интернет-гигантам, а практически любому интернет-проекту.&lt;/p&gt;
&lt;p&gt;За последний год &lt;em&gt;привнесение интерактивности в интернет-проекты&lt;/em&gt; -
пожалуй, одна из самых популярных тем, с которой ко мне &lt;a href="https://www.insight-it.ru/consulting/"&gt;обращаются за консультацией&lt;/a&gt;. В итоге я решил
не жадничать и поделиться с общественностью своими знаниями в этой
области, что в итоге должно вылиться в серию связанных статей
&lt;strong&gt;"Интерактивные сайты"&lt;/strong&gt;. В ней я хочу отразить практически пошаговую
инструкцию от А до Я для создания интерактивного интернет-приложения с
нуля или основываясь на существующем статичном проекте. Соответственно,
по ходу дела сделаю легко доступное оглавление по аналогии с
&lt;a href="https://www.insight-it.ru/highload/"&gt;архитектурой высоконагруженных интернет-проектов&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="orientirovochnye-temy-statei"&gt;Ориентировочные темы статей&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/interactive/2012/arkhitektura-interaktivnykh-sajjtov/"&gt;Общая архитектура&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/interactive/2012/klientskaya-chast-interaktivnogo-sajjta/"&gt;Организация клиентской части&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/interactive/2012/postoyannoe-soedinenie-mezhdu-brauzerom-i-serverom/"&gt;Постоянное соединение между браузером и сервером&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/interactive/2012/povtornoe-ispolzovanie-shablonov/"&gt;Повторное использование шаблонов&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/interactive/2012/servernaya-chast-interaktivnogo-sajjta-i-potoki-soobshhenijj/"&gt;Серверная часть интерактивного сайта и потоки сообщений&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/interactive/2012/optimizaciya-interaktivnykh-sajjtov/"&gt;Оптимизация&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Основной упор будет сделан именно на общую концепцию и сведение всех
компонентов воедино, как на серверной стороне, так и на клиентской.
Количество изобретаемых велосипедов постараюсь свести к минимуму: где-то
будут просто рекомендации по использованию публично доступных
технологий, где-то - сравнительные обзоры. Специфики каких-либо
определенных типов проектов постараюсь избегать.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Пожелания, предложения, советы и вопросы в комментариях к этому посту
очень приветствуются :)&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 01 Apr 2012 17:43:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-04-01:interactive/2012/anons-serii-statejj-interaktivnye-sajjty/</guid><category>JavaScript</category><category>архитектура</category><category>интерактив</category><category>интернет-приложения</category><category>клиентская оптимизация</category><category>Масштабируемость</category><category>сайты</category><category>технологии</category></item><item><title>Новое поколение MapReduce в Apache Hadoop</title><link>https://www.insight-it.ru//storage/2011/novoe-pokolenie-mapreduce-v-apache-hadoop/</link><description>&lt;p&gt;В большом бизнесе использование нескольких больших кластеров с
финансовой точки зрения более&amp;nbsp;эффективно, чем много маленьких. Чем
больше машин в кластере, тем большими наборами данных он может
оперировать, больше задач могут выполняться одновременно. Реализация
&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt; в &lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt;
&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; столкнулась с потолком масштабируемости на уровне
около 4000 машин в кластере. Разрабатывается следующее поколение Apaсhe
Hadoop MapReduce, &amp;nbsp;в котором появится общий планировщик ресурсов и
отдельный мастер для каждой отдельной задач, управляющий выполнением
программного кода. Так как простой оборудования по техническим причинам
обходится дорого на таком масштабе, высокий уровень доступности
проектируется с самого начала, ровно как и безопасность и
многозадачность, необходимые для поддержки одновременного использования
большого кластера многими пользователями. Новая архитектура также будет
более инновационной, гибкой и эффективной с точки зрения использования
вычислительных ресурсов.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="predistoriia"&gt;Предистория&lt;/h2&gt;
&lt;p&gt;Текущая реализация Hadoop MapReduce устаревает на глазах. Основываясь на
текущих тенденциях в размерах кластеров и нагрузок на них, JobTracker
требует кардинальных доработок, чтобы исправить его дефекты в области
масштабируемости, потребления памяти, многопоточности, надежности и
производительности. С точки зрения работы с Hadoop при каждом обновлении
кластера (даже если это просто багфикс), абсолютно все компоненты
кластера, так и приложений, которые на нем работают, должны быть
обновлены одновременно. Это так же очень неудобно, так как каждый раз
необходимо тестировать все приложения на совместимость с новой версией.&lt;/p&gt;
&lt;h2 id="trebovaniia"&gt;Требования&lt;/h2&gt;
&lt;p&gt;Прежде чем кардинально что-то менять в Hadoop mapreduce, необходимо
понять какие же основные требования предъявляются к вычислительным
кластерам на практике. Наиболее значительными требованиями к Hadoop
следующего поколения являются:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Надежность&lt;/li&gt;
&lt;li&gt;Доступность&lt;/li&gt;
&lt;li&gt;Масштабируемость - кластеры из как минимум 10 тысяч машин, 200 тысяч
    вычислительных ядер и даже больше&lt;/li&gt;
&lt;li&gt;Обратная и прямая совместимость - возможность быть уверенным, что
    приложение будет работать на новой версии так же, как оно работало
    на старой&lt;/li&gt;
&lt;li&gt;Контроль над обновлениями&lt;/li&gt;
&lt;li&gt;Предсказуемые задержки&lt;/li&gt;
&lt;li&gt;Эффективное использование ресурсов&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Среди менее значительных требований:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Поддержка альтернативных парадигм разработки (помимо MapReduce)&lt;/li&gt;
&lt;li&gt;Поддержка сервисов с коротким жизненным циклом&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Если учесть перечисленные выше требования, то становится очевидно, что
инфраструктура обработки данных в Hadoop должна быть кардинальным
образом изменена. В сообществе Hadoop люди в целом приходят к общему
мнению, что текущая архитектура MapReduce не способна решить текущие
задачи, которые перед ней ставится, и что требуется кардинальный
рефакторинг кодовой базы.&lt;/p&gt;
&lt;h2 id="mapreduce-sleduiushchego-pokoleniia"&gt;MapReduce следующего поколения&lt;/h2&gt;
&lt;p&gt;Фундаментальной идеей смены архитектуры является разделение двух
основных функций JobTracker'а на два отдельных части:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;управление ресурсами;&lt;/li&gt;
&lt;li&gt;планирования и мониторинга задач.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В итоге появляется несколько новых ролей:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ResourceManager&lt;/strong&gt; управляет глобальным распределением
    вычислительных ресурсов между приложениями;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplicationMaster&lt;/strong&gt; управляет планированием и координацией внутри
    приложения;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NodeManager&lt;/strong&gt; управляет процессами в рамках одной машины.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ApplicationMaster представляет собой библиотеку, с помощью которой можно
получить у ResourceManager квоту на вычислительные ресурсы и работать с
NodeManager(ами) для выполнения и мониторинга задач.&lt;/p&gt;
&lt;p&gt;ResourceManager поддерживает иерархическим очереди приложений, которым
может гарантированно выделяться некоторый процент ресурсов кластера. Его
функционал ограничивается планированием, никакого мониторинга и
отслеживания задач не происходит, а также нет никаких гарантий
перезапуска задач, провалившихся из-за проблем с оборудованием или
кодом. Планирование основывается на требованиях, которые выставляет
приложение с помощью ряда запросов ресурсов (среди них: запросы на
вычислительные ресурсы, память, дисковое пространство, сетевой доступ и
т.п.). Обратите внимание, что это значительное изменение по сравнению с
текущей моделью слотов фиксированного размера, которая является одной из
основных причин неэффективного использования ресурсов кластера на данный
момент.&lt;/p&gt;
&lt;p&gt;NodeManager - это агент, который работает на каждой машине и несет
ответственность за запуск контейнеров приложений, мониторинг
используемых ими ресурсов (плюс отчет планировщику).&lt;/p&gt;
&lt;p&gt;По одному ApplicationMaster запускается для каждого приложения, они
ответственны за запрос необходимых ресурсов у планировщика, запуск
задач, отслеживание статусов, мониторинг прогресса и обработку сбоев.&lt;/p&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Следующее поколение MapReduce" class="responsive-img" src="https://www.insight-it.ru/images/mapreduce-nextgen.jpg" title="Следующее поколение MapReduce"/&gt;&lt;/p&gt;
&lt;h2 id="uluchsheniia-po-sravneniiu-s-tekushchei-realizatsiei-mapreduce"&gt;Улучшения по сравнению с текущей реализацией MapReduce&lt;/h2&gt;
&lt;h3 id="masshtabiruemost"&gt;Масштабируемость&lt;/h3&gt;
&lt;p&gt;Разделение управления ресурсами и прикладными задачами позволяет
горизонтально расширять кластер более просто и эффективно. JobTracker
проводит значительную часть времени пытаясь управлять жизненным циклом
каждого приложения, что часто может приводить к различным
происшествиям - переход к отдельному менеджеру для каждого приложения
является значительным шагом вперед.&lt;/p&gt;
&lt;p&gt;Масштабируемость особенно важна в свете текущих трендов в оборудовании -
на данный момент Hadoop может быть развернут на кластере из 4000 машин.
Но 4000 средних машин 2009го года (т.е. по 8 ядер, 16Гб памяти, 4Тб
дискового пространства) только вдвое менее &amp;nbsp;ресурсоемки, чем 4000 машин
2011го года (16 ядер, 48гб памяти, 24Тб дискового пространства). Помимо
этого с точки зрения операционных издержек было выгоднее работать в еще
больших кластере от 6000 машин и выше.&lt;/p&gt;
&lt;h3 id="dostupnost"&gt;Доступность&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ResourceManager использует&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/e7095d3/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/zookeeper/"&gt;Apache ZooKeeper&lt;/a&gt; для обработки сбоев.
    Когда ResourceManager перестает работать, аналогичный процесс может
    быстро запуститься на другой машине благодаря тому, что состояние
    кластера было сохранено в ZooKeeper. При таком сценарии все
    запланированные и выполняющиеся приложения максимум лишь
    перезапустятся.&lt;/li&gt;
&lt;li&gt;ApplicationMaster - поддерживается создание точек восстановления на
    уровне приложений. ApplicationMaster может восстановить работу из
    состояния, сохраненного в HDFS, в случае сбоя.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="sovmestimost-protokola"&gt;Совместимость протокола&lt;/h3&gt;
&lt;p&gt;Это позволит различным версиям клиентов и серверов Hadoop общаться между
собой. Помимо решения многих существующих проблем с обновлением, в
будующих релизах появится возможность последовательного обновления кода
без простоя системы в целом - очень большое достижения с точки зрения
системного администрирования.&lt;/p&gt;
&lt;h3 id="innovatsionnost-i-gibkost"&gt;Инновационность и гибкость&lt;/h3&gt;
&lt;p&gt;Основным плюсом предложенной архитектуры является тот факт, что
MapReduce по сути становится просто пользовательской библиотекой.
Вычислительная же система (ResourceManager и NodeManager) становятся
полностью независимыми от специфики MapReduce.&lt;/p&gt;
&lt;p&gt;Клиенты получат возможность одновременного использования разных версий
MapReduce в одном и том же кластере. Это становится тривиальным, так как
отдельная копия ApplicationMaster'а запускается для каждого приложения.
Это дает гибкость в исправлении багов, улучшений и новых возможностей,
так как полное обновление кластер перестает быть обязательной
процедурой. Это позволяет клиентам обновлять их приложения до новых
версий MapReduce вне зависимости от обновлений кластера.&lt;/p&gt;
&lt;h3 id="effektivnost-ispolzovaniia-vychislitelnykh-resursov"&gt;Эффективность использования вычислительных ресурсов&lt;/h3&gt;
&lt;p&gt;ResourceManager использует общую концепцию для управления ресурсами и
планирования по отношению к каждому конкретному приложению. Каждая
машина в кластере на концептуальном уровне рассматривается просто как
набор ресурсов: память, процессор, ввод-вывод и др. Все машины
взаимозаменяемы и приложение может быть назначено на любую из них,
основываясь на доступных и запрашиваемых ресурсах. При этом приложения
работают в контейнерах, изолированно от других приложений, что дает
сильную поддержку многозадачности.&lt;/p&gt;
&lt;p&gt;Таким образом эта схема избавляет от текущего механизма map и reduce
слотов в Hadoop, который негативно влияет на эффективную утилизацию
вычислительных ресурсов.&lt;/p&gt;
&lt;h3 id="podderzhka-drugikh-paradigm-programmirovaniia-pomimo-mapreduce"&gt;Поддержка других парадигм программирования помимо MapReduce&lt;/h3&gt;
&lt;p&gt;В предложенной архитектуре используется общий механизм вычислений, не
привязанный конкретно к MapReduce, что позволит использовать и другие
парадигмы. Имеется возможность реализовать собственный
ApplicationMaster, способный запрашивать ресурсы у ResourceManager и
использовать их в соответствии с задачей, при этом сохраняются общие
принципы изоляции и гарантированного наличия полученных ресурсов. Среди
потенциально поддерживаемых парадигм можно назвать MapReduce, MPI,
Мaster-Worker, итеративные модели. Все они могут одновременно работать
на одном и том же кластере. Это особенно актуально для приложений
(например К-средний или Page Rank), где &lt;a href="https://www.insight-it.ru/python/2011/piccolo-postroenie-raspredelennykh-sistem-v-11-raz-bystree-hadoop/"&gt;другие подходы более чем на порядок эффективнее&lt;/a&gt; MapReduce.&lt;/p&gt;
&lt;h2 id="vyvody_1"&gt;Выводы&lt;/h2&gt;
&lt;p&gt;Apache Hadoop, и в частности Hadoop MapReduce - очень успешный
opensource проект по обработке больших объемов данных. Предложенный
Yahoo путь его переработки направлен на исправление недостатков
архитектуры текущей реализации, при этом повышая доступность,
эффективность использования ресурсов и предоставляя поддержку других
парадигм распределенных вычислений.&lt;/p&gt;
&lt;p&gt;Осталось дело за малым - собственно реализовать задуманное! :)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/336fc03c/" rel="nofollow" target="_blank" title="http://developer.yahoo.com/blogs/hadoop/posts/2011/02/mapreduce-nextgen/"&gt;Источник информации&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="/feed/"&gt;Подписаться на RSS можно здесь.&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 19 Feb 2011 21:23:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-02-19:storage/2011/novoe-pokolenie-mapreduce-v-apache-hadoop/</guid><category>Apache</category><category>Apache Hadoop</category><category>Hadoop</category><category>архитектура</category><category>кластер</category><category>кластеризация</category><category>кластерные вычисления</category><category>Масштабируемость</category><category>разработка</category><category>технологии</category></item><item><title>Архитектура Plenty of Fish</title><link>https://www.insight-it.ru//highload/2010/arkhitektura-plenty-of-fish/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/fa2360fd/" rel="nofollow" target="_blank" title="http://www.plentyoffish.com/"&gt;Plenty of Fish&lt;/a&gt; представляет собой очень
популярный сервис онлайн знакомств, насчитывающий более 45 миллионов
посетителей в месяц и 30+ миллионов просмотров страниц в сутки (что
составляет около 500-600 страниц в секунду). Но это не самая интересная
часть истории... Все это управляется единственным человеком при
использовании нескольких серверов, при этом он тратит на работу всего
пару часов в день и зарабатывает 6 миллионов долларов на рекламе от
Google. Завидуете? Я тоже :) Как же ему удалось соединить столько
влюбленных пар, используя так мало ресурсов?&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Данный пост является переводом &lt;a href="https://www.insight-it.ru/goto/e06f86e0/" rel="nofollow" target="_blank" title="http://highscalability.com/plentyoffish-architecture"&gt;англоязычной
статьи&lt;/a&gt;, автор
оригинала: Todd Hoff.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/35763c73/" rel="nofollow" target="_blank" title="http://channel9.msdn.com/ShowPost.aspx?PostID=331501#331501"&gt;Channel9 интервью с Markus Frind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/fa591b8c/" rel="nofollow" target="_blank" title="http://plentyoffish.wordpress.com/%20target="&gt;Блог Markus Frind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/ed31cb93/" rel="nofollow" target="_blank" title="http://www.readwriteweb.com/archives/plentyoffish_one_billion.php"&gt;Plentyoffish: компания одного человека может стоить 1 миллиард
долларов&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Microsoft Windows&lt;/li&gt;
&lt;li&gt;ASP.NET&lt;/li&gt;
&lt;li&gt;IIS&lt;/li&gt;
&lt;li&gt;Akamai CDN&lt;/li&gt;
&lt;li&gt;Foundry ServerIron Load Balancer&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;PlentyOfFish (POF) имеет 1.2 миллиарда просмотров страниц в месяц, в
среднем 500 тысяч уникальных авторизованных пользователей в день.
Пиковый сезон приходится на январь каждого года, когда эти цифры
возрастают на 30%.&lt;/li&gt;
&lt;li&gt;POF имеет единственного сотрудника: создатель и генеральный директор
Markus Frind.&lt;/li&gt;
&lt;li&gt;Зарабатывает до 10 миллионов долларов в год на рекламе от Google,
работает при этом только около двух часов в день.&lt;/li&gt;
&lt;li&gt;30+ миллионов просмотров страниц в день (500 - 600 страниц в секунду).&lt;/li&gt;
&lt;li&gt;1.2 миллиарда просмотров страниц и 45 миллионов посетителей в месяц.&lt;/li&gt;
&lt;li&gt;Имеет &lt;a href="https://www.insight-it.ru/goto/ca40875e/" rel="nofollow" target="_blank" title="http://ru.wikipedia.org/wiki/CTR_(%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%BD%D0%B5%D1%82)"&gt;CTR&lt;/a&gt; в 5-10 раз выше, чем Facebook.&lt;/li&gt;
&lt;li&gt;Находится в top 30 сайтов США по данным Competes Attention, top 10 в Канаде и top 30 в Великобритании.&lt;/li&gt;
&lt;li&gt;Нагрузка балансируется между двумя веб-серверами с 2 Quad Core Intel
Xeon X5355 @ 2.66Ghz, 8GB RAM (используется около 800 MB), 2 жесткими
дисками, работают под управлением Windows x64 Server 2003.&lt;/li&gt;
&lt;li&gt;3 сервера баз данных. Информация об их конфигурации не предоставляется.&lt;/li&gt;
&lt;li&gt;Приближается к 64000 одновременных соединений и 2 миллионам просмотрам
страниц в час.&lt;/li&gt;
&lt;li&gt;Интернет-канал в 1Gbps, из которых используется только 200Mbps.&lt;/li&gt;
&lt;li&gt;1 TB трафика от отдачи 171 миллионов изображений через Akamai.&lt;/li&gt;
&lt;li&gt;6TB система хранения данных для обработки миллионов полноразмерных
изображений, которые загружаются на сайт каждый месяц.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="chto-vnutri"&gt;Что внутри?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Модель монетизации заключалась в использовании рекламы от Google.
Match.com, для сравнения, получает 300 миллионов долларов в год, в
основном с платных подписок. Источник дохода POF должен измениться,
чтобы позволить ему получать больше выручки от имеющихся пользователей.
Планируется нанять больше сотрудников, в частности людей, которые будут
заниматься продажей рекламы напрямую вместо того, чтобы полностью
полагаться на AdSense.&lt;/li&gt;
&lt;li&gt;При 30 миллионах просмотрах страниц в день можно зарабатывать неплохие
деньги на рекламе, даже
если&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/b1c7d720/" rel="nofollow" target="_blank" title="http://en.wikipedia.org/wiki/Cost_per_mille"&gt;CPM&lt;/a&gt; будет всего 5-10
центов.&lt;/li&gt;
&lt;li&gt;Akamai используется для отдачи более 100 миллионов изображений в день.
Если на странице 8 изображений и каждое загружается за 100 миллисекунд -
их загрузка займет почти секунду, так что распределение изображений
целесообразно.&lt;/li&gt;
&lt;li&gt;Десятки миллионов изображений отдаются с серверов POF, но большинство из
них размером меньше 2KB и практически полностью закешированы в
оперативной памяти.&lt;/li&gt;
&lt;li&gt;Все динамично. Практически никакой статики.&lt;/li&gt;
&lt;li&gt;Все исходящие данные сжимаются с использованием Gzip, что обходится
всего 30% использованием процессорного времени. Используется много
вычислительных ресурсов, но зато существенно сокращается использование
пропускной способности интернет-канала.&lt;/li&gt;
&lt;li&gt;Кэширование ASP .NET не используется, так как данные теряют свою
актуальность практически сразу же.&lt;/li&gt;
&lt;li&gt;Встроенные компоненты ASP также не используется. Почти все написано с
чистого листа. Ничего не может быть более сложным, чем кучка простых
if-then-else и циклов. Все максимально элементарно.&lt;/li&gt;
&lt;li&gt;Балансировка нагрузки:&lt;/li&gt;
&lt;li&gt;IIS произвольно ограничивает общее количество соединений до 64000,
таким образом балансировщик нагрузки был добавлен для обработки большего
количества одновременных соединений. Вариант с добавлением второго IP
адреса и использованием round robin DNS также рассматривался, но вариант
с балансировщиком нагрузки выглядел более избыточным и позволял более
легко расширять количество серверов. Помимо этого ServerIron позволял
использовать более продвинутую функциональность, вроде блокировки ботов
и балансировку запросов по cookies, сессиям или IP-адресам
пользователей.&lt;/li&gt;
&lt;li&gt;Windows Network Load Balancing (NLB) функция не использовалась, так
как не поддерживает привязку сессий к серверам. Обходным путем было бы
хранение сессионных данных в базе данных или общей файловой системе.&lt;/li&gt;
&lt;li&gt;8-12 NLB серверов могут объединяться в кластер и может использоваться
неограниченное количество таких кластеров. Схема DNS round robin может
использоваться для распределения запросов между кластерами. Теоретически
такая архитектура могла бы позволить 70 веб-серверам обрабатывать более&amp;nbsp;300 тысяч одновременных соединений.&lt;/li&gt;
&lt;li&gt;NLB имеет опцию для отправки каждого пользователя на конкретный
сервер, таким образом не используется внешнее хранилище для сессионных
данных и если сервер выходит из строя - пользователи просто
разлогиниваются из системы. Если это состояние включает в себя например
корзину интернет-магазина или какую-то другую важную информацию, то
такой подход мог бы показаться&amp;nbsp;неприемлемым, но для сайта знакомств это
было бы не так критично.&lt;/li&gt;
&lt;li&gt;Было решено, что хранение и получение сессионных данных программными
средствами слишком дорого. Аппаратная балансировка нагрузка проще:
пользователи просто назначаются конкретным серверам и в случае сбоя
сервера назначенным ему пользователям предлагается пройти процесс
авторизации еще раз.&lt;/li&gt;
&lt;li&gt;Покупка ServerIron была дешевле и проще, чем использование NLB.
Многие крупные сайты используют их для создания пулов TCP соединений,
автоматическому определению ботов и так далее. ServerIron может делать
намного больше, чем просто балансировать нагрузку и такие функции
достаточно привлекательные за эту цену.&lt;/li&gt;
&lt;li&gt;Была большая проблема с выбором системы размещения рекламы. Многие из
них хотели несколько сотен тысяч в год и многолетний контракт.&lt;/li&gt;
&lt;li&gt;В процессе избавления от ASP.NET повторителей и использование взамен
конкатенации строк или response.write. Если у вас миллионы просмотров
страниц в день - просто напишите весь код для отображения на экране
пользователя.&lt;/li&gt;
&lt;li&gt;Большинство изначальных вложений ушло на построение SAN. Избыточность
любой ценой.&lt;/li&gt;
&lt;li&gt;Рост был за счет вирусного эффекта. Портал начал набирать популярность в
Канаде, затем о нем узнали в Великобритании и Австралии, и только потом
в США.&lt;/li&gt;
&lt;li&gt;База данных:&lt;/li&gt;
&lt;li&gt;Одна база данных является основной.&lt;/li&gt;
&lt;li&gt;Две базы данных для поиска. Поисковые запросы распределяются по их типу.&lt;/li&gt;
&lt;li&gt;Производительность наблюдается через диспетчер задач. Когда
появляются пики - ситуация рассматривается более детально. Проблемы
обычно заключались в блокировках на уровне СУБД. Собственно говоря почти
всегда это были проблемы с базами данных, очень редко они возникают на
уровне .NET. Так как POF не использует библиотеки .NET, отследить
проблемы с производительностью оказывается достаточно просто. Если бы
использовалось много уровней framework'ов, поиск мест, где скрываются
проблемы, был бы трудным и утомляющим.&lt;/li&gt;
&lt;li&gt;Если Вы делаете запрос к базе данных 20 раз при отображении одной
страницы, &amp;nbsp;Вы проиграли в любом случае, вне зависимости от того, что Вы
будете делать.&lt;/li&gt;
&lt;li&gt;Разделяйте запросы чтения и записи к базе данных. Если у вас нет
избыточного количества оперативной памяти не следование этому правилу
может заставить систему зависнуть на несколько секунд.&lt;/li&gt;
&lt;li&gt;Постарайтесь делать базы данных только для чтения.&lt;/li&gt;
&lt;li&gt;Денормализуйте данные. Если Вам приходится доставать данные из 20
разных таблиц, попробуйте сделать просто одну таблицу, где будут лежать
все нужные для чтения данные.&lt;/li&gt;
&lt;li&gt;Один день может проработать почти что угодно, но когда Ваша база
данных удвоится - использованные подход может внезапно перестать
работать.&lt;/li&gt;
&lt;li&gt;Если система делает только что-то одно, она будет делать это реально
хорошо. Только записывайте данные и все будет нормально. Только читайте
данные и все будет нормально. Делайте и то и другое - и все испортится.
База данных погрязнет в проблемах с блокировками.&lt;/li&gt;
&lt;li&gt;Если Вы полностью используете вычислительные мощности, Вы либо
делаете что-то не так, либо Ваша система на самом деле очень
оптимизирована. Если вы можете разместить всю базу в оперативной
памяти - обязательно делайте это.&lt;/li&gt;
&lt;li&gt;Процесс разработки выглядит примерно следующим образом: появляется идея,
быстро реализуется и выдается пользователям в пределах 24 часов. Отклик
от пользователей получается по слежению за тем, что они делают на сайте:
выросло количество сообщений на пользователя? среднее время сессий
выросло? Если пользователям новая фишка не пришлась по вкусу - просто
уберите её.&lt;/li&gt;
&lt;li&gt;При небольшом количестве серверов системные сбои достаточно редки и
краткосрочны. Наибольшими сложностями были проблемы с DNS, когда
некоторые интернет-провайдеры говорили, что POF больше не существует. Но
так как сайт бесплатен, пользователи нормально относятся к небольшим
периодам его недоступности. Люди часто не замечают простой сайта, так
как думают, что это какая-то проблема у них, с интернет-соединением или
еще чем-то.&lt;/li&gt;
&lt;li&gt;Переход от миллиона пользователей к 12 миллионам пользователей был
большим прыжком. Система может обслуживать и 60 миллионов пользователей
с двумя веб-серверами.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Часто смотрите на конкурентов для идей новых функциональных
возможностей.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Рассмотрите использование чего-то вроде S3, когда система начнет
требовать географической балансировки.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Вам не нужны миллионы в финансировании, размашистая инфраструктура и
целое здание сотрудников для того, чтобы создать вебсайт мирового
уровня, который обслуживает кучу пользователей и приносит неплохие
деньги. Все что нужно - всего лишь привлекательная идея, которая
понравится большому количеству идей, сайт, который становится популярным
благодаря слухам, а также опыт и видение для построения сайта, не
наступая на типичные "грабли". Вот и все, что Вам нужно :-)&lt;/li&gt;
&lt;li&gt;Необходимость - мать всех изменений.&lt;/li&gt;
&lt;li&gt;Когда вы растете быстро, но не слишком быстро, у Вас появляется шанс
расти, модифицировать и адаптироваться.&lt;/li&gt;
&lt;li&gt;Максимальное использование оперативной памяти решает массу проблем.
После этого рост возможен просто за счет использование более мощных
серверов.&lt;/li&gt;
&lt;li&gt;В начале старайтесь держать все максимально простым. Практически все
дают этот же самый совет, а Markus говорит, что все что он делает -
всего лишь очевидный здравый смысл. Но то что просто, не всегда означает
всего лишь осмысленную вещь. Создание простых вещей является результатом
многих лет практического опыта.&lt;/li&gt;
&lt;li&gt;Поддерживайте время доступа к базе данных быстрым и у Вас не будет
проблем.&lt;/li&gt;
&lt;li&gt;Одной из основных причин, по которой POF может работать с таким
небольшим количеством сотрудников и оборудования, является использование
CDN для отдачи активно используемого контента. Использование CDN может
оказаться секретным соусом для многих крупных сайтов. Markus считает,
что в top 100 не существует ни одного сайта, не использующего CDN. Без
CDN время загрузки страницы в Австралии возросло бы до 3-4 секунд только
за счет изображений.&lt;/li&gt;
&lt;li&gt;Реклама на Facebook принесла плохие результаты. Из 2000 кликов только 1
человек регистрировался. С CTR равным 0.04% Facebook выдавал 0.4 клика
на 1000 показов рекламы (CPM). При 5 центах CPM = 12.5 центов за клик,
50 центах CPM = 1.25\$ за клик. 1 доллар CPM = 2.50\$ за клик. 15\$ CPM
= 37.50\$ за клик.&lt;/li&gt;
&lt;li&gt;Это просто продавать несколько миллионов просмотров страниц с высоким
CPM, но НАМНОГО сложнее продавать миллиарды просмотров с высоким CPM,
как это делают Myspace и Facebook.&lt;/li&gt;
&lt;li&gt;Модель монетизации, основанная на рекламе, ограничивает Ваши доходы. Вам
придется переходить к платной модели чтобы повышать прибыль.
Генерировать 100 миллионов долларов в год за счет бесплатного сайта
практически невозможно - Вам потребуется слишком большой рынок.&lt;/li&gt;
&lt;li&gt;Повышение количества просмотров за счет Facebook не работает для сайтов
знакомств. Иметь посетителя на собственном сайте намного более
прибыльно. Большинство просмотров страниц на Facebook находятся за
пределами США и Вам придется делить 5 центов CPM с Facebook.&lt;/li&gt;
&lt;li&gt;Предложение пользователям при регистрации получить информацию об ипотеке
или каком-то другом продукте, может стать неплохим источником
дополнительной выручки.&lt;/li&gt;
&lt;li&gt;Вы не можете постоянно прислушиваться к отзывам пользователей. Кому-то
всегда будут нравиться новые функции, а кто-то всегда будет их
ненавидеть, но только часть из них сообщит Вам об этом. Вместо этого
лучше смотреть как новые функции влияют на то, чем люди на самом деле
занимаются, просто смотря на Ваш сайт и статистику его использования.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 18 Jan 2010 16:43:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-01-18:highload/2010/arkhitektura-plenty-of-fish/</guid><category>Akamai CDN</category><category>ASP</category><category>ASP .NET</category><category>dating</category><category>Foundry</category><category>IIS</category><category>Microsoft</category><category>online</category><category>Plenty of Fish</category><category>POF</category><category>ServerIron</category><category>Windows</category><category>Windows Server</category><category>архитектура</category><category>Архитектура Plenty of Fish</category><category>Масштабируемость</category><category>сайт знакомств</category></item><item><title>Архитектура Stack Overflow</title><link>https://www.insight-it.ru//highload/2010/arkhitektura-stack-overflow/</link><description>&lt;p&gt;&lt;img alt="Stack Overflow" class="right" src="https://www.insight-it.ru/images/stack-overflow-logo.png" title="Stack Overflow"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/33fc61d9/" rel="nofollow" target="_blank" title="https://stackoverflow.com/"&gt;Stack Overflow&lt;/a&gt; является любимым многими
программистами сайтом, где можно задать профессиональный вопрос и
получить ответы от коллег. Этот проект был написан двумя никому не
известными парнями, о которых никто никогда раньше не слышал. Хорошо, не
совсем так. Stack Overflow был создан топовыми программистами и звездами
блогосферы:&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/374a081/" rel="nofollow" target="_blank" title="http://www.codinghorror.com/blog/"&gt;Jeff Atwood&lt;/a&gt; и&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/31657700/" rel="nofollow" target="_blank" title="http://www.joelonsoftware.com/"&gt;Joel Spolsky&lt;/a&gt;. В этом отношении Stack
Overflow похож на ресторан, владельцами которого являются знаменитости.
По оценкам Joel'а около 1/3 программистов всего мира использовали этот
интернет-ресурс, так что должно быть он представляет собой что-то
достаточно полезное и интересное.&lt;/p&gt;
&lt;p&gt;Одним из ключевых моментов в истории Stack Overflow является
использование вертикального масштабирования, как достаточно
работоспособного решения достаточного большого класса проблем. Не смотря
на то, что публика на сегодняшний день больше склоняется к подходу с
использованием горизонтальным масштабирования и&amp;nbsp;не-SQL баз данных.&lt;/p&gt;
&lt;p&gt;Если Вы стремитесь к масштабу Google, у Вас нет другого выхода, как
двигаться в направлении не-SQL. Но Stack Overflow - это не Google, ровно
как и подавляющее большинство других сайтов. Когда Вы задумываетесь о
возможных вариантов дизайна Вашего проекта, попробуйте учесть и историю
Stack Overflow, она тоже имеет право на жизнь. В этот век многоядерных
машин с большим объемом оперативной памяти и невероятными темпами
развития методов параллельного программирования, вертикальное
масштабирование все еще является жизнеспособной стратегией и не должна
сразу же отбрасываться в сторону просто так как это теперь больше не
модно. Возможно в один прекрасный день мы получим лучшее из обоих миров,
но на сегодняшний момент перед нами лежит большой болезненный выбор
стратегии масштабирования, от которого определенно зависит судьба Вашего
проекта.&lt;/p&gt;
&lt;p&gt;Joel любит похвастаться тем, что они достигли производительности,
сравнимой с другими сайтами аналогичных размеров, используя в 10 раз
меньше оборудования. Он удивляется, работали над этими сайтами
по-настоящему хорошие программисты. Давайте взглянем на то, как им это
удалось, и дадим Вам возможность побыть судьей.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;&lt;em&gt;Перевод &lt;a href="https://www.insight-it.ru/goto/98b904e0/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2009/8/5/stack-overflow-architecture.html"&gt;статьи&lt;/a&gt;, автор оригинала - Todd Hoff. Возможно будет еще один пост с менее формальной информацией на ту же тему.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;16 миллионов просмотров страниц в месяц&lt;/li&gt;
&lt;li&gt;3 миллионов уникальных пользователей в месяц (для сравнения: Facebook
насчитывает около 77 миллионов уникальных пользователей в месяц)&lt;/li&gt;
&lt;li&gt;6 миллионов посещений в месяц&lt;/li&gt;
&lt;li&gt;86% трафика приходит с Google&lt;/li&gt;
&lt;li&gt;9 миллионов активных программистов во всем мире и 30% пользуются Stack
Overflow&lt;/li&gt;
&lt;li&gt;Более дешевые лицензии были получены через программу
Microsoft&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/205088ae/" rel="nofollow" target="_blank" title="http://www.microsoft.com/BizSpark"&gt;BizSpark&lt;/a&gt;. Скорее всего
они заплатили около 11000\$ за лицензии на ОС и MSSQL.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Стратегия монетизации: ненавязчивая реклама, вакансии, конференции
DevDays, достижения других смежных ниш (Server Fault, Super User),
разработка&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/631e88c2/" rel="nofollow" target="_blank" title="https://stackexchange.com/"&gt;StackExchange&lt;/a&gt; и возможно
каких-то других систем рейтингов для программистов.&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Microsoft ASP.NET MVC&lt;/li&gt;
&lt;li&gt;SQL Server 2008&lt;/li&gt;
&lt;li&gt;C#&lt;/li&gt;
&lt;li&gt;Visual Studio 2008 Team Suite&lt;/li&gt;
&lt;li&gt;jQuery&lt;/li&gt;
&lt;li&gt;LINQ to SQL&lt;/li&gt;
&lt;li&gt;Subversion&lt;/li&gt;
&lt;li&gt;Beyond Compare 3&lt;/li&gt;
&lt;li&gt;VisualSVN 1.5&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Веб уровень:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 x Lenovo ThinkServer RS110 1U&lt;/li&gt;
&lt;li&gt;4 ядра, 2.83 Ghz, 12 MB L2 cache&lt;/li&gt;
&lt;li&gt;500 GB жесткие диски, зеркалирование RAID1&lt;/li&gt;
&lt;li&gt;8 GB RAM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Уровень базы данных:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 x Lenovo ThinkServer RD120 2U&lt;/li&gt;
&lt;li&gt;8 ядер, 2.5 Ghz, 24 MB L2 cache&lt;/li&gt;
&lt;li&gt;48 GB RAM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Четвертый сервер был добавлен для запуска
&lt;a href="https://www.insight-it.ru/goto/d3829019/" rel="nofollow" target="_blank" title="https://superuser.com/"&gt;superuser.com&lt;/a&gt;. Все сервера вместе обеспечивают
работу &lt;a href="https://www.insight-it.ru/goto/33fc61d9/" rel="nofollow" target="_blank" title="https://stackoverflow.com/"&gt;Stack Overflow&lt;/a&gt;,&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/9b0a2d16/" rel="nofollow" target="_blank" title="https://serverfault.com/"&gt;Server
Fault&lt;/a&gt;, и&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/d3829019/" rel="nofollow" target="_blank" title="https://superuser.com/"&gt;Super User&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;QNAP TS-409U NAS для резервного копирования данных. Было принято решение
не использовать "облачные" решения, так как вызванные ими дополнительные
5GB трафика ежедневно были бы накладными.&lt;/li&gt;
&lt;li&gt;Сервера располагаются у&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/4f3f2e08/" rel="nofollow" target="_blank" title="http://www.peakinternet.com/"&gt;Peak Internet&lt;/a&gt;. В
основном из-за впечатляющей детализации технических ответов и разумных
расценок.&lt;/li&gt;
&lt;li&gt;Полнотекстный поиск в SQL Server активно используется для реализации
поиска по сайту и выявления повторных вопросов. Lucene .NET
рассматривается как достаточно заманчивая альтернатива.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;p&gt;Данный список является сборником уроков от Jeff и Joel, а также из
комментариев к их записям:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Если Вы комфортно себя чувствуете в деле управления серверами - не
бойтесь покупать их. Две основных проблемы с издержками аренды
оборудования:&lt;ol&gt;
&lt;li&gt;невероятные цены на дополнительную оперативную память и
жесткие диски;&lt;/li&gt;
&lt;li&gt;хостинг-провайдеры на самом деле не могут управлять
чем-либо за Вас.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Делайте одноразовые более крупные инвестиции в оборудование, чтобы
избежать быстро растущих ежемесячных издержек по аренде, которые
окажутся более высокими в долгосрочном периоде.&lt;/li&gt;
&lt;li&gt;Обновляйте сетевые драйвера. Производительность запросто может
удвоиться.&lt;/li&gt;
&lt;li&gt;Использование 48GB RAM требует обновления до MS Enterprise edition.&lt;/li&gt;
&lt;li&gt;Оперативная память невероятно дешевая. Используйте возможности по её
расширению по максимуму для получения практически бесплатной
производительности. У Dell, например, переход от 4GB памяти до 128GB
стоит всего 4378\$.&lt;/li&gt;
&lt;li&gt;Stack Overflow скопировали ключевую часть структуры базы данных у
Wikipedia. Это обернулось огромной ошибкой, для исправления которой
потребуется большой и болезненный рефакторинг базы данных. Основным
направлением изменений будет избавление от излишних операций по
объединению данных в большом количестве ключевых запросов. Это ключевой
урок, который стоит усвоить у гигантских много-терабайтных схем (вроде
Google BigTable), которые полностью избавлены от операций объединения
данных. Этот вопрос был достаточно важен для Stack Overflow, так как их
база данных практически полностью располагается в оперативной памяти и
операции join по прежнему требуют относительно много вычислительных
ресурсов.&lt;/li&gt;
&lt;li&gt;Производительность CPU оказывается на удивление важным фактором для
серверов баз данных. Переход от 1.86 GHz, к 2.5 GHz, и к 3.5 GHz
процессорам дает практически линейный прирост к времени выполнения
типичных запросов. Исключение: запросы, которые затрагивают не только
оперативную память.&lt;/li&gt;
&lt;li&gt;Когда оборудование арендуется, обычно никто не платит за дополнительную
оперативную память, если только вы не на помесячном контракте.&lt;/li&gt;
&lt;li&gt;В 90% случаев наиболее узким местом является база данных.&lt;/li&gt;
&lt;li&gt;При небольшом количестве серверов, &amp;nbsp;ключевым компонентом издержек
становится не место в стойках, электроэнергия, интернет-канал, сервера
или программное обеспечение, а СЕТЕВОЕ ОБОРУДОВАНИЕ. Вам потребуется как
минимум гигабитное соединение между уровнями веб-серверов и баз данных.
Между интернетом и веб-серверами потребуется firewall, маршрутизатор и
VPN. К моменту добавления второго веб-сервера понадобится решение для
балансировки нагрузки. Суммарная стоимость такого оборудования может
запросто вдвое превосходить стоимость пяти серверов.&lt;/li&gt;
&lt;li&gt;EC2 предназначен для горизонтального масштабирования, для того чтобы
нагрузка могла быть распределена между большим количеством машин
(достаточно хорошая идея, если Вы планируете расширяться). Еще больше
смысла в таком подходе появляется, если вы планируете масштабироваться
по необходимости (то есть добавлять и убирать машины в зависимости от
уровня нагрузки).&lt;/li&gt;
&lt;li&gt;Горизонтальное масштабирование может проходить относительно
безболезненно только при использовании open source программного
обеспечения. В противном случае вертикальное масштабирование значит
сокращение издержек, связанных с лицензиями, в ущерб стоимости
оборудования, а горизонтальное масштабирование - наоборот: экономия на
оборудовании, но требуется существенно больше лицензий на программное
обеспечение.&lt;/li&gt;
&lt;li&gt;RAID-10 отлично работает для баз данных с высокой нагрузкой операций
чтения и записи.&lt;/li&gt;
&lt;li&gt;Разделяйте работу приложений и баз данных таким образом, чтобы они могли
масштабироваться независимо друг от друга. Например, базы данных могут
масштабироваться вертикально, а сервера приложений - горизонтально.&lt;/li&gt;
&lt;li&gt;Приложения должны хранить все информацию о своем состоянии в базе данных
для обеспечения возможности роста путем простого добавления серверов
приложений в кластер.&lt;/li&gt;
&lt;li&gt;Одна из основных проблем со стратегией вертикального масштабирования -
недостаток избыточности. Кластеризация добавляет надежности, но когда
стоимость каждого сервера высока - это не так просто реализовать.&lt;/li&gt;
&lt;li&gt;Некоторые приложения могут масштабироваться линейно относительно числа
процессоров. Но зачастую будут использоваться механизмы блокировки, что
приведет к сериализации вычислений и в итоге к существенному уменьшению
эффективности приложения.&lt;/li&gt;
&lt;li&gt;С более крупными серверами, занимающими от 7U в стойке, электроэнергия и охлаждение становятся критичными вопросами. Возможно использование
чего-то среднего между 1U и 7U может облегчить Ваши взаимоотношения с
датацентром.&lt;/li&gt;
&lt;li&gt;С добавлением все новых и новых серверов баз данных издержки на лицензии SQL Server могут стать очень существенными. Если Вы начнете с
вертикального масштабирования и постепенно начнете переходить к
горизонтальному с использованием не open source продуктов, возможно это
сильно ударит по Вашему финансовому состоянию. Это справедливо, что в
этой заметке речь идет не совсем об архитектуре проекта. Мы знаем об их
серверах, об используемом наборе инструментов, об их двухуровневой
схеме, где база данных используется напрямую из кода веб-серверов. Но мы
не знаем практически ничего о самой реализации, например таких мелочей
как теги. Если Вам интересен этот вопрос, возможно Вам удастся получить
интересующую Вас информацию из &lt;a href="https://www.insight-it.ru/goto/61237733/" rel="nofollow" target="_blank" title="http://sqlserverpedia.com/wiki/Understanding_the_StackOverflow_Database_Schema"&gt;описания их схемы базы данных&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 08 Jan 2010 00:31:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-01-08:highload/2010/arkhitektura-stack-overflow/</guid><category>ASP</category><category>ASP .NET</category><category>Beyond Compare 3</category><category>C++</category><category>highload</category><category>JQuery</category><category>Lenovo</category><category>Lenovo ThinkServer</category><category>LINQ</category><category>Microsoft</category><category>MSSQL</category><category>MVC</category><category>Server Fault</category><category>SQL Server 2008</category><category>Stack Overflow</category><category>Subversion</category><category>Super User</category><category>Visual Studio 2008 Team Suite</category><category>VisualSVN</category><category>архитектура</category><category>архитектура Stack Overflow</category><category>архитектура высоконагруженных сайтов</category><category>Масштабируемость</category></item><item><title>Архитектура MySpace</title><link>https://www.insight-it.ru//highload/2009/arkhitektura-myspace/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/f989d588/" rel="nofollow" target="_blank" title="http://www.myspace.com"&gt;MySpace.com&lt;/a&gt; является одним из наиболее быстро
набирающих популярность сайтов в Интернете с 65 миллионами пользователей
и 260000 регистрациями в день. Этот сайт часто подвергается критике
из-за не достаточной производительности, хотя на самом деле MySpace
удалось избежать ряда проблем с масштабируемостью, с которыми
большинство других сайтов неизбежно сталкивались. Как же им это
удалось?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Данная статья является переводом статьи &lt;a href="https://www.insight-it.ru/goto/e9f0b809/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2009/2/12/myspace-architecture.html"&gt;MySpace
Architecture&lt;/a&gt;,
автором которой является Todd Hoff. Когда-то давно один из читателей
этого блога просил меня осветить и эту тему, тогда я так и не решился
из-за отсутствия моего личного интереса, но сейчас снова случайно
наткнулся на эту статью и подумал: а почему бы и нет?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/45549701/" rel="nofollow" target="_blank" title="http://www.infoq.com/news/2009/02/MySpace-Dan-Farino"&gt;Презентация: за сценой MySpace.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8d5c1d4d/" rel="nofollow" target="_blank" title="http://www.baselinemag.com/c/a/Projects-Networks-and-Storage/Inside-MySpacecom/"&gt;Внутри MySpace.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ASP .NET 2.0&lt;/li&gt;
&lt;li&gt;Windows&lt;/li&gt;
&lt;li&gt;IIS&lt;/li&gt;
&lt;li&gt;MSSQL Server&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="chto-vnutri"&gt;Что внутри?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;300 миллионов пользователей.&lt;/li&gt;
&lt;li&gt;Отдает 100Gbps в Интернет. 10Gbps из них является HTML контентом.&lt;/li&gt;
&lt;li&gt;4,500+ веб серверов со связкой: Windows 2003 / IIS 6.0 / ASP .NET.&lt;/li&gt;
&lt;li&gt;1,200+ кэширующих серверов, работающих на 64-bit Windows 2003. На
    каждом 16GB объектов находятся в кэше в оперативной памяти.&lt;/li&gt;
&lt;li&gt;500+ серверов баз данных, работающих на 64-bit Windows и SQL Server
    2005.&lt;/li&gt;
&lt;li&gt;MySpace обрабатывает 1.5 миллиарда просмотров страниц в день, а
    также 2.3 миллионов одновременно работающих пользователей в течении
    дня.&lt;/li&gt;
&lt;li&gt;Вехи по количеству пользователей:&lt;ul&gt;
&lt;li&gt;500 тысяч пользователей: простая архитектура перестает
справляться&lt;/li&gt;
&lt;li&gt;1 миллион пользователей: вертикальное партиционирование временно
спасает от основных болезненных вопросов с масштабированием&lt;/li&gt;
&lt;li&gt;3 миллиона пользователей: горизонтальное масштабирование
побеждает над вертикальным&lt;/li&gt;
&lt;li&gt;9 миллионов пользователей: сайт мигрирует на ASP.NET, создается
виртуализированная система хранения данных (SAN)&lt;/li&gt;
&lt;li&gt;26 миллионов пользователей: MySpace переходит на 64-битную
технологию.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;500 тысяч учетных записей было многовато для двух веб-серверов и
    одного сервера баз данных.&lt;/li&gt;
&lt;li&gt;На 1-2 миллионах учетных записей:&lt;ul&gt;
&lt;li&gt;Они использовали архитектуру базы данных, построенную на
концепции вертикального партиционирования, с отдельными базами
данных для разных частей сайта, которые использовались для
выполнения различных функций, таких как экран авторизации, профили
пользователей и блоги.&lt;/li&gt;
&lt;li&gt;Схема с вертикальным партиционированием помогала разделить
нагрузку как для операций чтения, так и для операций записи, а если
пользователям в друг оказывалась нужна новая функциональная
возможность - достаточно было просто добавить еще один сервер баз
данных для её обслуживания.&lt;/li&gt;
&lt;li&gt;MySpace переходит от использования систем хранения, подключенных
к серверам баз данных напрямую, к сетям хранения данных (SAN), при
таком подходе целый массив систем хранения объединяется вместе
специализированной сетью с высокой пропускной способностью, и
сервера баз данных также получают доступ к хранилищам через эту
сеть. Переход к SAN оказал положительное влияние как на
производительность, так и на доступность и надежность системы.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;На 3 миллионах учетных записей:&lt;ul&gt;
&lt;li&gt;Решение с вертикальным партиционированием не протянуло долго, так
как им приходилось реплицировать какую-то часть информации (например
информацию об учетных записях) по всем вертикальным частям базы
данных. С таким большим количеством операций репликации данных один
узел даже при незначительном сбое мог существенно замедлить
обновление информации во всей системе.&lt;/li&gt;
&lt;li&gt;Индивидуальные приложения вроде блогов на под-секциях сайта
достаточно быстро стали слишком большими для нормальной работы с
единственным сервером базы данных&lt;/li&gt;
&lt;li&gt;Произведена реорганизация всех ключевых данных для более логичной
организации в единственную базу данных&lt;/li&gt;
&lt;li&gt;Пользователи были разбиты на группы по миллиону в каждой и каждая
такая группа была перемещена на отдельный SQL Server&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;9&amp;ndash;17 миллионов учетных записей:&lt;ul&gt;
&lt;li&gt;Переход на ASP .NET, который требовал меньше ресурсов по
сравнению с их предыдущим вариантом архитектуры. 150 серверов,
использовавших новый код могли обработать нагрузку, для которой
раньше требовалось 246 серверов.&lt;/li&gt;
&lt;li&gt;Снова пришлось столкнуться с узким местом в системе хранения
данных. Реализация SAN решило какую-то часть старых проблем с
производительностью, но на тот момент потребности сайта начали
периодически превосходить возможности SAN по пропускной способности
операций ввода-вывода - той скорости, с которой она может читать и
писать данные на дисковые массивы.&lt;/li&gt;
&lt;li&gt;Столкнулись с лимитом производительности при размещении миллиона
учетных записей на одном сервере, ресурсы некоторых серверов начали
исчерпываться.&lt;/li&gt;
&lt;li&gt;Переход к виртуальному хранилищу, где весь SAN рассматривается
как одно большое общее место для хранения данных, без необходимости
назначать конкретные диски для хранения данных определенной части
приложения. MySpace на данный момент работает со стандартизированным
оборудованием от достаточно нового вендора SAN - 3PARdata&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Был добавлен кэширующий уровень &amp;mdash; прослойка из специализированных
    серверов, расположенных между веб-серверами и серверами данных, чья
    единственная задача была захватывать копии часто запрашиваемых
    объектов с данными в памяти и отдавать их веб-серверам для
    минимизации количества поиска данных в СУБД.&lt;/li&gt;
&lt;li&gt;26 миллионов учетных записей:&lt;ul&gt;
&lt;li&gt;Переход на 64-битные сервера с SQL Server на правах решения
проблемы с недостатком оперативной памяти. С тех пор их стандартный
сервер баз данных оснащен 64 GB RAM.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Горизонтальная федерация баз данных&lt;/strong&gt;. Базы данных
    партиционируются в зависимости от своего назначения. У них есть базы
    данных с профилями, электронными сообщениями и так далее. Каждая
    партиция основана на диапазоне пользователей. По миллиону в каждой
    базе данных. Таким образом, у них есть Profile1, Profile2 и все
    остальные базы данных вплоть до Profile300, если считать, что у них
    на данный момент зарегистрировано 300 миллионов учетных записей.&lt;/li&gt;
&lt;li&gt;Кэш ASP не используется, так как он не обеспечивает достаточного
    процента попаданий на веб серверах. Кэш, организованный как
    промежуточный слой, имеет существенно более высокое значение данного
    показателя.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Изоляция сбоев&lt;/strong&gt;. Внутри веб-сервера запросы сегментируются по
    базам данным. Разрешено использование только 7 потоков для работы с
    каждой базой данных. Таким образом, если база данных по каким-то
    причинам начинает работать медленно, только эти потоки замедлятся, в
    то время как остальные потоки будут успешно продолжать обрабатывать
    поток трафика.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="rabota-saita"&gt;Работа сайта&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Коллектор данных о производительности&lt;/strong&gt;. Централизованная система
    сбора информации о производительности через UDP. Такой подход более
    надежен, чем стандартный механизм Windows, а также позволяет любому
    клиенту подключиться и увидеть статистику.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Веб-система по просмотру дампов стеков процессов&lt;/strong&gt;. Можно просто
    сделать клик правой кнопкой мыши на проблемном сервере и увидеть
    дамп стека процессов, управляемых .NET. И это после привычки каждой
    раз удаленно подключаться к серверу, включать дебаггер и через
    полчаса получать свой ответ о том что же все таки происходит.
    Медленно, немасштабируемо и утомительно. Эта же система позволяет
    увидеть не просто стек процесса, но и предоставляет большое
    количество информации о контексте, в котором он работает.
    Обнаружение проблем намного проще при таком подходе, например можно
    легко увидеть, что база не отвечает, так как 90 ее потоков
    заблокировано.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Веб-система создания дампа heap-памяти&lt;/strong&gt;. Создает дамп всей
    выделенной памяти. Очень удобно и полезно для разработчиков.
    Сэкономьте часы на выполнение этой работы вручную.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Профайлер&lt;/strong&gt;. Прослеживает запрос от начала до конца и выводит
    подробный отчет. В нем можно увидеть URL, методы, статус, а также
    все, что поможет идентифицировать медленный запрос и его причины.
    Обнаруживает проблемы с блокировкой потоков, непредвиденными
    исключениями, другими словами все, что может оказаться интересным. В
    то же время остается очень легковесным решением. Работает на одной
    машине из каждой VIP (группа из 100 серверов) в production-среде.
    Опрашивает 1 поток каждые 10 секунд. Постоянно следит за системой в
    фоновом режиме.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Powershell&lt;/strong&gt;. Новая программная оболочка от Microsoft, которая
    работает в процессе и передаем объекты между командами вместо работы
    с текстовыми данными. MySpace разрабатывает множество так называемых
    commandlets'ов для поддержки различных операций.&lt;/li&gt;
&lt;li&gt;Разработана собственная технология асинхронной коммуникации для
    того, чтобы обойти проблемы с сетевыми проблемами Windows и работать
    с серверами как с группой. Например, она позволяет доставить файл
    .cs, скомпилировать его, запустить, и доставить результат обратно.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Развертывание&lt;/strong&gt;. Обновление кодовой базы происходит с помощью
    упомянутой выше собственной технологии. Ранее происходило до 5 таких
    обновлений в день, сейчас же они происходят лишь раз в неделю.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;С помощью стека Microsoft тоже можно делать большие веб-сайты.&lt;/li&gt;
&lt;li&gt;Стоит использовать кэширование с самого начала.&lt;/li&gt;
&lt;li&gt;Кэш является более подходящим местом для хранения временных данных,
    не требующих персистентности, например информации о пользовательских
    сессиях.&lt;/li&gt;
&lt;li&gt;Встроенные в операционные систему возможности, например по
    обнаружению DDoS-атака, могут приводить к необъяснимым сбоям.&lt;/li&gt;
&lt;li&gt;Храните свои данные в географически удаленных датацентрах для
    минимизации проблем, связанных со сбоями в электросети.&lt;/li&gt;
&lt;li&gt;Рассматривайте возможности использования виртуализированных систем
    хранения данных или кластерных файловых систем с самого начала. Это
    позволит существенно параллелизировать операции ввода-вывода, а
    также увеличивать дисковое пространство без необходимости какой-либо
    реорганизации.&lt;/li&gt;
&lt;li&gt;Разрабатывайте утилиты для работы с production окружением.
    Невозможно смоделировать все ситуации в тестовой среде.
    Масштабируемость и все различные варианты использования API не могут
    быть симулированы в процессе тестирования качества программного
    обеспечения. Обычные пользователи и хакеры обязательно найдут такие
    способы использования вашего продукта, о которых вы даже никогда и
    не подумаете в процессе тестирования, хотя конечно большая часть все
    же обнаружима в процессе QA тестирования.&lt;/li&gt;
&lt;li&gt;Когда это возможно - лучше просто использовать дополнительное
    оборудование для решения проблем. Это намного проще, чем изменять
    поведение программного обеспечения для того чтобы решать задачи
    как-то по-другому. Примером может служить добавление нового сервера
    на каждый миллион пользователей. Возможно было бы более эффективным
    изменить подход к самой работе с СУБД, но на практике все же проще и
    дешевле добавлять все новые и новые сервера. По крайней мере на
    данный момент.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 21 Dec 2009 16:15:00 +0300</pubDate><guid>tag:www.insight-it.ru,2009-12-21:highload/2009/arkhitektura-myspace/</guid><category>ASP</category><category>ASP .NET</category><category>highload</category><category>IIS</category><category>Microsoft</category><category>MSSQL</category><category>MySpace</category><category>myspace.com</category><category>online</category><category>Windows</category><category>Windows Server</category><category>архитектура</category><category>архитектура MySpace</category><category>Масштабируемость</category></item><item><title>Архитектура LinkedIn</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-linkedin/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/801d7bb4/" rel="nofollow" target="_blank" title="https://www.linkedin.com"&gt;LinkedIn&lt;/a&gt; является крупнейшей в мире
социальной сетью для профессионалов. Популярность этого проекта может
быть далека, от более общетематических социальных сетей, таких как,
скажем Facebook, но, тем не менее, нагрузка на серверную часть проекта
создается пользователями серьезная. О том как этот проект с ней
справляется и пойдет речь далее.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="predislovie"&gt;Предисловие&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Сообщение о публикации двух презентаций c JavaOne 2008 о LinkedIn и их
&lt;a href="https://www.insight-it.ru/goto/36e64126/" rel="nofollow" target="_blank" title="http://hurvitz.org/blog/2008/06/linkedin-architecture"&gt;обобщении&lt;/a&gt; от
Overn Hurvitz пронеслось по русскоязычным новостным ресурсам уже
достаточно давно, но время черкнуть пару строк обо всем этом нашлось у
меня только сейчас.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/aef5ee94/" rel="nofollow" target="_blank" title="http://www.slideshare.net/linkedin/linkedins-communication-architecture"&gt;LinkedIn - A Professional Social Network Built with Java&amp;trade; Technologies and Agile Practices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/515b891c/" rel="nofollow" target="_blank" title="http://www.slideshare.net/linkedin/linked-in-javaone-2008-tech-session-comm"&gt;LinkedIn Communication Architecture&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="statistika"&gt;Статистика&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;22 миллиона пользователей;&lt;/li&gt;
&lt;li&gt;4+ миллиона уникальных посетителей в день;&lt;/li&gt;
&lt;li&gt;40 миллионов просмотров страниц в день;&lt;/li&gt;
&lt;li&gt;2 миллиона поисковых запросов в день;&lt;/li&gt;
&lt;li&gt;ежедневно отправляются 250 тысяч приглашений;&lt;/li&gt;
&lt;li&gt;1 миллион ответов в день;&lt;/li&gt;
&lt;li&gt;2 миллиона электронных сообщений ежедневно.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/solaris/"&gt;Solaris&lt;/a&gt; (как x86, так и SPARC)&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/tomcat/"&gt;Tomcat&lt;/a&gt; и &lt;a href="/tag/jetty/"&gt;Jetty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/oracle/"&gt;Oracle&lt;/a&gt; и &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Никакого ORM&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/activemq/"&gt;ActiveMQ&lt;/a&gt; для JMS&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/lucene/"&gt;Lucene&lt;/a&gt; в качестве основы для поиска&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/spring/"&gt;Spring&lt;/a&gt; в роли "клея"&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="servernaia-arkhitektura"&gt;Серверная архитектура&lt;/h3&gt;
&lt;h4&gt;2003-2005&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;одно монолитное веб-приложение;&lt;/li&gt;
&lt;li&gt;одна общая база данных;&lt;/li&gt;
&lt;li&gt;сетевой граф кэшируется в памяти в "Облаке";&lt;/li&gt;
&lt;li&gt;поиск пользователей реализован с помощью &lt;a href="/tag/lucene/"&gt;Lucene&lt;/a&gt;, он
    работал на той же машине, что и "Облако", так как поиск был
    отфильтрован в соответствии с сетью пользователя, таким образом было
    удобно совмещать эти две функции на одной машине;&lt;/li&gt;
&lt;li&gt;веб-приложение напрямую обновляет базу данных, а она, в свою
    очередь, обновляет "Облако".&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2006&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Добавлена репликация для уменьшения нагрузки на основную базу
    данных. Реплики предоставляют данные в режиме "только для чтения", а
    репликация ведется в асинхронном режиме с помощью дополнительного
    компонента под названием Databus, с его появлением обновление данных
    стало выглядеть следующим образом:&lt;ul&gt;
&lt;li&gt;сначала какие-либо изменения происходят в веб-приложении;&lt;/li&gt;
&lt;li&gt;веб-приложение обновляет основную базу данных;&lt;/li&gt;
&lt;li&gt;она, в свою очередь, отправляет обновления на Databus;&lt;/li&gt;
&lt;li&gt;далее уже Databus обновляет: реплики, Облако и поисковый индекс.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Поиск был вынесен на отдельный сервер.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2008&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;веб-приложение само по себе практически ничего не делает: бизнес
    логика распределена по отдельным сервисам;&lt;/li&gt;
&lt;li&gt;веб-приложение все так же предоставляет пользователям графический
    интерфейс, но для его генерации она теперь вызывает сервисы;&lt;/li&gt;
&lt;li&gt;каждый сервис имеет свою специфическую базу данных (т.е.
    вертикальное сегментирование);&lt;/li&gt;
&lt;li&gt;такой подход позволяет другим приложениям (помимо основного)
    получать доступ к LinkedIn, такие приложения были созданы для
    работодателей, рекламных служб, и так далее.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="oblako"&gt;Облако&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;"Облаком" в LinkedIn называют сервер, который кэширует весь граф
    социальной сети в памяти;&lt;/li&gt;
&lt;li&gt;его размеры: 22 миллиона вершин и 120 миллионов ребер;&lt;/li&gt;
&lt;li&gt;занимает 12GB оперативной памяти;&lt;/li&gt;
&lt;li&gt;одновременно держится в памяти в 40 экземплярах;&lt;/li&gt;
&lt;li&gt;построение Облака из данных, в дисковой системе, занимает 8 часов;&lt;/li&gt;
&lt;li&gt;обновления происходят в режиме реального времени с помощью Databus;&lt;/li&gt;
&lt;li&gt;во время остановки данные записываются на диск;&lt;/li&gt;
&lt;li&gt;кэш реализован с помощью C++, а доступ предоставляется по JNI;&lt;/li&gt;
&lt;li&gt;они выбрали именно C++ так как требовалось использовать минимум
    оперативной памяти, а также, задержки, связанные с Garbage
    Collection, были неприемлемыми.&lt;/li&gt;
&lt;li&gt;размещение всех данных в памяти является ограничением, но, как
    удалось выяснить в LinkedIn, разбиение графов на части - не самая
    тривиальная задача.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Облако кэширует целиком весь граф социальной сети LinkedIn, но на
практике же пользователям требуется видеть его со своей точки зрения.
Данная задача является вычислительно сложной, по-этому она выполняется
лишь один раз при создании новой сессии, а затем система поддерживает
результат в кэше. Такой подход требует 2 MB оперативной памяти на
каждого активного пользователя. В течении сессии такой кэш обновляется
только если сам пользователь сделал какие-либо изменения в нем, если же
изменение вызвано другими пользователями - владелец сессии не заметит
изменений.&lt;/p&gt;
&lt;p&gt;Помимо этого используется кэширование профилей пользователей средствами
&lt;a href="/tag/ehcache/"&gt;EHcache&lt;/a&gt;. Одновременно в памяти хранится до 2 миллионов
профилей (из 22 миллионов). Изначально планировалось использовать
алгоритм &lt;abbr title="Least Frequently Used"&gt;LFU&lt;/abbr&gt;, но оказалось,
что иногда &lt;a href="/tag/ehcache/"&gt;EHcache&lt;/a&gt; зависал секунд на 30 во время
перерасчета &lt;abbr title="Least Frequently Used"&gt;LFU&lt;/abbr&gt;, таким
образом было принято решение о использовании вместо него алгоритма
&lt;abbr title="Least Recently Used"&gt;LRU&lt;/abbr&gt;.&lt;/p&gt;
&lt;h3 id="arkhitektura-kommunikatsii"&gt;Архитектура коммуникации&lt;/h3&gt;
&lt;p&gt;Как известно, пользователи практически любой социальной сети генерируют
огромное количество сообщений в единицу времени, причем каждый тип
сообщений обычно требует индивидуального подхода, но в целом их можно
разделить на две категории: постоянные и временные. В LinkedIn
разработчики построили по отдельному сервису, для обработки каждой из
этих категорий. Каждый из них определенно заслуживает отдельного
внимания, так как общего в них мало.&lt;/p&gt;
&lt;h4&gt;Сервис постоянных сообщений&lt;/h4&gt;
&lt;p&gt;Этот коммуникационный сервис выполняет все операции, связанные с
постоянными сообщениями: приватными сообщениями и электронной почтой.
Перед ним ставится вполне тривиальный ряд задач: доставлять сообщения
получателям и сохранять их на постоянной основе, но на самом деле этим
все не ограничивается: должны также поддерживаться, скажем, доставка
сообщений с задержкой, массовые рассылки, отмена отправки сообщения,
возможность добавления в сообщения какого-либо интерактивного контента.
Реализован он был примерно следующим образом:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;вся система работает асинхронно и активно использует JMS;&lt;/li&gt;
&lt;li&gt;клиенты отправляют сообщения так же через JMS;&lt;/li&gt;
&lt;li&gt;далее сообщения перенаправляются с помощью сервиса маршрутизации в
    соответствующий почтовый ящик или напрямую в обработку электронной
    почты;&lt;/li&gt;
&lt;li&gt;доставка сообщений происходит как с помощью Pull (клиенты
    запрашивают свои сообщения), так и с использованием Push (т.е.
    отправки сообщений);&lt;/li&gt;
&lt;li&gt;помимо этого используется &lt;a href="/tag/spring/"&gt;Spring&lt;/a&gt; с их собственными
    закрытыми расширениями, использующими HTTP-RPC.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Приемы, способствующие масштабируемости&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Функциональное сегментирование:&lt;/strong&gt; отправленные, полученные,
    архивные сообщения. &lt;em&gt;(т.е. вертикальное сегментирование)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Классовое сегментирование:&lt;/strong&gt; пользовательские, гостевые,
    корпоративные почтовые ящики.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Сегментирование по диапазонам:&lt;/strong&gt; по идентификаторам пользователей
    или по лексикографическим диапазонам самих сообщений. &lt;em&gt;(т.е.
    горизонтальное сегментирование)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Асинхронное выполнение операций&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Сервис сетевых обновлений&lt;/h4&gt;
&lt;p&gt;Этот сервис обеспечивает работу любых временных уведомлений, например,
вызванных изменением статуса пользователей в контакт-листах. Такие
сообщения должны с течением времени удаляться из-за быстрой потери
актуальности, а также должна поддерживаться группировка и приоритезация
сообщений. Функционирование этого сервиса оказалось не настолько
очевидно, по сравнению с предыдущим, так что до итогового варианта было
перепробовано масса менее удачных решений, но обо всем по порядку.&lt;/p&gt;
&lt;h5&gt;Изначальная архитектура (до 2007 года)&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;используется много серверов, которые могут содержать обновления;&lt;/li&gt;
&lt;li&gt;клиенты отправляют запросы на каждый сервис отдельно: вопросы,
    обновления профилей и т.д.&lt;/li&gt;
&lt;li&gt;на сбор всех данных требовалось относительно много времени.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В 2008 году вся эта система поэтапно эволюционировала собственно в сам
сервис сетевых обновлений:&lt;/p&gt;
&lt;h5&gt;Первая итерация&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;клиент отправляет единственный запрос сервису сетевых обновлений;&lt;/li&gt;
&lt;li&gt;этот сервис в свою очередь параллельно отправляет всем остальным сервисам соответствующие запросы.&lt;/li&gt;
&lt;li&gt;результаты агрегируются и все вместе возвращаются клиенту;&lt;/li&gt;
&lt;li&gt;весь процесс основывается на Pull.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Вторая итерация&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;стал использоваться метод Push: каждый раз, когда происходит
    какое-либо событие, они помещаются в пользовательский "почтовый
    ящик", в момент запроса пользователя ему возвращается просто
    содержимое, уже ожидающее своего звездного часа в специально том
    самом "ящике";&lt;/li&gt;
&lt;li&gt;такой подход сильно ускоряет процесс чтения, так как на тот
    момент данные уже готовы;&lt;/li&gt;
&lt;li&gt;с другой стороны, какая-то часть данных может так никогда и не
    понадобиться, что приводит к бесполезным передвижениям данных и
    лишнему используемому дисковому пространству;&lt;/li&gt;
&lt;li&gt;небольшая часть обработки данных все же производится уже в
    момент запроса пользователя (например, объединение нескольких
    обновлений от определенного пользователя в одно);&lt;/li&gt;
&lt;li&gt;обновления хранятся в &lt;abbr title="Character Large OBject"&gt;CLOB&lt;/abbr&gt;'ах: по одному &lt;abbr title="Character Large OBject"&gt;CLOB&lt;/abbr&gt;'у на каждый тип
    обновления для каждого пользователя (то есть в сумму около 15 &lt;abbr title="Character Large OBject"&gt;CLOB&lt;/abbr&gt;'ов на каждого пользователя);&lt;/li&gt;
&lt;li&gt;сначала использовался размер &lt;abbr title="Character Large OBject"&gt;CLOB&lt;/abbr&gt;'ов равный 8 KB,
    что было явно больше требуемого и приводило к существенному
    количеству неиспользуемого дискового пространства.&lt;/li&gt;
&lt;li&gt;вместо &lt;abbr title="Character Large OBject"&gt;CLOB&lt;/abbr&gt;'ов можно
    было бы использовать дополнительные таблици по одной на каждый
    тип обновлений, но в этом случае пришлось бы постоянно удалять
    из них устаревшие записи, что было бы чрезвычайно неэффективно.&lt;/li&gt;
&lt;li&gt;в дополнение к этому использовался &lt;abbr title="Java Management eXtensions"&gt;JMX&lt;/abbr&gt; для
    мониторинга и изменения конфигурации в реальном времени, что
    оказалось очень удобным и полезным.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Третья итерация&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Цель: повысить производительность путем сокращения количества
    обновлений &lt;abbr title="Character Large OBject"&gt;CLOB&lt;/abbr&gt;'ов,
    так как они требуют много вычислительных ресурсов.&lt;/li&gt;
&lt;li&gt;Был добавлен буфер: колонки в таблицах типа &lt;code&gt;varchar(4000)&lt;/code&gt;, в
    которых данные помещались изначально. При полном заполнении
    ячейки данные перемещаются в &lt;abbr title="Character Large OBject"&gt;CLOB&lt;/abbr&gt;; это позволило
    на порядок сократить количество их обновлений.&lt;/li&gt;
&lt;li&gt;Уменьшен размер самих сообщений об обновлениях.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="i-naposledok-paru-sovetov-ot-linkedin"&gt;И напоследок пару советов от LinkedIn&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;нельзя бесконечно долго ограничиваться одной базой данных:
    используйте много баз данных как с вертикальным, так и с
    горизонтальным сегментированием данных;&lt;/li&gt;
&lt;li&gt;забудьте о ссылочной целостности и кросс-серверных JOIN'ах;&lt;/li&gt;
&lt;li&gt;забудьте о 100% целостности данных;&lt;/li&gt;
&lt;li&gt;при большом масштабе издержки могут стать проблемой: оборудование,
    базы данных, лицензии, системы хранения данных, электроэнергия и так
    далее;&lt;/li&gt;
&lt;li&gt;как только вы станете достаточно крупны и популярны, спаммеры и
    прочие злые люди не заставят себя долго ждать;&lt;/li&gt;
&lt;li&gt;не забывайте про кэширование!!!&lt;/li&gt;
&lt;li&gt;используйте асинхронные потоки данных;&lt;/li&gt;
&lt;li&gt;аналитика и построение отчетов может стать непростой задачей,
    постарайтесь задуматься о них заранее в процессе планирования
    системы;&lt;/li&gt;
&lt;li&gt;имейте всегда ввиду, что Ваша система может упасть в любой момент;&lt;/li&gt;
&lt;li&gt;не стоит недооценивать траекторию своего роста.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="p-s"&gt;P.S.&lt;/h3&gt;
&lt;p&gt;Когда уже закончил переводить в голову пришла мысль, что если читателям
будет интересно взглянуть на оригинальные презентации (хотябы ради
иллюстрационного материала, который там вполне нагляден), то было бы
проще сделать это прямо здесь, так что вот, для Вашего же удобства:&lt;/p&gt;
&lt;div class="video-container"&gt;&lt;iframe allowfullscreen="" frameborder="0" height="355" marginheight="0" marginwidth="0" scrolling="no" src="//www.slideshare.net/slideshow/embed_code/key/uHbsRNnQFZwThD" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" width="425"&gt; &lt;/iframe&gt;&lt;/div&gt;
&lt;div class="video-container"&gt;&lt;iframe allowfullscreen="" frameborder="0" height="355" marginheight="0" marginwidth="0" scrolling="no" src="//www.slideshare.net/slideshow/embed_code/key/16CML2N96CDeWv" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" width="425"&gt; &lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Кстати если Вы еще не успели подписаться на &lt;a href="/feed/"&gt;RSS&lt;/a&gt; - сейчас
самое время!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 11 Sep 2008 04:00:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-09-11:highload/2008/arkhitektura-linkedin/</guid><category>ActiveMQ</category><category>C++</category><category>EHcache</category><category>Java</category><category>Jetty</category><category>LinkedIn</category><category>Lucene</category><category>MySQL</category><category>Oracle</category><category>Solaris</category><category>Spring</category><category>Tomcat</category><category>архитектура</category><category>архитектура LinkedIn</category><category>Масштабируемость</category></item><item><title>Архитектура Mailinator</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-mailinator/</link><description>&lt;p&gt;Ваш пьяный друг когда-либо вдохновлял Вас на создание первого в своем
роде интернет-сервиса, который пришелся бы по вкусу миллионам
пользователей и при этом неприхотливо обрабатывал миллиарды электронных
писем ежегодно? Именно так Paul Tyma и создал Mailinator.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/43449c5a/" rel="nofollow" target="_blank" title="http://www.mailinator.com"&gt;Mailinator&lt;/a&gt; представляет собой бесплатный,
не требующий инсталляции, сервис для разрушения планов злобных спаммеров
путем предоставления регистрации "одноразовых" почтовых адресов. Если Вы
не не будете публиковать в Сети свой настоящий интернет-адрес - спаммеру
не будут слать вам письма, вместо этого они будут спамить Mailinator :-)&lt;/p&gt;
&lt;p&gt;Как же Mailinator справляется со своей ролью анти-спам супергероя?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="istochniki-informatsiia"&gt;Источники информация&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Да-да, это снова перевод
&lt;a href="https://www.insight-it.ru/goto/1b9578f1/" rel="nofollow" target="_blank" title="http://highscalability.com/mailinator-architecture"&gt;статьи&lt;/a&gt; от
&lt;a href="https://www.insight-it.ru/goto/f3f1b405/" rel="nofollow" target="_blank" title="http://highscalability.com/user/todd-hoff"&gt;Todd&lt;/a&gt;'а (цифры правда не
первой свежести, но все же). На что-то более глобальное я в ближайшее
время способен не буду, в основном благодаря незаметно подкравшейся
сессии и, отчасти, работе.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/fd89ec4b/" rel="nofollow" target="_blank" title="http://mailinator.blogspot.com/2007/01/architecture-of-mailinator.html"&gt;The Architecture of Mailinator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a94e81a2/" rel="nofollow" target="_blank" title="http://mailinator.blogspot.com/2007/02/mailinators-2006-stats.html"&gt;Mailinator's 2006 Stats&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/tomcat/"&gt;Tomcat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/java/"&gt;Java&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="statistika"&gt;Статистика&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Сервис обработал: 1.29 миллиардов электронных писем за 2007 год.
    450.74 миллионов за 2006. 280.68 миллионов за 2005.&lt;/li&gt;
&lt;li&gt;В период пиковых нагрузок обрабатывается 6.5 миллионов электронных
    писем в сутки или 4513 сообщений в минуту или 75 в секунду.&lt;/li&gt;
&lt;li&gt;Mailinator работает на всего одном весьма средненьком компьютере с
    AMD Athlon 2GHz процессором, 1 GB оперативной памяти (которая
    используется не целиком) и низкопроизводительным IDE жестким диском
    объемом 80 GB. И она в общем-то загружена далеко не полностью.&lt;/li&gt;
&lt;li&gt;Mailinator работает месяцами без присмотра и теряется очень
    небольшое количество сообщений, даже при постоянных спам-атаках и
    высоких пиковых нагрузках.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="arkhitektura"&gt;Архитектура&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Так как система бесплатна, она не должна быть идеальной. Таким
    образом основные цели:&lt;ul&gt;
&lt;li&gt;Создание системы, которая ценит выживание превыше всего, даже
пользователей. Основным ключом является именно выживание, так как
Mailinator вынужден ежедневно отражать спам-атаки.&lt;/li&gt;
&lt;li&gt;Предоставить пользователям 99,99% доступность и точность данных.
Более высокие гарантии будут существенно менее практичными и
приведут к большим затратам. И так как сервис бесплатен, этот
небольшой риск для пользователей становится просто частью правил
игры.&lt;/li&gt;
&lt;li&gt;Поддержка следующей модели сервиса: пользователь регистрируется
где-то, заходит в Mailinator, жмет на пришедшую ссылку и забывает об
этом. Это означает, что письма не должны храниться постоянно. Они
могут размещаться в оперативной памяти, так как являются временными
(живут три-четыре часа). Если Вам нужен обычный настоящий почтовый
ящик - воспользуйтесь любым другим соответствующим сервисом.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Изначально письма обрабатывались следующим образом:&lt;ul&gt;
&lt;li&gt;Sendmail получал письмо в общий ящик на диске.&lt;/li&gt;
&lt;li&gt;Java-приложение доставало сообщение используя IMAP и/или POP (с
течением времени это менялось) и удаляло их.&lt;/li&gt;
&lt;li&gt;Система загружала все письма в память и оставляла их там.&lt;/li&gt;
&lt;li&gt;Наиболее старые сообщения вытеснялись как только накапливался
лимит в 20000 сообщений.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Данный принцип работал вполне неплохо:&lt;ul&gt;
&lt;li&gt;Он стабилен и работал месяцами без каких-либо проблем.&lt;/li&gt;
&lt;li&gt;Использовался практически весь гигабайт оперативной памяти.&lt;/li&gt;
&lt;li&gt;Проблемы начались, когда количество сообщений в сутки начало
превышать 800000. Система начала давать сбои из-за использования
жесткого диска между Mailinator и email подсистемой.&lt;/li&gt;
&lt;li&gt;Наиболее старые сообщения вытеснялись как только накапливался
лимит в 20000 сообщений.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Новая архитектура:&lt;ul&gt;
&lt;li&gt;Идея заключалась в отказе от временного хранения данных на жестком
диске путем полного переписывания всей системы с нуля.&lt;/li&gt;
&lt;li&gt;Веб-приложение, почтовый сервер и все хранилище писем
функционируют в рамках одной JVM.&lt;/li&gt;
&lt;li&gt;Sendmail был заменен на специально написанный для этого проекта
SMTP сервер. Так как природа Mailinator не требовала полноценного
SMTP сервера. Mailinator не отправляет писем, основная цель -
принимать или отвергать входящие письма. Это является недостатком
многоуровневой архитектуры. Она часто является залогом успеха в
процессе масштабирования веб-приложения, но порой она может и
наоборот полностью убить всю производительность благодаря неверному
принятию ответственных решений. Решение о создании собственного SMTP
сервера было достаточно интересным и смелым, многие другие
руководители проектов вместо этого просто добавили бы дополнительное
оборудование в систему. Это не было бы ошибкой, но, согласитесь,
создание своего собственного решения задачи - намного более
интересный подход.&lt;/li&gt;
&lt;li&gt;Сейчас Mailinator получает почту напрямую, обрабатывает ее и
хранит в оперативной памяти. Жесткие диски полностью обходятся и
практически не используются.&lt;/li&gt;
&lt;li&gt;Основное их применение - хранение сообщений в случае остановки
сервиса для того, чтобы они могли быть восстановлены при запуске.&lt;/li&gt;
&lt;li&gt;Ведение логов было отключено.&lt;/li&gt;
&lt;li&gt;Система использует менее 300 потоков. Это оказалось вполне
достаточно.&lt;/li&gt;
&lt;li&gt;При принятии сообщения, система пропускает его через набор
фильтров и хранит его в памяти только в том случае, если все фильтры
были успешно пройдены.&lt;/li&gt;
&lt;li&gt;Каждый почтовый адрес ограничен только 10 письмами, так что
популярные адреса вроде joe@mailinator.com не могут "взорвать"
систему.&lt;/li&gt;
&lt;li&gt;Письма не могут превышать 100 kb, а все приложения автоматически
уничтожаются. Это позволяет существенно сэкономить в плане
используемой оперативной памяти..&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Электронные письма сжимаются в оперативной памяти:&lt;ul&gt;
&lt;li&gt;99% писем никто даже не открывает, компрессия позволяет сэкономить
место в оперативной памяти. Письмо разжимается в исходное состояние
только если кто-то решает его открыть.&lt;/li&gt;
&lt;li&gt;Mailinator может хранить около 80000 писем в оперативной памяти,
используя лишь 300 MB памяти, по сравнению с 20000 писем, занимающих
1 GB без использования компрессии.&lt;/li&gt;
&lt;li&gt;С таким подходом к хранению писем, они живут в среднем 3-4 часа.&lt;/li&gt;
&lt;li&gt;В память поместится и 200000 писем, но на практике это и не
требуется.&lt;/li&gt;
&lt;li&gt;Оперативная память ценна, а процессорное время - вовсе нет. Именно
из-за этого используется компрессия для экономии памяти и
использования излишков вычислительных мощностей.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mailinator не гарантирует анонимность или приватность:&lt;ul&gt;
&lt;li&gt;Любой пользователь может получить доступ к любому почтовому ящику.&lt;/li&gt;
&lt;li&gt;Отказ от ограничений доступа делает схему работы системы намного
более простой.&lt;/li&gt;
&lt;li&gt;Со стороны пользователя такой подход очень прост, так как не
требуется абсолютно никакой регистрации. Когда сайт требует ввести
почтовый адрес достаточно лишь просто ввести любой адрес Mailinator.
Вам не нужно создавать отдельный аккаунт. Банальный ввод адреса
создает почтовый ящик. Все просто.&lt;/li&gt;
&lt;li&gt;На практике же, не смотря на вышесказанное, пользователи все же
получают изрядную степень приватности.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Стремление к выживанию требует агрессивной борьбы со спамом:&lt;ul&gt;
&lt;li&gt;Mailinator не имеет ничего против спама, но так как спама приходит
нереально много, когда он подвергает риску работоспособность сервиса
приходится его фильтровать.&lt;/li&gt;
&lt;li&gt;Этот факт привел к правилу: если Вы делаете что-то (получаете спам
или что-то еще), что мешает работе системы - Ваши письма не будут
приниматься и Вы можете быть временно заблокированы.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Для успешного приема письмо должно пройти следующую цепочку
    фильтров:&lt;ul&gt;
&lt;li&gt;Все письма, которые не смогли быть доставлены, отклоняются.&lt;/li&gt;
&lt;li&gt;При слишком большом количестве писем с одного IP они перестают
приниматься.&lt;/li&gt;
&lt;li&gt;Слишком много писем с одинаковой темой не принимаются.&lt;/li&gt;
&lt;li&gt;Письма, содержащие в заголовках запрещенные сервисом слова, также
не попадают в почтовые ящики.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Выживание в условиях наплыва писем с одного IP адреса:&lt;ul&gt;
&lt;li&gt;Для этого типа фильтрации используется AgingHashMap. Когда сервис
получает очередное письмо, IP помещается в массив и счетчик,
соответствующий этому ключу, увеличивается на единицу в момент
получения каждого последующего письма с этого IP.&lt;/li&gt;
&lt;li&gt;Спустя определенное время без получения писем с IP,
соответствующие ему счетчик обнуляется.&lt;/li&gt;
&lt;li&gt;Когда счетчик достигает определенного порога, IP блокируется,
предотвращая поток сообщений.&lt;/li&gt;
&lt;li&gt;Этим простым методом пользуются многие интернет-ресурсы для защиты
различных своих компонентов, например комментариев. В роли хранилища
для такого массива при распределенном функционировании системы часто
используют &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Защита от "зомби" атак:&lt;ul&gt;
&lt;li&gt;Спам может приходить и с больших координированных сетей с разными
IP адресами, как раз участников таких сетей и называют "зомби".
Одинаковые письма приходят со множества разных адресов, так что
защита по IP адресам становится бессильна.&lt;/li&gt;
&lt;li&gt;Этот фильтр несколько более сложный, чем блокировка по IP, так как
требуется достать из письма строку с заголовком, да и их сравнение -
несколько ресурсоемкая задача.&lt;/li&gt;
&lt;li&gt;Когда около 20 писем с одинаковыми темами приходят в течении 2
минут, этот заголовок блокируется на час.&lt;/li&gt;
&lt;li&gt;Что интересно, Mailinator не хранит заблокированные темы вечно,
так как это значило бы, что этот список неуклонно рос и приходилось
бы вечно отслеживать соответствия с ним. Это никак не приемлемо для
мимолетной природы Mailinator. Более комплексные алгоритмы защиты от
спама нужны лишь только если ставятся цели с более жесткой борьбой
со спама, для Mailinator же данный вариант - наиболее эффективный.&lt;/li&gt;
&lt;li&gt;Этим фильтром блокируется около 9% писем.&lt;/li&gt;
&lt;li&gt;Mailinator фильтрует сообщения только по теме и IP, так что
системе не приходится прочитывать и анализировать все письмо
целиком. Это позволяет неплохо сэкономить на вычислительных ресурсах
при достаточно эффективной итоговой фильтрации.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Для уменьшения угрозы DDoS атак:&lt;ul&gt;
&lt;li&gt;Все соединения, неактивные какое-то время обрываются.&lt;/li&gt;
&lt;li&gt;Mailinator отвечает отправителям писем очень медленно, 10, 20 или
даже 30 секунд, даже для небольших объемов данных. Это замедляет
работу спаммеров, пытающихся отправлять спам как можно быстрее, и
заставляет их лишний раз задуматься о целесообразности отправки
снова спама на этот адрес. Период ожидания уменьшается во время
повышенных нагрузок на сервис, так что письма не теряются из-за
этого.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="podvodim-itogi"&gt;Подводим итоги&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Идеальность - всего лишь ловушка.&lt;/strong&gt; Как много систем были
    кардинально усложнены лишь для того, чтобы достичь 100%-го
    результата во всех аспектах. Если Вы участвовали в подобных
    совещаниях, Вы понимаете о чем идет речь. О нет, мы не можем сделать
    этого, так как есть 0,01% шанс, что что-то пойдет не так. Лучше
    спросите себя: насколько неидеальными можно позволить себе быть,
    чтобы все равно оставаться достаточно неплохим сервисом?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;То, что Вы отвергаете, ничуть не менее важно, чем то, что Вы
    оставляете в системе.&lt;/strong&gt; Существует масса концепций по построению
    архитектуры системы. Нужно не только выбрать подходящие, но и
    отказаться от тех, которые излишни.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Знайте предназначение своей системы и разрабатывайте ее в
    соответствии с этим.&lt;/strong&gt; Быть всем для всех значит быть ничем для
    никого. Временное хранение электронных писем, позволяя небольшой
    части спама пробиться через фильтры, в совокупности с не 100%
    временем работы системы производят достаточно хорошее впечатление на
    пользователей. Построение собственного SMTP-сервера необходимо лишь
    в случае, если у Вас есть весомые аргументы в пользу того, что он
    Вам необходим. Далеко не факт, что такая идея придет в голову,
    возможно выбор пал бы и на более тривиальное решение, связанное
    просто с добавлением дополнительного оборудования.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Постарайтесь как можно быстрее свести механизм работы системы к
    наиболее общему случаю.&lt;/strong&gt; Очень большой процент писем отвергается,
    так что это оправданно сделать это как можно раньше, чтобы
    минимизировать ресурсы, требуемые для их обработки. Найдите способ
    сделать это как можно быстрее в отношении наиболее частых случаев.
    то очень часто становится важным компонентом стратегии
    масштабирования.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Эффективность часто означает "постройте это самостоятельно".&lt;/strong&gt;
    Готовые решения обычно решают большой спектр задач, но на практике
    часто нужна лишь небольшая часть функционала, в таких случаях можно
    написать небольшой компонент с нуля самостоятельно, чтобы он мог
    выполнять только нужные функции, но более эффективно.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Небольшое количество сбоев - вполне допустимо.&lt;/strong&gt; Все
    заблокированные адреса не должны быть запомнены навечно. Позвольте
    этим спискам генерироваться на основе локальных данных, а не
    глобального состояния. Это очень простая и эффективная архитектура.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/java/"&gt;Java&lt;/a&gt; совсем не обязательно должна быть медленной.&lt;/strong&gt;
    На эту тему сказано уже достаточно.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Избегайте работы с жесткими дисками.&lt;/strong&gt; Многие приложения требуют
    работы с дисковой системой, но очень часто именно она оказывается
    узким местом в системе. Можете ли Вы обойтись без него, используя
    более креативные подходы к архитектуре системы?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ограничте использование ресурсов.&lt;/strong&gt; Задайте рамки для размеров
    почтовых ящиков и других подобных элементов системы, это позволит
    избежать неконтролируемых скачков нагрузок. Неограниченное
    использование ресурсов недопустимо при ограниченности ресурсов.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Сжимайте данные.&lt;/strong&gt; Компрессия данных может стать неплохим
    достижением в попытках сэкономить оперативную память. Можно
    сократить использование памяти вдвое с лишь небольшой дополнительной
    нагрузкой, связанной с компрессией и декомпрессией информации. Если
    обмен данными происходит локально, достаточно лишь закодировать
    данные и предоставить API для доступа к данным без полной
    декомпрессии.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Используйте фиксированные объемы ресурсов для обработки запросов.&lt;/strong&gt; Многие приложения не могут контролировать используемые
    ресурсы, в частности - оперативную память, таким образом они могут
    порой давать сбой при использовании излишне больших ее объемов. Для
    более стабильной работы стоит ограничить используемые ресурсы и
    откладывать выполнение новых задач пока они используются полностью.
    Для управление доступом к ресурсам можно использовать определенную
    логику в зависимости от ситуации: по времени, по приоритету,
    "честный" доступ, но так как ресурсы ограничены, система несколько
    ослабнет под серьезной нагрузкой.&lt;/li&gt;
&lt;li&gt;Если данные не хранятся длительное время, они не могут стать
    причиной возбуждения судебного дела о нарушении чьих-либо прав.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Пользуйтесь тем, что знаете лучше всего.&lt;/strong&gt; Этот урок не раз
    оправдывал себя. Paul знал Java лучше, чем что-либо еще, именно
    по-этому он заставил приложение на этом языке работать и выполнить
    все поставленные задачи.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Найдите свои собственные Mailinator'ы.&lt;/strong&gt; Конечно, Mailinator
    является очень небольшой системой. В более крупной системе этот
    проект был бы лишь небольшой дополнительной возможностью, но такие
    системы обычно состоят просто из нескольких подпроектов размером с
    Mailinator. А что если подойти к разработке некоторых из них так же
    как и к Mailinator?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;KISS работает, правда довольно редко.&lt;/strong&gt; Простота систем часто
    обсуждается, но практические примеры появляются достаточно редко.
    Чаще всего разговор остается на уровне: твоя система сложная, а
    моя - простая, просто так как она моя. Mailinator является хорошим
    примером простой архитектуры системы.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Надежность является функцией архитектуры системы.&lt;/strong&gt; Для построения
    системы, эффективно использующей память и выживающей серьезные атаки
    спаммеров, потребовалось серьезно подойти к каждому уровню ее
    архитектуры.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 24 Jun 2008 18:17:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-06-24:highload/2008/arkhitektura-mailinator/</guid><category>Java</category><category>Linux</category><category>online</category><category>Tomcat</category><category>архитектура</category><category>архитектура Mailinator</category><category>электронная почта</category></item><item><title>Архитектура 37signals</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-37signals/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/7457badf/" rel="nofollow" target="_blank" title="http://www.37signals.com"&gt;37signals&lt;/a&gt; больше всего известны благодаря
выпуску в свет &lt;a href="/tag/ruby-on-rails/"&gt;Ruby on Rails&lt;/a&gt; грамотному его
использованию для запуска их очень популярных продуктов: Basecamp,
Highrise, Backpack и Campfire. RoR как обычно пытаются винить во всех
проблемах с производительностью, но 37signals казалось бы справляется с
большой нагрузкой, используя вполне разумное количество вычислительных
ресурсов.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Этот текст является переводом
&lt;a href="https://www.insight-it.ru/goto/23721415/" rel="nofollow" target="_blank" title="http://highscalability.com/37signals-architecture"&gt;статьи&lt;/a&gt;, автор -
&lt;a href="https://www.insight-it.ru/goto/f3f1b405/" rel="nofollow" target="_blank" title="http://highscalability.com/user/todd-hoff"&gt;Todd Hoff&lt;/a&gt;. Извиняюсь за не
сильно оригинальный контент (да и, как выяснилось, практически не
технической направленности), но на написание своих полноценных текстов у
меня последнее время все никак не хватает то креатива, то времени :( .&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/d744e02/" rel="nofollow" target="_blank" title="http://www.37signals.com/svn/posts/749-ask-37signals-numbers"&gt;Спросите 37signals:
    цифры?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/aed84967/" rel="nofollow" target="_blank" title="http://www.37signals.com/svn/posts/753-ask-37signals-how-do-you-process-credit-cards"&gt;Спросите 37signals: как вы работаете с кредитными
    картами?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/df60cd7c/" rel="nofollow" target="_blank" title="http://www.37signals.com/svn/posts/759-behind-the-scenes-at-37signals-support"&gt;За сценой 37signals:
    поддержка.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/61db4355/" rel="nofollow" target="_blank" title="http://www.37signals.com/svn/posts/772-ask-37signals-why-did-you-restart-highrise"&gt;Спросите 37signals: почему вы перезапустили
    Highrise?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/ruby-on-rails/"&gt;Ruby on Rails&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/xen/"&gt;Xen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/amazon-s3/"&gt;Amazon S3&lt;/a&gt; для хранения изображений&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="statistika"&gt;Статистика&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;30 серверов: от простых однопроцессорных файловых серверов до
    восьмипроцессорных серверов приложений, в сумме около 100
    процессоров и 200 GB оперативной памяти.&lt;/li&gt;
&lt;li&gt;В планах диагональное масштабирование: уменьшение количества
    серверов до 16, но более производительных - в сумме 92 процессоров и
    230 GB RAM.&lt;/li&gt;
&lt;li&gt;Виртуализация средствами &lt;a href="/tag/xen/"&gt;Xen&lt;/a&gt; для более гибкого
    управления системой.&lt;/li&gt;
&lt;li&gt;Basecamp (управление проектами):&lt;ul&gt;
&lt;li&gt;2000000 пользователей с учетными записями&lt;/li&gt;
&lt;li&gt;13200000 задач&lt;/li&gt;
&lt;li&gt;9200000 сообщений&lt;/li&gt;
&lt;li&gt;12200000 комментариев&lt;/li&gt;
&lt;li&gt;5500000 записей о распределении времени&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Backpack (управление информацией для личного использования и малого
    бизнеса):&lt;ul&gt;
&lt;li&gt;Около миллиона страниц&lt;/li&gt;
&lt;li&gt;6800000 задач&lt;/li&gt;
&lt;li&gt;1500000 записей&lt;/li&gt;
&lt;li&gt;829000 фотографий&lt;/li&gt;
&lt;li&gt;370000 файлов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Общая статистика проектов (на ноябрь 2007 г.):&lt;ul&gt;
&lt;li&gt;5.9 TB загруженных пользователями данных&lt;/li&gt;
&lt;li&gt;888 GB загружено (upload)&lt;/li&gt;
&lt;li&gt;2 TB скачано (download)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="arkhitektura"&gt;Архитектура&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Кэширование средствами &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; уже используется,
    но они ищут способы более активно его применять. Позволяет достичь
    впечатляющих результатов в плане производительности.&lt;/li&gt;
&lt;li&gt;Методы для составления URL используются вместо ручного их
    составления.&lt;/li&gt;
&lt;li&gt;Используются стандартные ActiveRecord запросы, но при необходимости
    они используют и ручную настройку.&lt;/li&gt;
&lt;li&gt;Иногда они дорабатывают Rails, если сталкиваются с проблемами с
    производительностью.&lt;/li&gt;
&lt;li&gt;Amazon S3 используется для хранения данных, загруженных
    пользователями. Разработчики очень довольны результатами.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="rabota-s-kreditnymi-kartami"&gt;Работа с кредитными картами&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Ежемесячное выписывание счетов.&lt;/em&gt; Это позволяет компаниям,
    занимающимся кредитными картами, чувствовать себя более комфортно,
    так как им не придется столкнуться с огромным количеством работы,
    если ваша фирма вдруг выйдет из бизнеса. Пользователям такой подход
    также по душе, так как издержки в итоге оказываются относительно
    невелики, а также не требуется подписание контракта, имеется
    возможность пользоваться сервисами необходимый промежуток времени и
    оплачивать именно ту сумму, которую потратили.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Получение учетной записи продавца услуг.&lt;/em&gt; Кто-то определенно должен
    обрабатывать операции с кредитными картами. Они используют Chase
    Bank. Воспользуйтесь услугами кого-нибудь, кому вы доверяете, а
    когда масштаб ваших сделок станет существенным - будет возможность
    получить более выгодные условия контракта.&lt;/li&gt;
&lt;li&gt;Они используют authorize.net в роли шлюза для процессинга операций с
    кредитными картами.&lt;/li&gt;
&lt;li&gt;Ежемесячным выписыванием счетов занимается специально написанная для
    этого проекта система. Она запускается каждую ночь, отправляет счета
    нужным людям и записывает результаты выполненных операций.&lt;/li&gt;
&lt;li&gt;В случае успеха счет-фактура высылается по электронной почте.&lt;/li&gt;
&lt;li&gt;В случае каких-либо сбоев - пользователю отправляется объяснение
    причин.&lt;/li&gt;
&lt;li&gt;Если три раза с помощью кредитной карты не удается оплатить счет -
    учетная запись замораживается до тех пор, пока не будет предоставлен
    платежеспособный номер кредитной карты.&lt;/li&gt;
&lt;li&gt;Обработка ошибок является критичным моментом, так как проблемы с
    оплатой возникают достаточно часто.&lt;/li&gt;
&lt;li&gt;Все продукты конвертируются для использования централизованным
    биллинговым сервисом.&lt;/li&gt;
&lt;li&gt;Необходимо быть совместимыми с &lt;a href="https://www.insight-it.ru/goto/963f6a2d/" rel="nofollow" target="_blank" title="http://en.wikipedia.org/wiki/PCI_DSS"&gt;PCI DSS (Payment Card Industry Data
    Security Standard)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Пользуйтесь услугами шлюзов, это позволит не хранить номера
    кредитных карт на своем сайте, что сильно упрощает жизнь в плане
    безопасности. Некоторые из них предоставляют и биллинговые услуги,
    что позволяет также не заниматься этим самостоятельно.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="podderzhka-polzovatelei"&gt;Поддержка пользователей&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Campfire используется для работы с клиентами. По сути эта система
    представляет собой групповой веб-чат с расширенными возможностями в
    виде опциональной защиты паролем, личных сообщений, обмена файлами,
    предпросмотра изображений, а также коллективного принятия решений.&lt;/li&gt;
&lt;li&gt;Поднимаемые вопросы используются для поиска ошибок в коде, а
    обновление данных в SVN отображается прямо в процессе общения.
    Bugtracking система обходится стороной, что казалось бы должно лишь
    усложнять исправление неполадок, например из-за отсутствия
    возможности проследить связь между конкретным изменением в коде и
    вызвавшей его неполадкой.&lt;/li&gt;
&lt;li&gt;Зато служба поддержки может решать проблемы клиентов с помощью
    общения в чате в реальном времени, с возможностью обмена
    изображениями и снимками экранов, демонстрирующими неисправность.&lt;/li&gt;
&lt;li&gt;Разработчики также всегда доступны с помощью Campfire для помощи в
    решении проблем клиентов.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="podvodim-itogi"&gt;Подводим итоги&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Возьмите пример с &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; и постройте все внутренние
    функции в виде сервисов с самого начала. Это позволит более легко
    использовать их в разных продуктах и прозрачно обновлять
    предоставляемые возможности.&lt;/li&gt;
&lt;li&gt;Не храните номера кредитных карт внутри своей базы данных - это
    существенно снижает риски, связанные с безопасностью.&lt;/li&gt;
&lt;li&gt;Разработчики и пользователи должны иметь возможность легко общаться
    друг с другом публично. Пользователи получают сервис намного более
    высокого качества, если общаются напрямую с разработчиками, которые
    решают все проблемы прямо в рамках нормального хода процесса
    разработки. Это позволяет избежать нескольких излишних промежуточных
    этапов при обработке заявок о неисправности. Помимо этого
    разработчикам предоставляется возможность узнать пользовательское
    мнение об их продукте, что делает дальнейшее развитие проекта более
    эффективным. Потенциальные клиенты могут убедиться в отзывчивости
    компании, просто посмотрев на такое общение, что подталкивает их к
    более охотной регистрации.&lt;/li&gt;
&lt;li&gt;Развивайте свой продукт добавлением новых функций, которые нужны
    конкретным клиентам, а не тех, которые когда-нибудь может быть
    кому-нибудь понадобятся.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 05 Jun 2008 18:49:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-06-05:highload/2008/arkhitektura-37signals/</guid><category>Amazon S3</category><category>Memcached</category><category>MySQL</category><category>RoR</category><category>Ruby on Rails</category><category>Xen</category><category>архитектура</category><category>архитектура 37signals</category><category>Масштабируемость</category></item><item><title>it's a pic</title><link>https://www.insight-it.ru//misc/2008/its-a-pic/</link><description>&lt;p&gt;&lt;img alt="it's a pic logo" class="left" src="https://www.insight-it.ru/images/itsapic-logo.png" title="логотип"/&gt;
Не удивлюсь, если заголовок этого поста вам не сказал ровным счетом
ничего - это вполне логично. Именно эту ситуацию я и хотел бы сегодня
исправить: &lt;strong&gt;it's a pic&lt;/strong&gt; представляет собой...
&lt;!--more--&gt;
...очередной интернет-проект. Хотели увидеть что-то более
грандиозное? - читайте дальше!&lt;/p&gt;
&lt;p&gt;Начать наверное стоит с обозначения основной сути: поисковая система
изображений, ориентированная на глобальный рынок. Да-да, мы уже видели
поиск картинок в исполнении Google/Yahoo!/MSN/Яндекс/Рамблер (нужное
подчеркнуть) - скажете вы, так в чем же разница?&lt;/p&gt;
&lt;p&gt;Сейчас объясню. Никогда не возникало мысли, что частенько поиск картинок
в обычных поисковых системах по большей части выдает всякий бред, очень
слабо коррелирующий с тем, что Вы на самом деле искали? Основная их
проблема заключается в том, что способов провести ассоциацию между
текстом и изображением не так-то много. Чаще всего в их распоряжении
лишь HTML-документы, ссылающиеся на изображение. То есть на основании
атрибута &lt;code&gt;alt&lt;/code&gt; у тэга &lt;code&gt;&amp;lt;img /&amp;gt;&lt;/code&gt; и изредка anchor-текста обычных
ссылок, поисковая система должна составить представление о том, что же
на самом деле изображено в графическом файле. Варианты ручного
построения таких соответствий тоже существуют, но либо нужно платить
огромнейшему количеству человек за рутинную работу (что-то на грани
фантастики - количество изображений в Сети измеряется числом с слишком
большим количеством нулей) или подталкивать людей заниматься этим
бесплатно, оформив это, например, в виде online-игры. Обычно в таких
играх двум участникам одновременно предоставляется один и тот же набор
изображений, а их задачей является последовательно вводить свои
ассоциации связанные с текущим изображением. Если они оба ввели одно и
то же слово - оно ассоциируется с изображением, а пользователям
начисляются виртуальные очки. В общем поиск изображений по ключевым
словам - задача, связанная с массой проблем и неточностей.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It's a pic&lt;/strong&gt; является как раз поисковой системой, призванной избавить
людей, ищущих изображения от всех этих проблем с неточностью и
некорректностью результатов. Чтобы не придумывать каких-то временных
решений проблемы было решено искоренить основательно: основная идея
заключается в использовании в качестве критерия поиска не набор ключевых
слов, а просто изображение. Сказать, что два изображения похожи,
компьютеру намного проще, чем сказать что на картинке нарисован,
например, жираф - именно на это и делает ставку этот проект.&lt;/p&gt;
&lt;p&gt;Выглядит это примерно следующим образом: допустим Вы хотите найти
побольше изображений заката и выбрать наиболее приглянувшееся, для этого
достаточно загрузить в систему с локального компьютера изображение
заката (хотя если оно уже присутствует в Сети - можно и просто указать
URL) и собственно говоря нажать кнопку "Найти" - вот и все! Вот ваши
результаты:&lt;/p&gt;
&lt;p&gt;&lt;img alt="пример работы it's a pic" class="responsive-img" src="https://www.insight-it.ru/images/itsapic-scr.jpg" title="пример работы"/&gt;&lt;/p&gt;
&lt;p&gt;Наверное Вы уже заметили, что написав приличную часть поста я так до сих
пор и не дал ссылки на саму поисковую систему. У этого есть достаточно
простая причина - проект находится в стадии закрытого
&amp;beta;-тестирования (что вы собственно говоря могли
прочитать и на скриншоте чуть выше). Так что недостаточная точность
поиска вполне объясняется скромной базой данных изображений - можно
заметить на все том же скриншоте семизначную цифру количества
изображений в его базе. Но даже из такого небольшого количества
изображений системе удается достаточно точно выбрать похожие на образец
экземпляры и отсортировать их в соответствии с их релевантностью
оригиналу.&lt;/p&gt;
&lt;p&gt;Наверняка у Вас снова напрашивается вопрос: а как же я собственно попал
в закрытую бету проекта и узнал так много о нем еще до его запуска? Нет,
мне никто так до сих пор и не дает эксклюзивной информации о проектах,
но эта информация была получена и не из Сети. Не буду тянуть и раскрою
все карты: я просто-напросто с недавних пор участвую в этом проекте.
Собственно говоря одной из основных моих задач является вывод этой
системы из закрытой бета-версии в открытую, то есть обеспечить
работоспособность алгоритмов при несколько больших нагрузках, чем
один-два разработчика одновременно, ищущих что-то просто для проверки и
тестирования.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 27 May 2008 19:35:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-05-27:misc/2008/its-a-pic/</guid><category>it's a pic</category><category>online</category><category>архитектура</category><category>изображение</category><category>информационные технологии</category><category>поиск</category><category>поисковые системы</category></item><item><title>Архитектура Google Talk</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-googletalk/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/8bce017/" rel="nofollow" target="_blank" title="http://www.google.com/talk"&gt;Google Talk&lt;/a&gt; представляет собой сервис
мгновенного обмена сообщениями от Google. В основе этого сервиса лежит
&lt;abbr title="eXtensible Messaging and Presence Protocol"&gt;XMPP&lt;/abbr&gt; протокол, более известный как &lt;em&gt;Jabber&lt;/em&gt;. В России среди &lt;abbr title="Instant Messaging"&gt;IM&lt;/abbr&gt;-сервисов
несомненно наиболее широко распространен &lt;em&gt;ICQ&lt;/em&gt;, но количество русских
пользователей &lt;em&gt;Jabber&lt;/em&gt; тоже неуклонно растет.&lt;/p&gt;
&lt;p&gt;Вам когда-нибудь доводилось задумываться какое количество сообщений
приходится обрабатывать такого рода сервисам? Допустим есть абстрактный
&lt;abbr title="Instant Messaging"&gt;IM&lt;/abbr&gt;-сервис, которым пользуется миллион пользователей, в среднем каждый из
них отправляет сто текстовых сообщений. Сколько всего сообщений
обработал и доставил сервис? Сто миллионов? Наивно!
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="vvedenie"&gt;Введение&lt;/h3&gt;
&lt;p&gt;Сервисы мгновенного обмена на самом деле подвергаются существенно
большей нагрузке, чем это может показаться на первый взгляд. Давайте
взглянем на расшифровку аббревиатуры &lt;abbr title="eXtensible Messaging and Presence Protocol"&gt;XMPP&lt;/abbr&gt;: eXtensible Messaging and
Presence Protocol. Обмен сообщениями - лишь одна из его функций,
наиболее важная же его часть остается "за сценой" - отображение
присутствия пользователей &lt;em&gt;online&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Давайте посмотрим на наш абстрактный пример с точки зрения присутствия:
пускай им пользуется все тот же миллион пользователей, когда один из них
включил компьютер и появился online - он должен уведомить весь свой
список контактов об этом событии, а также узнать кто из них находится
online. Если этот список велик, то такое элементарное событие может
обернуться для сервиса далеко не одной сотней обработанных и
доставленных сообщений. Помимо простого изменения статуса online/offline
подобную цепочку сообщений может генерировать и любое другое изменение
статуса: связанное с отсутствием пользователя около компьютера или с
изменением небольшого текстового сообщения, которое обычно отображается
в контакт листе рядом с ником пользователя и призвано отображать текущее
его состояние, занятие или чего там только не пишут (эта функция не
всегда предоставляется &lt;abbr title="Instant Messaging"&gt;IM&lt;/abbr&gt;-сервисами, но наверняка многим знакома по
&lt;em&gt;ICQ&lt;/em&gt;, если не по &lt;em&gt;Jabber&lt;/em&gt;). Все эти сообщения как раз и стоят за
"presence" в аббревиатуре &lt;abbr title="eXtensible Messaging and Presence Protocol"&gt;XMPP&lt;/abbr&gt;, суммарный траффик, ими генерируемый,
может в несколько раз превышать траффик от собственно самих текстовых
сообщений.&lt;/p&gt;
&lt;p&gt;Если учесть факты, описанные в предыдущем абзаце, не трудно догадаться,
что зависимость суммарного количества presence-сообщений от количества
пользователей &lt;abbr title="Instant Messaging"&gt;IM&lt;/abbr&gt;-сервиса далеко не линейна. Их количество за какой-то
период времени можно очень приблизительно посчитать как произведение
трех параметров: количества пользователей online, средней длины списка
контактов среди них и количества изменений статуса каждым пользователем.
А каждый дополнительный пользователь в системе так или иначе увеличивает
как минимум два из этих трех параметров.&lt;/p&gt;
&lt;p&gt;Введение несколько затянулась, а проблема масштабируемости &lt;abbr title="eXtensible Messaging and Presence Protocol"&gt;XMPP&lt;/abbr&gt;-сервисов
я думаю теперь стала очевидна, так что сейчас очень подходящий момент,
чтобы вернуться к основной теме разговора - сервису Google Talk и том,
как команда его разработчиков решает эту проблему.&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;Наверное уже стало заметно, что это не очередной перевод, а лично мной
написанный текстик. Так что сразу выдам
&lt;a href="https://www.insight-it.ru/goto/bd0c001f/" rel="nofollow" target="_blank" title="http://video.google.com/videoplay?docid=6202268628085731280"&gt;видео&lt;/a&gt;,
являющееся основным источником информации, и продолжу.&lt;/p&gt;
&lt;h3 id="arkhitektura"&gt;Архитектура&lt;/h3&gt;
&lt;p&gt;Со стороны Google (о котором я, кстати говоря, &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-google/"&gt;уже писал&lt;/a&gt;) было бы глупо строить
сервис мгновенного обмена сообщениями в стороне от остальных
коммуникационных сервисов, предоставляемых этой компанией. Еще до своего
публичного старта Google Talk был интегрирован в почтовый сервис
&lt;a href="https://www.insight-it.ru/goto/af1829ed/" rel="nofollow" target="_blank" title="http://www.gmail.com"&gt;GMail&lt;/a&gt; и социальную сеть
&lt;a href="https://www.insight-it.ru/goto/31afe99f/" rel="nofollow" target="_blank" title="http://www.orkut.com"&gt;Orkut&lt;/a&gt;: эти сервисы просто запрашивали у Google
Talk присутствие online пользователей из своего списка контактов при
возникновении соответствующих событий, но при этом не отображали
результаты в своих страницах. Таким образом разработчики получили
возможность оценить предстоящие нагрузки и готовность сервиса к
публичному запуску намного более точно, чем они могли бы это сделать
средствами синтетических тестов.&lt;/p&gt;
&lt;p&gt;В отношении распределения нагрузок, сразу же был выбран и реализован
подход, связанный с разбиением пользователей на группы и распределением
работы с каждой отдельной группой по разным серверам. Это позволило
избежать всей той эволюции серверной части приложения от одного сервера
до большого кластера, что впрочем вполне оправданно, так как сразу же
после запуска сервису предстояло столкнуться с огромным количеством
пользователей и не ничуть не меньшей нагрузкой. Разработчики не забыли и
сразу же предусмотреть безболезненный перенос пользователей с одного
сервера на другой без видимых для него изменений, это позволило очень
гибко изменять количество серверов в системе.&lt;/p&gt;
&lt;p&gt;С точки зрения интеграции сервиса с другими проектами
&lt;a href="/tag/google/"&gt;Google&lt;/a&gt;, очень важно было предоставить определенный
уровень абстракции для взаимодействия в виде API и набора адресов, по
которым необходимо обращаться к сервису. Придерживаясь одного API можно
производить практически любые архитектурные или программные изменения в
рамках проекта таким образом, что все его пользователи и проекты, в
которые он интегрирован, просто не заметят что что-то изменилось.
Адреса, к которым происходит обращение при обмене данных, так же
являются своеобразной абстракцией - можно переместить сервис в новый
датацентр и благодаря DNS трафик будет направляться в нужное место.&lt;/p&gt;
&lt;p&gt;С другой стороны необходимо учитывать и программное обеспечение
работающие ниже уровнем, чем собственно код приложения: особенно ядро
операционной системы и используемые библиотеки. В данном случае большую
роль играет количество открытых TCP соединений, так как &lt;abbr title="Instant Messaging"&gt;IM&lt;/abbr&gt; требует
большое их количество, но активность в них не велика.&lt;/p&gt;
&lt;p&gt;Разработчики Google Talk постарались как можно больше внимания уделить
возможным сбоям и связанным с ними ситуациям. Любое даже запланированное
временное прекращение функционирования какой-то части системы может
резко увеличить нагрузку на остальную часть, даже если это просто
перезагрузка части системы - из-за очистившегося кэша серверы снова
начнут полноценно функционировать далеко не сразу, не говоря уже о
непредвиденных сбоях, когда последствия намного более глобальны. Для
своевременного устранения потенциальных проблем как с общем
функционированием системы, так и с недостаточной производительностью,
ведутся логи для всех этапов обработки запросов, а также предусмотрена
возможность профайлинга прямо на работающих в системе серверах.&lt;/p&gt;
&lt;p&gt;Но не стоит забывать и о клиентской части программного обеспечения:
какая-нибудь глупая ошибка в коде клиента сервиса запросто может
устроить DDoS атаку на сервис, что и случилось с одной из ранних версий
клиента Google Talk. Помимо этого необходимо поддерживать совместимость
разных версий клиентских приложений.&lt;/p&gt;
&lt;h3 id="zakliuchenie"&gt;Заключение&lt;/h3&gt;
&lt;p&gt;Благодаря описанным выше принципам Google Talk удается обрабатывать
каждое из миллиардов сообщений в день менее чем за 100 миллисекунд.
Тесная интеграция с другими сервисами &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; позволила
проекту сразу же получить невероятную популярность, а продуманный подход
к разработке сервиса позволил справиться с огромной нагрузкой.&lt;/p&gt;
&lt;p&gt;На этот раз статья получилась скорее о специфике сервиса, чем о его
реализации. Технической информации найти практически не удалось, так что
очень кратко все, но надеюсь и в таком варианте было достаточно
интересно почитать. Напоследок хочу порекомендовать &lt;a href="/feed/"&gt;подписаться на RSS&lt;/a&gt;, если не хотите пропустить публикацию новых постов.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 22 May 2008 16:39:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-05-22:highload/2008/arkhitektura-googletalk/</guid><category>Google</category><category>Google Talk</category><category>IM</category><category>Jabber</category><category>Java</category><category>Linux</category><category>online</category><category>sharding</category><category>XMPP</category><category>архитектура</category><category>архитектура Google Talk</category><category>присутствие</category></item><item><title>Масштабируемые веб-архитектуры</title><link>https://www.insight-it.ru//theory/2008/masshtabiruemye-veb-arkhitektury/</link><description>&lt;p&gt;&lt;img alt="Масштабируемость" class="right" src="https://www.insight-it.ru/images/display.png"/&gt;
Уже немало слов было сказано по этой теме как в моем блоге, так и за
его пределами. Мне кажется настал подходящий момент для того, чтобы
перейти от частного к общему и попытаться взглянуть на данную тему
отдельно от какой-либо успешной ее реализации.&lt;/p&gt;
&lt;p&gt;Приступим?
&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Для начала имеет смысл определиться с тем, о чем мы вообще будем
говорить. В данном контексте перед веб-приложением ставятся три основные
цели:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;масштабируемость&lt;/strong&gt; - способность своевременно реагировать на
    непрерывный рост нагрузки и непредвиденные наплывы пользователей;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;доступность&lt;/strong&gt; - предоставление доступа к приложению даже в случае
    чрезвычайных обстоятельств;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;производительность&lt;/strong&gt; - даже малейшая задержка в загрузке страницы
    может оставить негативное впечатление у пользователя.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Основной темой разговора будет, как не трудно догадаться,
масштабируемость, но и остальные цели не думаю, что останутся в стороне.
Сразу хочется сказать пару слов про доступность, чтобы не возвращаться к
этому позднее, подразумевая как "само собой разумеется": любой сайт так
или иначе стремится к тому, чтобы функционировать максимально стабильно,
то есть быть доступным абсолютно всем своим потенциальным посетителям в
абсолютно каждый момент времени, но порой случаются всякие
непредвиденные ситуации, которые могут стать причиной временной
недоступности. Для минимизации потенциального ущерба доступности
приложения необходимо избегать наличия компонентов в системе,
потенциальный сбой в которых привел бы к недоступности какой-либо
функциональности или данных (или хотябы сайта в целом). Таким образом
каждый сервер или любой другой компонент системы должен иметь хотябы
одного дублера (не важно в каком режиме они будут работать: параллельно
или один "подстраховывает" другой, находясь при этом в пассивном
режиме), а данные должны быть реплицированы как минимум в двух
экземплярах (причем желательно не на уровне RAID, а на разных физических
машинах). Хранение нескольких резервных копий данных где-то отдельно от
основной системы (например на специальных сервисах или на отдельном
кластере) также поможет избежать многих проблем, если что-то пойдет не
так. Не стоит забывать и о финансовой стороне вопроса: подстраховка на
случай сбоев требует дополнительных существенных вложений в
оборудование, которые имеет смысл стараться минимизировать.&lt;/p&gt;
&lt;p&gt;Масштабируемость принято разделять на два направления:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Вертикальная масштабируемость&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Увеличение производительности каждого компонента системы c целью
повышения общей производительности.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Горизонтальная масштабируемость&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Разбиение системы на более мелкие структурные компоненты и
разнесение их по отдельным физическим машинам (или их группам) и/или
увеличение количества серверов параллельно выполняющих одну и ту же
функцию.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Так или иначе, при разработке стратегии роста системы приходится искать
компромис между ценой, временем разработки, итоговой производительность,
стабильностью и еще массой других критериев. С финансовой точки зрения
вертикальная масштабируемость является далеко не самым привлекательным
решением, ведь цены на сервера с большим количеством процессоров всегда
растут практически экспоненциально относительно количества процессоров.
Именно по-этому наиболее интересен горизонтальный подход, так как именно
он используется в большинстве случаев. Но и вертикальная
масштабируемость порой имеет право на существование, особенно в
ситуациях, когда основную роль играет время и скорость решения задачи, а
не финансовый вопрос: ведь купить БОЛЬШОЙ сервер существенно быстрее,
чем практически заново разрабатывать приложения, адаптируя его к работе
на большом количестве параллельно работающих серверов.&lt;/p&gt;
&lt;p&gt;Закончив с общими словами давайте перейдем к обзору потенциальных
проблем и вариантов их решений при горизонтальном масштабировании.
Просьба особо не критиковать - на абсолютную правильность и
достоверность не претендую, просто "мысли вслух", да и даже упомянуть
все моменты данной темы у меня определенно не получится.&lt;/p&gt;
&lt;h3 id="servery-prilozhenii"&gt;Серверы приложений&lt;/h3&gt;
&lt;p&gt;В процессе масштабирования самих приложений редко возникают проблемы,
если при разработке всегда иметь ввиду, что каждый экземпляр приложения
должен быть непосредственно никак не связан со своими "коллегами" и
должен иметь возможность обработать абсолютно любой запрос пользователя
вне зависимости от того где обрабатывались предыдущие запросы данного
пользователя и что конкретно он хочет от приложения в целом в текущий
момень.&lt;/p&gt;
&lt;p&gt;Далее, обеспечив независимость каждого отдельного запущенного
приложения, можно обрабатывать все большее и большее количество запросов
в единицу времени просто увеличивая количество параллельно
функционирующих серверов приложений, участвующих в системе. Все
достаточно просто (относительно).&lt;/p&gt;
&lt;h3 id="balansirovka-nagruzki"&gt;Балансировка нагрузки&lt;/h3&gt;
&lt;p&gt;Следущая задача - равномерно распределить запросы между доступными
серверами приложений. Существует масса подходов к решению этой задачи и
еще больше продуктов, предлагающих их конкретную реализацию.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Оборудование&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Сетевое оборудование, позволяющее распределять нагрузку между
несколькими серверами, обычно стоит достаточно внушительные суммы,
но среди прочих вариантов обычно именно этот подход предлагает
наивысшую производительность и стабильность (в основном благодаря
качеству, плюс такое оборудование иногда поставляется парами,
работающими по принципу
&lt;a href="https://www.insight-it.ru/goto/a40f2b94/" rel="nofollow" target="_blank" title="http://en.wikipedia.org/wiki/Heartbeat_%28program%29"&gt;HeartBeat&lt;/a&gt;).
В этой индустрии достаточно много серьезных брендов, предлагающих
свои решения - есть из чего выбрать: &lt;em&gt;Cisco&lt;/em&gt;, &lt;em&gt;Foundry&lt;/em&gt;, &lt;em&gt;NetScalar&lt;/em&gt;
и многие другие.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Программное обеспечение&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;В этой области еще большее разнообразие возможных вариантов.
Получить программно производительность сопоставимую с аппаратными
решениями не так-то просто, да и HeartBeat придется обеспечивать
программно, но зато оборудование для функционирования такого решения
представляет собой обычный сервер (возможно не один). Таких
программных продуктов достаточно много, обычно они представляют
собой просто HTTP-серверы, перенаправляющие запросы своим коллегам
на других серверах вместо отправки напрямую на обработку
интерпретатору языка программирования. Для примера можно упомянуть,
скажем, &lt;a href="/tag/nginx/"&gt;nginx&lt;/a&gt; с &lt;code&gt;mod_proxy&lt;/code&gt;. Помимо этого имеют
место более экзотические варианты, основанные на DNS, то есть в
процессе определения клиентом IP-адреса сервера с необходимым ему
интернет-ресурсов адрес выдается с учетом нагрузки на доступные
сервера, а также некоторых географических соображений.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Каждый вариант имеет свой ассортимент положительных и отрицательных
сторон, именно по-этому однозначного решения этой задачи не существует -
каждый вариант хорош в своей конкретной ситуации. Не стоит забывать, что
никто не ограничивает Вас в использовании лишь одного из них, при
необходимости может запросто быть реализована и практически произвольная
комбинация из них.&lt;/p&gt;
&lt;h3 id="resursoemkie-vychisleniia"&gt;Ресурсоемкие вычисления&lt;/h3&gt;
&lt;p&gt;Во многих приложениях используются какие-либо сложные механизмы, это
может быть конвертирование видео, изображений, звука, или просто
выполнение каких-либо ресурсоемких вычислений. Такие задачи требует
отдельного внимания если мы говорим о Сети, так как пользователь
интернет-ресурса врядли будет счастлив наблюдать за загружающейся
несколько минут страницей в ожидании лишь для того, чтобы увидеть
сообщение вроде: "Операция завершена успешно!".&lt;/p&gt;
&lt;p&gt;Для избежания подобных ситуаций стоит постараться минимизировать
выполнение ресурсоемких операций синхронно с генерацией интернет
страниц. Если какая-то конкретная операция не влияет на новую страницу,
отправляемую пользователю, то можно просто организовать &lt;em&gt;очередь&lt;/em&gt;
заданий, которые необходимо выполнить. В таком случае в момент когда
пользователь совершил все действия, необходимые для начала операции,
сервер приложений просто добавляет новое задание в очередь и сразу
начинает генерировать следущую страницу, не дожидаясь результатов. Если
задача на самом деле очень трудоемкая, то такая очередь и обработчики
заданий могут располагаться на отдельном сервере или кластере.&lt;/p&gt;
&lt;p&gt;Если результат выполнения операции задействован в следующей странице,
отправляемой пользователю, то при асинхронном ее выполнении придется
несколько схитрить и как-либо отвлечь пользователя на время ее
выполнения. Например, если речь идет о конвертировании видео в &lt;strong&gt;flv&lt;/strong&gt;,
то например можно быстро сгенерировать скриншот с первым кадром в
процессе составления страницы и подставить его на место видео, а
возможность просмотра динамически добавить на страницу уже после, когда
конвертирование будет завершено.&lt;/p&gt;
&lt;p&gt;Еще один неплохой метод обработки таких ситуаций заключается просто в
том, чтобы попросить пользователя "зайти попозже". Например, если сервис
генерирует скриншоты веб-сайтов из различных браузеров с целью
продемонстрировать правильность их отображения владельцам или просто
интересующимся, то генерация страницы с ними может занимать даже не
секунды, а минуты. Наиболее удобным для пользователя в такой ситуации
будет предложение посетить страницу по указанному адресу через
столько-то минут, а не ждать у моря погоды неопределенный срок.&lt;/p&gt;
&lt;h3 id="sessii"&gt;Сессии&lt;/h3&gt;
&lt;p&gt;Практически все веб-приложения каким-либо образом взаимодействуют со
своими посетителями и в подавляющем большинстве случаев в них
присутствует необходимость отслеживать перемещения пользователей по
страницам сайта. Для решения этой задачи обычно используется механизм
&lt;em&gt;сессий&lt;/em&gt;, который заключается в присвоении каждому посетителю
уникального идентификационного номера, который ему передается для
хранения в cookies или, в случае их отсутствия, для постоянного
"таскания" за собой через GET. Получив от пользователя некий ID вместе с
очередным HTTP-запросом сервер может посмотреть в список уже выданных
номеров и однозначно определить кто его отправил. С каждым ID может
ассоциироваться некий набор данных, который веб-приложение может
использовать по своему усмотрению, эти данные обычно по-умолчанию
хранятся в файле во временной директории на сервере.&lt;/p&gt;
&lt;p&gt;Казалось бы все просто, но... но запросы посетителей одного и того же
сайта могут обрабатывать сразу несколько серверов, как же тогда
определить не был ли выдан полученный ID на другом сервере и где вообще
хранятся его данные?&lt;/p&gt;
&lt;p&gt;Наиболее распространенными решениями является централизация или
децентрализация сессионных данных. Несколько абсурдная фраза, но,
надеюсь, пара примеров сможет прояснить ситуацию:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Централизованное хранение сессий&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Идея проста: создать для всех серверов общую "копилку", куда они
смогут складывать выданные ими сессии и узнавать о сессиях
посетителей других серверов. В роли такой "копилки" теоретически
может выступать и просто примонтированная по сети файловая система,
но по некоторым причинам более перспективным выглядит использование
какой-либо СУБД, так как это избавляет от массы проблем, связанных с
хранением сессионных данных в файлах. Но в варианте с общей базой
данных не стоит забывать, что нагрузка на него будет неуклонно расти
с ростом количества посетителей, а также стоит заранее предусмотреть
варианты выхода из проблематичных ситуаций, связанных с
потенциальными сбоями в работе сервера с этой СУБД.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Децентрализованное хранение сессий&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Наглядный пример - хранение сессий в &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;,
изначально расчитанная на распределенное хранение данных в
оперативной памяти система позволит получать всем серверам быстрый
доступ к любым сессионным данным, но при этом (в отличии от
предыдущего способа) какой-либо единый центр их хранения будет
отсутствовать. Это позволит избежать узких мест с точек зрения
производительности и стабильности в периоды повышенных нагрузок.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;В качестве альтернативы сессиям иногда используют похожие по
предназначению механизмы, построенные на cookies, то есть все
необходимые приложению данные о пользователе хранятся на клиентской
стороне (вероятно в зашифрованном виде) и запрашиваются по мере
необходимости. Но помимо очевидных преимуществ, связанных с отсутствием
необходимости хранить лишние данные на сервере, возникает ряд проблем с
безопасностью. Данные, хранимые на стороне клиента даже в зашифрованном
виде, представляют собой потенциальную угрозу для функционирования
многих приложений, так как любой желающий может попытаться
модифицировать их в своих интересах или с целью навредить приложению.
Такой подход хорош только если есть уверенность, что абсолютно любые
манипуляции с хранимые у пользователей данными безопасны. Но можно ли
быть уверенными на 100%?&lt;/p&gt;
&lt;h3 id="staticheskii-kontent"&gt;Статический контент&lt;/h3&gt;
&lt;p&gt;Пока объемы статических данных невелики - никто не мешает хранить их в
локальной файловой системе и предоставлять доступ к ним просто через
отдельный легковесный веб-сервер вроде &lt;a href="/tag/lighttpd/"&gt;lighttpd&lt;/a&gt; (я
подразумеваю в основном разные формы медиа-данных), но рано или поздно
лимит сервера по дисковому пространству или файловой системы по
количеству файлов в одной директории будет достигнут, и придется думать
о перераспределении контента. Временным решением может стать
распределение данных по их типу на разные сервера, или, возможно,
использование иерархической структуры каталогов.&lt;/p&gt;
&lt;p&gt;Если статический контент играет одну из основных ролей в работе
приложения, то стоит задуматься о применении распределенной файловой
системы для его хранения. Это, пожалуй, один из немногих способов
горизонтально масштабировать объем дискового пространства путем
добавления дополнительных серверов без каких-либо кардинальных изменений
в работе самого приложения. На какой именно кластерной файловой системе
остановить свой выбор ничего сейчас советовать не хочу, я уже
опубликовал далеко не один обзор конкретных реализаций - попробуйте
прочитать их все и сравнить, если этого мало - вся остальная Сеть в
Вашем распоряжении.&lt;/p&gt;
&lt;p&gt;Возможно такой вариант по каким-либо причинам будет нереализуем, тогда
придется "изобретать велосипед" для реализации на уровне приложения
принципов схожих с сегментированием данных в отношении СУБД, о которых я
еще упомяну далее. Этот вариант также вполне эффективен, но требует
модификации логики приложения, а значит и выполнение дополнительной
работы разработчиками.&lt;/p&gt;
&lt;p&gt;Альтернативой этим подходам выступает использование так называемых
&lt;strong&gt;Content Delievery Network&lt;/strong&gt; - внешних сервисов, обеспечивающих
доступность Вашего контента пользователям за определенное материальное
вознаграждение сервису. Преимущество очевидно - нет необходимости
организовывать собственную инфраструктуру для решения этой задачи, но
зато появляется другая дополнительная статья расходов. Список таких
сервисов приводить не буду, если кому-нибудь понадобится - найти будет
не трудно.&lt;/p&gt;
&lt;h3 id="keshirovanie"&gt;Кэширование&lt;/h3&gt;
&lt;p&gt;Кэширование имеет смысл проводить на всех этапах обработки данных, но в
разных типах приложений наиболее эффективными являются лишь некоторые
методы кэширования.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;СУБД&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Практически все современные СУБД предоставляют встроенные механизмы
для кэширования результатов определенных запросов. Этот метод
достаточно эффективен, если Ваша система регулярно делает одни и те
же выборки данных, но также имеет ряд недостатков, основными из
которых является инвалидация кэша всей таблицы при малейшем ее
изменении, а также локальное расположение кэша, что неэффективно при
наличии нескольких серверов в системе хранения данных.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Приложение&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;На уровне приложений обычно производится кэширование объектов любого
языка программирования. Этот метод позволяет вовсе избежать
существенной части запросов к СУБД, сильно снижая нагрузку на нее.
Как и сами приложения такой кэш должен быть независим от конкретного
запроса и сервера, на котором он выполняется, то есть быть доступным
всем серверам приложений одновременно, а еще лучше - быть
распределенным по нескольким машинам для более эффективной
утилизации оперативной памяти. Лидером в этом аспекте кэширования по
праву можно назвать &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;, о котором я в свое
время уже успел &lt;a href="https://www.insight-it.ru/storage/2008/obzor-memcached/"&gt;подробно рассказать&lt;/a&gt;.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;HTTP-сервер&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Многие веб-серверы имеют модули для кэширования как статического
контента, так и результатов работы скриптов. Если страница редко
обновляется, то использование этого метода позволяет без каких-либо
видимых для пользователя изменений избегать генерации страницы в
ответ на достаточно большую часть запросов.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Reverse proxy&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Поставив между пользователем и веб-сервером прозрачный
прокси-сервер, можно выдавать пользователю данные из кэша прокси
(который может быть как в оперативной памяти, так и дисковым), не
доводя запросы даже до HTTP-серверов. В большинстве случаев этот
подход актуален только для статического контента, в основном разных
форм медиа-данных: изображений, видео и тому подобного. Это
позволяет веб-серверам сосредоточиться только на работе с самими
страницами.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Кэширование по своей сути практически не требует дополнительных затрат
на оборудование, особенно если внимательно наблюдать за использованием
оперативной памяти остальными компонентами серверами и утилизировать все
доступные "излишки" под наиболее подходящие конкретному приложению формы
кэша.&lt;/p&gt;
&lt;p&gt;Инвалидация кэша в некоторых случаях может стать нетривиальной задачей,
но так или иначе универсального решения всех возможных проблем с ней
связанных написать не представляется возможным (по крайней мере лично
мне), так что оставим этот вопрос до лучших времен. В общем случае
решение этой задачи ложится на само веб-приложение, которое обычно
реализует некий механизм инвалидации средствами удаления объекта кэша
через определенный &lt;em&gt;период времени&lt;/em&gt; после его создания или последнего
использования, либо "вручную" при возникновении определенных &lt;em&gt;событий&lt;/em&gt;
со стороны пользователя или других компонентов системы.&lt;/p&gt;
&lt;h3 id="bazy-dannykh"&gt;Базы данных&lt;/h3&gt;
&lt;p&gt;На закуску я оставил самое интересное, ведь этот неотъемлемый компонент
любого веб-приложения вызывает больше проблем при росте нагрузок, чем
все остальные вместе взятые. Порой даже может показаться, что стоит
вообще отказаться от горизонтального масштабирования системы хранения
данных в пользу вертикального - просто купить тот самый БОЛЬШОЙ сервер
за шести- или семизначную сумму не-рублей и не забивать себе голову
лишними проблемами.&lt;/p&gt;
&lt;p&gt;Но для многих проектов такое кардинальное решение (и то, по большому
счету, временное) не подходит, а значит перед ними осталась лишь одна
дорога - горизонтальное масштабирование. О ней и поговорим.&lt;/p&gt;
&lt;p&gt;Путь практически любого веб проекта с точки зрения баз данных начинался
с одного простого сервера, на котором работал весь проект целиком. Затем
в один прекрасный момент наступает необходимость вынести СУБД на
отдельный сервер, но и он со временем начинает не справляться с
нагрузкой. Подробно останавливаться на этих двух этапах смысла особого
нет - все относительно тривиально.&lt;/p&gt;
&lt;p&gt;Следующим шагом обычно бывает &lt;strong&gt;master-slave&lt;/strong&gt; с асинхронной репликацией
данных, как работает эта схема уже неоднократно упоминалось в блоге, но,
пожалуй, повторюсь: при таком подходе все операции записи выполняются
лишь на одном сервере (master), а остальные сервера (slave) получают
данные напрямую от "мастера", обрабатывая при этом лишь запросы на
чтение данных. Как известно, операции чтения и записи любого веб-проекта
всегда растут пропорционально росту нагрузки, при этом сохраняется почти
фиксированным соотношение между обоими типами запросов: на каждый запрос
на обновление данных обычно приходится в среднем около десятка запросов
на чтение. Со временем нагрузка растет, а значит растет и количество
операций записи в единицу времени, а сервер-то обрабатывает их всего
один, а затем он же еще и обеспечивает создание некоторого количества
копий на других серверах. Рано или поздно издержки операций репликации
данных станут быть настолько высоки, что этот процесс станет занимать
очень большую часть процессорного времени каждого сервера, а каждый
slave сможет обрабатывать лишь сравнительно небольшое количество
операций чтения, и, как следствие, каждый дополнительный slave-сервер
начнет увеличивать суммарную производительность лишь незначительно, тоже
занимаясь по большей части лишь поддержанием своих данных в соответствии
с "мастером".&lt;/p&gt;
&lt;p&gt;Временным решением этой проблемы, возможно, может стать замена
master-сервера на более производительный, но так или иначе не выйдет
бесконечно откладывать переход на следующий "уровень" развития системы
хранения данных: &lt;strong&gt;"sharding"&lt;/strong&gt;, которому я совсем недавно посвятил
&lt;a href="https://www.insight-it.ru/theory/2008/segmentirovanie-bazy-dannykh/"&gt;отдельный пост "Сегментирование баз данных"&lt;/a&gt;.
Так что позволю себе остановиться на нем лишь вкратце: идея заключается
в том, чтобы разделить все данные на части по какому-либо признаку и
хранить каждую часть на отдельном сервере или кластере, такую часть
данных в совокупности с системой хранения данных, в которой она
находится, и называют сегментом или &lt;em&gt;shard&lt;/em&gt;&amp;rsquo;ом. Такой подход позволяет
избежать издержек, связанных с реплицированием данных (или сократить их
во много раз), а значит и &lt;em&gt;существенно&lt;/em&gt; увеличить общую
производительность системы хранения данных. Но, к сожалению, переход к
этой схеме организации данных требует массу издержек другого рода. Так
как готового решения для ее реализации не существует, приходится
модифицировать логику приложения или добавлять дополнительную
"прослойку" между приложением и СУБД, причем все это чаще всего
реализуется силами разработчиков проекта. Готовые продукты способны лишь
облегчить их работу, предоставив некий каркас для построения основной
архитектуры системы хранения данных и ее взаимодействия с остальными
компонентами приложения.&lt;/p&gt;
&lt;p&gt;На этом этапе цепочка обычно заканчивается, так как сегментированные
базы данных могут горизонтально масштабироваться для того, чтобы в
полной мере удовлетворить потребности даже самых высоконагруженных
интернет-ресурсов. К месту было бы сказать пару слов и о собственно
самой структуре данных в рамках баз данных и организации доступа к ним,
но какие-либо решения сильно зависят от конкретного приложения и
реализации, так что позволю себе лишь дать пару общих рекомендаций:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Денормализация&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Запросы, комбинирующие данные из нескольких таблиц, обычно при
прочих равных требуют большего процессорного времени для выполнения,
чем запрос, затрагивающий лишь одну таблицу. А производительность,
как уже упоминалось в начале повествования, чрезвычайно важна на
просторах Сети.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Логическое разбиение данных&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Если какая-то часть данных всегда используется отдельно от основной
массы, то иногда имеет смысл выделить ее в отдельную независимую
систему хранения данных.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Низкоуровневая оптимизация запросов&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Ведя и анализируя логи запросов, можно определить наиболее медленные
из них. Замена найденных запросов на более эффективные с той же
функциональностью может помочь более рационально использовать
вычислительные мощности.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;В этом разделе стоит упомянуть еще один, более специфический, тип
интернет-проектов. Такие проекты оперируют данными, не имеющими четко
формализованную структуру, в таких ситуациях использование реляционных
СУБД в качестве хранилища данных, мягко говоря, нецелесообразно. В этих
случаях обычно используют менее строгие базы данных, с более примитивной
функциональностью в плане обработки данных, но зато они способны
обрабатывать огромные объемы информации не придираясь к его качеству и
соответствию формату. В качестве основы для такого хранилища данных
может служить кластерная файловая система, а для анализа же данных в
таком случае используется механизм под названием
&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;, принцип его работы я расскажу лишь вкратце,
так как в полном своем масштабе он несколько выходит за рамки данного
повествования.&lt;/p&gt;
&lt;p&gt;Итак, мы имеем на входе некие произвольные данные в не факт что
правильно соблюденном формате. В результате нужно получить некое
итоговое значение или информацию. Согласно данному механизму практически
любой анализ данных можно провести в следующие два этапа:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Map&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Основной целью данного этапа является представление произвольных
входных данных в виде промежуточных пар ключ-значение, имеющих
определенный смысл и формально оформленных. Результаты подвергаются
сортировке и группированию по ключу, а после чего передаются на
следующий этап.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Reduce&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Полученные после &lt;strong&gt;map&lt;/strong&gt; значения используются для финального
вычисления требуемых итоговых данных.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Каждый этап каждого конкретного вычисления реализуется в виде
независимого мини-приложения. Такой подход позволяет практически
неограниченно распараллеливать вычисления на огромном количестве машин,
что позволяет в мгновения обрабатывать объемы практически произвольных
данных. Для этого достаточно лишь запустить эти приложения на каждом
доступном сервере одновременно, а затем собрать воедино все результаты.&lt;/p&gt;
&lt;p&gt;Примером готового каркаса для реализации работы с данными по такому
принципу служит opensource проект Apache Foundation под названием
&lt;a href="https://www.insight-it.ru/storage/2008/hadoop/"&gt;&lt;em&gt;Hadoop&lt;/em&gt;&lt;/a&gt;, о котором я уже неоднократно
рассказывал ранее, да и &lt;a href="https://www.insight-it.ru/goto/1a5b89d0/" rel="nofollow" target="_blank" title="http://ru.wikipedia.org/wiki/Hadoop"&gt;статейку в Википедию&lt;/a&gt; написал в свое время.&lt;/p&gt;
&lt;h3 id="vmesto-zakliucheniia"&gt;Вместо заключения&lt;/h3&gt;
&lt;p&gt;Если честно, мне с трудом верится, что я смог написать настолько
всеобъемлющий пост и сил на подведение итогов уже практически не
осталось. Хочется лишь сказать, что в разработке крупных проектов важна
каждая деталь, а неучтенная мелочь может стать причиной провала. Именно
по-этому в этом деле учиться стоит не на своих ошибках, а на чужих.&lt;/p&gt;
&lt;p&gt;Хоть может быть этот текст и выглядит как некое обобщение всех постов из
серии &lt;a href="https://www.insight-it.ru/highload/"&gt;"Архитектуры высоконагруженных систем"&lt;/a&gt;, но врядли он
станет финальной точкой, надеюсь мне &lt;a href="/feed/"&gt;найдется что сказать&lt;/a&gt; по
этой теме и в будущем, может быть однажды это будет основано и на личном
опыте, а не просто будет результатом переработки массы полученной мной
информации. Кто знает?...&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 12 May 2008 09:00:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-05-12:theory/2008/masshtabiruemye-veb-arkhitektury/</guid><category>online</category><category>архитектура</category><category>веб-приложение</category><category>веб-проект</category><category>веб-сервер</category><category>геораспределение</category><category>информационные технологии</category><category>кэширование</category><category>Масштабируемость</category><category>распределенные вычисления</category><category>сегментирование баз данных</category></item><item><title>Архитектура Twitter</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-twitter/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/c2919313/" rel="nofollow" target="_blank" title="https://www.twitter.com"&gt;Twitter&lt;/a&gt; стартовал как побочный подпроект, но
не смотря на это темпы его роста были впечатляющими: путь от 0 до
миллионов просмотров страниц занял всего несколько коротких месяцев.
Ранние решения о проектировании системы неплохо справлялись с небольшими
нагрузками, но они быстро таяли под напором огромного количества
пользователей, желающих разослать весточки всем своим друзьям с ответом
на простой вопрос: а чем ты занимаешься?&lt;/p&gt;
&lt;p&gt;Поначалу все винили &lt;a href="/tag/ror/"&gt;Ruby on Rails&lt;/a&gt; во всех проблемах с
масштабированием, но Blaine Cook, главный архитектор Twitter, встал на
его защиту:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Основной для нас на самом деле является проблема горизонтального
масштабирования, с этой точки зрения &lt;a href="/tag/ror/"&gt;Ruby on Rails&lt;/a&gt; ничем
не хуже других языков программирования или framework'ов: переход на
"более быстрый" язык программирования дал бы нам 10-20% прирост
производительности, в то время архитектурные преобразования, легко
реализованные средствами &lt;a href="/tag/ror/"&gt;Ruby on Rails&lt;/a&gt;, сделали Twitter
быстрее на 10000%.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Даже если &lt;a href="/tag/ror/"&gt;Ruby on Rails&lt;/a&gt; оказался невиновен, как же тогда
Twitter научился с его помощью рости до все больших и больших высот?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Этот текст является продолжением &lt;a href="https://www.insight-it.ru/highload/"&gt;серии переводов&lt;/a&gt;, автор
&lt;a href="https://www.insight-it.ru/goto/9736f7f8/" rel="nofollow" target="_blank" title="http://highscalability.com/scaling-twitter-making-twitter-10000-percent-faster"&gt;оригинала&lt;/a&gt; -
Todd Hoff. На этот раз написать что-либо своими силами у меня не
сложилось, все мысли ушли на другой пост, который я скоро опубликую, а
перевод этот получился несколько менее строгим, чем обычно, но я думаю
ничего страшного.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/1a76cc37/" rel="nofollow" target="_blank" title="http://video.google.com/videoplay?docid=-7846959339830379167"&gt;Scaling Twitter Video&lt;/a&gt;
    от Blaine Cook.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a004222e/" rel="nofollow" target="_blank" title="http://www.slideshare.net/Blaine/scaling-twitter"&gt;Scaling Twitter Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7541c4c6/" rel="nofollow" target="_blank" title="http://talklikeaduck.denhaven2.com/articles/2007/06/22/good-news"&gt;Good News&lt;/a&gt;
    блог пост от Rick Denatale&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/96735c2c/" rel="nofollow" target="_blank" title="http://pragmati.st/2007/5/20/scaling-twitter"&gt;Scaling Twitter&lt;/a&gt; блог
    пост от Patrick Joyce&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7267856d/" rel="nofollow" target="_blank" title="http://readwritetalk.com/2007/09/05/biz-stone-co-founder-twitter/"&gt;Twitter API Traffic is 10x Twitter&amp;rsquo;s Site&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/5eb63819/" rel="nofollow" target="_blank" title="http://www.slideshare.net/britt/a-small-talk-on-getting-big-113066"&gt;A Small Talk on Getting Big. Scaling a Rails App &amp;amp; all that Jazz&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/ruby-on-rails/"&gt;Ruby on Rails&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/erlang/"&gt;Erlang&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mongrel/"&gt;Mongrel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/munin/"&gt;Munin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/nagios/"&gt;Nagios&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/google-analytics/"&gt;Google Analytics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/awstats/"&gt;AWStats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="statistika"&gt;Статистика&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Более 350000 пользователей. Точная цифра, как обычно, держится в
    секрете.&lt;/li&gt;
&lt;li&gt;Около 600 запросов в секунду.&lt;/li&gt;
&lt;li&gt;В среднем система поддерживает 200-300 соединений в секунду.
    Максимум обычно достигается при значении 800.&lt;/li&gt;
&lt;li&gt;MySQL обрабатывает примерно 2400 запросов в секунду.&lt;/li&gt;
&lt;li&gt;180 экземпляров приложений на Rails, использующих Mongrel как
    веб-сервер.&lt;/li&gt;
&lt;li&gt;1 MySQL сервер (одна большая машина с 8 ядрами) и 1 slave,
    используемый лишь для статистики и отчетов.&lt;/li&gt;
&lt;li&gt;30+ процессов для выполнения произвольных работ.&lt;/li&gt;
&lt;li&gt;8 Sun X4100&lt;/li&gt;
&lt;li&gt;Обработка запроса обычно занимает у Rails 200 миллисекунд.&lt;/li&gt;
&lt;li&gt;В среднем ответ на запрос к базе данных занимает 50-100 миллисекунд.&lt;/li&gt;
&lt;li&gt;Более 16 GB выделено под &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="arkhitektura"&gt;Архитектура&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Проект столкнулся с массой проблем, связанных с масштабируемостью.
    Маленькая птичка частенько давала сбои.&lt;/li&gt;
&lt;li&gt;Изначально не было реализовано никаких форм мониторинга, графиков
    или статистики, это очень затрудняло обнаружение м решение
    возникающих проблем. Впоследствии были внедрены &lt;a href="/tag/munin/"&gt;Munin&lt;/a&gt;
    и &lt;a href="/tag/nagios/"&gt;Nagios&lt;/a&gt;. Разработчики столкнулись с некоторыми
    трудностями при использовании этих продуктов в
    &lt;a href="/tag/solaris/"&gt;Solaris&lt;/a&gt;. Помимо этого был использован сервис Google
    Analytics, но от него обычно мало толку, особенно когда страницы
    даже не загружаются.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Активное использование кэширования средствами &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Например, если подсчет количества чего-либо выполняется медленно,
намного эффективнее один раз запомнить результат в
&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;, чем каждый раз считать его заново.&lt;/li&gt;
&lt;li&gt;Получение информации о статусе своих друзей - непростая задача.
Вместо использования запросов информация о статусе друзей
обновляется в кэше. База данных совсем не используется. Такой подход
позволяет получить предсказуемое время отклика (ограниченное сверху примерно 20 миллисекундами).&lt;/li&gt;
&lt;li&gt;Объекты ActiveRecord настолько велики, что кэширование их
нецелесообразно. Критичные атрибуты хранятся в хэше, а остальная их часть подвергается "ленивой загрузке" в момент запроса на доступ.&lt;/li&gt;
&lt;li&gt;90% запросов являются запросами к API. Таким образом кэширование
страниц или их фрагментов становится бессмысленным, зато никто не мешает им кэшировать сами API запросы.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Внутренняя организация работы с сообщениями:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Сообщения очень активно используются: производители генерируют
сообщения, они образуются в очереди, а затем распространяются по
потребителем.&lt;/li&gt;
&lt;li&gt;Основная функция Twitter заключается в реализации
своеобразного моста между различными форматами электронных сообщений
(SMS, электронная почта, сервисы мгновенного обмена сообщениями и так далее).&lt;/li&gt;
&lt;li&gt;Чтобы инвалидировать в кэше информацию можно просто отправить внутреннее сообщение, зачем выполнять все действия синхронно?&lt;/li&gt;
&lt;li&gt;Изначально этот механизм основывался на DRb (distributed Ruby) -
библиотека, позволяющая отправлять и принимать сообщения сообщения
между удаленными Ruby-объектами по TCP/IP. Но она была несколько
странноватой, да и являлось потенциально слабым местом с точки зрения стабильности.&lt;/li&gt;
&lt;li&gt;Со временем сервис перевели на Rinda, представляющую собой набор
общих для всей системы очередей. Но и у нее были недостатки: все очереди были постоянными, а данные терялись при сбоях.&lt;/li&gt;
&lt;li&gt;Следующей попыткой был Erlang. Но однажды возникла проблема: каким
образом сломавшийся сервер может продолжать работать, но при этом в
очереди откуда-то возникли целых 20000 ожидающих пользователей? Разработчики не знали. На лицо явный недостаток документации...&lt;/li&gt;
&lt;li&gt;В конце концов решение было разработано своими силами: Twitter
выпустил &lt;a href="/tag/starling/"&gt;Starling&lt;/a&gt;, распределенный легковесный
сервер очередей, написанный на Ruby и поддерживающий протокол memcache. Сейчас серверная часть Twitter управляется именно им.&lt;/li&gt;
&lt;li&gt;Распределенные очереди позволяют переживать сбои путем записи их
на диск в критических ситуациях. Другие крупные интернет-проекты также часто пользуются таким подходом.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Работа с SMS осуществляется с помощью сторонних сервисов и
    предоставляемых ими шлюзов. Достаточно дорогое удовольствие.&lt;/li&gt;
&lt;li&gt;Развертывание:&lt;ul&gt;
&lt;li&gt;Просто запускаются дополнительные сервера с mongrel, более элегантного решения пока нет.&lt;/li&gt;
&lt;li&gt;Все внутренние ошибки выдаются пользователям, если обслуживающий
их mongrel сервер на данный момент заменяется.&lt;/li&gt;
&lt;li&gt;Все сервера останавливаются одновременно. Отключение их по одному
по определенным причинам не используется.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Неправильное использование сервиса:&lt;ul&gt;
&lt;li&gt;Много времени сервис был не доступен, так как люди проходились
специальными программами по сайту с целью добавить всех кто
попадался под руку в друзья. 9000 друзей за 24 часа. Это
просто-напросто останавливало работу сайта.&lt;/li&gt;
&lt;li&gt;Были разработаны средства для своевременного обнаружения таких
ситуаций.&lt;/li&gt;
&lt;li&gt;Будте беспощадными, таких пользователей нужно просто удалять.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Сегментирование:&lt;ul&gt;
&lt;li&gt;Пока оно только в планах, сейчас оно не используется.&lt;/li&gt;
&lt;li&gt;В будущем оно будет основываться на времени, а не на
пользователях, так как запросы обычно очень локальны по времени.&lt;/li&gt;
&lt;li&gt;Сегментирование будет не так просто реализовать благодаря
автоматическому запоминанию результатов выполнения функций для
последующего повторного их использования. Никто не даст гарантии,
что операции "только для чтения" на самом деле будут таковыми
являться. Запись в slave, работающий в режиме read-only, - не самая
лучшая идея.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;API Twitter генерирует в 10 раз больше трафика, чем сам сайт.&lt;ul&gt;
&lt;li&gt;Их API - самая важная вещь из всех, что они разработали.&lt;/li&gt;
&lt;li&gt;Простота сервиса позволила разработчикам строить свои приложения
поверх инфраструктуры Twitter, привнося все новые и новые идеи.
Например, Twitterrific - красивый способ использовать Twitter в
небольшой команде.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Мониторинг используется для остановки слишком больших процессов.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="podvodim-itogi"&gt;Подводим итоги&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Общайтесь со своим сообществом. Не прячьтесь и не пытайтесь решить
    абсолютно все проблемы самостоятельно. Много отличных людей будут
    готовы помочь, достаточно лишь попросить.&lt;/li&gt;
&lt;li&gt;Рассматривайте вашу стратегию масштабирования как бизнес-план.
    Соберите советы помощников для того чтобы облегчить для себя
    принятие решений.&lt;/li&gt;
&lt;li&gt;Стройте свой проект сами. Twitter потратил много времени, пытаясь
    приспособить готовые решения других людей, которые казалось бы
    должны работать, но это оказалось не совсем так. Лучше построить
    какие-то вещи самостоятельно, чтобы иметь высокую степень контроля
    над ситуацией и иметь возможность привносить новые возможности как
    только они понадобились.&lt;/li&gt;
&lt;li&gt;Ставьте перед своими пользователями разумные ограничения. На обычных
    пользователей это не повлияет, но когда кому-нибудь взбредет в
    голову попытаться сломать систему (а такой человек рано или поздно
    найдется) - они сыграют свою роль и спасут работоспособность
    системы.&lt;/li&gt;
&lt;li&gt;Не делайте базу данных центральным узким местом системы, врядли Ваше
    приложение на самом деле требует гигантских операций по объединению
    данных из нескольких таблиц. Используйте кэширование, или проявите
    свою смекалку для поиска альтернативных способов достижения того же
    результата.&lt;/li&gt;
&lt;li&gt;Предусмотрите возможность сегментирования с самого начала, тогда
    перед Вами всегда будут открыты пути для дальнейшего
    масштабирования.&lt;/li&gt;
&lt;li&gt;Очень важно вовремя осознать, что сайт начинает работать медленно.
    Сразу стоит задуматься о системе отчетов для отслеживания
    потенциальных проблем.&lt;/li&gt;
&lt;li&gt;Оптимизируйте базу данных:&lt;ul&gt;
&lt;li&gt;Индексируйте все таблицы, Rails не будет делать это за Вас.&lt;/li&gt;
&lt;li&gt;Используйте "explain" для анализа выполнения запросов. Результаты
могут не совпадать с Вашими ожиданиями.&lt;/li&gt;
&lt;li&gt;Денормализуйте данные. Один только этот совет порой может спасти
ситуацию. Для примера, в Twitter хранят все ID друзей каждого
пользователя вместе, это позволило избежать многих ресурсоемких
запросов.&lt;/li&gt;
&lt;li&gt;Избегайте комплексного объединения данных из нескольких таблиц.&lt;/li&gt;
&lt;li&gt;Избегайте сканирования больших наборов данных.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Кэшируйте все, что только можно.&lt;/li&gt;
&lt;li&gt;Тестируйте все максимально тщательно:&lt;ul&gt;
&lt;li&gt;Когда Вы развертываете приложение, Вы должно быть уверены, что оно
будет работать корректно.&lt;/li&gt;
&lt;li&gt;Они используют полный набор средств для тестирования. Таким
образом, когда произошла неполадка в кэшировании, они узнали о ней
еще до того как она на самом деле произошла.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Длительно функционирующие процессы стоит оформить в виде daemon'ов.&lt;/li&gt;
&lt;li&gt;Используйте уведомления об исключительных ситуациях в совокупности с
    ведением логов, это необходимо для своевременного реагирования на
    них.&lt;/li&gt;
&lt;li&gt;Не делайте глупостей!&lt;ul&gt;
&lt;li&gt;Масштаб проект несколько меняет понятие "глупость".&lt;/li&gt;
&lt;li&gt;Пытаться загрузить 3000 друзей в память одновременно может
заставить сервер временно перестать функционировать, хотя когда
друзей было всего 4 - этот механизм прекрасно работал.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Большая часть производительности зависит не от использованного языка
    программирования, а от продуманной структуры приложения.&lt;/li&gt;
&lt;li&gt;Превратите свой сайт в открытый сервис с помощью создания API. Их
    API является ключом к успеху Twitter. Он позволяет пользователям
    создавать постоянно расширяющуюся экосистему вокруг Twitter,
    соревноваться с которой не так-то просто. Вы никогда не сможете
    сделать столько же работы, сколько смогут Ваши пользователи для Вас,
    Вам просто не хватит креативных идей. Так что не стесняйтесь,
    откройте свое приложение и сделайте интеграцию Вашего приложения с
    другими максимально простой и удобной!&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 10 May 2008 12:36:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-05-10:highload/2008/arkhitektura-twitter/</guid><category>AWStats</category><category>Erlang</category><category>Google Analytics</category><category>Memcached</category><category>mongrel</category><category>Munin</category><category>MySQL</category><category>Nagios</category><category>Ruby on Rails</category><category>Solaris</category><category>Starling</category><category>Twitter</category><category>архитектура</category><category>архитектура Twitter</category></item><item><title>Сегментирование базы данных</title><link>https://www.insight-it.ru//theory/2008/segmentirovanie-bazy-dannykh/</link><description>&lt;p&gt;&lt;img alt="Сегментирование базы данных" class="left" src="https://www.insight-it.ru/images/partitions.png"/&gt;
В процессе чтения моего блога у вас наверняка возникал вопрос: а что же
имеется в виду под фразой &lt;em&gt;сегментирование базы данных&lt;/em&gt;?
На самом деле это просто приглянувшийся мне вариант перевода термина
&lt;em&gt;sharding&lt;/em&gt; (или он же - &lt;em&gt;partitioning&lt;/em&gt;), в качестве альтернатив можно
было бы использовать &lt;em&gt;партиционирование&lt;/em&gt;, &lt;em&gt;секционирование&lt;/em&gt; или
что-нибудь еще менее звучное, суть от этого не меняется.&lt;/p&gt;
&lt;p&gt;Сильно сомневаюсь, что предыдущий абзац предоставил Вам полную
информацию по данному вопросу, так что позволю себе перейти к более
детальным ответам...
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="chto-eto-takoe"&gt;Что это такое?&lt;/h3&gt;
&lt;p&gt;Особенно в условиях Сети данные накапливаются и запрашиваются с
невероятной скоростью, рано или поздно даже самый мощный сервер
перестанет справляться с задачей хранения и предоставления данных.
Количество запросов в секунду, которое способен обеспечивать один сервер
ограничено ничуть не меньше, чем его дисковое пространство. Когда
запросы данных начинают поступать слишком интенсивно, чаще всего
прибегают к наиболее тривиальному решению: реплицирование данных на
несколько серверов и обработка запросов на чтение данных параллельно, а
записи - лишь на одном, классическая схема master-slave. Но и у такого
подхода есть предел, с ростом системы затраты вычислительных мощностей
на реплицирование данных рано или поздно начнут потреблять большую часть
процессорного времени, что сделает прирост производительности от
простого добавления в систему дополнительных серверов минимальным.
Единственным выходом из такой ситуации становится пересмотр архитектуры
всей системы хранения данным, одним из возможных исходов которого и
сожет стать сегментирование базы данных.&lt;/p&gt;
&lt;p&gt;Сама идея сегментирования проста: разбить все данные на части по
какому-либо признаку и хранить каждую часть на отдельном сервере или
кластере, такую часть данных в совокупности с системой хранения данных,
в которой она находится, и называют сегментом или shard'ом.&lt;/p&gt;
&lt;p&gt;Признак по которому разделяются данные должен быть максимально прост и
очевиден, ведь в случае необходимости получения конкретных данных именно
он будет служить путеводителем для определения в каком именно сегменте
эти данные необходимо искать. В большинстве ситуаций такая необходимость
возникает очень часто, а значит и проверка признака должна выполняться
очень быстро. Сами эти признаки можно распределить на несколько групп по
принципу их проверки:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Диапазон&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Самый простой и тривиальный вариант, выбирается один из параметров
какой-либо записи, например идентификационный номер, и проверяется
его принадлежность определенному диапазону, например все записи с ID
от 0 до 999 хранятся на одном сервере, от 1000 до 1999 - на другом,
и так далее.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Список&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Принцип остается такой же как и при использовании диапазонов, с той
лишь разницей что проверяется принадлежность параметра какому-либо
списку значений (который может состоять и из одного значения), а не
диапазону, например: людей можно разбить по районам проживания или
их имени.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Хэш-функция&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;О принципах работы хэширующих функций &lt;a href="https://www.insight-it.ru/security/2008/obratnogo-puti-net/"&gt;я уже рассказывал&lt;/a&gt;, так что
лишь вкратце опишу принцип такого подхода: для определения в каком
именно сегменте хранится та или иная запись, один из ее заранее
известных параметров передается хэширующей функции, возвращающей в
качестве результата номер сегмента от 0 до (n-1), где - n общее
количество сегментов.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Композиция&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;При дальнейшем росте объемов данных и нагрузке на систему можно
несколько усложнить ее работу, реализовав сегментирование на основе
композиции из нескольких упомянутых выше признаков.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;В условиях интернет-проектов данные обычно разбиваются по принадлежности
к пользователю, автором которых он является (или просто они как-либо
взаимосвязаны), с использованием хэш-функций.&lt;/p&gt;
&lt;h3 id="dlia-chego-eto-vse-nuzhno"&gt;Для чего это все нужно?&lt;/h3&gt;
&lt;p&gt;Возможно описание получилось несколько устрашающим, но на самом деле вся
эта история с распределением данных того стоит. Для начала стоит
отметить, что такой подход позволяет оперировать огромными объемами
данными с невероятной скоростью, благодаря параллельной обработке
запросов в абсолютно разных частях системы. Количество запросов,
обрабатываемых каждым узлом одновременно невелико, что не может не
сказаться на скорости обработки каждого запроса.&lt;/p&gt;
&lt;p&gt;Использование в качестве системы хранения данных для каждого сегмента
кластера, а не просто одного сервера, может существенно повысить
надежность системы в случае программных или аппаратных сбоев. Тем более
в случае использования master-slave репликации в рамках каждого
сегмента, у системы в целом все равно не будет единственного сервера для
обработке операций записи позволит минимизировать издержки
реплицирования данных.&lt;/p&gt;
&lt;h3 id="a-kak-zhe"&gt;А как же...&lt;/h3&gt;
&lt;h4&gt;...перераспределять данные?&lt;/h4&gt;
&lt;p&gt;При необходимости по тем или иным причинам изменить количество сегментов
возникновение ситуации, когда система нагружена неравномерно, очень
вероятно. Наиболее простым решением было бы простое перераспределение
записей, но как же его реализовать?&lt;/p&gt;
&lt;p&gt;Над этой проблемой стоит задуматься сразу же при переходе к
сегментированной архитектуре. Она может сначала показаться нерешаемой
без одновременного перемещения огромных массивов данных и временной
практически полной потери производительности, но это не так. Наиболее
элегантным решением является организация для системы некоторого сервиса
определения местоположения данных, в обязанности входит не только
определение номера сегмента по заранее определенному алгоритму, но и
постепенное перемещение данных в случае необходимости. Например, в
случае появления необходимости разбить сегмент на две части в связи с
приближающимся переполнением дискового простронства, данный сервис
начнет постепенно последовательно создавать копии записей на новом или
существующем слабо загруженном сегменте. Пока копирование каждой
конкретной записи не будет завершено операции чтения перенаправляются на
исходный сегмент. Как только процесс завершился - перенаправление данных
переключается на копию, а оригинал уничтожается, после чего система
переходит к перемещению следующей записи.&lt;/p&gt;
&lt;h4&gt;...выполнить операцию, затрагивающию разные сегменты?&lt;/h4&gt;
&lt;p&gt;Когда все данные хранились в одном месте - можно было бы выполнить один
сложноструктурированный запрос и получить все нужные данные, но при
таком распределении данных это не возможно. Для достижения такого же
результата необходимо выполнение нескольких запросов к различным
сегментам и аггрегация полученных данных на программном уровне.&lt;/p&gt;
&lt;h4&gt;...реализовать все это?&lt;/h4&gt;
&lt;p&gt;Один из самых интересных вопросов, которые можно было бы задать по
данной теме. Однозначно лучшего решения для реализации этого подхода не
существует. В большинстве случаев приходится реализовывать теорию на
практике своими силами, но готовые решения все же тоже существуют:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/2b245ae9/" rel="nofollow" target="_blank" title="http://www.enterprisedb.com/community/projects/gridsql.do"&gt;GridSQL&lt;/a&gt;
    от EnterpriseDB предоставляет систему сегментирования на базе
    &lt;a href="/tag/postgresql/"&gt;PostgreSQL&lt;/a&gt; (которую, кстати, не так давно под GPL
    опубликовали);&lt;/li&gt;
&lt;li&gt;Многие реляционные системы управления базами данных, такие как MySQL
    и Oracle, имеют собственную встроенную систему разбиения данных на
    партиции;&lt;/li&gt;
&lt;li&gt;В рамках проекта &lt;a href="https://www.insight-it.ru/goto/cacbd918/" rel="nofollow" target="_blank" title="http://www.hibernate.org"&gt;Hibernate&lt;/a&gt;
    разрабатывается библиотека, инкапсулирующая сегментирование данных;&lt;/li&gt;
&lt;li&gt;В &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-livejournal/"&gt;статье об архитектуре LiveJournal&lt;/a&gt; я рассказывал о спектре opensource-решений от разработчиков этого проекта для схожих задач.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Так или иначе ни один из них не является средством из серии "установил и
все сразу заработало", они лишь могут упростить реализацию такой
системы, существенную часть работы придется проделать самостоятельно.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ещё остались вопросы по теме?&lt;/em&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 01 May 2008 20:12:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-05-01:theory/2008/segmentirovanie-bazy-dannykh/</guid><category>partitioning</category><category>shard</category><category>sharding</category><category>архитектура</category><category>Масштабируемость</category><category>партиционирование</category><category>сегментирование</category><category>сегментирование баз данных</category><category>секционирование</category><category>СУБД</category></item><item><title>Архитектура LiveJournal</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-livejournal/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/e53eaf70/" rel="nofollow" target="_blank" title="http://www.livejournal.com"&gt;LiveJournal&lt;/a&gt; был одним из первых сервисов,
бесплатно предоставляющих всем желающим личный блог. Практически с
самого начала своего существования в далеком 1999 году проект столкнулся
с непрерывно растущим потоком желающих воспользоваться услугами сервиса.
Как же проекту удалось справиться с предоставлением маленького кусочка
интернета каждому желающему, обойдя при этом всех конкурентов?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Возможно Вы ожидали увидеть здесь очередной перевод статьи с
английского, но тогда придется Вас разочаровать, на этот раз я решил
попробовать свои силы в самостоятельном написании статьи на такую
серьезную тему. Просьба особо сильно помидорами в меня не кидаться :)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Основным источником информации послужила &lt;a href="https://www.insight-it.ru/goto/a945bdaf/" rel="nofollow" target="_blank" title="http://video.google.com/videoplay?docid=-8953828243232338732"&gt;презентация Brad Fitzpatrick&lt;/a&gt;
в Токио.&lt;/p&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; (&lt;a href="/tag/debian/"&gt;Debian Sarge&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/perl/"&gt;Perl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt; 4.0/4.1 в основном с InnoDB&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/perlbal/"&gt;Perlbal&lt;/a&gt;, веб-сервер и балансировщик нагрузки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; для распределенного кэширования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mogilefs/"&gt;MogileFS&lt;/a&gt;, распределенная файловая система&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/gearman/"&gt;Gearman&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/theshwartz/"&gt;TheShwartz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/djabberd/"&gt;djabberd&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="statistika"&gt;Статистика&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;на данный момент 15320315 учетных записей; &lt;em&gt;(10.04.08)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;из них активно используется 551589;&lt;/li&gt;
&lt;li&gt;наиболее активно сервис используется в США и Российской федерации, а
    2/3 пользователей - девушки и женщины;&lt;/li&gt;
&lt;li&gt;более 15 миллионов новых записей в блогах за месяц;&lt;/li&gt;
&lt;li&gt;более 50 миллионов просмотров страниц в день, при пиковой нагрузке -
    несколько тысяч в секунду &lt;em&gt;(сильно устаревшие цифры, 2004 год);&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;связь с внешним миром осуществляется через два BIG-IP (активный + в
    режиме ожидания) с автоматическим восстановлением работоспособности
    в случае сбоя в работе одного из них, защитой от DDoS, L7 набором
    правил, включая TCL;&lt;/li&gt;
&lt;li&gt;более сотни серверов, насчет конфигурации известен только тот факт,
    что практически на каждом сервере установлены огромные объемы
    оперативной памяти (более 12 GB) для эффективного кэширования.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="istoriia"&gt;История&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Все началось с одного обычного сервера. Он выполнял роль как
    веб-сервера так и базы данных. Единственный плюс такого подхода к
    организации работы оборудования - достаточно дешево. Само собой
    достаточно скоро этот сервер перестал справляться с нагрузкой.&lt;/li&gt;
&lt;li&gt;Следующим шагом было разнесение веб-сервера и базы данных на разные
    серверы, всего их получилось два. По прежнему имелось два узла, сбой
    в которых означал недоступность сервиса. По прежнему вычислительная
    мощность такой системы оставалась более чем скромной.&lt;/li&gt;
&lt;li&gt;Первым из тех двух серверов, как ни странно, перестал справляться с
    нагрузкой веб-сервер - докупили еще два. Веб-сервера три, внешний
    IP - один, теперь приходится как-то распределять нагрузку! А как
    добавить еще одну базу данных?&lt;/li&gt;
&lt;li&gt;Новый сервер баз данных был подключен в роли slave к исходному,
    данные в нем обновлялись с помощью репликации, а обрабатывал он
    только операции чтения, оставив все операции записи первому серверу.&lt;/li&gt;
&lt;li&gt;Есть предположения о том, к чему привело дальнейшее добавление новых
    серверов? Правильно - к полнейшему хаосу! Со временем стала
    возникать проблема масштабируемости баз данных. Операции чтения
    производились на каком-то одном сервере, но когда приходил запрос на
    запись данных, так или иначе данные приходилось производить
    обновление на каждом из slave серверов. В итоге выполнение
    синхронизации данных стало занимать подавляющее большинство
    процессорного времени slave серверов, что привело к отсутствию
    возможности продолжать масштабирование просто добавлением
    дополнительных серверов.&lt;/li&gt;
&lt;li&gt;Пришло время задуматься над архитектурой системы и распределением
    операций записи. Основной целью стало избавиться от такой серьезной
    избыточности данных, так как это было практически пустой тратой
    времени копировать одни и те же данные на десяток машин, да еще и с
    RAID на каждой из них.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Наиболее эффективным подходом в такой ситуации является
сегментирование базы данных. Все серверы баз данных разбиваются на
небольшие кластеры. Каждый пользователь системы прозрачно
привязывается к определенному кластеру, таким образом когда он
обновляет свой блог или какие-либо еще данные, запись ведется в
рамках только небольшой группы серверов, такой же принцип справедлив
и для чтения.&lt;/p&gt;
&lt;p&gt;Применительно к LiveJournal эту схему лучше всего демонстрирует один
из слайдов презентации, указанной в источниках информации:
&lt;img alt="Сегментирование базы данных в Livejournal" class="responsive-img" src="https://www.insight-it.ru/images/lj-scheme.jpeg" title="Механизм работы сегментированной базы данных в LiveJournal"/&gt;&lt;/p&gt;
&lt;p&gt;При работе такой системы не используется &lt;code&gt;auto_increment&lt;/code&gt; в
&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;, а также используется составной primary key из
номера пользователя и номера записи. Таким образом пространство имен
объектов разбито на группы, соответствующие конкретному
пользователю.&lt;/p&gt;
&lt;p&gt;Дальнейшим развитием решения проблемы излишней избыточности данных
может послужить отказ от кластеров, аналогичных по структуре
исходному для хранения сегментов базы данных. Это может быть как
вариант с общим на несколько серверов хранилищем данных, так и более
низкоуровневая репликация данных средствами &lt;abbr title="Distributed Replicated Block Device"&gt;DRBD&lt;/abbr&gt; в
совокупности с HeartBeat. Каждый из возможных вариантов
кластеризации MySQL имеет массу положительных и отрицательных
сторон, так что конкретного лидера среди них выделить достаточно
сложно. Возможно именно это и подтолкнуло разработчиков построить
собственное решение, комбинируя их с целью получения наилучшего
эффекта.&lt;/p&gt;
&lt;h3 id="programmnoe-obespechenie"&gt;Программное обеспечение&lt;/h3&gt;
&lt;p&gt;В ситуации, когда не удавалось найти готового программного решения для
какой-то конкретной задачи, они не боялись взяться за написание его
самостоятельно, это стало одним из основных компонентов успеха проекта.
Существенная часть программной платформы LiveJournal написана специально
для этого проекта и выпущено под свободной лицензией с открытым исходным
кодом, доступным в &lt;a href="https://www.insight-it.ru/goto/f135e7bd/" rel="nofollow" target="_blank" title="http://code.sixapart.com/"&gt;официальном SVN репозитории&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;memcached&lt;/h4&gt;
&lt;p&gt;Залогом быстрой загрузки любой страницы крупного интернет-проекта
является кэширование. Но как всегда возникает вопрос: а на каком уровне
обработки данных его стоит выполнять? Для динамических страниц
недопустимо кэширование на уровне готовых страниц. Можно кэшировать на
уровне &lt;code&gt;mod_perl&lt;/code&gt;, но по сути это пустая трата оперативной памяти, так
как создастся отдельный кэш для каждого потока &lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt;, и
количество промахов мимо кэша будет огромно. Кэширование запросов MySQL
или HEAP таблицы также не дали бы требуемого результата ввиду
чрезвычайной распределенности базы данных.&lt;/p&gt;
&lt;p&gt;Выходом из сложившейся ситуации стало написание собственной
распределенной системы кэширования объектов, получившей название
&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;. Она позволяет:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;использовать для кэширования свободную оперативную память
    практически любого компьютера, задействованного в системе;&lt;/li&gt;
&lt;li&gt;кэшировать объекты практически любого языка программирования в
    сериализованном виде: &lt;a href="/tag/perl/"&gt;Perl&lt;/a&gt;, &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;,
    &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, &lt;a href="/tag/c/"&gt;C++&lt;/a&gt; и так далее;&lt;/li&gt;
&lt;li&gt;использовать для передачи кэшируемых данных простой протокол, не
    требующий избыточности данных;&lt;/li&gt;
&lt;li&gt;избегать даже теоретической возможности полного сбоя работы
    кэшируещей системы в связи с полной равнозначностью серверов;&lt;/li&gt;
&lt;li&gt;достигать превосходной производительности при формировании HTML-кода
    страниц;&lt;/li&gt;
&lt;li&gt;в разы снизить нагрузку на базы данных в проекте любого масштаба.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Этот продукт на практике оказался более чем эффективен, о чем
свидетельствует его более чем успешное использование во многих
крупнейших веб-проектах.&lt;/p&gt;
&lt;h4&gt;Perlbal&lt;/h4&gt;
&lt;p&gt;При решении вопроса, связанного с балансировкой нагрузки между
веб-серверами, пришлось перепробовать далеко не один десяток готовых
решений, но, к сожалению, ни один из них не смог удовлетворить все
потребности проекта. Не растерявшись, разработчики написали свое решение
этой задачи и назвали его &lt;a href="/tag/perlbal/"&gt;Perlbal&lt;/a&gt;. Конкурентов у него
множество, начиная от решений на уровне оборудования, например от
Foundry, заканчивая proxy балансировщиками нагрузки встроенные в более
популярные веб-сервера, но, тем не менее, продукт получился достаточно
конкурентноспособным. Он удовлетворял всем требованиям, выдвигаемым
разработчиками проекта:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;быстрый;&lt;/li&gt;
&lt;li&gt;небольшой размер;&lt;/li&gt;
&lt;li&gt;"сообразительный";&lt;/li&gt;
&lt;li&gt;обработка "мертвых" узлов;&lt;/li&gt;
&lt;li&gt;может выступать как в роли reverse proxy, так и балансировщика
    нагрузки;&lt;/li&gt;
&lt;li&gt;базовый функционал классического веб-сервера;&lt;/li&gt;
&lt;li&gt;реализация внутреннего перенаправления данных;&lt;/li&gt;
&lt;li&gt;поддержка некоторых менее существенных трюков, реализованных обычно
    в виде plug-in'ов.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="/tag/perlbal/"&gt;Perlbal&lt;/a&gt; не так активно используется вне LiveJournal, по
сравнению с &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;, но для решения конкретной
задачи он подошел как нельзя лучше.&lt;/p&gt;
&lt;h4&gt;MogileFS&lt;/h4&gt;
&lt;p&gt;Идея распределенных файловых систем далеко не нова, достаточно вспомнить
лишь &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt; или любой ее opensource аналог. Сам факт создания
такой системы был очень легок, изначальная версия была написана за одни
выходные, но при доведении ее до требуемого уровня качества пришлось
попотеть. Решение о ее создании было развитием идеи распределения
операций записи. Общая принцип хранения файлов прост: каждый файл в ФС
относится к определенному классу файлов, который определяет все правила
работы с файлом, в основном механизм его реплицирования, об остальном
заботится сама система.&lt;/p&gt;
&lt;p&gt;Как и все файловые системы этого класса,
&lt;acronym title="oMgFileS"&gt;MogileFS&lt;/acronym&gt; работает на уровне
пользовательских приложений и использует достаточно тривиальные протокол
передачи данных и общую архитектуру: клиенты, управляющие серверы,
абстрактные базы данных, сервера для хранения самих данных - в этом
плане ничего нового придумано не было. Доступ к файлам осуществляется с
помощью HTTP-запросов PUT/GET либо через виртуальный NFS-раздел.
Единственной особенностью можно назвать уклон в построение собой
абстрактной прослойки между приложением и собственно кластером базы
данных (в случае LiveJournal - сегмента), используемой в роли
альтернативы более тривиальной master/slave схемы.&lt;/p&gt;
&lt;h4&gt;Gearman&lt;/h4&gt;
&lt;p&gt;&lt;acronym title="manaGer"&gt;Gearman&lt;/acronym&gt; по сути прост до безобразия,
но это не мешает ему быть чрезвычайно эффективным. Возможно Вы уже
догадались в чем суть этого еще одного продукта, написанного специально
для LJ, если уже навели курсор на акроним в начале этого абзаца, если же
нет - поясню: он управляет общей работой системы средствами
клиент-серверной архитектуры и высокопроизводительного бинарного
протокола. С их помощью он способен удаленно вызывать практически любые
процедуры на удаленных серверах с минимальными задержками во времени.
Казалось бы ничего особенного он сам по себе не делает, но на самом деле
он выполняет очень важную функцию: увеличивает степень параллельности
выполнения операций, необходимых для полноценного функционирования
проекта. Единственное &lt;strong&gt;но&lt;/strong&gt; в работе этого механизма заключается в том,
что он не предоставляет никаких гарантий успешности выполнения работ.&lt;/p&gt;
&lt;p&gt;В рамках LiveJournal &lt;acronym title="manaGer"&gt;Gearman&lt;/acronym&gt;
применяется в основном для:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;обработка изображений средствами Image::Magick вне perl-приложений;&lt;/li&gt;
&lt;li&gt;создание pool'а DBI соединений (DBD::Gofer + Gearman);&lt;/li&gt;
&lt;li&gt;уменьшением нагрузки, создаваемой отдельными компонентами системы;&lt;/li&gt;
&lt;li&gt;улучшения субъективного впечатления пользователей о быстродействии
    сервиса, благодаря выполнению части работ параллельно в фоновом
    режиме;&lt;/li&gt;
&lt;li&gt;выполнение блокирующего ресурсы кода отдельно от обработчиков
    различных событий.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;TheShwartz&lt;/h4&gt;
&lt;p&gt;В качестве альтернативы gearman'у для работ, для выполнения которых
необходимы некоторые гарантии успешности, а также некоторая
стабильность, была разработана эта библиотека. Общая схема работы
осталась та же: клиент-серверная, но за стабильность приходится
платить - производительность существенно ниже, возможно возникновение
задержек.&lt;/p&gt;
&lt;p&gt;Хоть эти два продукта и выполняют схожие функции, используются они
обычно в совокупности друг с другом, просто-напросто обрабатывая разные
типы работ.&lt;/p&gt;
&lt;p&gt;Основными сферами применения TheShwartz в LJ являются:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;отправка электронной почты (SMTP клиент);&lt;/li&gt;
&lt;li&gt;LJ Notifications: каждое событие может вызывать за собой цепочку из
    тысяч уведомлений по электронной почте, SMS, XMPP и так далее;&lt;/li&gt;
&lt;li&gt;отправка RPC сообщений внешним сервисам;&lt;/li&gt;
&lt;li&gt;внедрение Atom потоков;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;djabberd&lt;/h4&gt;
&lt;p&gt;Как всегда следуя принципу "чем проще - тем лучше", разработки LJ
написали этот крошечный daemon, лежащий в основе их Jabber/LJTalk. Он
способен спокойно работать с более чем 300 тысячами соединений,
используя очень скромное количество оперативной памяти для поддержания
каждого соединения.&lt;/p&gt;
&lt;p&gt;Основной причиной для написания собственного Jabber-сервера, стало
недостаточная расширяемость и масштабируемость существующих решений.
Была необходимость в реализации многих нестандартных функций, вроде
индивидуальных обработчиков пользовательских изображений и личных
данных, обычно в других решениях было доступно только изменение методов
аутентификации.&lt;/p&gt;
&lt;h3 id="podvodim-itogi"&gt;Подводим итоги&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Если перед Вами появилась нетривиальная задача - не бойтесь написать
    программное обеспечение для ее решения самостоятельно! Пускай,
    возможно, это потребует некторых дополнительных усилий, но масса
    преимуществ, связанных с полным соответствием требованиям
    конкретного проекта, превосходит все издержки дополнительной
    разработки.&lt;/li&gt;
&lt;li&gt;Невозможно масштабировать проект просто постоянно добавляя новые
    сервера, рано или поздно все же прийдется задуматься об его
    архитектуре;&lt;/li&gt;
&lt;li&gt;Распределение нагрузок и параллельное операций порой заслуживают
    того, чтобы разработчики обратили на них внимание;&lt;/li&gt;
&lt;li&gt;"Мы ненавидим изобретать колесо! Но тем не менее, если колесо не
    существует или оно квадратное, то мы не боимся изобретать круглое
    колесо." (с)&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 10 Apr 2008 00:24:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-04-10:highload/2008/arkhitektura-livejournal/</guid><category>Apache</category><category>Debian</category><category>djabberd</category><category>gearman</category><category>Memcached</category><category>MogileFS</category><category>MySQL</category><category>opensource</category><category>Perl</category><category>Perlbal</category><category>TheShwartz</category><category>архитектура</category><category>архитектура LiveJournal</category><category>Масштабируемость</category></item><item><title>Архитектура Digg</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-digg/</link><description>&lt;p&gt;Трафик, генерируемый более чем 1.2 миллионами пользователей
&lt;a href="https://www.insight-it.ru/goto/e072c5ff/" rel="nofollow" target="_blank" title="http://www.digg.com"&gt;Digg&lt;/a&gt;, знаменитых своей жаждой информации,
способен загнать любой невинный сайт за рамки его вычислительных
ресурсов и пропускной способности канала. Как же сам Digg справляется с
такой нагрузкой?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Этот текст - перевод
&lt;a href="https://www.insight-it.ru/goto/83eb8e27/" rel="nofollow" target="_blank" title="http://highscalability.com/digg-architecture"&gt;статьи&lt;/a&gt;, автор - &lt;a href="https://www.insight-it.ru/goto/f3f1b405/" rel="nofollow" target="_blank" title="http://highscalability.com/user/todd-hoff"&gt;Todd
Hoff&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/3c9da0db/" rel="nofollow" target="_blank" title="http://www.computerworld.com/action/article.do?command=viewArticleBasic&amp;amp;articleId=9017778"&gt;Как Digg.com использует LAMP для масштабирования&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/fa3d3949/" rel="nofollow" target="_blank" title="http://www.oreillynet.com/onlamp/blog/2006/04/digg_phps_scalability_and_perf.html"&gt;Масштабируемость и производительность PHP в Digg&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/lucene/"&gt;Lucene&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/apc/"&gt;APC PHP Accelerator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="statistika"&gt;Статистика&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Проект стартовал в конце 2004 года на одном сервере под управлением
    &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; с использованием &lt;a href="/tag/apache/"&gt;Apache 1.3&lt;/a&gt;, &lt;a href="/tag/php/"&gt;PHP
    4&lt;/a&gt; и &lt;a href="/tag/mysql/"&gt;MySQL 4.0&lt;/a&gt; (со стандартной системой
    хранения данных - MyISAM).&lt;/li&gt;
&lt;li&gt;Более 1.2 миллиона пользователей.&lt;/li&gt;
&lt;li&gt;Более 200 миллионов просмотров страниц в месяц.&lt;/li&gt;
&lt;li&gt;100 серверов расположены в нескольких датацентрах, из них:
    &amp;ndash; 20 серверов баз данных;
    &amp;ndash; 30 веб-серверов;
    &amp;ndash; несколько поисковых серверов, использующих Lucene;
    &amp;ndash; остальные используются для обеспечения избыточности.&lt;/li&gt;
&lt;li&gt;30 GB данных.&lt;/li&gt;
&lt;li&gt;Ни одна из проблем, с которыми пришлось столкнуться проекту не была
    связана с &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;, в основном они касались базы данных.&lt;/li&gt;
&lt;li&gt;Легковесная природа &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; позволила переместить
    вычислительные работы из базы данных в приложение для улучшения
    производительности.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="chto-vnutri"&gt;Что внутри?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Балансировщик нагрузки равномерно распределяет запросы между
    &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; серверами.&lt;/li&gt;
&lt;li&gt;MySQL используется по принципу master-slave:
    -&amp;nbsp; Сервера, обрабатывающие большое количество транзакций, используют
    движок InnoDB.
    -&amp;nbsp; Сервера, выполняющие аналитическую обработку данных в реальном
    времени, используют MyISAM.
    -&amp;nbsp; Снижения производительности при переходе с &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt; 4.1
    на версию 5 замечено не было.&lt;/li&gt;
&lt;li&gt;Для кэширования используется &lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Используется сегментирование баз данных.&lt;/li&gt;
&lt;li&gt;Особенности использования Digg существенно облегчают процесс
    масштабирования. Большинство посетителей просто просматривают
    главную страницу и уходят. Это приводит к тому, что 98% запросов к
    базе данных являются операциями чтения. Такое соотношение операций
    чтения и записи позволяет не беспокоиться о комплексной работе по
    проектированию операций записи, что позволяет намного проще
    масштабировать проект.&lt;/li&gt;
&lt;li&gt;Возникали проблемы, связанные с системой хранения данных, которые
    сообщали, что данные уже записаны на диск, когда на самом деле это
    было не так. Контроллеры делали это для создания впечатления более
    высокой производительности. Но на практике это приводило лишь к
    проблемам с целостностью данных. Это достаточно распространенная
    проблема, которую порой не так уж просто решить, правда все зависит
    от используемого оборудования.&lt;/li&gt;
&lt;li&gt;Для облегчения нагрузки на базы данных используется кэширование и
    &lt;a href="/tag/apc/"&gt;APC PHP Accelerator&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;С использованием рабочих потоков &lt;a href="/tag/apache/"&gt;Apache2&lt;/a&gt;, FastCGI и
    &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; акселератора возможно избежать необходимости каждый
    раз заново интерпретировать и компилировать PHP скрипты: скрипт
    компилируется только при первом обращении, что существенно ускоряет
    скорость его выполнения при последующих обращениях.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="podvodim-itogi"&gt;Подводим итоги&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Используйте возможность выбора движка для &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;. Если
    Вам нужны транзакции - используйте InnoDB, если нет - MyISAM.
    Например, если на master сервере расположены транзакционные таблицы,
    то для slave серверов можно использовать и MyISAM.&lt;/li&gt;
&lt;li&gt;В определенный момент рост стал невозможен путем добавления
    дополнительной оперативной памяти, пришлось продолжать рост путем
    изменения архитектуры.&lt;/li&gt;
&lt;li&gt;Люди часто жалуются, что Digg медлителен. Скорее это вызвано их
    огромными &lt;a href="/tag/javascript/"&gt;JavaScript&lt;/a&gt; библиотеками, чем работой их
    серверной системы.&lt;/li&gt;
&lt;li&gt;Стоит тщательно выбирать какие именно приложения развертывать. Они
    приложили все усилия, чтобы не использовать приложения, требующие
    больших вычислительных мощностей. Очевидно, что Digg работает на
    совершенно стандартной &lt;a href="/tag/lamp/"&gt;LAMP&lt;/a&gt; архитектуре, но тем не
    менее реализована она достаточно интересно. У инженеров часто
    возникает желание реализовать какой-либо дополнительный функционал,
    но всегда стоит иметь ввиду, что они могут разрушить инфраструктуру,
    если она не сможет расти теми же темпами. Так что с этим стоит
    повременить до тех пор пока система сможет выдерживать все
    необходимые нагрузки. Это приводит к планированию ресурсов, особенно
    большое внимание этому аспекту уделяет &lt;a href="/tag/flickr/"&gt;Flickr&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Вам остается лишь догадываться, сможет ли &lt;a href="/tag/digg/"&gt;Digg&lt;/a&gt; удержать
    свои позиции, если и дальше будет ограничивать добавление новых
    возможностей, или уступит более активно развивающимся сервисам
    социальных закладок? Возможно если бы была возможность увеличивать
    масштабы более простыми методами, более быстрое добавление новых
    функций и возможностей позволило бы более эффективно конкурировать
    на этом рынке? С другой стороны, просто добавление новых
    возможностей может и не поменять ситуацию кардинальным образом.&lt;/li&gt;
&lt;li&gt;Основные проблемы с масштабируемостью и производительностью связаны
    с обработкой данных и в большинстве случаев они не зависят от
    используемого языка программирования. Вы столкнетесь с ними при
    работе с &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;, &lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt;, или
    подставьте сюда Ваш любимый язык программирования.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 01 Apr 2008 20:49:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-04-01:highload/2008/arkhitektura-digg/</guid><category>APC</category><category>Digg</category><category>LAMP</category><category>Linux</category><category>Lucene</category><category>Memcached</category><category>MySQL</category><category>online</category><category>PHP</category><category>архитектура</category><category>архитектура Digg</category><category>интернет</category></item><item><title>Архитектура Wikimedia</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-wikimedia/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/687b5b81/" rel="nofollow" target="_blank" title="http://wikimedia.org"&gt;Wikimedia&lt;/a&gt; является платформой для
&lt;a href="https://www.insight-it.ru/goto/35f4fea7/" rel="nofollow" target="_blank" title="http://wikipedia.org"&gt;Wikipedia&lt;/a&gt;, &lt;a href="https://www.insight-it.ru/goto/389e980c/" rel="nofollow" target="_blank" title="http://wiktionary.org"&gt;Wiktionary&lt;/a&gt; и
еще семи менее крупных wiki-проектов. Этот документ очень пригодится
новичкам, пытающимся довести свои проекты до масштабов гигантских
вебсайтов. Здесь можно найти множество интересных деталей и
инновационных идей, которые уже успели доказать свою работоспособность
на самых посещаемых сайтах всего Интернета.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Перевод &lt;a href="https://www.insight-it.ru/goto/cd47c021/" rel="nofollow" target="_blank" title="http://highscalability.com/wikimedia-architecture"&gt;статьи&lt;/a&gt;.
Автор - &lt;a href="https://www.insight-it.ru/goto/f3f1b405/" rel="nofollow" target="_blank" title="http://highscalability.com/user/todd-hoff"&gt;Todd Hoff&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/c8e5f8d0/" rel="nofollow" target="_blank" title="http://www.nedworks.org/~mark/presentations/san/Wikimedia%20architecture.pdf"&gt;Архитектура Wikimedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/62eceab/" rel="nofollow" target="_blank" title="http://meta.wikimedia.org/wiki/Wikimedia_servers"&gt;Серверы Wikimedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/c56ad62f/" rel="nofollow" target="_blank" title="http://oracle2mysql.wordpress.com/2007/08/22/12/"&gt;scale-out vs scale-up&lt;/a&gt; из блога "Oracle to MySQL"&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/lvs/"&gt;LVS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/lucene/"&gt;Lucene&lt;/a&gt; для поиска&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt; для распределенного кэширования объектов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/lighttpd/"&gt;lighttpd&lt;/a&gt; для работы с изображениями&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="statitstika"&gt;Статитстика&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;8 миллионов статей распределены по сотням языковых подпроектов
    (английские, голландские, ...)&lt;/li&gt;
&lt;li&gt;В десятке самых высоконагруженных проектов по данным
    &lt;a href="https://www.insight-it.ru/goto/3b390e59/" rel="nofollow" target="_blank" title="http://alexa.com"&gt;Alexa&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Экспоненциальный рост: в терминах посетителей, трафика и серверов
    удвоение происходит каждые 4-6 месяцев&lt;/li&gt;
&lt;li&gt;30000 HTTP запросов в секунду в периоды пиковой нагрузки&lt;/li&gt;
&lt;li&gt;3 GBps трафик данных&lt;/li&gt;
&lt;li&gt;3 датацентра: Тампа, Амстердам, Сеул&lt;/li&gt;
&lt;li&gt;350 серверов, конфигурации варьируются от однопроцессорных Pentium 4
    с 512 MB оперативной памяти до двухпроцессорных Xeon Quad-Core с 16
    GB RAM.&lt;/li&gt;
&lt;li&gt;Управляется ~6 людьми&lt;/li&gt;
&lt;li&gt;Три кластера на трех разных континентах&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="arkhitektura"&gt;Архитектура&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Географическая балансировка нагрузки, основываясь на IP клиента,
    перенаправляет их на ближайший кластер. Происходит статическое
    отображение множества IP адресов на множество стран, а затем и на
    множество кластеров.&lt;/li&gt;
&lt;li&gt;Кэширование с помощью &lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt; группируется по типу
    контента: текст для wiki отдельно от изображений и больших
    статических файлов.&lt;/li&gt;
&lt;li&gt;На данный момент функционирует 55 &lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt; серверов, плюс
    еще 20 подготавливается к запуску.&lt;/li&gt;
&lt;li&gt;1000 HTTP запросов в секунду на каждый сервер, в периоды повышенной
    нагрузки эта цифра может достигать 2500.&lt;/li&gt;
&lt;li&gt;~ 100-250 MBps на сервер.&lt;/li&gt;
&lt;li&gt;~ 14000-32000 открытых соединений на каждом сервере.&lt;/li&gt;
&lt;li&gt;До 40 GB дискового кэша на каждом Squid сервере.&lt;/li&gt;
&lt;li&gt;До 4 дисков в каждом сервере (1U серверы).&lt;/li&gt;
&lt;li&gt;8 GB оперативной памяти, половину использует &lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/e669c01a/" rel="nofollow" target="_blank" title="http://www.powerdns.com"&gt;PowerDNS&lt;/a&gt; предоставляет геораспределение.&lt;/li&gt;
&lt;li&gt;В основном и региональных датацентрах текстовые и медиа кластеры
    построены на &lt;a href="/tag/lvs/"&gt;LVS&lt;/a&gt;,
    &lt;abbr title="Common Address Redundancy Protocol"&gt;CARP&lt;/abbr&gt;
&lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt;, кэш &lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt;. В основном датацентре
    также находится хранилище медиа-данных.&lt;/li&gt;
&lt;li&gt;Для того, чтобы обеспечить предоставление только последних версий
    страниц, всем Squid-серверам отправляются инвалидационные запросы.&lt;/li&gt;
&lt;li&gt;Централизованно управляемая и синхронизированная установка
    программного обеспечения для сотен серверов.&lt;/li&gt;
&lt;li&gt;MediaWiki отлично масштабируется с несколькими процессорами, так что
    закупаются двухпроцессорный четырех ядерные серверы (8 ядер на
    сервер).&lt;/li&gt;
&lt;li&gt;Одно и то же оборудование выполняет как задачи внешнего хранения
    данных, так и кэширования &lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt; используется для кэширования метаданных
    изображений, данных парсера, различий между ревизиями,
    пользователей, сессий. Метаданные, такие как история ревизий,
    отношений статей (ссылки, категории и так далее), учетные записи
    пользователей хранятся в основных базах данных&lt;/li&gt;
&lt;li&gt;Сам текст находится во внешних хранилищах данных.&lt;/li&gt;
&lt;li&gt;Статические (загруженные пользователями) файлы, например
    изображения, хранятся отдельно на сервере изображений, а метаданные
    (размер, тип и так далее) кэшируются в основной базе данных и
    объектном кэше.&lt;/li&gt;
&lt;li&gt;Отдельная база данных для каждой вики (не отдельный сервер!).&lt;/li&gt;
&lt;li&gt;Один master и много реплицированных slave серверов.&lt;/li&gt;
&lt;li&gt;Операции чтения равномерно распределяются по slave серверам,
    операции записи направляются на master.&lt;/li&gt;
&lt;li&gt;Иногда master используется и для операция чтения, когда репликация
    на slave еще не завершена.&lt;/li&gt;
&lt;li&gt;Внешнее хранение данных:&lt;ul&gt;
&lt;li&gt;Текст статей хранится на отдельных кластерах, которые представляют
собой простой средство хранения данных с возможностью только
дописывания новых данных. Такой подход позволяет сохранить
дорогостоящее место в высоконагруженных основных базах данных от
редко используемой информации.&lt;/li&gt;
&lt;li&gt;Благодаря этому появляются дополнительные неиспользованные ресурсы
на серверах приложений (порой 250-500 GB на сервер).&lt;/li&gt;
&lt;li&gt;На данной момент используются реплицируемые кластеры из 3
&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt; серверов, но в будущем это может измениться, так
как требуется более удобное управление ими.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="podvodim-itogi"&gt;Подводим итоги&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Сфокусируйтесь на архитектуре, а не на операциях или чем-то другом.&lt;/li&gt;
&lt;li&gt;Иногда кэширование обходится дороже, чем повторные вычисление или
    поиск данных в исходном источнике.&lt;/li&gt;
&lt;li&gt;Старайтесь избегать сложных алгоритмов, запросов к базе данных и
    тому подобного.&lt;/li&gt;
&lt;li&gt;Кэшируйте каждый результат, который дорого вам обошелся и является
    относительно локальным.&lt;/li&gt;
&lt;li&gt;Сфокусируйтесь на "горячих точках" в коде.&lt;/li&gt;
&lt;li&gt;Масштабируйтесь разделением:&lt;ul&gt;
&lt;li&gt;операций чтения и записи (master/slave);&lt;/li&gt;
&lt;li&gt;сложных операций и более частых и простых (группы запросов);&lt;/li&gt;
&lt;li&gt;больших, популярных вики и более мелких.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Улучшайте кэширование: временная и пространственная локализация
    данных, а также уменьшение набора данных на каждом сервере.&lt;/li&gt;
&lt;li&gt;Выполняйте компрессию текстовых данных, храните только изменения в
    статьях.&lt;/li&gt;
&lt;li&gt;Казалось бы простые вызовы библиотечных функций порой на практике
    могут занимать слишком много времени.&lt;/li&gt;
&lt;li&gt;Скорость поиска данных на диске ограничена, так что чем больше
    дисков - тем лучше!&lt;/li&gt;
&lt;li&gt;Масштабирование с использованием обычного оборудование не означает
    использование самых дешевых вещей, которые удастся найти. Серверы
    баз данных Wikipedia сегодня представляют собой 16GB RAM, двух- или
    четырех-ядерные серверы с 6 15000 rpm SCSI дисками, организованными
    в RAID 0. Возможно они бы и использовали более дешевые системы, но
    16 GB как раз хватает для размещения основного объема данных, а
    остальное берут на себя жесткие диски, это вполне соответствует
    потребностям системы, которую они построили. Примерно по таким же
    причинам их веб-сервера имеют 8 ядер, так как это позволяет достичь
    неплохой производительности &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; при достаточно простой
    организации балансировки нагрузки.&lt;/li&gt;
&lt;li&gt;Для масштабирования требуется выполнение массы работы, но если
    заранее этого не предусмотреть - понадобится сделать еще больше.
    MediaWiki изначально была написана для одного master сервера баз
    данных. Затем добавилась поддержка slave. Затем добавилось
    распределение по языкам и проектам. Дизайн системы с тех пор
    прекрасно выдерживает все нагрузки, но без очистки от новых узких
    мест системы не обошлось.&lt;/li&gt;
&lt;li&gt;Каждый, кто хочет разработать свою базу данных таким образом, чтобы
    она позволила недорого масштабироваться с уровня одного сервера до
    уровня десятки лучших сайтов Интернета, должен начать с обработки
    слегка устаревших данных на реплицированных slave серверах, при этом
    не забывать балансировать нагрузку операций чтения между slave
    серверами. Если это возможно - блоки данных (группы пользователей,
    учетных записей, или чего угодно) должны размещаться каждый на
    разных серверах. Можно делать это с самого начала используя
    виртуализацию, чтобы удостовериться в работоспособности архитектуры,
    когда вы еще "маленькие". Это &lt;strong&gt;намного&lt;/strong&gt; проще, чем когда вы
    делаете то же самое, но под ежемесячно удваивающейся нагрузкой.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 28 Mar 2008 15:32:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-03-28:highload/2008/arkhitektura-wikimedia/</guid><category>Apache</category><category>lighttpd</category><category>Linux</category><category>Lucene</category><category>LVS</category><category>Memcached</category><category>MySQL</category><category>PHP</category><category>Squid</category><category>архитектура</category><category>архитектура Wikimedia</category><category>геораспределение</category><category>Масштабируемость</category></item><item><title>Comet</title><link>https://www.insight-it.ru//frontend/2008/comet/</link><description>&lt;p&gt;Уже приготовились мыть посуду? Что ж, придется Вас разочаровать, сегодня
речь пойдет вовсе не о моющем средстве, а об одноименной технологии.&lt;/p&gt;
&lt;p&gt;Comet представляет собой архитектуру веб-приложений, основной
особенностью которой является тот факт, что отправка данных от сервера к
клиенту (в роли которого обычно выступает браузер) не требует
какого-либо запроса данных со стороны клиента. Это позволяет
пользователям приложения более оперативно реагировать на возникающие на
сервере события и быть в курсе процесса работы приложения без
необходимости непрерывно опрашивать сервер с помощью веб-клиента.
&lt;!--more--&gt;
Реализация этой технологии, как не сложно догадаться, основывается на
JavaScript. Основной идеей является поддержание долговременных
HTTP-соединений с каждым клиентом приложения и отправка новых данных
клиентам с их помощью как только произошло их обновление или
возникновение на сервере. Этот принцип послужил основой для
альтернативных названий этой технологии: &lt;em&gt;Server-push&lt;/em&gt;, &lt;em&gt;Reverse AJAX&lt;/em&gt;.
В случае классического AJAX (о котором тоже стоило бы сначала написать
статейку, потом может быть соберусь) клиент асинхронно отправляет
серверу запрос на получение какой-либо конкретной информации. В Comet же
клиент с сервером как бы меняются ролями: инициатором передачи
информации является сервер, а не клиент. Что же это меняет? Самым
наглядным примером послужит реализация постоянного обновления какой-либо
информации в реальном времени:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Классическая модель отправки запросов.&lt;/strong&gt; Для решения задачи
    понадобится постоянно обновлять всю страницу целиком, вручную
    пользователем или альтернативными способами автоматически. Совсем не
    вариант, будет генерироваться огромное количество запросов к
    серверу, каждый из которых будет очень существеннен по объему.
    Большое количество одновременно работающих пользователей такое
    приложение явно не выдержит, да и доступность данных в реальном
    времени не обеспечит.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AJAX.&lt;/strong&gt; По сравнению с предыдущим вариантом AJAX предоставляет
    массу преимуществ: размер передаваемых данных существенно
    уменьшается, особенно если использовать в качестве формата данных не
    XML, а JSON. Но тем не менее необходимость регулярно отправлять
    запросы серверу для получения обновленной информации останется. С
    ростом количества пользователей будет расти и количество запросов,
    открытие и закрытие которых будет неуклонно генерировать нагрузку на
    сервер. Для снижения нагрузки можно попытаться увеличивать интервал
    между запросами, но это лишь временная мера, обладающая существенным
    недостатком - увеличение этого промежутка времени повлечет за собой
    рост задержек между обновлением информации на сервере и обновлением
    клиентской части приложения.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Comet.&lt;/strong&gt; С каждым клиентом поддерживается постоянное
    HTTP-соединение, как только данные на сервере обновились - сервер
    сразу же отправляет по уже открытому соединению уведомление о
    необходимости провести изменения в клиентской части, это позволяет
    существенно сократить нагрузку на сервер и практически избавиться от
    задержек между обновлением данных на сервере и клиенте.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Для написания серверной части приложения может использоваться
практически любой язык программирования, наиболее распространенным
решением, пожалуй, является Java Servlet, запущенный в каком-либо
контейнере - Apache TomCat, Jetty или Sun GlassFish. Для реализации
этого подхода к организации общения клиента с сервером написано
достаточно большое количество framework'ов и библиотек, наиболее полный
список, я думаю, можно найти в &lt;a href="https://www.insight-it.ru/goto/b28f61a7/" rel="nofollow" target="_blank" title="http://en.wikipedia.org/wiki/Comet_%28programming%29#Implementations"&gt;английской википедии&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Но и для Comet-приложений рано или поздно возникает вопрос
масштабируемости, так как традиционные HTTP-сервера создают новый поток
(thread) для обслуживания очередных нескольких очередных новых
соединений. Каждый поток в состоянии обрабатывать только небольшое
количество HTTP-соединений, а так как в случае с Comet соединения
находятся в открытом состоянии неопределенно долго, для каждых
нескольких новых пользователей веб-приложения приходится создавать новый
поток. Количество одновременно существующих потоков не безгранично, что
в один прекрасный момент приводит к существенному росту издержек,
связанных с созданием новых и управлением существующими потоками, а
также все чаще и чаще возникающим отказам потокам в предоставлении
серверу необходимых вычислительных. В таких ситуациях некоторые
пользователи встречаются с неприемлимыми задержками в работе приложения
или непредвиденными сообщениями об ошибках. Наиболее простым и в то же
время эффективным решением подобной проблемы является горизонтальное
масштабирование Comet-серверов с балансировкой нагрузки на программном
или аппаратном уровне.&lt;/p&gt;
&lt;p&gt;Если Вас заинтересовала эта тема - могу посоветовать взглянуть на пару
достаточно интересных статей, в котором более с практической точки
зрения описывается этот подход к построению приложений:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/2c5c0107/" rel="nofollow" target="_blank" title="http://www.ibm.com/developerworks/ru/library/j-jettydwr/"&gt;Создание масштабируемых Comet-приложений с использованием Jetty и Direct Web Remoting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/4d14c647/" rel="nofollow" target="_blank" title="http://www.javaworld.com/javaworld/jw-03-2008/jw-03-asynchhttp.html"&gt;Asynchronous HTTP and Comet Architecture&lt;/a&gt;
    (eng.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Я же надеюсь написать статью более практической на направленности по
этой теме лишь в обозримом будущем, не пропустить этот момент Вам
поможет &lt;a href="/feed/"&gt;абонемент&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 26 Mar 2008 21:13:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-03-26:frontend/2008/comet/</guid><category>AJAX</category><category>Comet</category><category>HTTP</category><category>JavaScript</category><category>persistent</category><category>архитектура</category><category>информационные технологии</category></item><item><title>Архитектура Friends for Sale</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-friends-for-sale/</link><description>&lt;p&gt;&lt;img alt="Friends for Sale Logo" class="right" src="https://www.insight-it.ru/images/friends-for-sale.png" title="Friends for Sale"/&gt;
За три коротких месяца &lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/616a7ee4/" rel="nofollow" target="_blank" title="http://www.facebook.com/apps/application.php?id=7019261521"&gt;Friend for Sale&lt;/a&gt;&lt;/em&gt;
(рейтинговая система в условиях рыночной экономики) попала в десятку
лучших приложений &lt;em&gt;Facebook&lt;/em&gt;, непринужденно обрабатывая 200 запросов в
секунду и демонстрируя шокирующее количество просмотров страниц, за
месяц достигающее 300 миллионов просмотров. Все это дело рук двух
разработчиков, работающих не полный рабочий день, которые смогли создать
успешное веб-приложение, имея в своем распоряжении лишь кластер из
дюжины серверов и &lt;a href="/tag/ruby-on-rails/"&gt;Ruby on Rails&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Как Friends for Sale масштабируется для того, чтобы обеспечить торговлю
всеми этими красивыми людьми? Как Вы думаете, сколько стоят Ваши друзья
на открытом рынке?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Традиционная пара фраз, чтобы отдать должное
&lt;a href="https://www.insight-it.ru/goto/2ee4cfe9/" rel="nofollow" target="_blank" title="http://highscalability.com/friends-sale-architecture-300-million-page-view-month-facebook-ror-app"&gt;оригиналу&lt;/a&gt;
и его &lt;a href="https://www.insight-it.ru/goto/f3f1b405/" rel="nofollow" target="_blank" title="http://highscalability.com/user/todd-hoff"&gt;автору&lt;/a&gt;. Продолжаем:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ответы на стандартный набор вопросов от Siqi Chen и Alexander Le,
    создателей Friends for Sale;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/2266d3f8/" rel="nofollow" target="_blank" title="http://highscalability.com/docs/EmergingTechSIGPresentation.pdf"&gt;Virality on Facebook&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/ruby-on-rails/"&gt;Ruby on Rails&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/centos/"&gt;CentOS&lt;/a&gt; (64 bit)&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/capistrano/"&gt;Capistrano&lt;/a&gt; - для обновлений и перезапусков
    серверов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/nginx/"&gt;nginx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/starling/"&gt;Starling&lt;/a&gt; - распределенный сервер очередей&lt;/li&gt;
&lt;li&gt;Softlayer - хостинг&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/pingdom/"&gt;Pingdom&lt;/a&gt; - мониторинг&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/lvm/"&gt;LVM&lt;/a&gt; -   &lt;a href="https://www.insight-it.ru/goto/157d64d2/" rel="nofollow" target="_blank" title="http://magicmodels.rubyforge.org/magic_multi_connections/"&gt;Magic Multi-Connections Gem&lt;/a&gt; -
    разделение операций чтения и записи между серверами&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="statistika"&gt;Статистика&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Это Facebook приложение находится в десятке наиболее популярных;&lt;/li&gt;
&lt;li&gt;Около 600 тысяч активных пользователей;&lt;/li&gt;
&lt;li&gt;Полмиллиона уникальных посетителей ежедневно, и эта цифра неуклонно
    растет;&lt;/li&gt;
&lt;li&gt;Темпы роста проекта достигают 300% в месяц;&lt;/li&gt;
&lt;li&gt;200 запросов в секунду;&lt;/li&gt;
&lt;li&gt;5 TB трафика в месяц;&lt;/li&gt;
&lt;li&gt;Над проектом работают 2 разработчика и 1 админимтратор баз данных.&lt;/li&gt;
&lt;li&gt;4 сервера баз данных, 6 серверов приложений, 1 тестовый сервер и 1
    сервер для балансировки нагрузки:&lt;ul&gt;
&lt;li&gt;Каждый из серверов приложений содержит 4 ядра и 8 GB оперативной
памяти.&lt;/li&gt;
&lt;li&gt;На каждом из них работает 16 сервисов &lt;a href="/tag/mongrel/"&gt;mongrel&lt;/a&gt; (в
сумме - 96).&lt;/li&gt;
&lt;li&gt;4 GB оперативной памяти на каждом из них отведено под
&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Сервера баз данных имеют более серьезное оборудование: при тех же
4-х ядрах, они имеют 32 GB оперативной памяти и RAID 10 массив из
четырех 15000rpm SCSI дисков, работающих в режиме "master/slave".&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="davaite-znakomitsia"&gt;Давайте знакомиться&lt;/h3&gt;
&lt;h4&gt;Для чего нужна ваша система?&lt;/h4&gt;
&lt;p&gt;Наша система разработана в качестве платформы для нашего Facebook
приложения, Friends for Sale.
В целом оно представляет собой аналог рейтинговой системы
&lt;a href="https://www.insight-it.ru/goto/d7a8b770/" rel="nofollow" target="_blank" title="http://www.hotornot.com/"&gt;Hot-or-Not&lt;/a&gt; с некоторым добавлением рыночной
экономики. В момент проведения интервью это приложение было на 10-м
месте по популярности среди приложений Facebook.&lt;/p&gt;
&lt;p&gt;Описание этого приложения на самом Facebook гласит:&lt;/p&gt;
&lt;div class="card blue lighten-1"&gt;
&lt;div class="card-content white-text"&gt;
Покупайте и продавайте своих друзей как питомцев! Вы можете научить их
толкаться, отправлять подарки или просто представлять Вас в выгодном
свете.

Зарабатывайте как практичный инвестор в питомцев или как популярный
товар!
&lt;/div&gt;
&lt;/div&gt;
&lt;h4&gt;Почему вы решили построить эту систему?&lt;/h4&gt;
&lt;p&gt;Мы разработали ее скорее как эксперимент для того, чтобы проверить
удалось ли нам понять концепции и измерения вирусного эффекта в рамках
Facebook. Мне кажется нам это удалось. :)&lt;/p&gt;
&lt;h4&gt;С какими конкретными сложными задачами, связанными с дизайном, архитектурой или реализацией системы, вам пришлось столкнуться при построении системы?&lt;/h4&gt;
&lt;p&gt;Как и в любом Facebook приложении, каждый запрос является динамическим,
так что кэширование страниц невозможно. Так как приложение является
интерактивным, со множеством операций записи, определенные трудности
вызвало масштабирование базы данных.&lt;/p&gt;
&lt;h4&gt;Каковы были ваши&lt;/h4&gt;
&lt;p&gt;действия, направленные для решения этих задач?&lt;/p&gt;
&lt;p&gt;С самого начала мы активно использовали &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; -
для перезагрузки страницы совсем не требуется выполнение SQL запросов. В
основном мы использовали кэширование фрагментов Rails с индивидуальной
логикой актуальности.&lt;/p&gt;
&lt;h4&gt;Как вы оцениваете размеры вашей системы?&lt;/h4&gt;
&lt;p&gt;Вчера статистика показала более полумиллиона уникальных посетителей, и
эта цифра неуклонно растет.
За этот месяц было зарегистрировано более 300 миллионов просмотров
страниц.&lt;/p&gt;
&lt;h4&gt;Каковы показатели использования пропускной способности интернет-канала?&lt;/h4&gt;
&lt;p&gt;В прошлом месяце было потрачено 3 терабайта трафика, но в этом месяце
ожидается цифра не меньше 5 терабайт. Эти цифры состоят по большей части
из XHTML / CSS и нескольких небольших иконок.&lt;/p&gt;
&lt;h4&gt;Как много документов используется в системе? Сколько изображений? Какой объем данных?&lt;/h4&gt;
&lt;p&gt;По большому счету у нас нет уникальных документов... но зато у нас есть
около 10 миллионов профилей пользователей.
Единственными используемыми изображениями являются несколько
статических иконок.&lt;/p&gt;
&lt;h4&gt;Как вы оцениваете темпы роста вашей системы?&lt;/h4&gt;
&lt;p&gt;Месяц назад за сутки просматривалось около трех миллионов страниц, на
данный момент эта цифра достигла 10 миллионов в сутки. Из чего можно
сделать вывод, что ориентировочные темпы роста проекта составляют 300% в
месяц. Если говорить о ежесекундной нагрузке, то на данный момент она
составляет около 200 запросов в секунду.&lt;/p&gt;
&lt;h4&gt;Какая часть посетителей платит вам за участие в вашем проекте?&lt;/h4&gt;
&lt;p&gt;Он абсолютно бесплатен для пользователей.&lt;/p&gt;
&lt;h4&gt;Каковы показатели "текучести" пользователей?&lt;/h4&gt;
&lt;p&gt;В среднем около 1% в сутки, с ежедневным ростом в 3% от этой цифры, если
говорить в терминах новых установок .&lt;/p&gt;
&lt;h4&gt;Как много учетных записей активно принимали участие в проекте за последний месяц?&lt;/h4&gt;
&lt;p&gt;По данным &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; за последний месяц проект посетил 2.1
миллион уникальных пользоывтелей.&lt;/p&gt;
&lt;h4&gt;Какова архитектура вашей системы?&lt;/h4&gt;
&lt;p&gt;Она представляет собой относительно стандартный Rails кластер. В
качестве интерфейса между запросами пользователей и серверами приложений
используется proxy балансировщик нагрузки, который перенаправляет
запросы напрямую шести четырехядерным серверам приложений. На каждом
сервере приложений запущено 16 &lt;a href="/tag/mongrel/"&gt;mongrel&lt;/a&gt;'ов, что в сумме
дает 96. Балансировщик нагрузки перенаправляет запросы напрямую на порты
серверов &lt;a href="/tag/mongrel/"&gt;mongrel&lt;/a&gt;. В дополнение к этому на каждом сервере
приложений выделено 4 GB оперативной памяти под
&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;, а также работает локальный сервер
распределенного менеджера очередей &lt;a href="/tag/starling/"&gt;Starling&lt;/a&gt; и несколько
менее важных фоновых процессов.&lt;/p&gt;
&lt;p&gt;&lt;a href="/tag/subd/"&gt;СУБД&lt;/a&gt; работает на двух серверах (четыре ядра, 32 GB
оперативной памяти, четыре 15000rpm SCSI диска в RAID 10) в режиме
"master/slave". Для организации распределения операций чтения и записи
между серверами используется &lt;a href="https://www.insight-it.ru/goto/157d64d2/" rel="nofollow" target="_blank" title="http://magicmodels.rubyforge.org/magic_multi_connections/"&gt;Magic Multi-Connections Gem&lt;/a&gt; от Dr
Nic.&lt;/p&gt;
&lt;p&gt;На данный момент ведется работа над добавлением дополнительных серверов,
работающих в роли "slave", для обеспечения более эффективного
распределения нагрузки, избыточности и политик хранения запасных копий
данных. Помимо этого нам помогают Percona (ребята из
mysqlperformanceblog) с удаленной работой над архитектурой базы данных.&lt;/p&gt;
&lt;p&gt;Нашим хостинг-провайдером является Softlayer - он просто фантастический.
Основной проблемой был тот факт, что их балансировщик нагрузки не
справлялся со своей задачей ... поначалу у нас возникала масса проблем,
связанных с задержками и повисшими соединениями. Переход на отдельный
сервер с запущенным только nginx в режиме proxy балансировщика нагрузки
позволила решить все проблемы.&lt;/p&gt;
&lt;h4&gt;Каким образом планируется масштабировать архитектуру вашего проекта?&lt;/h4&gt;
&lt;p&gt;Каких-то конкретных планов нет. На уровне приложения система не
использует какие-либо общие ресурсы, так что все достаточно тривиально.
На уровне баз данных на данный момент все еще используется один сервер в
роли "master", но мы стараемся отложить неизбежный переход к
сегментированной базе данных на как можно более длительный срок. На
данный момент базы данных масштабируются вертикально, но со временем,
надеюсь, мы сможем от этого избавиться.&lt;/p&gt;
&lt;h4&gt;Назовите самые интересные уникальные факты о вашем проекте?&lt;/h4&gt;
&lt;p&gt;Я могу назвать:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Ни один из двух разработчиков ранее не имел опыта в крупномасштабных
    разработках на основе &lt;a href="/tag/rails/"&gt;Rails&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Наша траектория роста проекта достаточно редка в истории разработок
    с использованием &lt;a href="/tag/rails/"&gt;Rails&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;У нас практически не было возможностей для кэширования статических
    страниц - каждый запрос страницы приходилось обрабатывать
    &lt;a href="/tag/rails/"&gt;Rails&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Чему вам удалось научиться? Каков залог вашего успеха? Чего бы вам хотелось сделать по-другому в прошлом, если бы была такая возможность? Что бы вы оставили как есть?&lt;/h4&gt;
&lt;p&gt;Отличные хостинг, оборудование и архитектура БД являются очень важными
факторами. Мы привыкли пользоваться услугами хостинга Railsmachine,
который честно говоря является отличным провайдером shared хостинга, но
со временем они потеряли возможность выдерживать необходимую нагрузку. В
итоге почти месяц мы были едва способны отвечать на запросы браузеров
из-за проблем с оборудованием, хотя последующий переход на Softlayer
занял всего два часа. Стоит заранее выбирать качественный хостинг, если
планируется масштабирование проекта, смена хостинг-провайдера - не очень
веселое занятие.&lt;/p&gt;
&lt;p&gt;Основным выводом, который нам удалось сделать, является тот факт, что
причиной проблемы с масштабированием практически всегда является база
данных. Все без исключений проблемы с производительностью в итоге
сводились к серверу баз данных, конфигурации СУБД, эффективности
запросов или решению вопроса насчет необходимости использования
индексов.&lt;/p&gt;
&lt;p&gt;Определенно нам нужен был более качественный хостинг намного раньше.&lt;/p&gt;
&lt;p&gt;Мы определенно не сменим наш framework - &lt;a href="/tag/rails/"&gt;Rails&lt;/a&gt; был
незаменим при быстрой разработке приложения, нам удалось доказать, что
для масштабирования проекта на &lt;a href="/tag/ror/"&gt;RoR&lt;/a&gt; достаточно двух парней,
абсолютно не имеющих опыта в этом.&lt;/p&gt;
&lt;h4&gt;Кто входит в состав вашей команды?&lt;/h4&gt;
&lt;p&gt;У нас есть два разработчика, включая меня. Помимо этого недавно мы
начали пользоваться услугами помощи с DBA, о которой уже упоминалось.&lt;/p&gt;
&lt;h4&gt;Сколько всего людей участвует в проекте?&lt;/h4&gt;
&lt;p&gt;В технической части - два разработчика и один администратор баз данных,
работающий на контрактной основе.&lt;/p&gt;
&lt;h4&gt;Где они расположены с географической точки зрения?&lt;/h4&gt;
&lt;p&gt;Все участники проекта живут в районе SOMA, San Francisco.&lt;/p&gt;
&lt;h4&gt;Каковы обязанности каждого из участников проекта?&lt;/h4&gt;
&lt;p&gt;Оба разработчика проекта по совместительству являются и его создателями.
Поначалу я (Siqi) был ответственным за дизайн и разработку
пользовательского интерфейса, но так как у меня был некоторый опыт с
развертыванием систем я взял на себя и разработку управления сетевыми
операциями и развертывания. Мой коллега Alex был ответственным за
большую часть &lt;a href="/tag/rails/"&gt;Rails&lt;/a&gt; кода, вся логика приложения - его рук
дело.&lt;/p&gt;
&lt;p&gt;На данный момент я по большей части занимаюсь более техническими
моментами, такими как оптимизация сетевых операций и работы и репликации
&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;. С трудом получается вернуться к работе над
пользовательским интерфейсом - к тому, что мне по-настоящему нравится.
Но это был опыт, который явно стоило получить, так что я стараюсь
извлекать максимум выгоды из этого занятия.&lt;/p&gt;
&lt;h4&gt;У вас есть какая-то определенная философия менеджмента?&lt;/h4&gt;
&lt;p&gt;Да - найти самых умелых и сообразительных людей, сделать им наилучшее
возможное предложение и убраться с их пути. Самые лучшие менеджеры
должны уметь НЕ МЕШАТЬ работникам, так что я стараюсь максимально этому
следовать при работе с другими участниками проекта. Но, к сожалению, мне
удается это далеко не всегда.&lt;/p&gt;
&lt;h4&gt;Если ваша команда работает раздельно, как вам удается координировать свою работу?&lt;/h4&gt;
&lt;p&gt;Нам стоило бы задуматься об использования каких-либо эффективных средств
общения. Мне кажется, что использование удаленной работа / outsourcing'а
является по-настоящему сложной задачей - я предпочитаю обходиться без
этого в разработке основы системы. Для системного администрирования или
разработки архитектуры БД это было бы более оправданно.&lt;/p&gt;
&lt;h3 id="chto-vy-ispolzuete-dlia-razrabotki"&gt;Что вы используете для разработки?&lt;/h3&gt;
&lt;p&gt;Мы используем &lt;a href="/tag/rails/"&gt;Rails&lt;/a&gt; с несколькими plug-in'ами, самыми
важными являются cache-fu от Cris Wanstrath и magic multi connections от
Dr Nic. В качестве текстового редактора я предпочитаю vim с плагином
rails.vim.&lt;/p&gt;
&lt;h4&gt;Какие языки программирования используются?&lt;/h4&gt;
&lt;p&gt;&lt;a href="/tag/ruby-on-rails/"&gt;Ruby on Rails&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Сколько используется серверов?&lt;/h4&gt;
&lt;p&gt;На данный момент используется кластер из 12 серверов.&lt;/p&gt;
&lt;h4&gt;Как они используются?&lt;/h4&gt;
&lt;p&gt;4 сервера баз данных, 6 серверов приложений, 1 тестовый сервер и 1
сервер для балансировки нагрузки.&lt;/p&gt;
&lt;h4&gt;Кто их предоставляет?&lt;/h4&gt;
&lt;p&gt;Мы заказываем их у Softlayer - до подключения их к системе проходит
порой менее четырех часов, что очень неплохо.&lt;/p&gt;
&lt;h4&gt;Какая операционная система используется?&lt;/h4&gt;
&lt;p&gt;CentOS 5 (64 бит)&lt;/p&gt;
&lt;h4&gt;Какой http сервер используется?&lt;/h4&gt;
&lt;p&gt;nginx&lt;/p&gt;
&lt;h4&gt;Какая СУБД используется?&lt;/h4&gt;
&lt;p&gt;MySQL 5.1&lt;/p&gt;
&lt;h4&gt;Вы используете обратную proxy?&lt;/h4&gt;
&lt;p&gt;Мы просто используем встроенный в nginx proxy балансировщик нагрузки.&lt;/p&gt;
&lt;h4&gt;Как вы развертываете вышу систему в датацентре?&lt;/h4&gt;
&lt;p&gt;Мы используем хостинг выделенных серверов, Softlayer.&lt;/p&gt;
&lt;h4&gt;Какова ваша стратегия хранения данных?&lt;/h4&gt;
&lt;p&gt;Мы используем резервное копирование NAS помимо внутренних SCSI RAID
массивов.&lt;/p&gt;
&lt;h4&gt;Какой объем дискового пространства вам доступен?&lt;/h4&gt;
&lt;p&gt;На всех серверах в сумме около 5 TB.&lt;/p&gt;
&lt;h4&gt;Как вы наращиваете объем дискового пространства?&lt;/h4&gt;
&lt;p&gt;Спонтанно. Мы еще не выполнили каких-либо исследований в планировании
дискового пространство, но это было явно зря не сделано.&lt;/p&gt;
&lt;h4&gt;Вы используйте какой-либо сервис хранения информации?&lt;/h4&gt;
&lt;p&gt;Нет.&lt;/p&gt;
&lt;h4&gt;Вы используете виртуализацию хранимых данных?&lt;/h4&gt;
&lt;p&gt;Нет.&lt;/p&gt;
&lt;h4&gt;Как организована работа с сессиями?&lt;/h4&gt;
&lt;p&gt;На данный момент она поручена СУБД, но передача их обслуживания напрямую
memcached - достаточно несложная задача.&lt;/p&gt;
&lt;h4&gt;Как организована архитектура вашей БД?&lt;/h4&gt;
&lt;p&gt;На данный момент - "master/slave". Мы осуществляем переход к нескольким
"slave" с proxy балансировщиком нагрузки для режима "только для чтения".&lt;/p&gt;
&lt;h4&gt;Как организована балансировка нагрузки?&lt;/h4&gt;
&lt;p&gt;На программном уровне средствами nginx.&lt;/p&gt;
&lt;h4&gt;Какой framework / AJAX библиотеку вы используете?&lt;/h4&gt;
&lt;p&gt;Rails.&lt;/p&gt;
&lt;h4&gt;Какие средства распределенного управления задачами вы используете?&lt;/h4&gt;
&lt;p&gt;&lt;a href="/tag/starling/"&gt;Starling&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Как вы управляете рекламой в проекте?&lt;/h4&gt;
&lt;p&gt;Мы участвуем в нескольких рекламных сетях. Мы оцениваем эффективность
каждой рекламной сети с помощью eCPM на уровне приложения.&lt;/p&gt;
&lt;h4&gt;Имеете ли вы стандартную API на вашем сайте?&lt;/h4&gt;
&lt;p&gt;Нет.&lt;/p&gt;
&lt;h4&gt;Сколько человек в вашей команде?&lt;/h4&gt;
&lt;p&gt;2 разработчика.&lt;/p&gt;
&lt;h4&gt;Какими наборами способностей обладают участники вашей команды?&lt;/h4&gt;
&lt;p&gt;Я: дизайн пользовательского интерфейса, разработка, ограниченные знания
в Rails, оптимизация MySQL, развертывание Rails.&lt;/p&gt;
&lt;p&gt;Alex: разработка логики приложения, дизайн пользовательского интерфейса,
программная инженерия в целом.&lt;/p&gt;
&lt;h4&gt;Какие средства разработки вы используете?&lt;/h4&gt;
&lt;p&gt;Alex работает в OS X, а я предпочитаю Ubuntu. Для контроля за версиями
используется &lt;a href="/tag/svn/"&gt;SVN&lt;/a&gt;. В качестве текстового редактора я
использую VIM, а Alex - TextMate.&lt;/p&gt;
&lt;h4&gt;Как проходит процесс разработки?&lt;/h4&gt;
&lt;p&gt;На логическом уровне все упирается в тесты, мы проводим их достаточно
экстенсивно. На уровне приложения все ограничивается быстрыми итерациями
и не менее быстры тестированием.&lt;/p&gt;
&lt;h4&gt;Какова ваша стратегия кэширования объектов и контента?&lt;/h4&gt;
&lt;p&gt;Мы используем &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; без TTL и просто вручную
очищаем кэш при необходимости.&lt;/p&gt;
&lt;h4&gt;Как происходит кэширование на клиентской стороне?&lt;/h4&gt;
&lt;p&gt;Никак.&lt;/p&gt;
&lt;h4&gt;Как вы проверяете глобальную доступность и моделируете производительность для конечных пользователей?&lt;/h4&gt;
&lt;p&gt;Мы используем &lt;a href="/tag/pingdom/"&gt;Pingdom&lt;/a&gt; для внешнего мониторинга за
сайтом - они отлично справляются.&lt;/p&gt;
&lt;h4&gt;Как вы проверяете работоспособность ваших серверов и сетей?&lt;/h4&gt;
&lt;p&gt;На данный момент мы полагаемся на внешний мониторинг и ping мониторинг
от Softlayer. В перспективе мы рассматриваем FiveRuns как возможное
решение для мониторинга серверов.&lt;/p&gt;
&lt;h4&gt;Как вы строите на графиках или диаграммах сетевую и серверную статистику, а также тенденции?&lt;/h4&gt;
&lt;p&gt;Мы не занимаемся этим.&lt;/p&gt;
&lt;h4&gt;Как вы тестируете систему?&lt;/h4&gt;
&lt;p&gt;Сначала мы разворачиваем ее на тестовом сервере и проводим несколько
тестов, после чего разворачиваем систему уже на серверах приложений.&lt;/p&gt;
&lt;h4&gt;Как вы анализируете производительность?&lt;/h4&gt;
&lt;p&gt;Мы отслеживаем каждый SQL-запрос в процессе разработки, это позволяет
нам убедиться, что не выполняются никакие ненужные запросы или создание
экземпляра модели. Помимо этого мы не выполняем каких-либо тестов на
производительность.&lt;/p&gt;
&lt;h4&gt;Как вы обеспечиваете безопасность?&lt;/h4&gt;
&lt;p&gt;Тщательно.&lt;/p&gt;
&lt;h4&gt;Как вы решаете какие возможности добавить или оставить?&lt;/h4&gt;
&lt;p&gt;Решения основываются на отзывах пользователей и критическом взгляде на
них. Мы верим в простоту, так что нам приходится как следует все
взвесить перед добавлением каких-либо существенных возможностей.&lt;/p&gt;
&lt;h4&gt;Как вы реализуете веб-аналитику?&lt;/h4&gt;
&lt;p&gt;Мы используем собственную систему оценок для оптимизации вирусного
эффекта, но помимо этого пользуемся и услугами &lt;a href="https://www.insight-it.ru/goto/d303d8e3/" rel="nofollow" target="_blank" title="http://www.google.com/analytics"&gt;Google
Analytics&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Используете ли вы A/B тестирование?&lt;/h4&gt;
&lt;p&gt;Да, время от времени мы используем их для тонкой настройки аспектов
дизайна для того, чтобы оптимизировать его под вирусный эффект.&lt;/p&gt;
&lt;h4&gt;Как вы выполняете резервное копирование и восстановление?&lt;/h4&gt;
&lt;p&gt;Мы используем LVM для создания ежедневных и еженедельных инкрементальных
резервных копий.&lt;/p&gt;
&lt;h4&gt;Как выполняются обновления оборудования и программного обеспечения?&lt;/h4&gt;
&lt;p&gt;На данный момент мы делаем это вручную, за исключением развертывания
&lt;a href="/tag/rails/"&gt;Rails&lt;/a&gt; приложения. Для обновления и перезапуска серверов
приложений мы используем &lt;a href="/tag/capistrano/"&gt;Capistrano&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Как вы выполняете глобальные изменения в структуре базы данных при обновлениях?&lt;/h4&gt;
&lt;p&gt;Обычно мы начинаем переход с второстепенных серверах баз данных, а затем
просто переключаем основные.&lt;/p&gt;
&lt;h4&gt;Каковы ваши планы насчет защиты от сбоев и развития бизнеса?&lt;/h4&gt;
&lt;p&gt;Не самым лучшим образом...&lt;/p&gt;
&lt;h4&gt;Есть ли у вас отдельная операционная команда, работающая над сайтом?&lt;/h4&gt;
&lt;p&gt;Было бы неплохо, но нет :)&lt;/p&gt;
&lt;h4&gt;Используете ли вы &lt;abbr title="Content Delivery Network"&gt;CDN&lt;/abbr&gt;? Если да, то какую и для каких целей?&lt;/h4&gt;
&lt;p&gt;Нет.&lt;/p&gt;
&lt;h4&gt;Как выглядит модель ваших доходов?&lt;/h4&gt;
&lt;p&gt;&lt;abbr title="Costs per thousand impressions"&gt;CPM&lt;/abbr&gt;: больше просмотров страниц - больше денег. Помимо этого у нас бывают прямые
поощрительные предложения через нашу виртуальную валюту.&lt;/p&gt;
&lt;h4&gt;Как вы продвигаете ваш продукт?&lt;/h4&gt;
&lt;p&gt;Это же социальная сеть. Мы просто используем вирусный эффект для
поддержания роста проекта.&lt;/p&gt;
&lt;h4&gt;Используете ли вы какие-либо особенно интересные технологии или алгоритмы?&lt;/h4&gt;
&lt;p&gt;Я думаю Ruby запросто мог бы подойти под это определение, но на самом
деле нет - мы не проводим научных исследований, мы просто стараемся быть
полезными для посетителей.&lt;/p&gt;
&lt;h4&gt;Храните ли вы изображения в базе данных?&lt;/h4&gt;
&lt;p&gt;Нет, это бы была не самая лучшая идея.&lt;/p&gt;
&lt;h4&gt;Как много работы над организацией взаимодействия с пользователями приходится выполнять?&lt;/h4&gt;
&lt;p&gt;Я бы сказал, что никакой, если вам не приходилось раньше масштабировать
что-либо, и достаточно много, если приходилось. Достаточно сложно
сказать что именно станет проблемой до тех пор, пока на самом деле с
ними не столкнешься. Как только ты пройдешь через это, у тебя будет
достаточно знаний, чтобы осознанно проводить какую-либо работу в этом
направлении.&lt;/p&gt;
&lt;h4&gt;Приходилось ли вам сталкиваться с какими-либо сюрпризами, положительными или отрицательными?&lt;/h4&gt;
&lt;p&gt;Было удивительно, насколько ненадежным может оказаться поставщик
оборудования, и как может отличаться уровень технической поддержки
одного хостинг-провайдера по сравнению с другим. Одной из основных
вещей, которая вам понадобится при масштабировании системы - хостинг,
способный поддерживать ваши потребности.&lt;/p&gt;
&lt;p&gt;С другой стороны, было удивительно насколько далеко смогла наз завести
архитектура с одним "master" и несколькими "slave" на самом обыкновенном
оборудовании. Я думаю, что даже миллиард просмотров страниц в месяц
достижим при таком подходе к базе данных.&lt;/p&gt;
&lt;h4&gt;Как ваша система эволюционирует для соответствия новым требованиям к масштабируемости?&lt;/h4&gt;
&lt;p&gt;По большому счету она этого не делает, мы просто исправляем узкие места
в системе и смотрим что же будет дальше.&lt;/p&gt;
&lt;h4&gt;Кем вы восхищаетесь?&lt;/h4&gt;
&lt;p&gt;Brad Fitzpatrick за изобретение memcache, а также каждым, кому успешно
удалось горизонтально масштабировать свой проект.&lt;/p&gt;
&lt;h4&gt;Каковы ваши планы по изменению архитектуры в будущем?&lt;/h4&gt;
&lt;p&gt;Скоро предется переходить к сегментированной по пользователям базе
данных, так как скоро мы достигнем пределов базы данных по операциям
записи и размерам.&lt;/p&gt;
&lt;h3 id="ikh-mysli-o-virusnom-effekte-facebook"&gt;Их мысли о вирусном эффекте Facebook&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Facebook моделирует социальную сеть в цифровой форме максимально
    точно и полно, по крайней мере насколько это возможно.&lt;/li&gt;
&lt;li&gt;Построение социальной сети более важно, чем возможности,
    предоставляемые пользователям.&lt;/li&gt;
&lt;li&gt;Facebook позволяет быстро распространять новые приложения через
    социальную сеть.&lt;/li&gt;
&lt;li&gt;Идея вашего приложения должна быть социальной, затягивающей и
    универсальной.&lt;/li&gt;
&lt;li&gt;Социальный аспект является основой вирусного эффекта.&lt;/li&gt;
&lt;li&gt;"Затягивание" пользователей позволяет зарабатывать на нем.&lt;/li&gt;
&lt;li&gt;Универсальность дает необходимый потенциал.&lt;/li&gt;
&lt;li&gt;Friends for Sale - социальный проект, так как предоставляет
    возможность торговать своей частью социального графа.&lt;/li&gt;
&lt;li&gt;Он затягивает, так как в основе лежит в какой-то степени сумасшедшая
    идея, ненавязчивая, слегка флиртующая, и немного циничная.&lt;/li&gt;
&lt;li&gt;Он универсальный, так как все люди в какой-то степени самовлюбленны,
    знают себе цену, и хотят флиртовать с "горячими" людьми.&lt;/li&gt;
&lt;li&gt;Каждая часть приложения является потенциальной для вовлечения новых
    пользователей.&lt;/li&gt;
&lt;li&gt;Каждый пользователь в среднем приводит 1.4 новых, что является
    залогом экспонентациального роста.&lt;/li&gt;
&lt;li&gt;Для каждого нового пользователя отслеживается количество
    приглашений, нотификаций, записей на "стене", кликов в профиле и
    других факторов.&lt;/li&gt;
&lt;li&gt;Для каждого канала поступления новых пользователей вычисляются
    проценты нажавших, успешно вовлеченных и выходов из проекта.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="podvodim-itogi"&gt;Подводим итоги&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;На Facebook требуется масштабирование с самого начала. Дорога до
    миллиона просмотров страниц в сутки заняла 4 недели.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/ruby-on-rails/"&gt;Ruby on Rails&lt;/a&gt; может масштабироваться.&lt;/li&gt;
&lt;li&gt;При правильном подходе к архитектуре может масштабироваться
    практически все что угодно, сосредоточтесь на этом.&lt;/li&gt;
&lt;li&gt;Вам определенно нужна продуманная архитектура базы данных,
    качественный хостинг, а также правильно настроенное оборудование.&lt;/li&gt;
&lt;li&gt;С использованием кэширования и современных серверов, может пройти
    достаточно длительный период времени до тех пор, пока понадобится
    использование баз данных с более сложной структурой, такой как
    сегментирование.&lt;/li&gt;
&lt;li&gt;Социальная сеть - это реальность. Количество новых пользователей в
    хорошо реализованном Facebook приложении на самом деле ошеломляет.&lt;/li&gt;
&lt;li&gt;Большая часть проблем с производительностью в итоге сводится к базе
    данных. Лишний раз обратите внимание на конфигурацию СУБД, запросы и
    использование индексов.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Люди до сих пор пользуются Vi!&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 17 Mar 2008 21:44:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-03-17:highload/2008/arkhitektura-friends-for-sale/</guid><category>Capistrano</category><category>CentOS</category><category>Facebook</category><category>Friends for Sale</category><category>LVM</category><category>Memcached</category><category>mongrel</category><category>MySQL</category><category>nginx</category><category>online</category><category>Pingdom</category><category>Rails</category><category>RoR</category><category>Ruby on Rails</category><category>Starling</category><category>SVN</category><category>архитектура</category><category>интернет</category><category>Масштабируемость</category><category>социальные сети</category></item><item><title>Архитектура YouTube</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-youtube/</link><description>&lt;p&gt;Рост &lt;a href="https://www.insight-it.ru/goto/628b8f6a/" rel="nofollow" target="_blank" title="http://www.youtube.com"&gt;YouTube&lt;/a&gt; был феноменально быстр,
количество просмотров видео превысило 100 миллионов в сутки при том, что
только около пяти человек работало над масштабированием проекта. Как им
удается управлять предоставлением всех этих видеороликов своим
посетителям? Как они развивались с тех пор, как были приобретены
&lt;a href="/tag/google/"&gt;Google&lt;/a&gt;?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; (SuSe)&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/psyco/"&gt;psyco&lt;/a&gt;, динамический компилятор &lt;a href="/tag/python/"&gt;Python&lt;/a&gt; &amp;rarr;
    &lt;a href="/tag/c/"&gt;C&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/lighttpd/"&gt;lighttpd&lt;/a&gt; для видео&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="chto-vnutri"&gt;Что внутри?&lt;/h3&gt;
&lt;h4&gt;Статистика&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Поддержка обработки более 100 миллионов видеороликов в сутки&lt;/li&gt;
&lt;li&gt;Сервис был запущен в феврале 2005 года&lt;/li&gt;
&lt;li&gt;В марте 2006 года в среднем производилось около 30 миллионов
    просмотров видео в день&lt;/li&gt;
&lt;li&gt;К июлю 2006 года эта цифра достигла 100 миллионов просмотров в день&lt;/li&gt;
&lt;li&gt;Над проектом работают: 2 системных администратора, 2 архитектора
    масштабируемости программного обеспечения, 2 разработчика новых
    возможностей, 2 инженера по сетям, 1 архитектор баз данных&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Рецепт управления огромными темпами роста&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
   &lt;span class="n"&gt;identify_and_fix_bottlenecks&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
   &lt;span class="n"&gt;drink&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
   &lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
   &lt;span class="n"&gt;notice_new_bottleneck&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Этот цикл проходит далеко не одну итерацию ежедневно.&lt;/p&gt;
&lt;h3 id="veb-servery"&gt;Веб-серверы&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/netscalar/"&gt;NetScalar&lt;/a&gt; используется для балансировки нагрузки и
    кэширования статического контента.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; работает с включенным &lt;code&gt;mod_fast_cgi&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Запросы отправляются на обработку с помощью серверного приложения на
    &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Приложение взаимодействует с различными базами данных и другими
    источниками информации для формирования финальной
    &lt;a href="/tag/html/"&gt;HTML&lt;/a&gt;-страницы.&lt;/li&gt;
&lt;li&gt;Масштабирование обычно происходит просто добавлением дополнительных
    компьютеров.&lt;/li&gt;
&lt;li&gt;Код на &lt;a href="/tag/python/"&gt;Python&lt;/a&gt; обычно не является узким местом
    системы, он проводит большую часть времени заблокированным RPC.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt; предоставляет быстроту и гибкость в процессе
    разработки и развертывания. Этот факт является очень актуальным,
    если учесть кто является их конкурентами.&lt;/li&gt;
&lt;li&gt;На формирование страницы обычно уходит не более 100 миллисекунд.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/psyco/"&gt;psyco&lt;/a&gt;, динамический компилятор &lt;a href="/tag/python/"&gt;Python&lt;/a&gt; &amp;rarr;
    &lt;a href="/tag/c/"&gt;C&lt;/a&gt;, использует JIT подход к компилированию для оптимизации
    внутренних циклов&lt;/li&gt;
&lt;li&gt;Для интенсивных вычислений, таких как шифрование, используются
    расширения, написанные на &lt;a href="/tag/c/"&gt;C&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Какая-то часть заранее сгенерированного &lt;a href="/tag/html/"&gt;HTML&lt;/a&gt; хранится в
    кэше.&lt;/li&gt;
&lt;li&gt;Кэширование данных в &lt;a href="/tag/subd/"&gt;СУБД&lt;/a&gt; на уровне строк.&lt;/li&gt;
&lt;li&gt;Кэшируются полностью сформированные объекты &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Некие данные вычисляются и отправляется каждому серверу для
    кэширования в локальной оперативной памяти. Эта стратегия годится
    далеко не всегда, чаще всего более эффективен другой метод: самым
    быстрым кэшем является само серверное приложение, а отправка уже
    готовых данных остальным серверам для дальнейшей обработки обычно не
    занимает так много времени. Для организации такого подхода
    необходимы агенты, осуществляющие отслеживание изменений,
    предварительную обработку и отправку данных.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="upravlenie-video"&gt;Управление видео&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Издержки включают в себя затраты на пропускную способность каналов
    связи, приобретение нового оборудования и оплату огромных счетов за
    электроэнергию.&lt;/li&gt;
&lt;li&gt;Каждый видеоролик расположен на мини-кластере, что означает
    управление работой с ним группой из нескольких компьютеров.&lt;/li&gt;
&lt;li&gt;Использование кластеров влечет за собой:
    &amp;ndash; увеличение производительности пропорционально количеству дисков,
    на которых расположен контент;
    &amp;ndash; возможность поддержания функционирования всей системы даже в
    случае прекращения работоспособности части компьютеров;
    &amp;ndash; возможность организации создания резервных копий
    &lt;a href="/tag/online/"&gt;online&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;В роли HTTP-сервера для работы с видео используется
    &lt;a href="/tag/lighttpd/"&gt;lighttpd&lt;/a&gt;:
    &amp;ndash; Он способен дать фору &lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; в плане
    производительности предоставления статического контента;
    &amp;ndash; Для работы с событиями ввода-вывода используется &lt;strong&gt;epoll&lt;/strong&gt;;
    &amp;ndash; Многопоточная конфигурация способна обрабатывать большее
    количество соединений одновременно;&lt;/li&gt;
&lt;li&gt;Самая популярная часть контента размещается в &lt;abbr title="Content Delivery Network"&gt;CDN&lt;/abbr&gt;
    &amp;ndash; &lt;abbr title="Content Delivery Network"&gt;CDN&lt;/abbr&gt; реплицирует весь контент в разных частях системы;
    &amp;ndash; Компьютеры &lt;abbr title="Content Delivery Network"&gt;CDN&lt;/abbr&gt; в основном предоставляют данные напрямую из кэша в оперативной памяти, так как ассортимент популярного видео с течением времени
    меняется достаточно медленно.&lt;/li&gt;
&lt;li&gt;Менее популярный контент, количество просмотров в день которого
    варьируется в диапазоне от одного до двадцати, обычно размещается на
    серверах &lt;a href="/tag/youtube/"&gt;YouTube&lt;/a&gt;, расположенных в датацентрах на
    &lt;em&gt;colocation&lt;/em&gt;:
    &amp;ndash; Не смотря на тот факт, что такое видео может быть просмотрено
    всего несколько раз за день, количество таких роликов велико, что
    приводит к случайным блокировкам данных на жестких дисках;
    &amp;ndash; В такой ситуации кэширование практически бесполезно, инвестиции в
    кэширование контента с низкой вероятностью востребованности обычно
    является пустой тратой средств;
    &amp;ndash; Более детальная настройка низкоуровневых компонентов системы,
    таких как, например, RAID-контроллеры, в этой ситуации может
    достаточно положительно повлиять на производительность;
    &amp;ndash; Выбор оптимального размера оперативной памяти на каждой машине
    также очень важен: как недостаточное, так и излишнее ее количество
    не являются эффективными решениями.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Ключевые моменты&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Чем &lt;strong&gt;проще&lt;/strong&gt; - тем &lt;strong&gt;лучше&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;Старайтесь минимизировать количество устройств (маршрутизаторов,
    коммутаторов и тому подобных) между контентом и пользователями:
    далеко не факт, что все они будут способны выдерживать интенсивную
    нагрузку;&lt;/li&gt;
&lt;li&gt;Старайтесь использовать самое обыкновенное оборудование. Hi-end
    оборудование обычно влечет за собой рост издержек, связанных с
    сопутствующими процессами, например технической поддержкой, а также
    уменьшает вероятность нахождение решения той или иной проблемы с
    оборудованием в Сети;&lt;/li&gt;
&lt;li&gt;Используйте самые простые распространенные утилиты.
    &lt;a href="/tag/youtube/"&gt;YouTube&lt;/a&gt; использует идущий в комплекте с
    &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; набор утилит для построения системы именно на их
    основе;&lt;/li&gt;
&lt;li&gt;Не забывайте о случайных доступах к жестким дискам, эту, казалось
    бы, мелочь тоже стоит настроить.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="upravlenie-miniatiurami-video"&gt;Управление миниатюрами видео&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;На удивление сложно решаемая задача, особенно если необходима
    эффективность;&lt;/li&gt;
&lt;li&gt;Для каждого видео хранится 4 миниатюры, что приводит к существенному
    преобладанию количества миниатюр над количеством видеороликов;&lt;/li&gt;
&lt;li&gt;Миниатюры хранятся всего на нескольких компьютерах;&lt;/li&gt;
&lt;li&gt;Некоторые проблемы наблюдаются в связи с работой с большим
    количеством маленьких объектов:
    &amp;ndash; Проблемы на уровне операционной системы, связанные с большим
    количеством запросов на поиск данных, а также кэшем страниц и
    &lt;em&gt;inode&lt;/em&gt;'ов файловой системы;
    &amp;ndash; Ограничение на количество файлов в одной директории (особенно
    актуально для &lt;strong&gt;ext3&lt;/strong&gt;), возможно частичное решение в виде перехода
    к более иерархической структуре хранения данных, а также переходе к
    ядру &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; версии 2.6, что может привести к более чем
    стократному росту производительности, но в любом случае хранение
    такого огромного количества файлов в локальной файловой системе - не
    самая лучшая идея;
    &amp;ndash; Большое количество запросов в секунду, так как одна страница может
    содержать до 60 миниатюр различных видеороликов;
    &amp;ndash; В условиях таких нагрузок &lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; показывает плохую
    производительность;
    &amp;ndash; Проводились эксперименты с использованием &lt;a href="/tag/squid/"&gt;squid&lt;/a&gt;
    (обратной proxy) между &lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; и посетителями.
    Какое-то время такой вариант казался работоспособным, но с ростом
    нагрузки производительность начала падать. С обработки 300 запросов
    в секунду она упала до 20;
    &amp;ndash; Попытки использовать &lt;a href="/tag/lighttpd/"&gt;lighttpd&lt;/a&gt; также не
    завершились успехом: однопоточный режим не справлялся с задачей, а
    многопоточный требовал отдельного кэша для каждого потока, что
    сводило на нет его эффективность;
    &amp;ndash; С таким количеством изображений добавление в систему нового
    компьютера могло занимать более 24 часов;
    &amp;ndash; Перезагрузка занимала 6-10 часов, так как кэш должен был
    "разогреться" прежде чем перестать использовать данные с жестких
    дисков.&lt;/li&gt;
&lt;li&gt;Решением всех описанных выше проблем стала распределенная система
    хранения данных &lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt; от &lt;a href="/tag/google/"&gt;Google&lt;/a&gt;:
    &amp;ndash; Она позволяет избежать проблем, связанных с большим количеством
    файлов, так как объединяет маленькие файлы вместе.
    &amp;ndash; Она работает быстро и устойчива к сбоям, помимо этого она
    прекрасно приспособлена для работы по ненадежной сети.
    &amp;ndash; Уменьшает задержки, так как использует распределенный
    многоуровневый кэш, который способен работать даже между удаленными
    датацентрами.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="bazy-dannykh"&gt;Базы данных&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Раньше:
    &amp;ndash; &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt; использовалась для хранения данных:
    пользователей, тэгов, описаний и так далее.
    &amp;ndash; Данные хранились на монолитном RAID 10 массиве, состоящем из 10
    жестких дисков;
    &amp;ndash; Оборудование арендовалось, что негативно сказывалось на состоянии
    их кредитных карточек. В случае необходимости нового оборудования,
    на оформление заказа и доставку мог уходить далеко не один день.
    &amp;ndash; Они прошли через весь путь эволюции: сначала был один сервер,
    затем добавилось несколько дополнительных серверов, обслуживающих
    операции чтения, после чего они решили разбить базу данных на части,
    и, наконец, они пришли к полноценной распределенной архитектуре.
    &amp;ndash; Поначалу их система страдала от задержек, связанных с
    реплицированием. Основной сервер, обрабатывающий операции записи,
    являлся мощным сервером, работающим в многопоточном режиме, это было
    необходимо для своевременного выполнения большого объема работы.
    Второстепенные сервера, которые обрабатывали только операции чтения,
    асинхронно реплицировали данные в одном потоке, что влекло за собой
    возможность серьезного отставания некоторых из них.
    &amp;ndash; Обновления были причиной частого отсутствия необходимой информации
    в кэше, что заставляло сервера читать данные с жестких дисков. Этот
    факт сильно замедлял процесс чтения и репликации.
    &amp;ndash; Реплицирующая архитектура требует немалых вложений в оборудование,
    необходимого для поддержания постоянно растущих темпов записи
    информации.
    &amp;ndash; Основным из кардинальных решений, принятых в архитектуре системы
    было отделение обеспечения процесса просмотра видео от основного
    кластера. Основной целью посетителей является просмотр видео, а
    второстепенные задачи можно возложить и на менее производительный
    кластер.&lt;/li&gt;
&lt;li&gt;Сейчас:
    &amp;ndash; Используются распределенные базы данных;
    &amp;ndash; Сегментированная система &lt;em&gt;(прим.: &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-flickr/"&gt;по аналогии с Flickr&lt;/a&gt;)&lt;/em&gt;;
    &amp;ndash; Распределенные чтение и запись;
    &amp;ndash; Более эффективное расположение кэша, что ведет к уменьшению работы
    с жесткими дисками;
    &amp;ndash; Такая архитектура привела к 30%-й экономии на оборудовании;
    &amp;ndash; Задержки в реплицировании сведены к нулю;
    &amp;ndash; Размеры базы данных могут расти практически неограниченно&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="strategiia-razmeshcheniia-v-datatsentrakh"&gt;Стратегия размещения в датацентрах&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Поначалу использовались хостинг провайдеры, предоставляющие услуги
    colocation. Не самый экономичный подход, но тогда не было другого
    выхода.&lt;/li&gt;
&lt;li&gt;Хостинг провайдеры не могут поспеть за темпами роста проекта. Не
    всегда получается получить контроль над необходимым оборудованием
    или сделать необходимые соглашения о предоставлению сетевых услуг.&lt;/li&gt;
&lt;li&gt;Решением этой проблемы стало создание собственной базы для
    размещения оборудования. Появилась возможность настраивать абсолютно
    все и подписывать свои собственные контракты такого рода.&lt;/li&gt;
&lt;li&gt;Было использовано 5 или 6 разных датацентров в дополнение к
    &lt;abbr title="Content Delivery Network"&gt;CDN&lt;/abbr&gt;.&lt;/li&gt;
&lt;li&gt;Видео поступает из случайного датацентра, никаких специальных
    проверок не проводится. Если ролик становится достаточно
    популярным - он перемещается в
    &lt;abbr title="Content Delivery Network"&gt;CDN&lt;/abbr&gt;.&lt;/li&gt;
&lt;li&gt;Основным фактором, влияющим на доступность того или иного ролика
    является пропускная способность канала связи.&lt;/li&gt;
&lt;li&gt;Для изображений же более актуальны задержки, особенно если на одной
    страницы должно быть размещено под 60 изображений.&lt;/li&gt;
&lt;li&gt;Репликация изображений производится средствами
    &lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt;. В этом случае используются различные меры
    для определения ближайшего места, откуда можно получить необходимые
    данные.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="podvodim-itogi"&gt;Подводим итоги&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Остановитесь на секунду.&lt;/strong&gt; Креативные и рискованные трюки могут
    помочь справиться с задачей в краткосрочном периоде, но со временем
    понадобятся более продуманные решения.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Расставьте приоритеты.&lt;/strong&gt; Определите какие части Вашего сервиса
    являются более важными и стройте систему обеспечения ресурсами и
    усилиями именно в соответствии с поставленными приоритетами.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Выбирайте свои битвы.&lt;/strong&gt; Не бойтесь пользоваться аутсорсингом в
    некоторых ключевых сервисах. &lt;a href="/tag/youtube/"&gt;YouTube&lt;/a&gt; использует
    &lt;abbr title="Content Delivery Network"&gt;CDN&lt;/abbr&gt; для распределения
    своего наиболее популярного контента. Создание своей собственной
    подобной сети стоило бы им слишком много и потребовало бы слишком
    много времени. Возможно у Вас появятся подобные возможности в
    отношении Вашей системы.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Будьте проще!&lt;/strong&gt; Простота позволяет изменять архитектуру более
    быстро, что позволяет своевременно реагировать на возникающие
    проблемы. Никто на самом деле не знает что такое &lt;em&gt;простота&lt;/em&gt;, но если
    Вы не боитесь делать изменения, то это неплохой знак что вашей
    системе свойственна та самая &lt;em&gt;простота&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Сегментирование.&lt;/strong&gt; Сегментирование позволяет изолировать и
    ограничить дисковое пространство, процессорное время, оперативную
    память и ввод-вывод. Оно выполняется не только для повышения
    производительности операций записи.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Постоянная работа над поиском и устранением узких мест в
    системе:&lt;/strong&gt;
    &amp;ndash; на программном уровне это чаще всего бывает кэширование и работа с
    &lt;a href="/tag/subd/"&gt;СУБД&lt;/a&gt;;
    &amp;ndash; на уровне операционной системы - операции ввода-вывода;
    &amp;ndash; на уровне оборудования - оперативная память и RAID массивы.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Залог Вашего успеха - командная работа.&lt;/strong&gt; Хорошая команда разного
    рода специалистов должна понимать принцип системы вцелом и того, что
    лежит &lt;em&gt;под&lt;/em&gt; ней. Каждый должен знать свое дело: настраивать
    принтеры, подключать к системе новые компьютеры, строить сети и так
    далее. С отличной командой Вам по силам все что угодно.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;В отличии от &lt;a href="https://www.insight-it.ru/highload/"&gt;остальных&lt;/a&gt;, этот перевод
&lt;a href="https://www.insight-it.ru/goto/ea16b832/" rel="nofollow" target="_blank" title="http://highscalability.com/youtube-architecture"&gt;статьи&lt;/a&gt; от &lt;a href="https://www.insight-it.ru/goto/f3f1b405/" rel="nofollow" target="_blank" title="http://highscalability.com/user/todd-hoff"&gt;Todd Hoff&lt;/a&gt;'а уже был выполнен до
меня (при желании можно найти в любой поисковой системе), но я все равно
решил опубликовать свою версию просто для собственного развития и
полноты &lt;a href="https://www.insight-it.ru/highload/"&gt;коллекции&lt;/a&gt;, да и многим читателям, возможно,
покажется интересным. Что ж, перейдем к источнику информации оригинала:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/f5823d0b/" rel="nofollow" target="_blank" title="http://video.google.com/videoplay?docid=-6304964351441328559"&gt;Google Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 01 Mar 2008 16:07:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-03-01:highload/2008/arkhitektura-youtube/</guid><category>Apache</category><category>BigTable</category><category>lighttpd</category><category>Linux</category><category>MySQL</category><category>NetScalar</category><category>psyco</category><category>Python</category><category>YouTube</category><category>архитектура</category><category>архитектура YouTube</category><category>интернет</category><category>Масштабируемость</category><category>производительность</category><category>хранение данных</category></item><item><title>Hadoop</title><link>https://www.insight-it.ru//storage/2008/hadoop/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/30a7481/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/"&gt;Hadoop&lt;/a&gt; представляет собой платформу
для построения приложений, способных обрабатывать огромные объемы
данных. Система основывается на распределенном подходе к вычислениям и
хранению информации, основными ее особенностями являются:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Масштабируемость:&lt;/strong&gt; с помощью &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; возможно
    надежное хранение и обработка огромных объемов данных, которые могут
    измеряться петабайтами;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Экономичность:&lt;/strong&gt; информация и вычисления распределяются по
    &lt;a href="/tag/klaster/"&gt;кластеру&lt;/a&gt;, построенному на самом обыкновенном
    оборудовании. Такой кластер может состоять из тысяч узлов;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Эффективность:&lt;/strong&gt; распределение данных позволяет выполнять их
    обработку параллельно на множестве компьютеров, что существенно
    ускоряет этот процесс;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Надежность:&lt;/strong&gt; при хранении данных возможно предоставление
    избыточности, благодаря хранению нескольких копий. Такой подход
    позволяет гарантировать отсутствие потерь информации в случае сбоев
    в работе системы;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Кроссплатформенность:&lt;/strong&gt; так как основным языком программирования,
    используемым в этой системе является &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, развернуть
    ее можно на базе любой операционной системы, имеющей &lt;abbr title="Java Virtual Machine"&gt;JVM&lt;/abbr&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;h3 id="hdfs"&gt;HDFS&lt;/h3&gt;
&lt;p&gt;В основе всей системы лежит распределенная файловая система под
незамысловатым названием &lt;strong&gt;Hadoop Distributed File System&lt;/strong&gt;.
Представляет она собой вполне стандартную распределенную файловую
систему, но все же она обладает рядом особенностей:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Устойчивость к сбоям, разработчики рассматривали сбои в оборудовании
    скорее как норму, чем как исключение;&lt;/li&gt;
&lt;li&gt;Приспособленность к развертке на самом обыкновенном ненадежном
    оборудовании;&lt;/li&gt;
&lt;li&gt;Предоставление высокоскоростного потокового доступа ко всем данным;&lt;/li&gt;
&lt;li&gt;Настроена для работы с большими файлами и наборами файлов;&lt;/li&gt;
&lt;li&gt;Простая модель работы с данными: &lt;em&gt;один раз записали - много раз
    прочли&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;Следование принципу: &lt;em&gt;переместить вычисления проще, чем переместить
    данные&lt;/em&gt;;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Архитектура HDFS&lt;/h4&gt;
&lt;p&gt;Проще всего ее демонстрирует схема,
&lt;a href="https://www.insight-it.ru/goto/9c57006b/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/docs/current/images/hdfsarchitecture.gif"&gt;позаимствованная&lt;/a&gt; с официального сайта проекта и переведенная мной на руский:
&lt;img alt="Архитектура HDFS" class="responsive-img" src="https://www.insight-it.ru/images/hdfsarchitecture.jpg" title="Архитектура HDFS"/&gt;&lt;/p&gt;
&lt;p&gt;Действующие лица:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Namenode&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Этот компонент системы осуществляет всю работу с метаданными. Он
должен быть запущен только на одном компьютере в кластере. Именно он
управляет размещением информации и доступом ко всем данным,
расположенным на ресурсах кластера. Сами данные проходят с остальных
машин кластера к клиенту мимо него.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Datanode&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;На всех остальных компьютерах системы работает именно этот
компонент. Он располагает сами блоки данных в локальной файловой
системе для последующей передачи или обработки их по запросу
клиента. Группы узлов данных принято называть Rack, они
используются, например, в схемах репликации данных.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Клиент&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Просто приложение или пользователь, работающий с файловой системой.
В его роли может выступать практически что угодно.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Пространство имен &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt; имеет классическую иерархическую
структуру: пользователи и приложения имеют возможность создавать
директории и файлы. Файлы хранятся в виде блоков данных произвольной (но
одинаковой, за исключением последнего; по-умолчанию 64 mb) длины,
размещенных на &lt;strong&gt;Datanode&lt;/strong&gt;'ах. Для обеспечения отказоустойчивости блоки
хранятся в нескольких экземплярах на разных узлах, имеется возможность
настройки количества копий и алгоритма их распределения по системе.
Удаление файлов происходит не сразу, а через какое-то время после
соответствующего запроса, так как после получения запроса файл
перемещается в директорию &lt;strong&gt;/trash&lt;/strong&gt; и хранится там определенный период
времени на случай если пользователь или приложение передумают о своем
решении. В этом случае информацию можно будет восстановить, в противном
случае - физически удалить.&lt;/p&gt;
&lt;p&gt;Для обнаружения возникновения каких-либо неисправностей, &lt;strong&gt;Datanode&lt;/strong&gt;
периодически отправляют &lt;strong&gt;Namenode&lt;/strong&gt;'у сигналы о своей
работоспособности. При прекращении получения таких сигналов от одного из
узлов &lt;strong&gt;Namenode&lt;/strong&gt; помечает его как &lt;em&gt;"мертвый"&lt;/em&gt;, и прекращает какой-либо
с ним взаимодействие до возвращения его работоспособности. Данные,
хранившиеся на &lt;em&gt;"умершем"&lt;/em&gt; узле реплицируются дополнительный раз из
оставшихся &lt;em&gt;"в живых"&lt;/em&gt; копий и система продолжает свое функционирование
как ни в чем не бывало.&lt;/p&gt;
&lt;p&gt;Все коммуникации между компонентами файловой системы проходят по
специальным протоколам, основывающимся на стандартном &lt;strong&gt;TCP/IP&lt;/strong&gt;.
Клиенты работают с &lt;strong&gt;Namenode&lt;/strong&gt; с помощью так называемого
&lt;strong&gt;ClientProtocol&lt;/strong&gt;, а передача данных происходит по
&lt;strong&gt;DatanodeProtocol&lt;/strong&gt;, оба они &lt;em&gt;обернуты&lt;/em&gt; в &lt;strong&gt;Remote Procedure Call
(RPC)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Система предоставляет несколько интерфейсов, среди которых командная
оболочка &lt;strong&gt;DFSShell&lt;/strong&gt;, набор ПО для администрирования &lt;strong&gt;DFSAdmin&lt;/strong&gt;, а
также простой, но эффективный веб-интерфейс. Помимо этого существуют
несколько API для языков программирования: Java API, C pipeline, WebDAV
и так далее.&lt;/p&gt;
&lt;h3 id="mapreduce"&gt;MapReduce&lt;/h3&gt;
&lt;p&gt;Помимо файловой системы, &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; включает в себя framework
для проведения масштабных вычислений, обрабатывающих огромные объемы
данных. Каждое такое вычисление называется Job (задание) и состоит оно,
как видно из названия, из двух этапов:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Map&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Целью этого этапа является представление произвольных данных (на
практике чаще всего просто пары ключ-значение) в виде промежуточных
пар ключ-значение. Результаты сортируются и групируются по ключу и
передаются на следующий этап.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Reduce&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Полученные после &lt;strong&gt;map&lt;/strong&gt; значения используются для финального
вычисления требуемых данных. Практические любые данные могут быть
получены таким образом, все зависит от требований и функционала
приложения.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Задания выполняются, подобно файловой системе, на всех машинах в
кластере (чаще всего одних и тех же). Одна из них выполняет роль
управления работой остальных - &lt;strong&gt;JobTracker&lt;/strong&gt;, остальные же ее
бесприкословно слушаются - &lt;strong&gt;TaskTracker&lt;/strong&gt;. В задачи &lt;strong&gt;JobTracker&lt;/strong&gt;'а
входит составление расписания выполняемых работ, наблюдение за ходом
выполнения, и перераспределение в случае возникновения сбоев.&lt;/p&gt;
&lt;p&gt;В общем случае каждое приложение, работающее с этим framework'ом,
предоставляет методы для осуществления этапов &lt;strong&gt;map&lt;/strong&gt; и &lt;strong&gt;reduce&lt;/strong&gt;, а
также указывает расположения входных и выходных данных. После получения
этих данных &lt;strong&gt;JobTracker&lt;/strong&gt; распределяет задание между остальными
машинами и предоставляет клиенту полную информацию о ходе работ.&lt;/p&gt;
&lt;p&gt;Помимо основных вычислений могут выполняться вспомогательные процессы,
такие как составление отчетов о ходе работы, кэширование, сортировка и
так далее.&lt;/p&gt;
&lt;h3 id="hbase"&gt;HBase&lt;/h3&gt;
&lt;p&gt;&lt;img alt="HBase Logo" class="right" src="https://www.insight-it.ru/images/hbase-logo.png" title="HBase"/&gt;
В рамках &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; доступна еще и система хранения данных,
которую правда сложно назвать &lt;a href="/tag/subd/"&gt;СУБД&lt;/a&gt; в традиционном смысле
этого слова. Чаще проводят аналогии с проприетарной системой этого же
плана от &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; - &lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/12419d3d/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/hbase"&gt;HBase&lt;/a&gt; представляет собой
распределенную систему хранения больших объемов данных. Подобно
реляционным СУБД данные хранятся в виде таблиц, состоящих из строк и
столбцов. И даже для доступа к ним предоставляется язык запросов &lt;strong&gt;HQL&lt;/strong&gt;
(как ни странно - &lt;strong&gt;Hadoop Query Language&lt;/strong&gt;), отдаленно напоминающий
более распространенный &lt;a href="/tag/sql/"&gt;SQL&lt;/a&gt;. Помимо этого предоставляется
итерирующмй интерфейс для сканирования наборов строк.&lt;/p&gt;
&lt;p&gt;Одной из основных особенностей хранения данных в &lt;strong&gt;HBase&lt;/strong&gt; является
возможность наличия нескольких значений, соответствующих одной
комбинации таблица-строка-столбец, для их различения используется
информация о времени добавления записи. На концептуальном уровне таблицы
обычно представляют как набор строк, но физически же они хранятся по
столбцам, достаточно важный факт, который стоит учитывать при разработки
схемы хранения данных. Пустые ячейки не отображаются каким-либо образом
физически в хранимых данных, они просто отсутствуют. Существуют конечно
и другие нюансы, но я постарался упомянуть лишь основные.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HQL&lt;/strong&gt; очень прост по своей сути, если Вы уже знаете &lt;a href="/tag/sql/"&gt;SQL&lt;/a&gt;,
то для изучения его Вам понадобится лишь просмотреть по диагонали
коротенький вывод команды &lt;strong&gt;help;&lt;/strong&gt;, занимающий всего пару экранов в
консоли. Все те же &lt;strong&gt;SELECT&lt;/strong&gt;, &lt;strong&gt;INSERT&lt;/strong&gt;, &lt;strong&gt;UPDATE&lt;/strong&gt;, &lt;strong&gt;DROP&lt;/strong&gt; и так
далее, лишь со слегка измененным синтаксисом.&lt;/p&gt;
&lt;p&gt;Помимо обычно командной оболочки &lt;strong&gt;HBase Shell&lt;/strong&gt;, для работы с &lt;strong&gt;HBase&lt;/strong&gt;
также предоставлено несколько API для различных языков программирования:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/f059ad5e/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/hbase/docs/current/api/index.html"&gt;Java&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/e44fcd5/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/Jython"&gt;Jython&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8282e2e2/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/HbaseRest"&gt;REST&lt;/a&gt; и&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/185bb3f7/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/ThriftApi"&gt;Thrift&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="zakliuchenie"&gt;Заключение&lt;/h3&gt;
&lt;p&gt;&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; является отличным решением для построения
высоконагруженных приложений, которое уже активно используется
&lt;a href="https://www.insight-it.ru/goto/ab057c2a/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/PoweredBy"&gt;множеством интернет-проектов&lt;/a&gt;.
В последующих постах на эту тему я постараюсь описать процесс
развертывания этой системы и написания приложений, работающих по
принципу &lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;. Не пропустить момент их публикации
Вам может помочь подписка на &lt;a href="/feed/"&gt;RSS-ленту&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 22 Feb 2008 22:41:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-02-22:storage/2008/hadoop/</guid><category>Hadoop</category><category>HBase</category><category>HDFS</category><category>Java</category><category>MapReduce</category><category>архитектура</category><category>информационные технологии</category><category>кластер</category><category>Масштабируемость</category><category>распределенные вычисления</category><category>технология</category></item><item><title>Архитектура Amazon</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-amazon/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/300f3b10/" rel="nofollow" target="_blank" title="http://amazon.com"&gt;Amazon&lt;/a&gt; вырос из крошечной книжной лавки в один из
крупнейших магазинов вселенной. Они добились этого благодаря их
инновационному подходу к обзорам, рекомендациям и оценке продукции.-more--&amp;gt;&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Как и &lt;a href="https://www.insight-it.ru/highload/"&gt;многие статьи&lt;/a&gt; об архитектурах высоконагруженных
систем на этом блоге, эта запись представляет собой перевод
&lt;a href="https://www.insight-it.ru/goto/59bb645b/" rel="nofollow" target="_blank" title="http://highscalability.com/amazon-architecture"&gt;статьи&lt;/a&gt;, автором
которой является &lt;a href="https://www.insight-it.ru/goto/f3f1b405/" rel="nofollow" target="_blank" title="http://highscalability.com/user/todd-hoff"&gt;Todd Hoff&lt;/a&gt;.
Источниками информации для оригинала послужили:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/292cd03b/" rel="nofollow" target="_blank" title="http://glinden.blogspot.com/2006/05/early-amazon-end.html"&gt;Ранний Amazon&lt;/a&gt;
    от Greg Linden&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7a5f17fb/" rel="nofollow" target="_blank" title="http://news.com.com/2100-1001-275155.html"&gt;Как Linux позволил Amazon сэкономить миллионы&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/de1af2aa/" rel="nofollow" target="_blank" title="http://www.se-radio.net/index.php?post_id=157593"&gt;Интервью с Werner Vogels'ом&lt;/a&gt; -
    техническим директором Amazon&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a27efde/" rel="nofollow" target="_blank" title="http://www.webperformancematters.com/journal/2007/8/21/asynchronous-architectures-4.html"&gt;Асинхронные архитектуры&lt;/a&gt; -
    краткий пересказ речи Werner Vogels'а от Cris Loosley&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/5439ba1f/" rel="nofollow" target="_blank" title="http://www.acmqueue.com/modules.php?name=Content&amp;amp;pa=showpage&amp;amp;pid=388"&gt;Познание технологической платформы Amazon&lt;/a&gt; -
    диалог с Werner Vogels&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/bf438042/" rel="nofollow" target="_blank" title="http://www.allthingsdistributed.com/"&gt;Блог Werner Vogels'а&lt;/a&gt; -
    построение масштабируемых распределенных систем&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/oracle/"&gt;Oracle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/c/"&gt;C++&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/perl/"&gt;Perl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mason/"&gt;Mason&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/java/"&gt;Java&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/jboss/"&gt;Jboss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/servlet/"&gt;Сервлеты&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="statistika"&gt;Статистика&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Более чем 55 миллионов учетных записей активных покупателей.&lt;/li&gt;
&lt;li&gt;Более миллиона активных розничных партнеров по всему Миру.&lt;/li&gt;
&lt;li&gt;Для построения страницы осуществляется доступ к 100-150 сервисам.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="arkhitektura"&gt;Архитектура&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Что мы на самом деле подразумеваем под словом
    &lt;a href="/tag/masshtabiruemost/"&gt;"масштабируемость"&lt;/a&gt;? Обычно говорят, что
    сервис является масштабируемым, если в случае расширения ресурсов
    системы производительность растет пропорционально. Рост
    производительности обычно означает увеличение количества выполняемых
    в единицу времени работ, но с другой стороны он может означать и
    рост объемов выполняемых работ, например размер обрабатываемых
    наборов данных.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; пришлось претерпеть большое архитектурное
    преобразование в процессе перехода от двух-уровневой монолитной
    системы к полностью распределенной децентрализованной платформе для
    сервисов и приложений.&lt;/li&gt;
&lt;li&gt;Все началось с одного приложения, обменивающегося данными с
    внутренним интерфейсом, написанного на &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Оно росло. За годы усилий, направленных на масштабирование,
    &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; сфокусировался на масштабировании &lt;a href="/tag/bd/"&gt;баз данных&lt;/a&gt; для хранения постоянно растущего объема информации
    о предметах, покупателях, заказах, для поддержки нескольких
    интернациональных сайтов. В 2001 году стало ясно, что исходное
    веб-приложение больше не в состоянии
    &lt;a href="/tag/masshtabiruemost/"&gt;масштабироваться&lt;/a&gt; такими темпами. Базы
    данных были разбиты на маленькие части и для каждой их них был
    построен отдельный &lt;a href="/tag/interfejs/"&gt;интерфейс&lt;/a&gt;, выполненный в виде
    сервиса, который являлся единственным способом получить доступ к
    данным.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/bd/"&gt;Базы данных&lt;/a&gt; стали общим ресурсом, что затрудняло рост
    бизнеса в целом. Интерфейсы, связанные с пользователями и базами
    данных, были сильно ограничены в своей эволюции, так как они
    одновременно использовались множеством разных команд разработчиков и
    процессов.&lt;/li&gt;
&lt;li&gt;Их &lt;a href="/tag/arkhitektura/"&gt;архитектура&lt;/a&gt; тесно связана и построена вокруг
    сервисов. Ориентированная на сервисы архитектура дала им необходимый
    уровень изоляции для построения множества программных компонентов
    быстро и независимо.&lt;/li&gt;
&lt;li&gt;Система выросла до сотен сервисов и не меньшего количества серверов
    приложений, агрегирующих информацию, полученную от сервисов.
    Приложение, генерирующее страницы для
    &lt;a href="https://www.insight-it.ru/goto/300f3b10/" rel="nofollow" target="_blank" title="http://amazon.com"&gt;Amazon.com&lt;/a&gt;, является одним из таких серверов.
    То же самое можно сказать и про приложения, служащие в роли
    интерфейса для Веб-сервисов, сервиса, обслуживающего покупателя,
    интерфейса для продавцов.&lt;/li&gt;
&lt;li&gt;Многие другие &lt;a href="/tag/tekhnologiya/"&gt;технологии&lt;/a&gt; очень трудно
    масштабировать до размеров &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt;, особенно
    технологии коммуникационной инфраструктуры. Они отлично работают до
    какого-то предела в размерах системы, а после перестают справляться
    с выполнения своих обязанностей. Именно это подтолкнуло
    &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; на создание своих
    &lt;a href="/tag/tekhnologiya/"&gt;технологий&lt;/a&gt; в этой области.&lt;/li&gt;
&lt;li&gt;Не ограничиваясь одним конкретным подходом, некоторые части системы
    используют &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;/&lt;a href="/tag/jboss/"&gt;Jboss&lt;/a&gt;, но они являются
    всего лишь &lt;a href="/tag/servlet/"&gt;сервлетами&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/c/"&gt;C++&lt;/a&gt; используется для обработки запросов, в то время как
    &lt;a href="/tag/perl/"&gt;Perl&lt;/a&gt; и &lt;a href="/tag/mason/"&gt;Mason&lt;/a&gt; - для составления контента.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; предпочитает не пользоваться промежуточным
    &lt;a href="/tag/po/"&gt;программным обеспечением&lt;/a&gt;, так как оно в большинстве
    случаев является каркасом, а не средством разработки. Если
    используется промежуточное &lt;a href="/tag/po/"&gt;программное обеспечение&lt;/a&gt;, то
    разработчик становится &lt;em&gt;заперт&lt;/em&gt; в использование тех принципов
    разработки, которые выбрал разработчик промежуточного &lt;a href="/tag/po/"&gt;ПО&lt;/a&gt;.
    Если появится необходимость использовать какие-либо другие решения,
    ничего не выйдет - вы заперты. Один и тот же цикл используется для
    обработки всех типов событий: сообщений, задержек в передаче данных,
    &lt;em&gt;AJAX&lt;/em&gt;, и так далее. Слишком громоздко. Если бы промежуточное
    &lt;a href="/tag/po/"&gt;программное обеспечение&lt;/a&gt; было бы доступно в виде более
    мелких компонентов, скорее на правах средства разработки, чем
    каркаса для системы, тогда &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; был бы более
    заинтересован в нем.&lt;/li&gt;
&lt;li&gt;Кажется, что &lt;a href="/tag/soap/"&gt;SOAP&lt;/a&gt; веб стек собирается заново решать все
    те же проблемы распределенных систем.&lt;/li&gt;
&lt;li&gt;Если предложить разработчиком на выбор работу над &lt;a href="/tag/soap/"&gt;SOAP&lt;/a&gt;
    и &lt;a href="/tag/rest/"&gt;REST&lt;/a&gt; веб-сервисами, то только 30% выберут
    &lt;a href="/tag/soap/"&gt;SOAP&lt;/a&gt;, это скорее всего будут разработчики на .NET и
    &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, привыкшие использовать WSDL файлы для генерации
    интерфейсов удаленных объектов. Оставшиеся 70% выберут
    &lt;a href="/tag/rest/"&gt;REST&lt;/a&gt; - это будут пользователи &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; и
    &lt;a href="/tag/perl/"&gt;Perl&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Обе категории разработчиков имеют возможность получить интерфейс к
    объектам &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt;. Разработчики заинтересованы просто
    выполнить свою работу, не заботясь о том, что происходит на другом
    конце провода.&lt;/li&gt;
&lt;li&gt;Идея &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; заключалась в построении открытого
    сообщества вокруг своих сервисов. Веб-сервисы были выбраны благодаря
    своей простоте. Но так это выглядит только снаружи. Внутри же
    находится архитектура, ориентированная на сервисы. Доступ к данным
    может быть получен только через соответстыующий
    &lt;a href="/tag/interfejs/"&gt;интерфейс&lt;/a&gt;. Этот процесс описан в WSDL, но они
    используют свои собственные механизмы транспортировки и инкапсуляции
    данных.&lt;/li&gt;
&lt;li&gt;Команды разработчиков очень небольшие и организуются вокруг сервисов&lt;ul&gt;
&lt;li&gt;Сервисы являются независимыми единицами предоставления функционала
в рамках &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Если у разработчика возникает новая бизнес-идея или проблема,
которую ему хотелось бы решить, он собирает команду для ее решения
или реализации. Количество участников ограничено 8-10 людьми.
Команды из такого количества человек обычно называют
&lt;em&gt;пиццерийными&lt;/em&gt;, так как для того, чтобы ее накормить достаточно двух
пицц.&lt;/li&gt;
&lt;li&gt;Команды очень небольшие, но они уполномочены решать поставленную
задачу любыми доступными способами, именно так, как они считают
нужным.
&amp;ndash; В качестве примера задачи, поставленной перед такой командой,
может служить поиск фраз в рамках книги, уникальных для конкретного
текста.
&amp;ndash; Экстенсивное A/B тестирование используется для интеграции новых
сервисов. Они смотрят на произведенное влияние на систему и
выполняют экстенсивные измерения.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Развертывание&lt;ul&gt;
&lt;li&gt;Они создают специальную инфраструктуру для управления
зависимостями и развертывания.&lt;/li&gt;
&lt;li&gt;Цель состоит в том, чтобы иметь все необходимые сервисы
развернутыми на новом оборудовании, в том числе код приложений,
системы мониторинга и лицензирования и так далее.&lt;/li&gt;
&lt;li&gt;Результатом развертывания является виртуальная машина, которая
запускается с помощью &lt;strong&gt;EC2&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Работа с покупателями для того, чтобы убедиться, что внедрение нового сервиса того стоит&lt;ul&gt;
&lt;li&gt;Фокусировка на конкретно на тех возможностях, которые планируется
предоставить покупателям&lt;/li&gt;
&lt;li&gt;Разработчики принуждаются работать в первую очередь с упором на
предоставление пользователям новых возможностей, а не на внедрение
новых технологий и уже после этого осознавание того, зачем это
делалось&lt;/li&gt;
&lt;li&gt;Все начинается с пресс-релиза о новых возможностях,
предоставляемых пользователям, а после чего ведется работа по
определению того факта, планировалось ли все же что-то значимое для
пользователей или нет?&lt;/li&gt;
&lt;li&gt;Дизайн должен быть минимален. Простота - залог успеха, когда речь
идет о больших распределенных системах&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Управление состояниями, как основная проблема крупномасштабных систем&lt;ul&gt;
&lt;li&gt;Изнутри они теоретически могут предоставить практически
бесконечный объем дискового пространства.&lt;/li&gt;
&lt;li&gt;Не все, но многие операции имеют состояния. Например, оформление
покупки продукта.&lt;/li&gt;
&lt;li&gt;Сервис отслеживания последних открытых страниц использует
рекомендации, базирующиеся на идентификационных номерах сессий.&lt;/li&gt;
&lt;li&gt;Они следят за всем, так что в любом случае цель вовсе не в
поддержании состояний. Достаточно небольшой набор состояний требует
поддержания с помощью сессий. Сервисы уже хранят всю необходимую
информацию, остается лишь ими воспользоваться.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Три свойства системы или теорема Eric Brewer'а:&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Три свойства системы: &lt;em&gt;стабильность, доступность, переносимость
возможных распадений сети&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;В большинстве случаев для любой системы с общими данными
выполняются два свойства из трех&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Возможность разделения:&lt;/em&gt; распределение узлов по небольшим
группам, которые могут иметь доступ к другим группам, но не могут
получить доступ к конкретному произвольному узлу системы&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Стабильность:&lt;/em&gt; запишите какие-либо данные, а затем прочитайте их
же - получите те же самые данные обратно. Для распределенных систем
это далеко не всегда так.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Доступность:&lt;/em&gt; не всегда имеется возможность произвести чтение или
запись каких-либо данных. Система иногда сообщает, что она не может
произвести запись, так как она хочет остаться целостной.&lt;ul&gt;
&lt;li&gt;Для масштабирования системы необходимо разбиение ее на части, что приводит к выбору между стабильностью и доступностью. Необходимо найти некий баланс между ними.&lt;/li&gt;
&lt;li&gt;Выберите определенный подход в соответствии с нуждами сервиса.&lt;/li&gt;
&lt;li&gt;В процессе выбора продуктов приоритет предоставляется доступности: все запросы на добавление товаров в корзину учитываются, так как именно они приносят прибыль. Даже если возникают какие-либо ошибки, они скрываются от покупателя, и разработчики разбираются с ним позже.&lt;/li&gt;
&lt;li&gt;В процессе подтверждения заказа покупателем важна надежность, так как сразу несколько сервисов одновременно используют одни и те же данные: работа с кредитными картами, доставка, составление отчетов.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="podvodim-itogi"&gt;Подводим итоги&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Для того, чтобы строить реально масштабируемые системы, Вам
    необходимо изменить свой склад ума. Вероятностный подход к хаосу
    может принести неплохие результаты. В традиционных системах мы
    представляем себе идеальный мир, где не происходит никаких
    чрезвычайных ситуаций, а затем мы в этом же мире пытаемся построить
    реализацию по-настоящему сложных алгоритмов. При первом же удобном
    случае вся система гарантированно рушится, это реальность, пора бы
    уже к этому привыкнуть. Например, неплохим решением мог бы стать
    подход, использующий быструю перезагрузку и тем самым быстрое
    восстановление работоспособности. При достаточной избыточности
    данных и сервисов этот подход может дать практически 100%
    отказоустойчивость. Необходимо создание самовосстанавливающихся и
    самоорганизующихся операций.&lt;/li&gt;
&lt;li&gt;Создание инфраструктуры, в которой компоненты ничего друг с другом
    не разделяют. Сама инфраструктура может стать общим ресурсом для
    разработки и развертывания с теми же недостатками, что и совместные
    ресурсы в логике и на уровне данных. Это может вызвать запирание и
    блокировку данных. &lt;a href="/tag/arkhitektura/"&gt;Архитектура&lt;/a&gt;, ориентированная
    на сервисы, позволяет создание параллельных изолированных процессов
    разработки, позволяющих масштабировать будущие разработки для
    соответствия темпам роста.&lt;/li&gt;
&lt;li&gt;Откройте систему с помощью собственной API для создания
    экосистемы вокруг Ваших приложений.&lt;/li&gt;
&lt;li&gt;Единственный способ управлять большой распределенной системой -
    разрабатывать ее как можно более простой. Это достигается благодаря
    отсутствию скрытых требований и зависимостей в ее структуре.
    Минимизируйте использование технологий до того уровня, который Вам
    необходим для решения конкретно Ваших проблем и задач. Создание
    дополнительных искуственных и ненужных уровней в системе никогда не
    пойдет ей на пользу.&lt;/li&gt;
&lt;li&gt;Организация вокруг сервисов дает гибкость. Параллельная работа
    возможна, так как на выходе получается сервис. Этот факт резко
    сокращает время, необходимое для выхода на рынок. Построение
    инфраструктуры позволяет сервисам реализовываться очень быстро.&lt;/li&gt;
&lt;li&gt;Определенно будут возникать проблемы со всем, что пускает пыль в
    глаза еще до реальной реализации.&lt;/li&gt;
&lt;li&gt;Для внутреннего управления сервисами стоит использовать SLA.&lt;/li&gt;
&lt;li&gt;Кто угодно может быстро добавлять веб-сервисы к их продукту.
    Достаточно лишь реализовать часть продукта в виде сервиса и начать
    его использовать.&lt;/li&gt;
&lt;li&gt;Построение инфраструктуры производится для обеспечения
    производительности, надежности и контролирования издержек. После ее
    построения Вы никогда не сможете сказать после очередной неудачи,
    что в этом виновата компания &lt;em&gt;Х&lt;/em&gt;. Ваше &lt;a href="/tag/po/"&gt;программное обеспечение&lt;/a&gt; не всегда является более надежным, чем любой
    другой, но зато у Вас появляется возможность быстро устранять
    неполадки и развертывать ее, в отличии от продуктов других компаний.&lt;/li&gt;
&lt;li&gt;Используйте систему оценивания и целенаправленные обсуждения для
    отделения "хорошего" от "плохого". Бывшие сотрудники
    &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; в своих презентациях неоднократно
    демонстрировали свою глубоко засевшую привычку ставить покупателей
    перед выбором и смотреть какой из вариантов сработает лучшим
    образом, и уже на результатах такого рода тестов строить свои
    решения.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/1a817e43/" rel="nofollow" target="_blank" title="http://www.kaushik.net/avinash/"&gt;Avinash Kaushik&lt;/a&gt; называет это
    избавлением от "гиппопотамов", наиболее высоко оплачиваемых людей.
    Осуществляется оно с помощью A/B тестирований и веб-аналитиков. Если
    у вас есть выбор пути развития, реализуйте оба, позвольте людям ими
    пользоваться, и посмотрите какой из альтернативных результатов
    приведет в лучшим результатам.&lt;/li&gt;
&lt;li&gt;Создайте экономичную культуру. &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; использовал
    двери в роли столов, например.&lt;/li&gt;
&lt;li&gt;Знайте, что Вам необходимо. &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; имеет печальный
    опыт с ранней системой рекомендаций, которая не сработала: "Это было
    не то, что требовалось &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt;. Рекомендации книг в
    &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; требовали работы с разбросанными данными,
    всего лишь несколько рейтингов или покупок. Она должна работать
    быстро. Система должна иметь необходимый
    &lt;a href="/tag/masshtabiruemost/"&gt;масштаб&lt;/a&gt; для работы с массивным количеством
    клиентов и огромным каталогом. Все, что было необходимо: лишь
    усовершенствовать обнаружение книг из глубин каталога, откуда
    читатели не могли достать из самостоятельно."&lt;/li&gt;
&lt;li&gt;Работа в сторонних проектах, просто так как Вы в них заинтересованы,
    часто является намного более продуктивной и инновационной, чем
    просто работа за деньги. Никогда не недооценивайте мощь блуждания в
    той сфере, которая Вам интересна.&lt;/li&gt;
&lt;li&gt;Вовлеките всех в производство еды для собак. Пойдите на склад и
    упаковывайте книги во время рождественской суеты. Это называется
    командной работой.&lt;/li&gt;
&lt;li&gt;Создайте специальный сайт для тестирования нововведений перед
    выпуском их в вольное плавание.&lt;/li&gt;
&lt;li&gt;Непоколебимая, кластеризованная, реплицирующая, распределенная
    файловая система является идеальным решением для хранения данных,
    доступных только для чтения, используемых веб-серверами.&lt;/li&gt;
&lt;li&gt;Предусмотрите способы отменить изменения, если обновление не
    удалось. Если нужно, напишите соответствующие программные средства.&lt;/li&gt;
&lt;li&gt;Переключитесь на глубоко &lt;a href="/tag/soa/"&gt;сервис-ориентированную архитектуру&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Во время интервью обращайте внимание на три критерия: энтузиазм,
    креативность, компетентность. Самым крупным залогом успеха
    &lt;a href="https://www.insight-it.ru/goto/300f3b10/" rel="nofollow" target="_blank" title="http://amazon.com"&gt;Amazon.com&lt;/a&gt; был энтузиазм.&lt;/li&gt;
&lt;li&gt;Наймите Боба, кого-то кто знает свое дело, обладает невероятными
    способностями и знанием системы, и что самое важное, умеет решать
    даже самые невообразимые проблемы просто нырнув в них с головой.&lt;/li&gt;
&lt;li&gt;Инновация может прийти только снизу. Те, кто находится ближе всего к
    проблеме, являются наиболее вероятными людьми, кто смог бы ее
    решить. Любая организация, зависящая от инноваций, должна уметь
    пользоваться хаосом. Лояльность и подчинение - не наш метод.&lt;/li&gt;
&lt;li&gt;Креативность должна лезть из всех щелей.&lt;/li&gt;
&lt;li&gt;У всех должна быть возможность эксперементировать и учиться.
    Позиции, подчинение и традиции не должны играть какой-либо роли. Для
    процветания инновации балом должен править точный расчет.&lt;/li&gt;
&lt;li&gt;Выберите путь инноваций. Перед лицом всей компании, Jeff Bezos может
    дать старый кроссовок Nike в роли награды "Просто сделай это" тому,
    кто привнес инновацию.&lt;/li&gt;
&lt;li&gt;Не платите за производительность. Предоставьте хороший повод задрать
    нос и высокую оплату труда, но оставляйте это простым. Распознать
    выдающуюся работу можно и другими методами. Оплата по заслугам
    звучит неплохо, но в условиях большой организации это практически
    невозможно. Используйте не-денежные награды, такие как тот старый
    кроссовок. Если преподнести это как способ сказать спасибо, кто-то
    оценит.&lt;/li&gt;
&lt;li&gt;Вырастайте быстро. Большие парни вроде Barnes и Nobel у Вас на
    хвосте. &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; не был ни первым, ни вторым, ни даже
    третим книжным магазинам в &lt;a href="/tag/internet/"&gt;Сети&lt;/a&gt;, но их взгляд на
    работу и драйв в итоге позволили им вырваться вперед.&lt;/li&gt;
&lt;li&gt;В дата-центрах персонал проводит только 30% времени в работе над
    вопросами создания инфраструктуры, остальные 70% они проводят за
    размещения поставок тяжелого оборудования, управлением программным
    обеспечением, балансировкой нагрузок, техническими работами,
    изменениями в масштабе и так далее.&lt;/li&gt;
&lt;li&gt;Запретите клиентам прямой доступ к базе данных. Это значит появление
    возможность масштабировать сервис и делать его более надежным не
    вовлекая при этом клиентов. Это очень похоже на возможность Google
    независимо вносить улучшения в части системы, что приводит к
    улучшениям в работе всех остальных ее компонентов.&lt;/li&gt;
&lt;li&gt;Создайте единый универсальный механизм получения доступа к сервисам.
    Это позволяет более легко агрегировать информацию, полученную от
    сервисов, децентрализованно прокладывать маршруты передачи запросов,
    распределенно следить за ними, а также получать доступ к другим
    инфраструктурным механизмам.&lt;/li&gt;
&lt;li&gt;Предоставление свободного доступа ко всем сервисам
    &lt;a href="https://www.insight-it.ru/goto/300f3b10/" rel="nofollow" target="_blank" title="http://amazon.com"&gt;Amazon.com&lt;/a&gt; разработчикам со всех уголков Мира
    также было достаточно значимым компонентом успеха, так как это
    привлекло на порядок больше инноваций, чем они могли надеяться
    построить самостоятельно.&lt;/li&gt;
&lt;li&gt;Разработчики сами знают какими инструментами они владеют лучше
    всего, какие из них делают их наиболее продуктивными.&lt;/li&gt;
&lt;li&gt;Не накладывайте слишком много ограничений на инженеров.
    Предоставляйте стимулы для использования некоторых вещей, например
    интеграцию с системами мониторинга и другими инструментами
    инфраструктуры. Для всего остального старайтесь предоставлять
    возможность командам функционировать максимально независимо.&lt;/li&gt;
&lt;li&gt;Разработчики, они как художники; они делают свою работу лучше всего
    только тогда, когда им предоставляют свободу это делать, но в любом
    случае им требуются качественные инструменты. Имейте много
    вспомогательных инструментов, имеющих само-помогающую природу.
    Поддерживайте окружение вокруг разработки сервисов, которое никогда
    не будет вмешиваться в сам процесс разработки.&lt;/li&gt;
&lt;li&gt;Вы построили это, вы и поддерживаете. Это позволяет разработчикам
    почувствовать повседневную работу их приложения, а также
    предоставляет им постоянный контакт с покупателями.&lt;/li&gt;
&lt;li&gt;Раз в пару лет разработчики должны проводить некоторое время в
    отделе по работе с клиентами. Это позволит им выслушать покупателей,
    ответить на электронные письма, и реально осознать влияние тех
    вещей, которые они реализовали с помощью как технологи.&lt;/li&gt;
&lt;li&gt;Пользуйтесь "голосом покупателя", который являлся бы реалистичной
    историей от покупателя о какой-то конкретной части сайта. Это
    поможет менеджерам и инженерам осознать тот факт, что все эти
    технологии построены для реальных людей. Статистика отдела по работе
    с клиентами является ранним индикатором того, что вы делаете что-то
    не так, а также указывает на то, что реально является болевыми
    точками для ваших покупателей.&lt;/li&gt;
&lt;li&gt;Инфраструктура &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt;, подобно &lt;a href="/tag/google/"&gt;Google&lt;/a&gt;,
    является огромным конкурентным преимуществом. Они могут строить
    комплексные приложения на основе примитивных сервисов, которые сами
    по себе просты до безобразия. Они могут независимо масштабировать
    свою работу, поддерживать доступность не распараллеленной системы,
    быстро реализовывать новые сервисы без необходимости массивных
    изменений в конфигурации.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 17 Feb 2008 21:47:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-02-17:highload/2008/arkhitektura-amazon/</guid><category>Amazon</category><category>C++</category><category>Java</category><category>Jboss</category><category>Linux</category><category>Mason</category><category>online</category><category>Oracle</category><category>Perl</category><category>REST</category><category>Servlet</category><category>SOAP</category><category>архитектура</category><category>архитектура Amazon</category><category>интер информационные технологии</category><category>Масштабируемость</category></item><item><title>Архитектура Flickr</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-flickr/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/f50a76e1/" rel="nofollow" target="_blank" title="http://www.flickr.com"&gt;Flickr&lt;/a&gt; является мировым лидером среди сайтов
размещения фотографий. Перед Flickr стоит впечатляющая задача, они
должны контролировать обширное море ежесекундно обновляющегося контента,
непрерывно пополняющиеся легионы пользователей, постоянный поток новых
предоставляемых пользователям возможностей, а делается все это при
постоянной поддержке отличной производительности. Как же они это
делают?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Как и предыдущий пост &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-google/"&gt;"Архитектура Google"&lt;/a&gt;, этот тоже является
переводом &lt;a href="https://www.insight-it.ru/goto/e7a0ee0d/" rel="nofollow" target="_blank" title="http://highscalability.com/flickr-architecture"&gt;статьи&lt;/a&gt; от
&lt;a href="https://www.insight-it.ru/goto/f3f1b405/" rel="nofollow" target="_blank" title="http://highscalability.com/user/todd-hoff"&gt;Todd'а Hoff'а&lt;/a&gt;. Возможно
читателям &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; был более интересен, но подход Flickr к
масштабируемости тоже более чем заслуживает внимания. Далее привожу
источники информации из оригинальной статьи:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/88014756/" rel="nofollow" target="_blank" title="http://www.niallkennedy.com/blog/uploads/flickr_php.pdf"&gt;Flickr и PHP&lt;/a&gt;
    (ранний документ)&lt;/li&gt;
&lt;li&gt;Планирование нагрузок на LAMP&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/6df1dabf/" rel="nofollow" target="_blank" title="http://www.bytebot.net/blog/archives/2007/04/25/federation-at-flickr-a-tour-of-the-flickr-architecture"&gt;Федерация Flickr: Тур по архитектуре Flickr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/9e0a13a1/" rel="nofollow" target="_blank" title="http://highscalability.com/book-building-scalable-web-sites"&gt;Построение масштабируемых веб-сайтов&lt;/a&gt;
    от Call Handerson'а из Flickr&lt;/li&gt;
&lt;li&gt;История войн баз данных #3: Tim O'Reilly о Flickr&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/d881b0d9/" rel="nofollow" target="_blank" title="http://www.iamcal.com/talks/"&gt;Cal Henderson's Talks&lt;/a&gt; - много
    полезных презентаций&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/sql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Сегментирование &lt;em&gt;(прим.: разбиение системы на части, обслуживающие
    каждая свою группу пользователей; называть можно было по-разному, но
    давайте остановимся на этом варианте перевода слова "Shards")&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt; для кэширования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt; в качестве обратной-прокси для html и
    изображений&lt;/li&gt;
&lt;li&gt;&lt;a href="/linux"&gt;Linux&lt;/a&gt; (&lt;a href="/tag/redhat/"&gt;RedHat&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/smarty/"&gt;Smarty&lt;/a&gt; в роли шаблонизатора&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/perl/"&gt;Perl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PEAR для парсинга e-mail и XML&lt;/li&gt;
&lt;li&gt;ImageMagick для обработки изображений&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/java/"&gt;Java&lt;/a&gt; для узлового сервиса&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/systemimager/"&gt;SystemImager&lt;/a&gt; для развертывания систем&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/ganglia/"&gt;Ganglia&lt;/a&gt; для мониторинга распределенных систем&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/subcon/"&gt;Subcon&lt;/a&gt; хранит важные системные конфигурационные файлы
    в SVN-репозитории для легкого развертывания на машины в кластере.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/cvsup/"&gt;Cvsup&lt;/a&gt; для распространения и обновления коллекций
    файлов по сети&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="statistika"&gt;Статистика&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Более четырех миллиардов запросов в день&lt;/li&gt;
&lt;li&gt;Примерно 35 миллионов фотографий в кэше &lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Около двух миллионов фотографий в оперативной памяти
    &lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Всего приблизительно 470 миллионов изображений, каждое представлено
    в 4 или 5 размерах&lt;/li&gt;
&lt;li&gt;38 тысяч запросов к &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; (12 миллионов
    объектов)&lt;/li&gt;
&lt;li&gt;2 петабайта дискового пространства&lt;/li&gt;
&lt;li&gt;Более 400000 фотографий добавляются ежедневно&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="arkhitektura"&gt;Архитектура&lt;/h3&gt;
&lt;p&gt;Симпатичное изображение архитектуры Flickr можно увидеть на &lt;a href="https://www.insight-it.ru/goto/d30e097b/" rel="nofollow" target="_blank" title="http://www.slideshare.net/techdude/scalable-web-architectures-common-patterns-and-approaches/138"&gt;этом слайде&lt;/a&gt;.
Краткое ее описание выглядит следующим образом:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Два ServerIron&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt; кэши&lt;/li&gt;
&lt;li&gt;Системы хранения NetApp&lt;/li&gt;
&lt;li&gt;Серверы &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; приложений&lt;/li&gt;
&lt;li&gt;Менеджер хранения данных&lt;/li&gt;
&lt;li&gt;Master-master сегменты&lt;/li&gt;
&lt;li&gt;Центральная база данных, структурированная по принципу Dual
Tree&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt; кластер&lt;/li&gt;
&lt;li&gt;Поисковая система&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Хранение данных&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Структура Dual Tree является индивидуальным набором модификаций для
&lt;a href="/tag/sql/"&gt;MySQL&lt;/a&gt;, позволяющим масштабировать систему путем добавления
новых мастер-серверов без использования кольцевой архитектуры. Эта
система позволяет экономить на масштабировании, так как варианты
мастер-мастер требовали бы удвоенных вложений в оборудование.&lt;/li&gt;
&lt;li&gt;Центральная база данных включает в себя таблицу пользователей,
состоящую из основных ключей пользователей (несколько уникальных
идентификационных номеров) и указатель на сегмент, на котором может быть
найдена остальная информация о конкретном пользователе.&lt;/li&gt;
&lt;li&gt;Использование выделенных серверов для статического контента&lt;/li&gt;
&lt;li&gt;Все, за исключением фотографий, хранится в базе данных&lt;/li&gt;
&lt;li&gt;Отсутствие состояний заключается в том, что в случае необходимости
    они имеют возможность передать пользователей от сервера к серверу,
    что стало намного проще для них после создания своего API&lt;/li&gt;
&lt;li&gt;В основе масштабируемости лежит репликация, но этот факт помогает
    лишь при обработке операций чтения&lt;/li&gt;
&lt;li&gt;Для поиска по определенной части базы данных создается отдельная
    копия этого фрагмента&lt;/li&gt;
&lt;li&gt;Использования горизонтального масштабирования для того чтобы можно
    было проще добавлять новые машины в систему&lt;/li&gt;
&lt;li&gt;Обработка изображений, полученных от пользователей по электронной
    почте, происходит с помощью &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Раньше система страдала от задержек связанных с организацией по
    принципу мастер-слуга. При слишком большой нагрузке они имели одну
    точку, которая теоретически могла дать сбой.&lt;/li&gt;
&lt;li&gt;Им было необходимо иметь возможность проводить технические работы во
    время непрерывной работы сайта, не прекращая его функционирование.&lt;/li&gt;
&lt;li&gt;Были проведены отличные работы по планированию распределения
    дискового пространства, более подробную информацию можно найти по
    ссылкам в разделе "Источники информации".&lt;/li&gt;
&lt;li&gt;Для обеспечения возможности масштабирования в будущем, они пошли по
    федеративному пути развития:&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Сегменты системы:&lt;/em&gt; Мои данные хранятся на моем сегменте, но
запись о Вашем комментарии хранится на Вашем сегменте.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Глобальное кольцо:&lt;/em&gt; Принцип работы схож с DNS, Вам необходимо
знать куда Вы хотите пойти и кто контролирует то место, куда Вы
собираетесь пойти.&lt;/li&gt;
&lt;li&gt;Логика на &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; устанавливает соединение с сегментом и
поддерживает целостность данных (10 строк кода с комментариями!)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Сегменты:&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Срез основной базы данных&lt;/li&gt;
&lt;li&gt;Активная репликация по принципу мастер-мастер: имеет несколько
недостатков в &lt;a href="/tag/sql/"&gt;MySQL&lt;/a&gt; 4.1. Автоматическое
инкрементирование идентификационных номеров используется для
поддержания системы в режиме одновременной активности обоих серверов
в паре&lt;/li&gt;
&lt;li&gt;Привязывание новых учетных записей к сегментам системы происходит
случайным образом&lt;/li&gt;
&lt;li&gt;Миграция пользователей проводится время от времени для того, чтобы
избавиться от проблем, связанных с излишне активными пользователями.
Необходима сбалансированность в этом процессе, особенно в случаях с
большим количеством фотографий&amp;hellip; 192 тысячи фотографий, 700 тысяч
тэгов, может занять несколько минут. Миграция выполняется вручную.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Нажатие на &lt;strong&gt;Favorite&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Получается информация об учетной записи владельца из кэша для
того, чтобы узнать к какому сегменту он привязан (допустим на
shard-5)&lt;/li&gt;
&lt;li&gt;Получается информация о моей учетной записи из кэша, более
конкретно - мой сегмент (например shard-13)&lt;/li&gt;
&lt;li&gt;Начинается "распределенная транзакция" для определения ответов на
вопросы: Кто добавил эту фотографию в избранное? Как изменился
список избранных фотографий?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Подобные вопросы могут задаваться любому сегменту, информация на них
    абсолютно избыточна.&lt;/li&gt;
&lt;li&gt;Для избавления от задержек, связанных с репликацией...&lt;ul&gt;
&lt;li&gt;при каждой загрузке страницы, пользователю предоставляется список
серверов&lt;/li&gt;
&lt;li&gt;если сервер не в состоянии ответить на запрос, запрос переходит к
следующему серверу в списке; если список кончился - выводится
сообщение об ошибке. При этом не используются постоянные соединения,
каждый раз создаются и разрываются новые соединения.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Запросы на чтение и запись от каждого пользователя ограничиваются
    рамками одного сегмента. Задержки репликации исчезают из поля зрения
    пользователей.&lt;/li&gt;
&lt;li&gt;Каждый сервер в рамках одного сегмента в обычном состоянии нагружен
    ровно на половину. Выключите половину серверов в каждом сегменте и
    система продолжит функционировать без изменений. Это значит, что
    один сервер внутри сегмента может взять на себя всю нагрузку
    второго, в то время как второй сервер может по каким либо причинам
    быть отключен от системы, например для проведения технических работ.
    Обновление оборудования производится очень просто: отключается
    половина сегмента, она же обновляется, подключается обратно, процесс
    повторяется для оставшейся половины.&lt;/li&gt;
&lt;li&gt;Периоды пиковой нагрузки также нарушают правило 50% нагрузки. В
    такие моменты система получает 6-7 тысяч запросов в секунду, в то
    время как на данный момент система может работать на
    пятидесятипроцентном уровне нагрузки только при четырех тысячах
    запросов в секунду.&lt;/li&gt;
&lt;li&gt;В среднем при загрузке одной страницы выполняется 27-35
    SQL-запросов. Списки избранных фотографий обрабатываются в реальном
    времени, ровно как и доступ через API к базе данных. Все требования
    к нагрузке в реальном времени выполняются без каких-либо
    недостатков.&lt;/li&gt;
&lt;li&gt;Более 36 тысяч запросов в секунду может выполняться не выходя за
    рамки возможностей системы, даже при резком росте трафика.&lt;/li&gt;
&lt;li&gt;Каждый сегмент содержит данные о более чем 400 тысячах
    пользователей.&lt;/li&gt;
&lt;li&gt;Многие данные хранятся в двух местах одновременно. Например,
    комментарий является частью между комментатором и автором
    комментируемого контента. Где его хранить? Как насчет обоих мест?
    Транзакции используются для предотвращения рассинхронизации данных:
    открывается первая транзакция, выполняется запись, открывается
    вторая транзакция, выполняется запись, подтверждается первая
    транзакция если все нормально, после чего вторая подтверждается
    только в случае если первая прошла успешно.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Поиск&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Используется два варианта поиска: поиск в рамках сегмента,
поддерживающий до 35 тысяч запросов в секунду, а также проприетарный
веб-поиск от Yahoo!&lt;/li&gt;
&lt;li&gt;В 90% случаев используется система от Yahoo!, за исключением
поиска по тэгу фотографий одного пользователя и массовых изменений
тэгов.&lt;/li&gt;
&lt;li&gt;Эту систему стоит рассматривать как аналог Lucene.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Оборудование&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;EMT64 под управлением RHEL 4 с 16 Gb оперативной памяти.&lt;/li&gt;
&lt;li&gt;6 жестких дисков с 15000rpm, объединены в RAID-10.&lt;/li&gt;
&lt;li&gt;Размер для пользовательских метаданных достигает 12 терабайт (это
не включает фотографии, для них цифры существенно больше).&lt;/li&gt;
&lt;li&gt;Используются 2U корпуса.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Резервное копирование данных&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ibbackup выполняется регулярно посредством cron daemon'а, на
каждом сегменте настроен на разное время.&lt;/li&gt;
&lt;li&gt;Каждую ночь делается снимок со всего кластера баз данных.&lt;/li&gt;
&lt;li&gt;Запись или удаление нескольких больших файлов с резервными копиями
одновременно на реплицирующую систему хранения может сильно
сократить производительность системы вцелом на последующие несколько
часов из-за процесса репликации. Выполнение этого на активно
работающей системе хранения фотографий было бы не самой лучшей
идеей.&lt;/li&gt;
&lt;li&gt;Содержание нескольких резервных копий всех Ваших данных требует
существенных материальных затрат, но оно того стоит. Особенно это
актуально для тех ситуаций, когда Вы понимаете, что что-то пошло не
так только спустя несколько дней после того как это случилось, в
таких случаях неплохо иметь, например, резервные копии 1, 3, 10 и
30-дневной давности.&lt;/li&gt;
&lt;li&gt;Фотографии хранятся в системе хранения данных. После загрузки
изображения система выдает различные его размеры, на чем ее работа
заканчивается. Метаданные и ссылки на файловые системы, где
расположены фотографии, хранятся в базе данных.&lt;/li&gt;
&lt;li&gt;Агрегация данных проходит очень быстро, так как она ограничена
пределами сегмента.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max_connections = 400&lt;/code&gt; соединений на каждый сегмент, неплохой запас.
Значение для кэша потоков установлено равным 45, так как не бывает
ситуаций когда более 45 пользователей одновременно выполняют
какие-либо действия с одним конкретным сегментом.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Тэги&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Тэги плохо вписываются в традиционную нормализованную схему
реляционной базы данных. Денормализация или активное кэширование -
единственные способы сгенерировать облако меток для сотен миллионов
тэгов в течении миллисекунд.&lt;/li&gt;
&lt;li&gt;Некоторые данные обрабатываются отдельными вычислительными
кластерами, которые сохраняют результаты своей работы в MySQL, так
как иначе вычисление сложных отношений заняло бы все процессорное
время основных серверов баз данных.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Направления для развития&lt;/h4&gt;
&lt;p&gt;Ускорение работы с помощью создания
организационного плана для непрерывной работы всей системы на уровне
нескольких датацентров, таким образом чтобы все датацентры имели
возможность получать запросы на общий уровень данных (как сами БД,
так и memcache и прочее) все вместе одновременно. Если все части
системы постоянно активны - время простоя оборудования будет сведено
к минимуму.&lt;/p&gt;
&lt;h3 id="podvodim-itogi"&gt;Подводим итоги&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Старайтесь думать о своем приложении как о чем-то большем, чем просто
    веб-приложении, тогда у Вас возможно появятся поддержка различных
    API, RSS и Atom ленты и многие другие возможности.&lt;/li&gt;
&lt;li&gt;Отсутствие состояний системы позволяет более легко выполнять
    модернизации не моргнув и глазом.&lt;/li&gt;
&lt;li&gt;Реструктуризация базы данных - не самое лучшее занятие.&lt;/li&gt;
&lt;li&gt;Планирование нагрузок должно проводиться уже на ранних этапах
    развития проекта&lt;/li&gt;
&lt;li&gt;Начинайте медленно. Не покупайте сразу много оборудования просто
    из-за того, что Вы рады/боитесь, что ваш сайт взорвется.&lt;/li&gt;
&lt;li&gt;Измеряйте реально, планирование нагрузок должно базироваться на
    реальных вещах, а не абстрактных.&lt;/li&gt;
&lt;li&gt;Внедряйте ведение логов и индивидуальные измерения для оценки
    реальных показателей на основе серверной статистики, статистика
    использования не менее важна чем серверная.&lt;/li&gt;
&lt;li&gt;Кэширование и оперативная память может стать ответом на все вопросы.&lt;/li&gt;
&lt;li&gt;Создавайте четкие уровни абстракции между работой базы данных,
    бизнес-логикой, логикой страниц, разметкой страниц и презентационным
    уровнем. Это позволяет ускорить циклы итеративной разработки.&lt;/li&gt;
&lt;li&gt;Разделение приложения на уровни позволяет каждому заниматься своим
    делом: разработчики могут строить логику страниц, в то время как
    дизайнеры работают с удобством работы для пользователей.&lt;/li&gt;
&lt;li&gt;Делайте релизы как можно чаще, пускай даже это будет происходить
    каждые полчаса.&lt;/li&gt;
&lt;li&gt;Забудьте о всех небольших эффективных вещах, предварительная
    оптимизация является корнем всего зла в примерно 97% всех случаев.&lt;/li&gt;
&lt;li&gt;Тестируйте в работе. Постройте архитектурные механизмы (флаги
    конфигурации, балансировку нагрузки, и так далее), которые позволят
    Вам разворачивать новое оборудование в (и из) работу.&lt;/li&gt;
&lt;li&gt;Забудьте об искусственных тестах, они годятся только для получения
    общего представления о нагрузках, но не для планирования.
    Искуственные тесты дают искусственные результаты, для настоящих
    тестов все же стоит пользоваться реальным временем выполнения задач.&lt;/li&gt;
&lt;li&gt;Найдите максимальное значения для всех показателей:&lt;ul&gt;
&lt;li&gt;Какой максимум чего-то, что может выполнять каждый сервер?&lt;/li&gt;
&lt;li&gt;Как близко параметр находится к максимуму и каковы тенденции?&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/sql/"&gt;MySQL&lt;/a&gt; (дисковый ввод/вывод?)&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt; (дисковый ввод/вывод? или процессорное время?)&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt; (процессорное время? или пропускная способность?)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Старайтесь учесть особенности использования Вашего приложения.&lt;ul&gt;
&lt;li&gt;Возможен ли резкий рост нагрузки, связанный с каким-либо событием?
Например: какое-либо бедствие, или может быть новость?&lt;/li&gt;
&lt;li&gt;Flickr получает на 20-40% больше новых фотографий в первый рабочий
день нового года, чем в любой пик в предыдущем году.&lt;/li&gt;
&lt;li&gt;По воскресеньям нагрузка в среднем на 40-50% выше, чем в любой
другой день недели.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Учтите возможность экспоненциального роста. Больше пользователей
    означает больше контента, больше контента означает больше
    соединений, больше соединений означает более активное использование.&lt;/li&gt;
&lt;li&gt;Планируйте возможные варианты управления работой системы в периоды
    пиковых нагрузок.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 08 Feb 2008 22:41:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-02-08:highload/2008/arkhitektura-flickr/</guid><category>Apache</category><category>Cvsup</category><category>flickr</category><category>Ganglia</category><category>Java</category><category>Linux</category><category>Memcached</category><category>MySQL</category><category>online</category><category>Perl</category><category>PHP</category><category>RedHat</category><category>shard</category><category>Smarty</category><category>Squid</category><category>Subcon</category><category>SystemImager</category><category>архитектура</category><category>архитектура Flickr</category><category>интернет</category><category>кластер</category><category>Масштабируемость</category><category>сервер</category></item><item><title>Архитектура Google</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-google/</link><description>&lt;p&gt;&lt;em&gt;Эта статья датируется 2008 годом, новая версия: &lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-google-2011/"&gt;Архитектура Google 2011&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="/tag/google/"&gt;Google&lt;/a&gt;&lt;/strong&gt; - Король масштабируемости.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Каждый хоть раз слышал о &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; благодаря их
всеобъемлющему, "умному" и быстрому поисковому сервису, но ни для кого
не секрет, что они не ограничиваются только им. Их платформа для
построения масштабируемых приложений позволяет выпускать множество
удивительно конкурентноспособных интернет-приложений, работающих на
уровне всего Интернета вцелом. Они ставят перед собой цель постоянно
строить все более и более производительную и масштабируемую архитектуру
для поддержки своих продуктов. Как же им это удается?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Сразу хочу сказать, что эта запись является переводом с английского,
автор &lt;a href="https://www.insight-it.ru/goto/31bfd110/" rel="nofollow" target="_blank" title="http://highscalability.com/google-architecture"&gt;оригинальной версии&lt;/a&gt; - &lt;a href="https://www.insight-it.ru/goto/f3f1b405/" rel="nofollow" target="_blank" title="http://highscalability.com/user/todd-hoff"&gt;Todd Hoff&lt;/a&gt;. Оригинал написан приблизительно в середине 2007 года, но по-моему до сих пор очень даже актуально.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Далее следует перечисление источников информации из оригинала:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/741bec4c/" rel="nofollow" target="_blank" title="http://video.google.com/videoplay?docid=-5699448884004201579"&gt;Video: Построение больших систем в Google&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/fae0d413/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/gfs.html"&gt;Google Lab: Файловая система Google (GFS)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/39138d08/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/mapreduce.html"&gt;Google Lab: MapReduce: упрощенная обработка данных на больших кластерах&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8667b351/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/bigtable.html"&gt;Google Lab: BigTable.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/dab5470e/" rel="nofollow" target="_blank" title="http://video.google.com/videoplay?docid=7278544055668715642"&gt;Video: BigTable: система распределенного хранения данных.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/87fff9b2/" rel="nofollow" target="_blank" title="http://www.baselinemag.com/article2/0,1540,1985514,00.asp"&gt;Как работает Google&lt;/a&gt;
    от David Carr в Baseline Magazine.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a426f3de/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/sawzall.html"&gt;Google Lab: интерпретирование данных. Параллельный анализ с помощью Sawzall.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/ed8bca67/" rel="nofollow" target="_blank" title="http://www.25hoursaday.com/weblog/2007/06/25/GoogleScalabilityConferenceTripReportMapReduceBigTableAndOtherDistributedSystemAbstractionsForHandlingLargeDatasets.aspx"&gt;Записи с конференции по масштабированию от Dare Obasonjo.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Большое разнообразие языков программирования: &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;,
    &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="chto-vnutri"&gt;Что внутри?&lt;/h3&gt;
&lt;h4&gt;Статистика&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;На 2006 год система включала в себя 450000 недорогих серверов&lt;/li&gt;
&lt;li&gt;За 2005 год было проиндексировано 8 миллиардов страниц. На данный
    момент&amp;hellip; кто знает?&lt;/li&gt;
&lt;li&gt;На момент написания оригинала Google включает в себя более 200
    &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt; кластеров. Один кластер может состоять из 1000 или
    даже 5000 компьютеров&lt;/li&gt;
&lt;li&gt;Десятки и сотни тысяч компьютеров получают данные из &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;
    кластеров, которые насчитывают более 5 петабайт дискового
    пространства. Суммарные пропускная способность операций записи и
    чтения между дата центрами может достигать 40 гигабайт в секунду&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt; позволяет хранить миллиарды ссылок (URL),
    сотни терабайт снимков со спутников, а также настройки миллионов
    пользователей&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;// Цифры не первой свежести конечно, но тоже неплохо.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;Стек&lt;/h4&gt;
&lt;p&gt;&lt;a href="/tag/google/"&gt;Google&lt;/a&gt; визуализирует свою инфраструктуру в виде
трехслойного стека:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Продукты:&lt;/em&gt; поиск, реклама, электронная почта, карты, видео, чат,
    блоги&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Распределенная инфраструктура системы:&lt;/em&gt; &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;,
    &lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt; и &lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Вычислительные платформы:&lt;/em&gt; множество компьютеров во множестве
    датацентров&lt;/li&gt;
&lt;li&gt;Легкое развертывание для компании при низком уровне издержек&lt;/li&gt;
&lt;li&gt;Больше денег вкладывается в оборудование для исключения возможности
    потерь данных&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Надежное хранение данных с помощью &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Надежное масштабируемое хранение данных крайне необходимо для любого
    приложения. &lt;strong&gt;GFS&lt;/strong&gt; является основой их платформы хранения
    информации&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;&lt;/strong&gt; - большая распределенная файловая система, способная хранить и обрабатывать огромные объемы информации&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Зачем строить что-либо самим вместо того, чтобы просто взять это с полки?&lt;/em&gt; Они контролируют абсолютно всю систему и именно эта платформа отличает их от всех остальных.&lt;/p&gt;
&lt;p&gt;Она предоставляет:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;высокую надежность дата центров&lt;/li&gt;
&lt;li&gt;масштабируемость до тысяч сетевых узлов
&amp;ndash; высокую пропускную способность операций чтения и записи&lt;/li&gt;
&lt;li&gt;поддержку больших блоков данных, размер которых может измеряться в
гигабайтах&lt;/li&gt;
&lt;li&gt;эффективное распределение операций между датацентрами для
избежания возникновения "узких мест" в системе&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;В системе существуют мастер-сервера и сервера, собственно хранящие
    информацию:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Мастер-сервера хранят метаданные для всех файлов. Сами данные
    хранятся блоками по 64 мегабайта на остальных серверах. Клиенты
    могут выполнять операции с метаданными на мастер-серверах, чтобы
    узнать на каком именно сервере расположены необходимые данные.&lt;/li&gt;
&lt;li&gt;Для обеспечения надежности один и тот же блок данных хранится
    в трех экземплярах на разных серверах, что обеспечивает
    избыточность на случай сбоев в работе какого-либо сервера.&lt;/li&gt;
&lt;li&gt;Новые приложения могут пользоваться как существующими
    кластерами, так и новыми, созданными специально для них.&lt;/li&gt;
&lt;li&gt;Ключ успеха заключается в том, чтобы быть уверенными в том,
    что у людей есть достаточно вариантов выбора для реализации их
    приложений. &lt;strong&gt;&lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;&lt;/strong&gt; может быть настроена для
    удовлетворения нужд любого конкретного приложения.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Работаем с данными при помощи MapReduce&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Теперь, когда у нас есть отличная система хранения, что же делать с
    такими объемами данных? Допустим, у нас есть много терабайт данных,
    равномерно распределенных между 1000 компьютерами. Коммерческие базы
    данных не могут эффективно масштабироваться до такого уровня, именно
    в такой ситуации в дело вступает технология
    &lt;strong&gt;&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;&lt;/strong&gt; является программной моделью и
    соответствующей реализацией обработки и генерации больших наборов
    данных. Пользователи могут задавать функцию, обрабатывающую пары
    ключ/значение для генерации промежуточных аналогичных пар, и
    сокращающую функцию, которая объединяет все промежуточные значения,
    соответствующие одному и тому же ключу. Многие реальные задачи могут
    быть выражены с помощью этой модели. Программы, написанные в таком
    функциональном стиле автоматически распараллеливаются и адаптируются
    для выполнения на обширных кластерах. Система берет на себя детали
    разбиения входных данных на части, составления расписания выполнения
    программ на различных компьютерах, управления ошибками, и
    организации необходимой коммуникации между компьютерами. Это
    позволяет программистам, не обладающим опытом работы с параллельными
    и распределенными системами, легко использовать все ресурсы больших
    распределенных систем.&lt;/li&gt;
&lt;li&gt;Зачем использовать &lt;strong&gt;&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;&lt;/strong&gt;?
    &amp;ndash; Отличный способ распределения задач между множеством компьютеров
    &amp;ndash; Обработка сбоев в работе
    &amp;ndash; Работа с различными типами смежных приложений, таких как поиск или
    реклама. Возможно предварительное вычисление и обработка данных,
    подсчет количества слов, сортировка терабайт данных и так далее
    &amp;ndash; Вычисления автоматически приближаются к источнику ввода-вывода&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;&lt;/strong&gt; использует три типа серверов:&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Master:&lt;/em&gt; назначают задания остальным типам серверов, а также
следят за процессом их выполнения&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Map:&lt;/em&gt; принимают входные данные от пользователей и обрабатывают
их, результаты записываются в промежуточные файлы&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Reduce:&lt;/em&gt; принимают промежуточные файлы от Map-серверов и
сокращают их указанным выше способом&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Например, мы хотим посчитать количество слов на всех страницах. Для
    этого нам необходимо передать все страницы, хранимые в &lt;strong&gt;GFS&lt;/strong&gt;, на
    обработку в &lt;strong&gt;MapReduce&lt;/strong&gt;. Этот процесс будет происходить на тысячах
    машин одновременно с полной координацией действий, в соответствии с
    автоматически составленным расписанием выполняемых работ, обработкой
    потенциальных ошибок, и передачей данных выполняемыми автоматически.&lt;ul&gt;
&lt;li&gt;Последовательность выполняемых действий выглядела бы следующим
образом: &lt;code&gt;GFS &amp;rarr; Map &amp;rarr; перемешивание &amp;rarr; Reduce &amp;rarr; запись результатов обратно в GFS&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Технология &lt;strong&gt;MapReduce&lt;/strong&gt; состоит из двух компонентов:
соответственно &lt;em&gt;map&lt;/em&gt; и &lt;em&gt;reduce&lt;/em&gt;. Map отображает один набор данных в
другой, создавая тем самым пары ключ/значение, которпыми в нашем
случае являются слова и их количества.&lt;/li&gt;
&lt;li&gt;В процессе перемешивания происходит агрегирование типов ключей.&lt;/li&gt;
&lt;li&gt;Reduction в нашем случае просто суммирует все результаты и
возвращает финальный результат.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;В процессе индексирования &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; подвергает поток
    данных обработке около 20 разных механизмов сокращения. Сначала идет
    работа над всеми записями и агрегированными ключами, после чего
    результат передается следующему механизму и второй механизм уже
    работает с результатами работы первого, и так далее.&lt;/li&gt;
&lt;li&gt;Программы могут быть очень маленькими, всего лишь от 20 до 50 строк
    кода.&lt;/li&gt;
&lt;li&gt;Единственной проблемой могут быть "отстающие компьютеры". Если один
    компьютер работает существенно медленнее, чем все остальные, это
    будет задерживать работу всей системы в целом.&lt;/li&gt;
&lt;li&gt;Транспортировка данных между серверами происходит в сжатом виде.
    Идея заключается в том, что ограничивающим фактором является
    пропускная способность канала и ввода-вывода, что делает резонным
    потратить часть процессорного времени на компрессию и декомпрессию
    данных.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Хранение структурированных данных в BigTable&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;BigTable&lt;/strong&gt; является крупномасштабной, устойчивой к потенциальным
    ошибкам, самоуправляемой системой, которая может включать в себя
    терабайты памяти и петабайты данных, а также управлять миллионами
    операций чтения и записи в секунду.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BigTable&lt;/strong&gt; представляет собой распределенный механизм хэширования,
    построенный поверх &lt;strong&gt;&lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;&lt;/strong&gt;, а вовсе не реляционную базу
    данных и, как следствие, не поддерживает &lt;a href="/tag/sql/"&gt;SQL&lt;/a&gt;-запросы и
    операции типа Join.&lt;/li&gt;
&lt;li&gt;Она предоставляет механизм просмотра данных для получения доступа к
    структурированным данным по имеющемуся ключу. &lt;strong&gt;&lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;&lt;/strong&gt;
    хранит данные не поддающиеся пониманию, хотя многим приложениям
    необходимы структурированные данные.&lt;/li&gt;
&lt;li&gt;Коммерческие базы данных попросту не могут масштабироваться до
    такого уровня и, соответственно, не могут работать с тысячами машин
    одновременно.&lt;/li&gt;
&lt;li&gt;С помощью контролирования своих низкоуровневых систем хранения
    данных, &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; получает больше возможностей по
    управлению и модификации их системой. Например, если им понадобится
    функция, упрощающая координацию работы между датацентрами, они
    просто могут написать ее и внедрить в систему.&lt;/li&gt;
&lt;li&gt;Подключение и отключение компьютеров к функционирующей системе никак
    не мешает ей просто работать.&lt;/li&gt;
&lt;li&gt;Каждый блок данных хранится в ячейке, доступ к которой может быть
    предоставлен как по ключу строки или столбца, так и по временной
    метке.&lt;/li&gt;
&lt;li&gt;Каждая строка может храниться в одной или нескольких таблицах.
    Таблицы реализуются в виде последовательности блоков по 64
    килобайта, организованных в формате данных под названием
    &lt;strong&gt;SSTable&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;В &lt;strong&gt;&lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt;&lt;/strong&gt; тоже используется три типа серверов:&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Master:&lt;/em&gt; распределяют таблицы по Tablet-серверам, а также следят
за расположением таблиц и перераспределяют задания в случае
необходимости.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Tablet:&lt;/em&gt; обрабатывают запросы чтения/записи для таблиц. Они
разделяют таблицы, когда те превышают лимит размера (обычно 100-200
мегабайт). Когда такой сервер прекращает функционирование по
каким-либо причинам, 100 других серверов берут на себя по одной
таблице и система продолжает работать как-будто ничего не произошло.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Lock:&lt;/em&gt; формируют распределенный сервис ограничения одновременного
доступа. Операции открытия таблицы для записи, анализа
Master-сервером или проверки доступа должны быть
взаимоисключающими.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Локальная группировка может быть использована для физического
    хранения связанных данных вместе, чтобы обеспечить лучшую
    локализацию ссылок на данные.&lt;/li&gt;
&lt;li&gt;Таблицы по возможности кэшируются в оперативной памяти серверов.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="oborudovanie"&gt;Оборудование&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Как эффективно организовать большую группу компьютеров с точки
    зрения издержек и производительности?&lt;/li&gt;
&lt;li&gt;Используется самое обыкновенное ультра-дешевое оборудование и поверх
    него строится программное обеспечение, способное спокойно пережить
    смерть любой части оборудования.&lt;/li&gt;
&lt;li&gt;Тысячекратный рост вычислительной мощности может быть достигнут с
    издержками в 33 раза меньшими, если воспользоваться толерантной к
    сбоям инфраструктурой, по сравнению с инфраструктурой, построенной
    на высоконадежных компонентах. Надежность строится поверх ненадежных
    компонентов.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;, домашнее размещение серверов, материнские платы
    предназначенные для персональных компьютеров, дешевые средства
    хранения данных.&lt;/li&gt;
&lt;li&gt;Цена за каждый ватт энергии в расчете на производительность не
    становится меньше, что ведет к большим проблемам связанным с
    энергообеспечением и охлаждением.&lt;/li&gt;
&lt;li&gt;Использование совместного размещения в своих и арендуемых
    датацентрах.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="raznoe"&gt;Разное&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Быстрый выпуск изменений более предпочтителен, чем ожидание.&lt;/li&gt;
&lt;li&gt;Библиотеки - превалирующий метод построения программ.&lt;/li&gt;
&lt;li&gt;Некоторые приложения предоставляются в виде сервисов.&lt;/li&gt;
&lt;li&gt;Инфраструктура управляет определением версий приложений таким
    образом, что они могут выпускать новые продукты, не боясь сломать
    работу какого-либо компонента системы.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="puti-razvitiia"&gt;Пути развития&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Поддержка географически распределенных кластеров.&lt;/li&gt;
&lt;li&gt;Создание единого глобального пространства имен для всех данных. На
    данный момент данные распределены по кластерам.&lt;/li&gt;
&lt;li&gt;Более автоматизированные передача и обработка данных&lt;/li&gt;
&lt;li&gt;Решение вопросов, связанных с поддержанием работоспособности
    сервисов даже в тех случаях, когда целый кластер отключается от
    системы в связи с техническими работами или каким-либо сбоем в
    работе.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="podvodim-itogi"&gt;Подводим итоги&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Инфраструктура может быть конкурентным преимуществом.&lt;/strong&gt; Это
    определенно так для Google. Они могут выпускать новые интернет
    сервисы быстрее, с меньшими издержками, на таком уровне, что мало
    кто сможет составить им конкуренцию. Подход многих компаний сильно
    отличается от подхода &lt;a href="/tag/google/"&gt;Google&lt;/a&gt;, эти компании
    рассматривают инфраструктуру как статью расходов, они обычно
    используют совсем другие технологии и совсем не задумываются о
    планировании и организации своей системы. Google позиционирует себя
    как компанию по построению систем, что является очень современным
    подходом к разработке программного обеспечения.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Охватывание нескольких дата центров до сих пор является нерешенной проблемой.&lt;/strong&gt; Большинство сайтов базируется в одном или двух дата
    центрах. Полное распределение сайта между несколькими датацентрами
    является хитрой задачей.&lt;/li&gt;
&lt;li&gt;Взгляните на &lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/30a7481/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/"&gt;Hadoop&lt;/a&gt;&lt;/em&gt;, если у Вас
    нет времени на собственноручное построение всей архитектуры с нуля.
    &lt;em&gt;Hadoop&lt;/em&gt; является opensource воплощением в жизнь многих идей здесь
    представленных.&lt;/li&gt;
&lt;li&gt;Часто недооцениваемым преимуществом платформенного подхода является
    тот факт, что даже неопытные разработчики могут быстро и качественно
    реализовывать трудоемкие приложения на базе платформы. Но если бы
    каждый проект требовал одинаково распределенной архитектуры, то это
    создало бы много проблем, так как люди, которые понимают как это
    делается, являются достаточно большой редкостью.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Совместная деятельность не всегда является таким уж плохим занятием.&lt;/strong&gt; Если все части системы работают взаимосвязанно, то
    улучшение в одной из них сразу и абсолютно прозрачно отразится
    положительным образом и на остальных компонентах системы. В
    противном случае такой эффект наблюдаться не будет.&lt;/li&gt;
&lt;li&gt;Построение самоуправляемых систем позволяет более легко
    перераспределять ресурсы между серверами, расширять систему,
    отключать некоторые компьютеры и элегантно проводить обновления.&lt;/li&gt;
&lt;li&gt;Производить длительные операции стоит параллельно.&lt;/li&gt;
&lt;li&gt;Всему, что было сделано Google, предшествовало искусство, а не
    только крупномасштабное развертывание системы.&lt;/li&gt;
&lt;li&gt;Учитывайте возможность &lt;strong&gt;компрессии данных&lt;/strong&gt;, она является очень
    неплохим решением, если остается лишнее процессорное время, но
    присутствует нехватка пропускной способности.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 31 Jan 2008 18:05:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-01-31:highload/2008/arkhitektura-google/</guid><category>BigTable</category><category>featured</category><category>GFS</category><category>Google</category><category>MapReduce</category><category>online</category><category>Sawzall</category><category>архитектура</category><category>архитектура Google</category><category>интернет</category><category>кластер</category><category>Масштабируемость</category><category>поиск</category><category>сервер</category><category>хранение данных</category></item></channel></rss>