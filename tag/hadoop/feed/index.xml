<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Insight IT</title><link>https://www.insight-it.ru/</link><description></description><atom:link href="https://www.insight-it.ru/tag/hadoop/feed/index.xml" rel="self"></atom:link><lastBuildDate>Wed, 15 Aug 2012 22:26:00 +0400</lastBuildDate><item><title>Архитектура Pinterest</title><link>https://www.insight-it.ru//highload/2012/arkhitektura-pinterest/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/c1302f36/" rel="nofollow" target="_blank" title="http://pinterest.com"&gt;Pinterest&lt;/a&gt; - по непонятным для меня причинам
популярная в определенных кругах социальная сеть, построенная вокруг
произвольных картинок чаще всего не собственного производства. Как и
&lt;a href="https://www.insight-it.ru/highload/2012/arkhitektura-instagram/"&gt;Instagram&lt;/a&gt;
проект довольно молодой, с очень похожей историей и стеком технологий.
Тем не менее, Pinterest определенно заслуживает внимания как один из
самых быстрорастущих по посещаемости вебсайтов за всю историю.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; &lt;a href="/tag/aws/"&gt;AWS&lt;/a&gt;&amp;nbsp;- хостинг и вспомогательные
    сервисы&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/nginx/"&gt;nginx&lt;/a&gt;&amp;nbsp;- вторичная балансировка нагрузки, отдача
    статики&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt;&amp;nbsp;- язык программирования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/django/"&gt;Django&lt;/a&gt;&amp;nbsp;- фреймворк&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&amp;nbsp;- основная СУБД&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;&amp;nbsp;- кэширование объектов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/redis/"&gt;Redis&lt;/a&gt;&amp;nbsp;- кэширование коллекций объектов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/solr/"&gt;Solr&lt;/a&gt;&amp;nbsp;- поиск&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt;&amp;nbsp;- анализ данных&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;3 миллиона уникальных посетителей в день&lt;/li&gt;
&lt;li&gt;18 миллионов уникальных посетителей в месяц&lt;/li&gt;
&lt;li&gt;4-я по популярности социальная сеть в США после
    &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt;,&amp;nbsp;&lt;a href="/tag/twitter/"&gt;Twitter&lt;/a&gt;&amp;nbsp;и
    &lt;a href="/tag/linkedin/"&gt;LinkedIn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Порядка 500 виртуальных машин в EC2&lt;/li&gt;
&lt;li&gt;80 миллионов объектов в S3&lt;/li&gt;
&lt;li&gt;410Тб пользовательских данных&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="razvitie"&gt;Развитие&lt;/h2&gt;
&lt;h4&gt;Март 2010&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;1 маленький виртуальный веб-сервер&lt;/li&gt;
&lt;li&gt;1 маленький виртуальный сервер MySQL&lt;/li&gt;
&lt;li&gt;Все это в Rackspace, 1 разработчик&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Январь 2011&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;1 сервер nginx для балансировки нагрузки, 4 веб-сервера&lt;/li&gt;
&lt;li&gt;2 сервера MySQL с master/slave репликацией&lt;/li&gt;
&lt;li&gt;3 сервера для отложенного выполнения задач&lt;/li&gt;
&lt;li&gt;1 сервер MongoDB&lt;/li&gt;
&lt;li&gt;Переехали на Amazon &lt;a href="/tag/ec2/"&gt;EC2&lt;/a&gt; + &lt;a href="/tag/s3/"&gt;S3&lt;/a&gt; +
    &lt;a href="/tag/cloudfront/"&gt;CloudFront&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Осень 2011&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;2 сервера nginx, 16 веб-серверов, 2 сервера для API&lt;/li&gt;
&lt;li&gt;5 функционально разделенных серверов MySQL с 9 read slave&lt;/li&gt;
&lt;li&gt;Кластер из 4 узлов Cassandra&lt;/li&gt;
&lt;li&gt;15 серверов Membase в 3 отдельных кластерах&lt;/li&gt;
&lt;li&gt;8 серверов memcached&lt;/li&gt;
&lt;li&gt;10 серверов Redis&lt;/li&gt;
&lt;li&gt;7 серверов для отложенной обработки задач&lt;/li&gt;
&lt;li&gt;4 сервера Elastic Search&lt;/li&gt;
&lt;li&gt;3 кластера MongoDB&lt;/li&gt;
&lt;li&gt;3 разработчика&lt;/li&gt;
&lt;li&gt;Если кто-то может объяснить зачем им сдался такой зоопарк, кроме как
    потестировать разные варианты, можете взять с полки пирожок.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Зима 2011-2012&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Заменили CloudFront на &lt;a href="/tag/akamai/"&gt;Akamai&lt;/a&gt; - вполне объяснимо,
    так как у Akamai намного лучше покрытие по миру, а
    качественный&amp;nbsp;&lt;a href="/tag/cdn/"&gt;CDN&lt;/a&gt; для сайта с большим количеством
    изображений - чуть ли не залог успеха.&lt;/li&gt;
&lt;li&gt;90 веб серверов и 50 серверов для API&lt;/li&gt;
&lt;li&gt;66 + 66 MySQL серверов на m1.xlarge инстансах EC2&lt;/li&gt;
&lt;li&gt;59 серверов Redis&lt;/li&gt;
&lt;li&gt;51 серверов memcached&lt;/li&gt;
&lt;li&gt;25+1 сервер для отложенной обработки задач на основе Redis&lt;/li&gt;
&lt;li&gt;Кластеризованный Solr&lt;/li&gt;
&lt;li&gt;6 разработчиков&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Весна-лето 2012&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Снова сменили CDN, на этот раз в пользу ранее неизвестного мне &lt;a href="/tag/edge-cast/"&gt;Edge
    Cast&lt;/a&gt;. Покрытие по всему миру довольно скромное,
    так что единственное логичное объяснение, которое мне приходит в
    голову - не потянули Akamai по деньгам.&lt;/li&gt;
&lt;li&gt;135 веб серверов и 75 серверов для API&lt;/li&gt;
&lt;li&gt;80 + 80 серверов MySQL&lt;/li&gt;
&lt;li&gt;110 серверов Redis&lt;/li&gt;
&lt;li&gt;60 серверов memcached&lt;/li&gt;
&lt;li&gt;60 + 2&amp;nbsp;сервера&amp;nbsp;для отложенной обработки задач на основе Redis&lt;/li&gt;
&lt;li&gt;25 разработчиков&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="vybor"&gt;Выбор&lt;/h2&gt;
&lt;h4&gt;Почему Amazon Ec2/S3?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Очень хорошая надежность, отчетность и поддержка&lt;/li&gt;
&lt;li&gt;Хорошие дополнительные сервисы: кэш, базы данных, балансировка
    нагрузки, &lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt; и т.п.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Новые виртуальные машины готовы за считанные секунды&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Почему MySQL?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Очень "зрелая", хорошо известная и любимая многими&lt;/li&gt;
&lt;li&gt;Редки катастрофичные потери данных&lt;/li&gt;
&lt;li&gt;Линейная зависимость времени отклика от частоты запросов&lt;/li&gt;
&lt;li&gt;Хорошая поддержка сторонним ПО (XtraBackup, Innotop, Maatkit)&lt;/li&gt;
&lt;li&gt;Надежное активное сообщество&lt;/li&gt;
&lt;li&gt;Отличная поддержка от Percona&lt;/li&gt;
&lt;li&gt;Бесплатна&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Почему memcached?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Очень "зрелый", отличная производительность, хорошо известный и
    любимый многими&lt;/li&gt;
&lt;li&gt;Никогда не ломается&lt;/li&gt;
&lt;li&gt;Бесплатен&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Почему Redis?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Много удобных &lt;strong&gt;структур данных&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Поддержка персистентности и репликации&lt;/li&gt;
&lt;li&gt;Также многим известен и нравится&lt;/li&gt;
&lt;li&gt;Стабильно хорошая производительность и надежность&lt;/li&gt;
&lt;li&gt;Также бесплатен&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;h4&gt;Сlustering vs Sharding&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Большую часть презентации, на основе которой написана данная статья
    (&lt;a href="https://www.insight-it.ru/goto/10005efe/" rel="nofollow" target="_blank" title="http://www.slideshare.net/eonarts/mysql-meetup-july2012scalingpinterest"&gt;ссылка&lt;/a&gt;,
    если не охота листать до секции источников информации), занимает
    раздел под названием "Clustering vs Sharding". В связи с путаницей в
    терминологии пришлось несколько раз перечитывать, чтобы понять к
    чему они клонят, сейчас попробую объяснить.&lt;/li&gt;
&lt;li&gt;Вообще есть два фундаментальных способа распределить данные между
    несколькими серверами:&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Вертикально:&lt;/strong&gt;&amp;nbsp;разные таблицы (или просто логически разные
    типы данных) разносятся на разные сервера.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Горизонтально:&lt;/strong&gt; каждая таблица разбивается на некоторое
    количество частей и эти части разносятся на разные сервера по
    определенному алгоритму.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;С первого взгляда казалось, что они пытаются вертикальное разбиение
    назвать &lt;em&gt;sharding&lt;/em&gt;, а горизонтальное - &lt;em&gt;clustering&lt;/em&gt;. Хотя вообще они
    почти синонимы и на русский я их обычно примерно одинаково перевожу.&lt;/li&gt;
&lt;li&gt;По факту же оказалось, что под словом clustering они понимают все
    программные продукты для хранения данных, которые имеют встроенную
    поддержку работы в кластере. В частности они имеют ввиду
    &lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt;, &lt;a href="/tag/membase/"&gt;Membase&lt;/a&gt;,
    &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt; и &lt;a href="/tag/riak/"&gt;Riak&lt;/a&gt;, которые прозрачно для
    пользователя горизонтально распределяют данные по кластеру.&lt;/li&gt;
&lt;li&gt;За словом &lt;em&gt;sharding&lt;/em&gt; в их терминологии стоит аналогичная схема
    собственной разработки, использующая огромное количество логических
    БД в MySQL, распределенных между меньшим количеством &lt;del&gt;физических серверов&lt;/del&gt; виртуальных машин. Именно по этому пути и пошли в
    Pinterest, плюс очень похожий подход используется в
    &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-facebook/"&gt;Facebook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;От себя добавлю, что хоть при наличии должных ресурсов разработка
    собственной системы распределения данных и может быть
    целесообразной, в большинстве случаев на начальном этапе проще
    основываться на готовых решениях вроде перечисленных выше. К слову в
    opensource доступны и основанные на MySQL подобные решения:&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/vitess/"&gt;Vitess&lt;/a&gt; от &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; /
    &lt;a href="/tag/youtube/"&gt;YouTube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/flockdb/"&gt;FlockDB&lt;/a&gt; от &lt;a href="/tag/twitter/"&gt;Twitter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;В их проекте данная подсистема развивалась следующим образом:&lt;ul&gt;
&lt;li&gt;1 БД + внешние ключи + join'ы&amp;nbsp;&amp;rarr;&lt;/li&gt;
&lt;li&gt;1 БД + денормализация + кэш&amp;nbsp;&amp;rarr;&lt;/li&gt;
&lt;li&gt;1 БД + master/slave + кэш&amp;nbsp;&amp;rarr;&lt;/li&gt;
&lt;li&gt;несколько функциональных разделенных БД + master/slave + кэш&amp;nbsp;&amp;rarr;&lt;/li&gt;
&lt;li&gt;вертикально и горизонтально разделенные БД (по
    идентификаторам) + по резервные БД (пассивный slave) + кэш&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;При использовании аналогичного решения остерегайтесь:&lt;ul&gt;
&lt;li&gt;Невозможности выполнять большинство запросов с join&lt;/li&gt;
&lt;li&gt;Отсутствия транзакций&lt;/li&gt;
&lt;li&gt;Дополнительных манипуляций для поддержания ограничений
    уникальности&lt;/li&gt;
&lt;li&gt;Необходимости тщательного планирования для изменений схемы&lt;/li&gt;
&lt;li&gt;Необходимости выполнения одного и того же запроса с последующей
    агрегацией для построения отчетов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Остальные моменты&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Кэширование многоуровневое:&lt;ul&gt;
&lt;li&gt;Коллекции объектов хранятся в списках Redis&lt;/li&gt;
&lt;li&gt;Сами объекты - в memcached&lt;/li&gt;
&lt;li&gt;На уровне SQL запросы в основном примитивны и написаны вручную,
    так что часты попадания в кэш MySQL&lt;/li&gt;
&lt;li&gt;Кэш файловой системы - само собой&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Еще пара фактов про кэширование в Pinterest:&lt;ul&gt;
&lt;li&gt;Кэш разбит также на несколько частей (шардов), для упрощения
    обслуживания и масштабирования&lt;/li&gt;
&lt;li&gt;В коде для кэширования используются Python'овские декораторы, на
    вид собственной разработки, хотя точно не уверен&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Балансировка нагрузки осуществляется в первую очередь за счет Amazon
    ELB, что позволяет легко подключать/отключать новые сервера
    посредством API.&lt;/li&gt;
&lt;li&gt;Так как большинство пользователей живут в США по ночам нагрузка
    сильно падает, что позволяет им по ночам отключать до 40%
    виртуальных машин. В пиковые часы EC2 обходится порядка 52$ в час,
    а по ночам - всего 15$.&lt;/li&gt;
&lt;li&gt;Elastic Map Reduce, основанный на Hadoop, используется для анализа
    данных и стоит всего несколько сотен долларов в месяц&lt;/li&gt;
&lt;li&gt;Текущие проблемы:&lt;ul&gt;
&lt;li&gt;Масштабирование команды&lt;/li&gt;
&lt;li&gt;Основанная на сервисах архитектура:&lt;ul&gt;
&lt;li&gt;Ограничения соединений&lt;/li&gt;
&lt;li&gt;Изоляция функционала&lt;/li&gt;
&lt;li&gt;Изоляция доступа (безопасность)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="uroki-ot-komandy-pinterest"&gt;Уроки от команды Pinterest&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;"Оно сломается. Все должно быть просто."&lt;/em&gt; - столько раз уже слышу
    это наставление, но ни разу не видел разработчиков, которые реально
    к нему прислушивались.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;"Кластеризация - страшная штука."&lt;/em&gt; - конечно страшная, большая и
    сложная. Но кому сейчас легко?&lt;/li&gt;
&lt;li&gt;&lt;em&gt;"Продолжайте получать удовольствие."&lt;/em&gt; - с этим не могу не
    согласиться, без удовольствия работать совершенно невозможно в любой
    сфере.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/10005efe/" rel="nofollow" target="_blank" title="http://www.slideshare.net/eonarts/mysql-meetup-july2012scalingpinterest"&gt;Scaling Pinterest @ MySQL Meetup&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;В презентации можно посмотреть примеры кода и SQL-запросов&lt;/li&gt;
&lt;li&gt;Если кто-то знает где можно посмотреть/послушать запись этого
    мероприятия - поделитесь ссылкой, пожалуйста&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/ac4a03d6/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2012/5/21/pinterest-architecture-update-18-million-visitors-10x-growth.html"&gt;Pinterest Architecture Update&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/db0647b0/" rel="nofollow" target="_blank" title="http://pinterest.com/about/careers/"&gt;Вакансии в Pinterest&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 15 Aug 2012 22:26:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-08-15:highload/2012/arkhitektura-pinterest/</guid><category>Akamai</category><category>Amazon</category><category>Apache Hadoop</category><category>AWS</category><category>CDN</category><category>CloudFront</category><category>django</category><category>EC2</category><category>Edge Cast</category><category>Hadoop</category><category>Memcached</category><category>MySQL</category><category>nginx</category><category>Pinterest</category><category>Python</category><category>Redis</category><category>S3</category><category>Solr</category><category>Архитектура Pinterest</category><category>архиткутера</category><category>Масштабируемость</category></item><item><title>Архитектура Tumblr</title><link>https://www.insight-it.ru//highload/2012/arkhitektura-tumblr/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/e90ae6ea/" rel="nofollow" target="_blank" title="http://www.tumblr.com"&gt;&lt;strong&gt;Tumblr&lt;/strong&gt;&lt;/a&gt; - одна из самых популярных в мире
платформ для блоггинга, которая делает ставку на привлекательный внешний
вид, юзабилити и дружелюбное сообщество. Хоть проект и не особо на слуху
в России, цифры говорят сами за себя: 24й по посещаемости сайт в США
с&amp;nbsp;15 миллиардами просмотров страниц в месяц. Хотите познакомиться с
историей этого проекта, выросшего из простого стартапа?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="vvedenie"&gt;Введение&lt;/h2&gt;
&lt;p&gt;Как и всем успешным стартапам, Tumblr удалось преодолеть опасную пропать
между начинающим проектом и широко известной компанией. Поиск правильных
людей, эволюция инфраструктуры, поддержка старых решений, паника по
поводу значительного роста посещаемости от месяца к месяцу, при этом в
команде только 4 технических специалиста - все это заставляло
руководство Tumblr принимать тяжелые решения о том над чем стоит
работать, а над чем - нет. Сейчас же технический персонал расширился до
20 человек и у них достаточно энергии для преодоления всех текущих
проблем и разработки новых интересных технических решений.&lt;/p&gt;
&lt;p&gt;Поначалу Tumblr был вполне типичным большим &lt;a href="/tag/lamp/"&gt;LAMP&lt;/a&gt;
приложением. Сейчас же они двигаются в направлении модели распределенных
сервисов, построенных вокруг существенно менее распространенных
технологий. Основные усилия сейчас вкладываются в постепенный уход от
&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; в пользу более "правильных" и "современных" решений,
оформленных в виде сервисов. Параллельно с переходом к новым технологиям
идут изменения и в команде проекта: от небольшой группы энтузиастов к
полноценной команде разработчиков, имеющей четкую структуру и сферы
ответственности, но тем не менее жаждущей реализовывать новый функционал
и обустраивать совершенно новую инфраструктуру проекта.&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/centos/"&gt;CentOS&lt;/a&gt; на серверах, &lt;a href="/tag/mac-os-x/"&gt;Mac OS X&lt;/a&gt; для
    разработки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; - основной веб-сервер&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;, &lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt;, &lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt; - языки
    программирования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/finagle/"&gt;Finagle&lt;/a&gt;&amp;nbsp;- асинхронный RPC сервер и клиент&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;, &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt;&amp;nbsp;- СУБД&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;,&amp;nbsp;&lt;a href="/tag/redis/"&gt;Redis&lt;/a&gt;&amp;nbsp;- кэширование&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/varnish/"&gt;Varnish&lt;/a&gt;, &lt;a href="/tag/nginx/"&gt;nginx&lt;/a&gt; - отдача статики&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/haproxy/"&gt;HAProxy&lt;/a&gt; - балансировка нагрузки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/kestrel/"&gt;kestrel&lt;/a&gt;, &lt;a href="/tag/gearman/"&gt;gearman&lt;/a&gt; - очередь задач&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt; - сериализация&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/kafka/"&gt;Kafka&lt;/a&gt; - распределенная шина сообщений&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; - обработка статистики&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/zookeeper/"&gt;ZooKeeper&lt;/a&gt; - хранение конфигурации и состояний
    системы&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/git/"&gt;git&lt;/a&gt; - система контроля версий&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/jenkins/"&gt;Jenkins&lt;/a&gt; - непрерывное тестирование&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Около 500 миллионов просмотров страниц в день&lt;/li&gt;
&lt;li&gt;Более 15 миллиардов просмотров страниц в месяц&lt;/li&gt;
&lt;li&gt;Посещаемость растет примерно на 30% в месяц&lt;/li&gt;
&lt;li&gt;Пиковые нагрузки порядка 40 тысяч запросов в секунду&lt;/li&gt;
&lt;li&gt;Около 20 технических специалистов в команде&lt;/li&gt;
&lt;li&gt;Каждый день создается около 50Гб новых постов и 2.7Тб обновлений списков
последователей&lt;/li&gt;
&lt;li&gt;Более 1Тб статистики обрабатывается в &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; ежедневно&lt;/li&gt;
&lt;li&gt;Используется порядка 1000 серверов:&lt;ul&gt;
&lt;li&gt;500 веб-серверов c Apache и PHP-приложением&lt;/li&gt;
&lt;li&gt;200 серверов баз данных (существенная их часть - резервные)&lt;ul&gt;
&lt;li&gt;47 пулов&lt;/li&gt;
&lt;li&gt;30 партиций (шардов)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;30 серверов &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;25 серверов Redis&lt;/li&gt;
&lt;li&gt;15 серверов Varnish&lt;/li&gt;
&lt;li&gt;25 серверов HAProxy&lt;/li&gt;
&lt;li&gt;8 серверов nginx&lt;/li&gt;
&lt;li&gt;14 серверов для очередей задач&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="tipichnoe-ispolzovanie"&gt;Типичное использование&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tumblr&lt;/strong&gt; используется несколько по-другому, чем другие социальные
сети:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;При более чем 50 миллионах постов в день, каждый из них попадает в
среднем к нескольким сотням читателей. Это и не несколько
пользователей с миллионами читателей (например, популярные личности
в Twitter) и не миллиарды личных сообщений.&lt;/li&gt;
&lt;li&gt;Ориентированность на длинные публичные сообщения, полные интересной
информацией и картинками/видео, заставляет пользователей проводить
долгие часы каждый день за чтением Tumblr.&lt;/li&gt;
&lt;li&gt;Большинство активных пользователей подписывается на сотни других
блоггеров, что практически гарантирует много страниц нового контента
при каждом заходе на сайт. В других социальных сетях поток новых
сообщений переполнен ненужным контентом и толком не читается.&lt;/li&gt;
&lt;li&gt;Как следствие, при сложившемся количестве пользователей, средней
аудиторией каждого и высокой активностью написания постов, системе
приходится обрабатывать и доставлять огромное количество информации.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Публичные блоги называют Tumblelog'ами, они не так динамичны и легко
кэшируются.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Сложнее всего масштабировать Dashboard, страницу, где пользователи в
реальном времени читают что нового у блоггеров, на которых они
подписаны:&lt;ul&gt;
&lt;li&gt;Кэширование практически бесполезно, так как для активных
пользователей запросы редко повторяются.&lt;/li&gt;
&lt;li&gt;Информация должна отображаться в реальном времени, быть целостной и
не "задерживаться".&lt;/li&gt;
&lt;li&gt;Около 70% просмотров страниц приходится именно на Dashboard, почти
все пользователи им пользуются.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="staraia-arkhitektura"&gt;Старая архитектура&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Когда проект только начинался, Tumblr размещался в Rackspace и последние выдавали каждому блогу с собственным доменом A-запись. Когда они переросли Rackspace, они не смогли полноценно мигрировать в новый
датацентр, в том числе из-за количества пользователей. Это было в 2007
году, но у них по-прежнему часть доменов ведут на Rackspace и
перенаправляются в новый датацентр с помощью HAProxy и Varnish. Подобных
"унаследованных" проблем у проекта очень много.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;С технической точки зрения проект прошел по пути типичной эволюции
&lt;strong&gt;LAMP&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Исторически разработан на &lt;strong&gt;PHP&lt;/strong&gt;, все началось с веб-сервера,
сервера баз данных и начало потихоньку развиваться.&lt;/li&gt;
&lt;li&gt;Чтобы справляться с нагрузкой они начали использовать memcache,
затем добавили кэширование целых страниц и статических файлов, потом
поставили HAProxy перед кэшами, после чего сделали партиционирование
на уровне &lt;strong&gt;MySQL&lt;/strong&gt;, что сильно облегчило им жизнь.&lt;/li&gt;
&lt;li&gt;Они делали все, чтобы выжать максимум из каждого сервера.&lt;/li&gt;
&lt;li&gt;Было разработано два сервиса на C: генератор уникальных
идентификаторов на основе HTTP и libevent, а также
&lt;a href="https://www.insight-it.ru/goto/533d5aae/" rel="nofollow" target="_blank" title="http://engineering.tumblr.com/post/7819252942/staircar-redis-powered-notifications"&gt;Staircar&lt;/a&gt;,
использующий Redis для обеспечения уведомлений в реальном времени на
Dashboard.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dashboard использует подход
&lt;a href="https://www.insight-it.ru/goto/f4d9020c/" rel="nofollow" target="_blank" title="http://www2.parc.com/istl/projects/ia/papers/sg-sigir92/sigir92.html"&gt;"разбрасывать-собирать"&lt;/a&gt;,
так как из-за отсортировонности данных по времени традиционные схемы
партиционирования работали не очень хорошо. По их прогнозам текущая
реализация позволит им рости еще в течении полугода.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="novaia-arkhitektura"&gt;Новая архитектура&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Приоритетным направлением стали технологии, основанные на JVM, по
причине более быстрой разработки и доступности квалифицированных
кадров. Мотивация несколько спорная, особенно если учесть, что речь
идет в первую очередь о &lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt;, а не
о&amp;nbsp;&lt;a href="/tag/scala/"&gt;Java&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Основная цель - вынести все из PHP приложения в отдельные сервисы, что
сделает его лишь тонким клиентом к внутреннему API.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Почему выбор пал именно на &lt;strong&gt;Scala&lt;/strong&gt; и &lt;strong&gt;Finagle&lt;/strong&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Многие разработчики имели опыт с Ruby и PHP, так что Scala был
привлекательным (цитата, логики мало)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/d7e3c54e/" rel="nofollow" target="_blank" title="https://github.com/twitter/finagle"&gt;Finagle&lt;/a&gt; был одним из основных
факторов в пользу JVM: это библиотека, разработанная в Twitter,
которая решает большинство распределенных задач вроде маршрутизации
запросов и обнаружение/регистрацию сервисов - не пришлось
реализовывать это все с нуля.&lt;/li&gt;
&lt;li&gt;В Scala не принято использовать общие состояния, что избавляет
разработчиков от забот с потоками выполнения и блокировками.&lt;/li&gt;
&lt;li&gt;Им очень нравится Thrift в роли программного интерфейса из-за его
высокой производительности (он кроссплатформенный и к JVM никак не
относится)&lt;/li&gt;
&lt;li&gt;Нравится &lt;a href="/tag/netty/"&gt;Netty&lt;/a&gt;, но не хочется связываться с Java, еще
один аргумент в пользу Scala.&lt;/li&gt;
&lt;li&gt;Рассматривали &lt;a href="/tag/node-js/"&gt;Node.js&lt;/a&gt;, но отказались так как под
JVM проще найти разработчиков, а также из-за отсутствия стандартов,
"лучших практик" и большого количества качественно протестированного
кода.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Старые внутренние сервисы также переписываются с C + libevent на Scala + Fingle.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Был создан общий каркас для построения внутренних сервисов:&lt;ul&gt;
&lt;li&gt;Много усилий было приложено для автоматизации управления
распределенной системой.&lt;/li&gt;
&lt;li&gt;Создан аналог скаффолдинга - используется некий шаблон для создания
каждого нового сервиса.&lt;/li&gt;
&lt;li&gt;Все сервисы выглядят одинаково с точки зрения системного
администратора: получение статистики, мониторинг, запуск и остановка
реализованы одинаково для всех сервисов.&lt;/li&gt;
&lt;li&gt;Созданы простые инструменты для сборки сервисов без вникания в
детали используемых стандартных решений.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Используется 6 внутренних сервисов, над которыми работает отдельная
команд. На запуск сервиса с нуля уходит около 2-3 недель.&lt;/li&gt;
&lt;li&gt;Новые, нереляционные СУБД, такие как HBase и Redis, вводятся в
эксплуатацию, но основным хранилищем по-прежнему остается сильно
партиционированный MySQL.&lt;/li&gt;
&lt;li&gt;HBase используется для сервиса сокращенных ссылок для постов, а также
всех исторических данных и аналитики. HBase хорошо справляется с
ситуациями, где необходимы миллионы операций записи в секунду, но он не
достаточно стабилен, чтобы полностью заменить проверенное временем
решение на MySQL в критичных для бизнеса задачах.&lt;/li&gt;
&lt;li&gt;Партиционированный MySQL плохо справляется с отсортированными по времени данными, так как один из серверов всегда оказывается существенно более "горячим", чем остальными. Также сталкивались с значительными задержками в репликации из-за большого количества параллельных операций добавления данных.&lt;/li&gt;
&lt;li&gt;Используется 25 серверов Redis с 8-32 процессами на каждом, что означает
порядка 300-400 экземпляров Redis в сумме.&lt;ul&gt;
&lt;li&gt;Используется для уведомлений в реальном времени на Dashboard (о
событиях вроде "кому-то понравился Ваш пост").&lt;/li&gt;
&lt;li&gt;Высокое соотношений операций записи к операциям чтения сделало MySQL
не очень подходящим кандидатом.&lt;/li&gt;
&lt;li&gt;Уведомления не так критичны, их потеря допустима, что позволило
отключить персистентность Redis.&lt;/li&gt;
&lt;li&gt;Был создан интерфейс между Redis и отложенными задачами в Finagle.&lt;/li&gt;
&lt;li&gt;Сервис коротких ссылок также использует Redis как кэш, а HBase для
постоянного хранения.&lt;/li&gt;
&lt;li&gt;Вторичный индекс Dashboard также построен вокруг Redis.&lt;/li&gt;
&lt;li&gt;Redis также используется для хранения задач Gearman, для чего был
написан memcache proxy на основе Finale.&lt;/li&gt;
&lt;li&gt;Постепенно отказываются от memcached в пользу Redis в роли основного
кэша. Производительность у них сопоставима.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Внутренним сервисам необходим доступ к потоку всех событий в системе
(создание, редактирование и удаление постов, нравится или не нравится и
т.п.), для чего была созданна внутренняя шина сообщений &lt;em&gt;(англ.
firehose, пожарный шланг)&lt;/em&gt;:&lt;ul&gt;
&lt;li&gt;Пробовали использовать в этой роли Scribe, но так как оно по сути
свелось к пропусканию логов через grep в реальном времени - нагрузки
оно не выдержало.&lt;/li&gt;
&lt;li&gt;Текущая реализация основана на Kafka, решению аналогичной задачи от
LinkedIn на Scala.&lt;/li&gt;
&lt;li&gt;MySQL также не рассматривался из-за большой доли операций записи.&lt;/li&gt;
&lt;li&gt;Внутри сервисы используют HTTP потоки для чтения данных, хотя Thrift
интерфейс также используется.&lt;/li&gt;
&lt;li&gt;Поток сообщений хранит события за последнюю неделю с возможностью
указать момент времени с которого считывать данные при открытии
соединения.&lt;/li&gt;
&lt;li&gt;Поддерживается абстракция "группы потребителей", которая позволяет
группе клиентов вместе обрабатывать один поток данных вместе и
независимо, то есть одно и то же сообщение не попадет дважды к
клиентам из одной группы.&lt;/li&gt;
&lt;li&gt;ZooKeeper используется для периодического сохранения текущей позиции
каждого клиента в потоке.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Новая архитектура Dashboard основана на принципе ячеек или ящиков
входящих сообщений:&lt;ul&gt;
&lt;li&gt;Каждая "ячейка" отвечает за группу пользователей и читает новые события
с шины сообщений, если один из её пользователей-подопечных подписан на
автора только что опубликованного поста, то пост добавляется в "почтовый
ящик" подписанного пользователя.&lt;/li&gt;
&lt;li&gt;Когда пользователь заходит в Dashboard его запрос попадает в его ячейку,
которая возвращает ему нужную часть непрочитанных постов.&lt;/li&gt;
&lt;li&gt;Каждая ячейка состоит из трех групп серверов:&lt;ul&gt;
&lt;li&gt;HBase для постоянного хранения копий постов и почтовых ящиков;&lt;/li&gt;
&lt;li&gt;Redis для кэширование свежих данных;&lt;/li&gt;
&lt;li&gt;Сервис, читающий данные из шины и предоставляющий доступ к ящикам
посредством Thrift.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;В HBase используется две таблицы:&lt;ul&gt;
&lt;li&gt;Отсортированный &lt;strong&gt;список идентификаторов постов&lt;/strong&gt; для каждого
пользователя в ячейке, именно в том виде, как они будут отображены в
итоге.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Копии всех постов&lt;/strong&gt; по идентификаторам, что позволяет выдать все
данные для отрисовки Dashboard без обращений к серверам вне одной
ячейки.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ячейки представляют собой независимые единицы, что позволяет легко
масштабировать систему при росте числа пользователей.&lt;/li&gt;
&lt;li&gt;Платой за относительно безболезненность масштабирования является
чрезвычайная избыточность данных: при том что ежедневно создается лишь
50Гб постов, суммарный объем данных в ячейках растет на 2.7Тб в день.&lt;/li&gt;
&lt;li&gt;Альтернативой было бы использование общего кластера со всеми постами, но
тогда он бы стал единственной точкой отказа и потребовалось бы делать
дополнительные удаленные запросы. Помимо этого выигрыш по объему был бы
не велик - списки идентификаторов занимают значительно больше места, чем
сами посты.&lt;/li&gt;
&lt;li&gt;Пользователи, которые подписаны или на которых подписаны миллионы других
пользователей, обрабатываются отдельно - страницы с их постами
генерируются не заранее (как описывалось выше), а при поступлении
запроса - это позволяет не тратить впустую много ресурсов (этот подход
называется выборочная материализация,
&lt;a href="https://www.insight-it.ru/goto/ab0d48a1/" rel="nofollow" target="_blank" title="http://research.yahoo.com/pub/3203"&gt;подробнее&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Количество пользователей в одной ячейке позволяет управлять балансом
между уровнем надежности и стоимостью содержания этой подсистемы.&lt;/li&gt;
&lt;li&gt;Параллельное чтение их шины сообщений оказывает серьезную нагрузку на
сеть, в дальнейшем из ячеек можно будет составить иерархию: только часть
будет читать напрямую из шины сообщений, а остальным сообщения будут
ретранслироваться.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tumblr географически по-прежнему находится в одном датацентре (если не
считать незначительное присутствие в Rackspace), распределение по
нескольким лишь в планах.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="razvertyvanie"&gt;Развертывание&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Начиналось как несколько rsync-скриптов для распространения
    PHP-приложения. Как только машин стало больше 200 такой подход стал
    занимать слишком много времени.&lt;/li&gt;
&lt;li&gt;Следующий вариант был основан на &lt;a href="/tag/capistrano/"&gt;Capistrano&lt;/a&gt;:
    были созданы три стадии процесса развертывания (разработка,
    тестирование, боевой). Неплохо справлялся с десятками серверов, но
    на сотнях также был слишком медленным, так как основывался на SSH.&lt;/li&gt;
&lt;li&gt;Итоговый вариант основан на &lt;strong&gt;Func&lt;/strong&gt;, решении от
    &lt;a href="/tag/redhat/"&gt;RedHat&lt;/a&gt;, позволившим заменить &lt;a href="/tag/ssh/"&gt;SSH&lt;/a&gt; на
    более легковесный протокол.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="razrabotka"&gt;Разработка&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Поначалу философия была такова, что каждый мог использовать любые
технологии, которые считал уместным. Но довольно скоро пришлось
стандартизировать стек технологий, чтобы было легче нанимать и вводить в
работу новых сотрудников, а также для более оперативного решения
технических проблем.&lt;/li&gt;
&lt;li&gt;Каждый разработчик имеет одинаковую заранее настроенную рабочую станцию,
которая обновляется посредством &lt;a href="/tag/puppet/"&gt;Puppet&lt;/a&gt;:&lt;ul&gt;
&lt;li&gt;Настроена публикация изменений, тестирование и развертывание новых
версий.&lt;/li&gt;
&lt;li&gt;Разработчики используют vim и Textmate.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Новый PHP код систематически инспектируется другими разработчиками.&lt;/li&gt;
&lt;li&gt;Внутренние сервисы подвергаются непрерывному тестированию посредством
Jenkins.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="struktura-komand"&gt;Структура команд&lt;/h2&gt;
&lt;p&gt;Проект разбит на 6 команд:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Инфраструктура:&lt;/strong&gt;&amp;nbsp;все, что ниже 5 уровня по модели OSI -
    маршрутизация, TCP/IP, DNS, оборудование и.т.п.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Платформа:&lt;/strong&gt;&amp;nbsp;разработка основного приложения, партиционирование
    SQL, взаимодействие сервисов.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Надежность (SRE):&lt;/strong&gt;&amp;nbsp;сфокусирована на текущие потребности с точки
    зрения надежности и масштабируемости.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Сервисы:&lt;/strong&gt;&amp;nbsp;занимается более стратегической разработкой того, что
    понадобится через один-два месяца.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Эксплуатация:&lt;/strong&gt;&amp;nbsp;отвечает за обнаружение и реагирование на
    проблемы, плюс тонкая настройка.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="naim"&gt;Найм&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;На интервью они обычно избегают математики и головоломок, основной
    упор идет в основном именно на те вещи, которым придется заниматься
    кандидату.&lt;/li&gt;
&lt;li&gt;Основной вопрос: будет ли он успешно решать поставленные задачи?
    Цель в том, чтобы найти отличных людей, а не в том, чтобы никого не
    брать.&lt;/li&gt;
&lt;li&gt;Разработчиков обязательно просят привести пример своего кода, даже
    во время телефонных интервью.&lt;/li&gt;
&lt;li&gt;Во время интервью кандидатов не ограничивают в наборе инструментов,
    можно даже гуглить.&lt;/li&gt;
&lt;li&gt;Поиск людей с опытом в крупных проектах достаточно сложен, так как
    всего нескольких компаниях по всему миру решают подобные проблемы.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Автоматизация - ключ к успеху крупного проекта.&lt;/li&gt;
&lt;li&gt;При партиционировании MySQL может масштабироваться, но лишь при
    преобладании операций чтения.&lt;/li&gt;
&lt;li&gt;Redis с отключенной персистентностью легко может заменить memcached.&lt;/li&gt;
&lt;li&gt;Scala достойно себя проявляет в роли языка программирования для
    внутренних сервисов, во многом благодаря обширной Java-экосистеме.&lt;/li&gt;
&lt;li&gt;Внедряйте новые технологии постепенно, поначалу работать с HBase и
    Redis было очень болезненно, они были включены в основной стек
    технологий только после испытаний в некритичных сервисах и
    подпроектах, где цена ошибки не так велика.&lt;/li&gt;
&lt;li&gt;Проект должен строиться вокруг навыков его команды, а не наоборот.&lt;/li&gt;
&lt;li&gt;Нужно нанимать людей только если они вписываются в команду и в
    состоянии довести работу до результата.&lt;/li&gt;
&lt;li&gt;При выборе технологического стека одну из ключевых ролей играет
    доступность соответствующих специалистов на кадровом рынке.&lt;/li&gt;
&lt;li&gt;Читайте публикации и статьи в блогах. Ключевые аспекты архитектуры,
    включая "ячейки" и частичную материализацию были позаимствованы из
    внешних источников.&lt;/li&gt;
&lt;li&gt;Поспрашивайте своих коллег, кто-то из них мог общаться с
    специалистами из
    &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-facebook/"&gt;Facebook&lt;/a&gt;,
    &lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-twitter-dva-goda-spustya/"&gt;Twitter&lt;/a&gt;,
    &lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-google-2011/"&gt;Google&lt;/a&gt;
    или
    &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-linkedin/"&gt;LinkedIn&lt;/a&gt; -
    если нет прямого доступа, всегда можно получить нужную информацию
    через одно-два "рукопожатия".&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Статья написана на основе &lt;a href="https://www.insight-it.ru/goto/1444dc9b/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2012/2/13/tumblr-architecture-15-billion-page-views-a-month-and-harder.html"&gt;интервью&lt;/a&gt;&amp;ensp;&lt;a href="https://www.insight-it.ru/goto/d7daa0cd/" rel="nofollow" target="_blank" title="http://www.linkedin.com/in/bmatheny"&gt;Blake Matheny&lt;/a&gt;, директора по разработке платформы Tumblr.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 21 Feb 2012 16:29:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-02-21:highload/2012/arkhitektura-tumblr/</guid><category>Apache</category><category>Capistrano</category><category>CentOS</category><category>Finagle</category><category>Func</category><category>gearman</category><category>Git</category><category>Hadoop</category><category>HAProxy</category><category>HBase</category><category>jenkins</category><category>kafka</category><category>Kestrel</category><category>LAMP</category><category>Mac OS X</category><category>Memcached</category><category>MySQL</category><category>nginx</category><category>PHP</category><category>puppet</category><category>Redis</category><category>Ruby</category><category>Scala</category><category>Thrift</category><category>Tumblr</category><category>Varnish</category><category>ZooKeeper</category></item><item><title>Аналитика в реальном времени от Facebook</title><link>https://www.insight-it.ru//highload/2011/analitika-v-realnom-vremeni-ot-facebook/</link><description>&lt;p&gt;&lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt; в &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt; завоевывает все более
и более крепкие позиции, в прошлый раз я рассказывал о применении HBase
в роли системы хранения данных для их новой системы обмена сообщений.
Вторым продуктом, который теперь полноценно использует данную
технологию, является система сбора и обработки статистики в реальном
времени под названием Insights. Социальные кнопки (см. слева от поста)
стали одним из основных источников трафика для многих сайтов, новая
система аналитики позволит владельцам сайтов и страниц лучше понимать
как пользователи взаимодействуют и оптимизировать свои интернет-ресурсы,
основываясь на данных в реальном времени. Итак, 20 миллиардов событий в
день (200 тысяч в секунду) с задержкой не более 30 секунд, как же можно
этого достичь?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="tseli-servisa"&gt;Цели сервиса&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Дать пользователям надежные счетчики в реальном времени по ряду
    метрик&lt;/li&gt;
&lt;li&gt;Предоставлять анонимные данных - нельзя узнать кто конкретно были
    эти люди&lt;/li&gt;
&lt;li&gt;Продемонстрировать ценность социальных плагинов и виджетов. Что они
    дают сайту или бизнесу?&lt;/li&gt;
&lt;li&gt;Концепция воронки: сколько людей увидело плагин, сколько совершило
    действие, сколько было привлечено пользователей обратно на
    интернет-ресурс&lt;/li&gt;
&lt;li&gt;Сделать данные более оперативными: сокращение частоты обновлений с
    48 часов до 30 секунд&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="zadachi"&gt;Задачи&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Множество типов метрик, более 100: показы, лайки, просмотры и клики
    в новостной ленте, демография и.т.д.&lt;/li&gt;
&lt;li&gt;Большой поток данных: 20 миллиардов событий в день&lt;/li&gt;
&lt;li&gt;Неравномерное распределение данных: за большинство контента
    практически не голосуют, но некоторые материалы набирают просто
    невероятное количество лайков&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="neudavshiesia-popytki"&gt;Неудавшиеся попытки&lt;/h2&gt;
&lt;h2 id="mysql"&gt;MySQL&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;В каждой строке идентификатор и значение счетчика&lt;/li&gt;
&lt;li&gt;Привело к очень высокой активности в СУБД&lt;/li&gt;
&lt;li&gt;Статистика считается за каждые сутки, создание новых счетчиков в
    полночь приводило к большому скачку операций записи&lt;/li&gt;
&lt;li&gt;Пришлось пробовать искать способы решать проблему распределения
    счетчиков, пробовали учитывать временные зоны пользователей&lt;/li&gt;
&lt;li&gt;Пики операций записи неминуемо вели к блокировкам&lt;/li&gt;
&lt;li&gt;Решение оказалось не очень хорошо подходящим для данной конкретной
    задачи&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="schetchiki-v-pamiati"&gt;Счетчики в памяти&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Казалось бы: если столкнулись с проблемами ввода-вывода - надо
    перенести все в память&lt;/li&gt;
&lt;li&gt;Никаких проблем с масштабируемостью - счетчики находятся в памяти,
    обновление практически мгновенно, легко распределить по серверам&lt;/li&gt;
&lt;li&gt;Но на практике оказалось, что при таком подходе теряется точность,
    видимо из-за неатомарности операций или других последствий столь
    прямолинейной реализации, подробностей нет&lt;/li&gt;
&lt;li&gt;Погрешность даже в 1% посчитали неприемлемой и от данного варианта
    отказались&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="mapreduce"&gt;MapReduce&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Предыдущий вариант реализации был создан с помощью
    &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; + &lt;a href="/tag/hive/"&gt;Hive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Подход гибкий, легко справляется с большим входящим потоком
    информации и объемами данным&lt;/li&gt;
&lt;li&gt;Основной минус: даже близко не в реальном времени, слишком
    комплексная система&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="cassandra"&gt;Cassandra&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Вариант с &lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt; рассматривался, но так и не
    был реализован&lt;/li&gt;
&lt;li&gt;Причины были опубликованы достаточно сомнительные: высокие
    требования к доступности данных и производительности записи&lt;/li&gt;
&lt;li&gt;По всем данным у Cassandra нет абсолютно никаких проблем ни с одним,
    ни с другим&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="pobeditel-hbase-scribe-ptail-puma_1"&gt;Победитель: HBase + Scribe + Ptail + Puma&lt;/h1&gt;
&lt;p&gt;В целом система, на которой остановился выбор, выглядит следующим
образом:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HBase хранит все данные на распределенном кластере&lt;/li&gt;
&lt;li&gt;Используется система логов, новые данные дописываются в конец&lt;/li&gt;
&lt;li&gt;Система обрабатывает события и записывает результат в хранилище&lt;/li&gt;
&lt;li&gt;Пользовательский интерфейс забирает данные из хранилища и отображает
    пользователям&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Как обрабатывается один запрос:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Пользователь жмет на кнопку Like&lt;/li&gt;
&lt;li&gt;Браузер инициирует AJAX запрос к серверу Facebook&lt;/li&gt;
&lt;li&gt;Запрос записывается в лог в HDFS с помощью Scribe&lt;/li&gt;
&lt;li&gt;Ptail - внутренний инструмент для чтения данных из нескольких
    Scribe-логов, на выходе данные разделяются на три потока, которые
    отправляются в разные кластеры в разных датацентрах:&lt;ul&gt;
&lt;li&gt;Просмотры плагинов и виджетов&lt;/li&gt;
&lt;li&gt;Просмотры в новостной ленте&lt;/li&gt;
&lt;li&gt;Действия (плагины + новостная лента)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Puma - механизм для пакетной записи данных в HBase для снижения
    влияния "горячих" материалов:&lt;ul&gt;
&lt;li&gt;HBase может справиться с очень большим потоком операций записи,
    но популярные материалы могут заставить упереться во ввод-вывод
    даже её&lt;/li&gt;
&lt;li&gt;В среднем пакет запросов собирается в течении 1.5 секунд,
    хотелось бы больше - но из-за огромного количества URL очень
    быстро заканчивается оперативная память&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Отображение данных пользователю:&lt;ul&gt;
&lt;li&gt;Сам код для отображения написан на &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Работа с HBase осуществляется из &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Для взаимодействия по традиции используется
    &lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Система кэширования используется для ускорения отображения страниц:&lt;ul&gt;
&lt;li&gt;Чем более старые данные запрашиваются, тем реже они
    пересчитываются&lt;/li&gt;
&lt;li&gt;Многое зависит от типа запрашиваемой статистики&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MapReduce:&lt;ul&gt;
&lt;li&gt;Данные передаются для дальнейшего анализа с помощью Hive&lt;/li&gt;
&lt;li&gt;Сами логи удаляются через какой-то период времени&lt;/li&gt;
&lt;li&gt;Помимо этого старая система анализа статистики все еще в
    действии, она используется для регулярных проверок результатов
    новой системы, а также в роли запасного плана, если что-то
    пойдет не так&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="o-proekte"&gt;О проекте&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Продолжительность 5 месяцев&lt;/li&gt;
&lt;li&gt;2 с половиной разработчика самой системы&lt;/li&gt;
&lt;li&gt;2 разработчика пользовательского интерфейса&lt;/li&gt;
&lt;li&gt;Всего было задействовано около 14 человек, включая менеджмент,
    дизайнера и системных администраторов&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="napravleniia-razvitiia"&gt;Направления развития&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Списки популярных материалов: при текущем подходе их составление
    является сложной задачей&lt;/li&gt;
&lt;li&gt;Отдельные пользовательские счетчики&lt;/li&gt;
&lt;li&gt;Обобщение приложения для использования с другими сервисами, а не
    только с социальными плагинами&lt;/li&gt;
&lt;li&gt;Распределение системы на несколько датацентров&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;p&gt;У новых систем аналитики и сообщений много общего: большой поток
входящих данных для записи, HBase и требование работы в реальном
времени. Facebook делает ставку на HBase, Hadoop и HDFS, не смотря на
громоздкость системы, когда другие предпочитают Cassandra из-за её
простой схемы масштабирования, поддержку нескольких датацентров и
легкость в использовании. Какой путь окажется выигрышным - покажет
время.&lt;/p&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/c847fcc5/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2011/3/22/facebooks-new-realtime-analytics-system-hbase-to-process-20.html"&gt;Facebook's New Realtime Analytics System: HBase To Process 20 Billion Events Per Day&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/618b7573/" rel="nofollow" target="_blank" title="http://www.facebook.com/note.php?note_id=10150103900258920"&gt;Building Realtime Insights&lt;/a&gt; (&lt;a href="https://www.insight-it.ru/goto/6abcef48/" rel="nofollow" target="_blank" title="http://www.facebook.com/video/video.php?v=707216889765&amp;amp;oid=9445547199&amp;amp;comments"&gt;видео&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 24 Mar 2011 19:14:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-03-24:highload/2011/analitika-v-realnom-vremeni-ot-facebook/</guid><category>Facebook</category><category>Hadoop</category><category>HBase</category><category>Insights</category><category>Ptail</category><category>Puma</category><category>Scribe</category></item><item><title>Архитектура Twitter. Два года спустя.</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-twitter-dva-goda-spustya/</link><description>&lt;p&gt;В далеком 2008м я уже публиковал статью про &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-twitter/"&gt;архитектуру Twitter&lt;/a&gt;, но время летит
стремительно и она уже абсолютно устарела. За это время аудитория
Twitter росла просто фантастическими темпами и многое поменялось и с
технической точки зрения. Интересно что новенького у одного из самых
популярных социальных интернет-проектов?&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;3 год, 2 месяца и 1 день потребовалось Twitter, чтобы набрать 1
    миллиард твитов&lt;/li&gt;
&lt;li&gt;На сегодняшний день, чтобы отправить миллиард твитов пользователям
    нужна всего одна неделя&lt;/li&gt;
&lt;li&gt;752% рост аудитории за 2008 год&lt;/li&gt;
&lt;li&gt;1358% рост аудитории за 2009 год&amp;nbsp;(без учета API, по данным comScore)&lt;/li&gt;
&lt;li&gt;175 миллионов зарегистрированных пользователей на сентябрь 2010 года&lt;/li&gt;
&lt;li&gt;460 тысяч регистраций пользователей в день&lt;/li&gt;
&lt;li&gt;9й сайт в мире по популярности (по данным Alexa, год назад был на 12
    месте)&lt;/li&gt;
&lt;li&gt;50 миллионов твитов в день год назад, 140 миллионов твитов в день
    месяц назад, 177 миллионов твитов в день на 11 марта 2011г.&lt;/li&gt;
&lt;li&gt;Рекорд по количеству твитов за секунду 6939, установлен через минуту
    после того, как Новый Год 2011 наступил в Японии&lt;/li&gt;
&lt;li&gt;600 миллионов поисков в день&lt;/li&gt;
&lt;li&gt;Лишь 25% трафика приходится на веб сайт, остальное идет через API&lt;/li&gt;
&lt;li&gt;Росто числа мобильных пользователей за последний год 182%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;6 миллиардов&lt;/strong&gt; запросов к API в день, около 70 тысяч в секунду&lt;/li&gt;
&lt;li&gt;8, 29, 130, 350, 400 - это количество сотрудников Twitter на январь
    2008, январь 2009, январь 2010, январь и март 2011, соответственно&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Самая свежая &lt;a href="https://www.insight-it.ru/goto/682783c0/" rel="nofollow" target="_blank" title="http://blog.twitter.com/2011/03/numbers.html"&gt;статистика про Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; + &lt;code&gt;mod_proxy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/unicorn/"&gt;Unicorn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt; +&amp;nbsp;&lt;a href="/tag/ror/"&gt;Ruby on Rails&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/flock/"&gt;Flock&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/kestrel/"&gt;Kestrel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/scribe/"&gt;Scribe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt;, &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt; и &lt;a href="/tag/pig/"&gt;Pig&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Сравните с аналогичным разделом предыдущей статьи о Twitter - увидите
много новых лиц, подробнее ниже.&lt;/p&gt;
&lt;h2 id="oborudovanie"&gt;Оборудование&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Сервера расположены в NTT America&lt;/li&gt;
&lt;li&gt;Никаких облаков и виртуализации, существующие решения страдают
    слишком высокими задержками&lt;/li&gt;
&lt;li&gt;Более тысячи серверов&lt;/li&gt;
&lt;li&gt;Планируется переезд в собственный датацентр&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="chto-takoe-tvit"&gt;Что такое твит?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Сообщение длиной до 140 символов + метаданные&lt;/li&gt;
&lt;li&gt;Типичные запросы:&lt;ul&gt;
&lt;li&gt;по идентификатору&lt;/li&gt;
&lt;li&gt;по автору&lt;/li&gt;
&lt;li&gt;по @упоминаниям пользователей&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Процесс обработки запроса в Twitter" class="responsive-img" src="https://www.insight-it.ru/images/twitter-request-flow.jpeg" title="Процесс обработки запроса в Twitter"/&gt;&lt;/p&gt;
&lt;h3 id="unicorn"&gt;Unicorn&lt;/h3&gt;
&lt;p&gt;Сервер приложений для Rails:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Развертывание новых версий кода &lt;strong&gt;без простоя&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;На 30% меньше расход вычислительных ресурсов и оперативной памяти,
    по сравнению с другими решениями&lt;/li&gt;
&lt;li&gt;Перешли с &lt;code&gt;mod_proxy_balancer&lt;/code&gt; на &lt;code&gt;mod_proxy_pass&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="rails"&gt;Rails&lt;/h3&gt;
&lt;p&gt;Используется в основном для генерации страниц, работа за сценой
реализована на чистом Ruby или Scala.&lt;/p&gt;
&lt;p&gt;Столкнулись со следующими проблемами:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Проблемы с кэшированием, особенно по части инвалидации&lt;/li&gt;
&lt;li&gt;ActiveRecord генерирует не самые удачные SQL-запросы, что замедляло
    время отклика&lt;/li&gt;
&lt;li&gt;Высокие задержки в очереди и при репликации&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="memcached"&gt;memcached&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;memcached не идеален. Twitter начал сталкиваться с Segmentation
    Fault в нем очень рано.&lt;/li&gt;
&lt;li&gt;Большинство стратегий кэширования основываются на длинных TTL
    (более минуты).&lt;/li&gt;
&lt;li&gt;Вытеснение данных делает его непригодным для важных конфигурационных
    данных (например флагов "темного режима", о котором пойдет речь
    ниже).&lt;/li&gt;
&lt;li&gt;Разбивается на несколько пулов для улучшения производительности и
    снижения риска вытеснения.&lt;/li&gt;
&lt;li&gt;Оптимизированная библиотека для доступа к memcached из Ruby на
    основе libmemcached + FNV hash, вместо чистого Ruby и md5.&lt;/li&gt;
&lt;li&gt;Twitter является одним их наиболее активных проектов, участвующих в
    разработке libmemcached.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="mysql"&gt;MySQL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Разбиение больших объемов данных является тяжелой задачей.&lt;/li&gt;
&lt;li&gt;Задержки в репликации и вытеснение данных из кэша является причиной
    нарушения целостности данных с точки зрения конечного пользователя.&lt;/li&gt;
&lt;li&gt;Блокировки создают борьбу за ресурсы для популярных данных.&lt;/li&gt;
&lt;li&gt;Репликация однопоточна и происходит недостаточно быстро.&lt;/li&gt;
&lt;li&gt;Данные социальных сетей плохо подходят для реляционных СУБД:&lt;ul&gt;
&lt;li&gt;NxN отношения, социальный граф и обход деревьев - не самые
    подходящие задачи для таких баз данных&lt;/li&gt;
&lt;li&gt;Проблемы с дисковой подсистемой (выбор файловой системы,
    noatime, алгоритм планирования)&lt;/li&gt;
&lt;li&gt;ACID практически не требуется&lt;/li&gt;
&lt;li&gt;Для очередей также практически непригодны&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Twitter сталкивался с большими проблемами касательно таблиц
    пользователей и их статусов&lt;/li&gt;
&lt;li&gt;Читать данные с мастера при Master/Slave репликации = медленная
    смерть&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="flockdb"&gt;FlockDB&lt;/h3&gt;
&lt;p&gt;Масштабируемое хранилище для данных социального графа:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Разбиение данных через Gizzard&lt;/li&gt;
&lt;li&gt;Множество серверов MySQL в качестве низлежащей системы хранения&lt;/li&gt;
&lt;li&gt;В Twitter содержит 13 миллиардов ребер графа и обеспечивает 20 тысяч
    операций записи и 100 тысяч операций чтения в секунду&lt;/li&gt;
&lt;li&gt;Грани хранятся и индексируются в обоих направлениях&lt;/li&gt;
&lt;li&gt;Поддерживает распределенный подсчет количества строк&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/4fe0530b/" rel="nofollow" target="_blank" title="https://github.com/twitter/flockdb"&gt;Open source!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Среднее время на выполнение операций:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Подсчет количества строк: 1мс&lt;/li&gt;
&lt;li&gt;Временные запросы: 2мс&lt;/li&gt;
&lt;li&gt;Запись: 1мс для журнала, 16мс для надежной записи&lt;/li&gt;
&lt;li&gt;Обход дерева: 100 граней/мс&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Подробнее про эволюцию систем хранения данных в Twitter &lt;a href="https://www.insight-it.ru/goto/32077a90/" rel="nofollow" target="_blank" title="http://www.slideshare.net/nkallen/q-con-3770885"&gt;в презентации
Nick Kallen&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="cassandra"&gt;Cassandra&lt;/h3&gt;
&lt;p&gt;Распределенная система хранения данных, ориентированная на работу в
реальном времени:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Изначально разработана в &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Очень высокая производительность на запись&lt;/li&gt;
&lt;li&gt;Из слабых сторон: высокая задержка при случайном доступе&lt;/li&gt;
&lt;li&gt;Децентрализованная, способна переносить сбои оборудования&lt;/li&gt;
&lt;li&gt;Гибкая схема данных&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Планируется полный переход на нее по
    следующему алгоритму:&lt;/del&gt;&lt;ul&gt;
&lt;li&gt;&lt;del&gt;Все твиты пишутся и в Cassandra
    и в MySQL&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Динамически часть операций
    чтения переводится на Cassandra&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Анализируется реакция системы,
    что сломалось&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Полностью отключаем чтение из
    Cassandra, чиним неисправности&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Начинаем сначала&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/e83e4e8e/" rel="nofollow" target="_blank" title="http://engineering.twitter.com/2010/07/cassandra-at-twitter-today.html"&gt;Обновление:&lt;/a&gt;&lt;/strong&gt; стратегия по поводу использования Cassandra изменилась, попытки
    использовать её в роли основного хранилища для твитов прекратились,
    но она продолжает использоваться для аналитики и географической
    информации.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Подробнее почему Twitter пришел к решению использовать Cassandra можно
прочитать &lt;a href="https://www.insight-it.ru/goto/ffc31d1/" rel="nofollow" target="_blank" title="http://www.slideshare.net/ryansking/scaling-twitter-with-cassandra"&gt;в отдельной презентации&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Помимо всего прочего Cassandra&amp;nbsp;&lt;del&gt;планируется использовать&lt;/del&gt; используется для аналитики в реальном времени.&lt;/p&gt;
&lt;h3 id="scribe"&gt;Scribe&lt;/h3&gt;
&lt;p&gt;Пользователи Twitter генерируют огромное количество данных, около 15-25
Гб в минуту, более 12 Тб в день, и эта цифра удваивается несколько раз
в год.&lt;/p&gt;
&lt;p&gt;Изначально для сбора логов использовали &lt;code&gt;syslog-ng&lt;/code&gt;, но он очень быстро
перестал справляться с нагрузкой.&lt;/p&gt;
&lt;p&gt;Решение нашлось очень просто: &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt; столкнулся с
аналогичной проблемой и разработал проект Scribe, который был
опубликован в opensource.&lt;/p&gt;
&lt;p&gt;По сути это фреймворк для сбора и агрегации логов, основанный на
&lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt;. Вы пишете текст для логов и указываете
категорию, остальное он берет на себя.&lt;/p&gt;
&lt;p&gt;Работает локально, надежен даже в случае потери сетевого соединения,
каждый узел знает только на какой сервер передавать логи, что позволяет
создавать масштабируемую иерархию для сбора логов.&lt;/p&gt;
&lt;p&gt;Поддерживаются различные системы для записи в данным, &amp;nbsp;в том числе
обычные файлы и HDFS (о ней ниже).&lt;/p&gt;
&lt;p&gt;Этот продукт полностью решил проблему Twitter со сбором логов,
используется около 30 различных категорий. В процессе использования была
создана и опубликована масса доработок. Активно сотрудничают с командой
Facebook в развитии проекта.&lt;/p&gt;
&lt;h3 id="hadoop"&gt;Hadoop&lt;/h3&gt;
&lt;p&gt;Как Вы обычно сохраняете 12Тб новых данных, поступающих каждый день?&lt;/p&gt;
&lt;p&gt;Если считать, что средняя скорость записи современного жесткого диска
составляет 80Мбайт в секунду, запись 12Тб данных заняла бы почти 48
часов.&lt;/p&gt;
&lt;p&gt;На одном даже очень большом сервере данную задачу не решить, логичным
решением задачи стало использование кластера для хранения и анализа
таких объемов данных.&lt;/p&gt;
&lt;p&gt;Использование кластерной файловой системы добавляет сложности, но
позволяет меньше заботиться о деталях.&lt;/p&gt;
&lt;p&gt;Hadoop Distributed File System (HDFS) предоставляет возможность
автоматической репликации и помогает справляться со сбоями оборудования.&lt;/p&gt;
&lt;p&gt;MapReduce framework позволяет обрабатывать огромные объемы данных,
анализируя пары ключ-значение.&lt;/p&gt;
&lt;p&gt;Типичные вычислительные задачи, которые решаются с помощью Hadoop в
Twitter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Вычисление связей дружбы в социальном графе (&lt;code&gt;grep&lt;/code&gt; и &lt;code&gt;awk&lt;/code&gt; не
    справились бы, self join в MySQL на таблицах с миллиардами строк -
    тоже)&lt;/li&gt;
&lt;li&gt;Подсчет статистики (количество пользователей и твитов, например
    подсчет количества твитов занимает 5 минут при 12 миллиардах
    записей)&lt;/li&gt;
&lt;li&gt;Подсчет PageRank между пользователями для вычисления репутации.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В твиттер используется бесплатный дистрибутив от Cloudera, версия Hadoop
0.20.1, данные храняться &lt;a href="https://www.insight-it.ru/goto/1ac5bba3/" rel="nofollow" target="_blank" title="https://github.com/kevinweil/hadoop-lzo"&gt;в сжатом по алгоритму LZO виде&lt;/a&gt;, библиотеки для работы с
данными опубликованы под названием
&lt;a href="https://www.insight-it.ru/goto/a1b5430e/" rel="nofollow" target="_blank" title="https://github.com/kevinweil/elephant-bird"&gt;elephant-bird&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="pig"&gt;Pig&lt;/h3&gt;
&lt;p&gt;Для того чтобы анализировать данные с помощью MapReduce обычно
необходимо разрабатывать код на Java, что далеко не все умеют делать, да
и трудоемко это.&lt;/p&gt;
&lt;p&gt;Pig представляет собой высокоуровневый язык, позволяющий
трансформировать огромные наборы данных шаг за шагом.&lt;/p&gt;
&lt;p&gt;Немного напоминает SQL, но намного проще. Это позволяет писать в 20 раз
меньше кода, чем при анализе данных с помощью обычных MapReduce работ.
Большая часть работы по анализу данных в Twitter осуществляется с
помощью Pig.&lt;/p&gt;
&lt;h3 id="dannye"&gt;Данные&lt;/h3&gt;
&lt;p&gt;Полу-структурированные данные:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;логи Apache, RoR, MySQL, A/B тестирования, процесса регистрации&lt;/li&gt;
&lt;li&gt;поисковые запросы&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Структурированные данные:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Твиты&lt;/li&gt;
&lt;li&gt;Пользователи&lt;/li&gt;
&lt;li&gt;Блок-листы&lt;/li&gt;
&lt;li&gt;Номера телефонов&lt;/li&gt;
&lt;li&gt;Любимые твиты&lt;/li&gt;
&lt;li&gt;Сохраненные поиски&lt;/li&gt;
&lt;li&gt;Ретвиты&lt;/li&gt;
&lt;li&gt;Авторизации&lt;/li&gt;
&lt;li&gt;Подписки&lt;/li&gt;
&lt;li&gt;Сторонние клиенты&lt;/li&gt;
&lt;li&gt;География&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Запутанные данные:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Социальный граф&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Что же они делают с этим всем?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Подсчет математического ожидания, минимума, максимума и дисперсии
    следующих показателей:&lt;ul&gt;
&lt;li&gt;Количество запросов за сутки&lt;/li&gt;
&lt;li&gt;Средняя задержка, 95% задержка&lt;/li&gt;
&lt;li&gt;Распределение кодов HTTP-ответов (по часам)&lt;/li&gt;
&lt;li&gt;Количество поисков осуществляется каждый день&lt;/li&gt;
&lt;li&gt;Количество уникальных запросов и пользователей&lt;/li&gt;
&lt;li&gt;Географическое распределение запросов и пользователей&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Подсчет вероятности, ковариации, влияния:&lt;ul&gt;
&lt;li&gt;Как отличается использование через мобильные устройства?&lt;/li&gt;
&lt;li&gt;Как влияет использование клиентов сторонних разработчиков?&lt;/li&gt;
&lt;li&gt;Когортный анализ&lt;/li&gt;
&lt;li&gt;Проблемы с сайтом (киты и роботы, подробнее ниже)&lt;/li&gt;
&lt;li&gt;Какие функциональные возможности цепляют пользователей?&lt;/li&gt;
&lt;li&gt;Какие функциональные возможности чаще используются популярными
    пользователями?&lt;/li&gt;
&lt;li&gt;Корректировка и предложение поисковых запросов&lt;/li&gt;
&lt;li&gt;A/B тестирование&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Предсказания, анализ графов, естественные языки:&lt;ul&gt;
&lt;li&gt;Анализ пользователей по их твитам, твитов, на которые они
    подписаны, твитам их фоловеров&lt;/li&gt;
&lt;li&gt;Какая структура графа ведет к успешным популярным сетям&lt;/li&gt;
&lt;li&gt;Пользовательская репутация&lt;/li&gt;
&lt;li&gt;Анализ эмоциональной окраски&lt;/li&gt;
&lt;li&gt;Какие особенности заставляют людей ретвитнуть твит?&lt;/li&gt;
&lt;li&gt;Что влияет на глубину дерева ретвитов ?&lt;/li&gt;
&lt;li&gt;Долгосрочное обнаружение дубликатов&lt;/li&gt;
&lt;li&gt;Машинное обучение&lt;/li&gt;
&lt;li&gt;Обнаружения языка&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Подробнее про обработку данных &lt;a href="https://www.insight-it.ru/goto/3d4649ef/" rel="nofollow" target="_blank" title="http://www.slideshare.net/kevinweil/nosql-at-twitter-nosql-eu-2010"&gt;в презентации Kevin Weil&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="hbase"&gt;HBase&lt;/h3&gt;
&lt;p&gt;Twitter начинают строить настоящие сервисы на основе Hadoop, например
поиск людей:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HBase используется как изменяемая прослойка над HDFS&lt;/li&gt;
&lt;li&gt;Данные экспортируются из HBase c помощью периодической MapReduce
    работы:&lt;ul&gt;
&lt;li&gt;На этапе Map используются также данные из FlockDB и нескольких
    внутренних сервисов&lt;/li&gt;
&lt;li&gt;Собственная схема разбиения данных&lt;/li&gt;
&lt;li&gt;Данные подтягиваются через высокопроизводительный, горизонтально
    масштабируемый сервис на Scala (&lt;a href="https://www.insight-it.ru/goto/917f8c95/" rel="nofollow" target="_blank" title="http://www.slideshare.net/al3x/building-distributed-systems-in-scala"&gt;подробнее о построении распределенных сервисов на Scala&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;На основе HBase разрабатываются и другие продукты внутри Twitter.&lt;/p&gt;
&lt;p&gt;Основными её достоинствами являются гибкость и легкая интеграция с
Hadoop и Pig.&lt;/p&gt;
&lt;p&gt;По сравнению с Cassandra:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"Их происхождение объясняет их сильные и слабые стороны"&lt;/li&gt;
&lt;li&gt;HBase построен на основе системы по пакетной обработке данных,
    высокие задержки, работает далеко не в реальном времени&lt;/li&gt;
&lt;li&gt;Cassandra построена с нуля для работы с низкими задержками&lt;/li&gt;
&lt;li&gt;HBase легко использовать при анализе данных как источник или место
    сохранения результатов, Cassandra для этого подходит меньше, но они
    работают над этим&lt;/li&gt;
&lt;li&gt;HBase на данный момент единственную точку отказа в виде мастер-узла&lt;/li&gt;
&lt;li&gt;В твиттере HBase используется для аналитики, анализа и создания
    наборов данных, а Cassandra - для онлайн систем&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="loony"&gt;Loony&lt;/h3&gt;
&lt;p&gt;Централизованная система управления оборудованием.&lt;/p&gt;
&lt;p&gt;Реализована с использованием:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/django/"&gt;Django&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8927633f/" rel="nofollow" target="_blank" title="http://www.lag.net/paramiko"&gt;Paraminko&lt;/a&gt; (реализация протокола SSH
    на Python, разработана и опубликована в opensource в Twitter)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Интегрирована с LDAP, анализирует входящую почту от датацентра и
автоматически вносит изменения в базу.&lt;/p&gt;
&lt;h3 id="murder"&gt;Murder&lt;/h3&gt;
&lt;p&gt;Система развертывания кода и ПО, основанная на протоколе BitTorrent.&lt;/p&gt;
&lt;p&gt;Благодаря своей P2P природе позволяет обновить более тысячи серверов за
30-60 секунд.&lt;/p&gt;
&lt;h3 id="kestrel"&gt;Kestrel&lt;/h3&gt;
&lt;p&gt;Распределенная очередь, работающая по протоколу memcache:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;set&lt;/code&gt; - поставить в очередь&lt;/li&gt;
&lt;li&gt;&lt;code&gt;get&lt;/code&gt; - взять из очереди&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Особенности:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Отсутствие строгого порядка выполнения заданий&lt;/li&gt;
&lt;li&gt;Отсутствие общего состояния между серверами&lt;/li&gt;
&lt;li&gt;Разработана на &lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="daemony"&gt;Daemon'ы&lt;/h3&gt;
&lt;p&gt;Каждый твит обрабатывается с помощью daemon'ов.&lt;/p&gt;
&lt;p&gt;В unicorn обрабатываются только HTTP запросы, вся работа за сценой
реализована в виде отдельных daemon'ов.&lt;/p&gt;
&lt;p&gt;Раньше использовалось много разных демонов, по одному на каждую задачу
(Rails), но перешли к меньшему их количеству, способному решать
несколько задач одновременно.&lt;/p&gt;
&lt;h3 id="kak-oni-spravliaiutsia-s-takimi-tempami-rosta"&gt;Как они справляются с такими темпами роста?&lt;/h3&gt;
&lt;p&gt;Рецепт прост, но эффективен, подходит практически для любого
интернет-проекта:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;обнаружить самое слабое место в системе;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;принять меры по его устранению;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;перейти к следующему самому слабому месту.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;На словах звучит и правда примитивно, но на практике нужно предпринять
ряд мер, чтобы такой подход был бы реализуем:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Автоматический сбор метрик (причем в агрегированном виде)&lt;/li&gt;
&lt;li&gt;Построение графиков (RRD, Ganglia)&lt;/li&gt;
&lt;li&gt;Сбор и анализ&amp;nbsp;логов&lt;/li&gt;
&lt;li&gt;Все данные должны получаться с минимальной задержкой, как можно
    более близко к реальному времени&lt;/li&gt;
&lt;li&gt;Анализ:&lt;ul&gt;
&lt;li&gt;Из данных необходимо получать &lt;em&gt;информацию&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Следить за динамикой показателей: стало лучше или хуже?&lt;/li&gt;
&lt;li&gt;Особенно при развертывании новых версий кода&lt;/li&gt;
&lt;li&gt;Планирование использования ресурсов намного проще, чем решение
    экстренных ситуаций, когда они на исходу&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Примерами агрегированных метрик в Twitter являются "киты" и "роботы",
вернее их количество в единицу времени.&lt;/p&gt;
&lt;h5&gt;Что такое "робот"?&lt;/h5&gt;
&lt;p&gt;&lt;img alt="Twitter Робот" class="responsive-img" src="https://www.insight-it.ru/images/twitter-bot.jpg" title="Twitter Робот"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ошибка внутри Rails (HTTP 500)&lt;/li&gt;
&lt;li&gt;Непойманное исключение&lt;/li&gt;
&lt;li&gt;Проблема в коде или нулевой результат&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Что такое "кит"?&lt;/h5&gt;
&lt;p&gt;&lt;img alt="Twitter Кит" class="responsive-img" src="https://www.insight-it.ru/images/twitter-whale.jpg" title="Twitter Кит"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HTTP ошибка 502 или 503&lt;/li&gt;
&lt;li&gt;В твиттер используется фиксированный таймаут в 5 секунд (лучше
    кому-то показать ошибку, чем захлебнуться в запросах)&lt;/li&gt;
&lt;li&gt;Убитый слишком длинный запрос к базе данных (mkill)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Значительное превышение нормального количества китов или роботов в
минуту является поводом для беспокойством.&lt;/p&gt;
&lt;p&gt;Реализован этот механизм простым bash-скриптом, который просматривает
агрегированные логи за последние 60 секунд, подсчитывает количество
китов/роботов и рассылает уведомления, если значение оказалось выше
порогового значения. Подробнее про работу команды оперативного
реагирования &lt;a href="https://www.insight-it.ru/goto/30562be/" rel="nofollow" target="_blank" title="http://www.slideshare.net/netik/billions-of-hits-scaling-twitter"&gt;в презентации John Adams&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="temnyi-rezhim"&gt;"Темный режим"&lt;/h3&gt;
&lt;p&gt;Для экстренных ситуаций в Twitter предусмотрен так называемый "темный
режим", который представляет собой набор механизмов для отключения
тяжелых по вычислительным ресурсам или вводу-выводу функциональных
частей сайта. Что-то вроде стоп-крана для сайта.&lt;/p&gt;
&lt;p&gt;Имеется около 60 выключателей, в том числе и полный режим "только для
чтения".&lt;/p&gt;
&lt;p&gt;Все изменения в настройках этого режима фиксируются в логах и сообщаются
руководству, чтобы никто не баловался.&lt;/p&gt;
&lt;h2 id="podvodim-itogi_1"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Не бросайте систему на самотек, начинайте собирать метрики и их
    визуализировать как можно раньше&lt;/li&gt;
&lt;li&gt;Заранее планируйте рост требуемых ресурсов и свои действия в случае
    экстренных ситуаций&lt;/li&gt;
&lt;li&gt;Кэшируйте по максимуму все, что возможно&lt;/li&gt;
&lt;li&gt;Все инженерные решения не вечны, ни одно из решений не идеально, но
    многие будут нормально работать в течение какого-то периода времени&lt;/li&gt;
&lt;li&gt;Заранее начинайте задумываться о плане масштабирования&lt;/li&gt;
&lt;li&gt;Не полагайтесь полностью на memcached и базу данных - они могут Вас
    подвести в самый неподходящий момент&lt;/li&gt;
&lt;li&gt;Все данные для запросов в реальном времени должны находиться в
    памяти, диски в основном для записи&lt;/li&gt;
&lt;li&gt;Убивайте медленные запросы (mkill) прежде, чем они убьют всю систему&lt;/li&gt;
&lt;li&gt;Некоторые задачи могут решаться путем предварительного подсчета и
    анализа, но далеко не все&lt;/li&gt;
&lt;li&gt;Приближайте вычисления к данным по возможности&lt;/li&gt;
&lt;li&gt;Используйте не mongrel, а unicorn для RoR&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Спасибо за внимание, &lt;a href="/feed/"&gt;жду Вас снова&lt;/a&gt;! Буду рад, если Вы
&lt;a href="https://www.insight-it.ru/goto/26b8fa1/" rel="nofollow" target="_blank" title="http://twitter.com/blinkov"&gt;подпишитесь на меня в Twitter&lt;/a&gt;, с
удовольствием пообщаюсь со всеми читателями :)&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 05 Mar 2011 20:47:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-03-05:highload/2011/arkhitektura-twitter-dva-goda-spustya/</guid><category>Apache</category><category>Cassandra</category><category>featured</category><category>Flock</category><category>FlockDB</category><category>Hadoop</category><category>HBase</category><category>Kestrel</category><category>Memcached</category><category>MySQL</category><category>Pig</category><category>Ruby</category><category>Ruby on Rails</category><category>Scala</category><category>Scribe</category><category>Twitter</category><category>Unicorn</category><category>архитектура Twitter</category><category>интернет-проекты</category><category>Масштабируемость</category><category>социальная сеть</category></item><item><title>Новое поколение MapReduce в Apache Hadoop</title><link>https://www.insight-it.ru//storage/2011/novoe-pokolenie-mapreduce-v-apache-hadoop/</link><description>&lt;p&gt;В большом бизнесе использование нескольких больших кластеров с
финансовой точки зрения более&amp;nbsp;эффективно, чем много маленьких. Чем
больше машин в кластере, тем большими наборами данных он может
оперировать, больше задач могут выполняться одновременно. Реализация
&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt; в &lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt;
&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; столкнулась с потолком масштабируемости на уровне
около 4000 машин в кластере. Разрабатывается следующее поколение Apaсhe
Hadoop MapReduce, &amp;nbsp;в котором появится общий планировщик ресурсов и
отдельный мастер для каждой отдельной задач, управляющий выполнением
программного кода. Так как простой оборудования по техническим причинам
обходится дорого на таком масштабе, высокий уровень доступности
проектируется с самого начала, ровно как и безопасность и
многозадачность, необходимые для поддержки одновременного использования
большого кластера многими пользователями. Новая архитектура также будет
более инновационной, гибкой и эффективной с точки зрения использования
вычислительных ресурсов.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="predistoriia"&gt;Предистория&lt;/h2&gt;
&lt;p&gt;Текущая реализация Hadoop MapReduce устаревает на глазах. Основываясь на
текущих тенденциях в размерах кластеров и нагрузок на них, JobTracker
требует кардинальных доработок, чтобы исправить его дефекты в области
масштабируемости, потребления памяти, многопоточности, надежности и
производительности. С точки зрения работы с Hadoop при каждом обновлении
кластера (даже если это просто багфикс), абсолютно все компоненты
кластера, так и приложений, которые на нем работают, должны быть
обновлены одновременно. Это так же очень неудобно, так как каждый раз
необходимо тестировать все приложения на совместимость с новой версией.&lt;/p&gt;
&lt;h2 id="trebovaniia"&gt;Требования&lt;/h2&gt;
&lt;p&gt;Прежде чем кардинально что-то менять в Hadoop mapreduce, необходимо
понять какие же основные требования предъявляются к вычислительным
кластерам на практике. Наиболее значительными требованиями к Hadoop
следующего поколения являются:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Надежность&lt;/li&gt;
&lt;li&gt;Доступность&lt;/li&gt;
&lt;li&gt;Масштабируемость - кластеры из как минимум 10 тысяч машин, 200 тысяч
    вычислительных ядер и даже больше&lt;/li&gt;
&lt;li&gt;Обратная и прямая совместимость - возможность быть уверенным, что
    приложение будет работать на новой версии так же, как оно работало
    на старой&lt;/li&gt;
&lt;li&gt;Контроль над обновлениями&lt;/li&gt;
&lt;li&gt;Предсказуемые задержки&lt;/li&gt;
&lt;li&gt;Эффективное использование ресурсов&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Среди менее значительных требований:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Поддержка альтернативных парадигм разработки (помимо MapReduce)&lt;/li&gt;
&lt;li&gt;Поддержка сервисов с коротким жизненным циклом&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Если учесть перечисленные выше требования, то становится очевидно, что
инфраструктура обработки данных в Hadoop должна быть кардинальным
образом изменена. В сообществе Hadoop люди в целом приходят к общему
мнению, что текущая архитектура MapReduce не способна решить текущие
задачи, которые перед ней ставится, и что требуется кардинальный
рефакторинг кодовой базы.&lt;/p&gt;
&lt;h2 id="mapreduce-sleduiushchego-pokoleniia"&gt;MapReduce следующего поколения&lt;/h2&gt;
&lt;p&gt;Фундаментальной идеей смены архитектуры является разделение двух
основных функций JobTracker'а на два отдельных части:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;управление ресурсами;&lt;/li&gt;
&lt;li&gt;планирования и мониторинга задач.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В итоге появляется несколько новых ролей:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ResourceManager&lt;/strong&gt; управляет глобальным распределением
    вычислительных ресурсов между приложениями;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplicationMaster&lt;/strong&gt; управляет планированием и координацией внутри
    приложения;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NodeManager&lt;/strong&gt; управляет процессами в рамках одной машины.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ApplicationMaster представляет собой библиотеку, с помощью которой можно
получить у ResourceManager квоту на вычислительные ресурсы и работать с
NodeManager(ами) для выполнения и мониторинга задач.&lt;/p&gt;
&lt;p&gt;ResourceManager поддерживает иерархическим очереди приложений, которым
может гарантированно выделяться некоторый процент ресурсов кластера. Его
функционал ограничивается планированием, никакого мониторинга и
отслеживания задач не происходит, а также нет никаких гарантий
перезапуска задач, провалившихся из-за проблем с оборудованием или
кодом. Планирование основывается на требованиях, которые выставляет
приложение с помощью ряда запросов ресурсов (среди них: запросы на
вычислительные ресурсы, память, дисковое пространство, сетевой доступ и
т.п.). Обратите внимание, что это значительное изменение по сравнению с
текущей моделью слотов фиксированного размера, которая является одной из
основных причин неэффективного использования ресурсов кластера на данный
момент.&lt;/p&gt;
&lt;p&gt;NodeManager - это агент, который работает на каждой машине и несет
ответственность за запуск контейнеров приложений, мониторинг
используемых ими ресурсов (плюс отчет планировщику).&lt;/p&gt;
&lt;p&gt;По одному ApplicationMaster запускается для каждого приложения, они
ответственны за запрос необходимых ресурсов у планировщика, запуск
задач, отслеживание статусов, мониторинг прогресса и обработку сбоев.&lt;/p&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Следующее поколение MapReduce" class="responsive-img" src="https://www.insight-it.ru/images/mapreduce-nextgen.jpg" title="Следующее поколение MapReduce"/&gt;&lt;/p&gt;
&lt;h2 id="uluchsheniia-po-sravneniiu-s-tekushchei-realizatsiei-mapreduce"&gt;Улучшения по сравнению с текущей реализацией MapReduce&lt;/h2&gt;
&lt;h3 id="masshtabiruemost"&gt;Масштабируемость&lt;/h3&gt;
&lt;p&gt;Разделение управления ресурсами и прикладными задачами позволяет
горизонтально расширять кластер более просто и эффективно. JobTracker
проводит значительную часть времени пытаясь управлять жизненным циклом
каждого приложения, что часто может приводить к различным
происшествиям - переход к отдельному менеджеру для каждого приложения
является значительным шагом вперед.&lt;/p&gt;
&lt;p&gt;Масштабируемость особенно важна в свете текущих трендов в оборудовании -
на данный момент Hadoop может быть развернут на кластере из 4000 машин.
Но 4000 средних машин 2009го года (т.е. по 8 ядер, 16Гб памяти, 4Тб
дискового пространства) только вдвое менее &amp;nbsp;ресурсоемки, чем 4000 машин
2011го года (16 ядер, 48гб памяти, 24Тб дискового пространства). Помимо
этого с точки зрения операционных издержек было выгоднее работать в еще
больших кластере от 6000 машин и выше.&lt;/p&gt;
&lt;h3 id="dostupnost"&gt;Доступность&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ResourceManager использует&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/e7095d3/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/zookeeper/"&gt;Apache ZooKeeper&lt;/a&gt; для обработки сбоев.
    Когда ResourceManager перестает работать, аналогичный процесс может
    быстро запуститься на другой машине благодаря тому, что состояние
    кластера было сохранено в ZooKeeper. При таком сценарии все
    запланированные и выполняющиеся приложения максимум лишь
    перезапустятся.&lt;/li&gt;
&lt;li&gt;ApplicationMaster - поддерживается создание точек восстановления на
    уровне приложений. ApplicationMaster может восстановить работу из
    состояния, сохраненного в HDFS, в случае сбоя.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="sovmestimost-protokola"&gt;Совместимость протокола&lt;/h3&gt;
&lt;p&gt;Это позволит различным версиям клиентов и серверов Hadoop общаться между
собой. Помимо решения многих существующих проблем с обновлением, в
будующих релизах появится возможность последовательного обновления кода
без простоя системы в целом - очень большое достижения с точки зрения
системного администрирования.&lt;/p&gt;
&lt;h3 id="innovatsionnost-i-gibkost"&gt;Инновационность и гибкость&lt;/h3&gt;
&lt;p&gt;Основным плюсом предложенной архитектуры является тот факт, что
MapReduce по сути становится просто пользовательской библиотекой.
Вычислительная же система (ResourceManager и NodeManager) становятся
полностью независимыми от специфики MapReduce.&lt;/p&gt;
&lt;p&gt;Клиенты получат возможность одновременного использования разных версий
MapReduce в одном и том же кластере. Это становится тривиальным, так как
отдельная копия ApplicationMaster'а запускается для каждого приложения.
Это дает гибкость в исправлении багов, улучшений и новых возможностей,
так как полное обновление кластер перестает быть обязательной
процедурой. Это позволяет клиентам обновлять их приложения до новых
версий MapReduce вне зависимости от обновлений кластера.&lt;/p&gt;
&lt;h3 id="effektivnost-ispolzovaniia-vychislitelnykh-resursov"&gt;Эффективность использования вычислительных ресурсов&lt;/h3&gt;
&lt;p&gt;ResourceManager использует общую концепцию для управления ресурсами и
планирования по отношению к каждому конкретному приложению. Каждая
машина в кластере на концептуальном уровне рассматривается просто как
набор ресурсов: память, процессор, ввод-вывод и др. Все машины
взаимозаменяемы и приложение может быть назначено на любую из них,
основываясь на доступных и запрашиваемых ресурсах. При этом приложения
работают в контейнерах, изолированно от других приложений, что дает
сильную поддержку многозадачности.&lt;/p&gt;
&lt;p&gt;Таким образом эта схема избавляет от текущего механизма map и reduce
слотов в Hadoop, который негативно влияет на эффективную утилизацию
вычислительных ресурсов.&lt;/p&gt;
&lt;h3 id="podderzhka-drugikh-paradigm-programmirovaniia-pomimo-mapreduce"&gt;Поддержка других парадигм программирования помимо MapReduce&lt;/h3&gt;
&lt;p&gt;В предложенной архитектуре используется общий механизм вычислений, не
привязанный конкретно к MapReduce, что позволит использовать и другие
парадигмы. Имеется возможность реализовать собственный
ApplicationMaster, способный запрашивать ресурсы у ResourceManager и
использовать их в соответствии с задачей, при этом сохраняются общие
принципы изоляции и гарантированного наличия полученных ресурсов. Среди
потенциально поддерживаемых парадигм можно назвать MapReduce, MPI,
Мaster-Worker, итеративные модели. Все они могут одновременно работать
на одном и том же кластере. Это особенно актуально для приложений
(например К-средний или Page Rank), где &lt;a href="https://www.insight-it.ru/python/2011/piccolo-postroenie-raspredelennykh-sistem-v-11-raz-bystree-hadoop/"&gt;другие подходы более чем на порядок эффективнее&lt;/a&gt; MapReduce.&lt;/p&gt;
&lt;h2 id="vyvody_1"&gt;Выводы&lt;/h2&gt;
&lt;p&gt;Apache Hadoop, и в частности Hadoop MapReduce - очень успешный
opensource проект по обработке больших объемов данных. Предложенный
Yahoo путь его переработки направлен на исправление недостатков
архитектуры текущей реализации, при этом повышая доступность,
эффективность использования ресурсов и предоставляя поддержку других
парадигм распределенных вычислений.&lt;/p&gt;
&lt;p&gt;Осталось дело за малым - собственно реализовать задуманное! :)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/336fc03c/" rel="nofollow" target="_blank" title="http://developer.yahoo.com/blogs/hadoop/posts/2011/02/mapreduce-nextgen/"&gt;Источник информации&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="/feed/"&gt;Подписаться на RSS можно здесь.&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 19 Feb 2011 21:23:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-02-19:storage/2011/novoe-pokolenie-mapreduce-v-apache-hadoop/</guid><category>Apache</category><category>Apache Hadoop</category><category>Hadoop</category><category>архитектура</category><category>кластер</category><category>кластеризация</category><category>кластерные вычисления</category><category>Масштабируемость</category><category>разработка</category><category>технологии</category></item><item><title>Еще раз про HBase</title><link>https://www.insight-it.ru//storage/2008/eshe-raz-pro-hbase/</link><description>&lt;p&gt;Некоторое время назад &lt;a href="https://www.insight-it.ru/goto/5bbb805b/" rel="nofollow" target="_blank" title="http://neuronus.blogspot.com"&gt;Neuronus&lt;/a&gt; в одном
из комментариев к посту &lt;a href="https://www.insight-it.ru/storage/2008/hadoop-vozvrashhaetsya/"&gt;"Hadoop возвращается"&lt;/a&gt; не согласился с
моим кратким определением HBase как "нереляционная база данных"
(позаимствованным, собственно говоря, откуда-то с официального портала
продукта). Этот факт подтолкнул меня попытаться найти более корректное
определение в англоязычных источниках информации, получилось вполне
успешно. Хочется прочитать более детально что к чему? Вперед!
&lt;!--more--&gt;
Если Вам уже приходилось иметь дело с этой системой,возможно Вы уже
поняли, что самым сложным этапом работы является просто-напросто
осознавание того, чем она на самом деле является. Обычно приходится
мысленно отказаться от всех привычек, доставшихся при работе с
традиционный RDBMS, и начинать постигать базовые принципы организации
хранения данных с нуля.&lt;/p&gt;
&lt;p&gt;Стоит напомнить, что проект позиционируется как opensource реализация
&lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt; от &lt;a href="/tag/google/"&gt;Google&lt;/a&gt;. Да, проекты
реализованы разными людьми на разных языках программирования, но
общие идеи и принципы функционирования у них сильно пересекаются.
Наиболее значимой общей характеристикой у них является очень схожие
модели данных (о чем
&lt;a href="https://www.insight-it.ru/goto/aef0a732/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/HbaseArchitecture#head-daee3a0ce7a6892096ffb43f3cc3e0310d047f48"&gt;упоминается&lt;/a&gt;
в вики HBase), а в свою очередь &lt;a href="https://www.insight-it.ru/goto/8667b351/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/bigtable.html"&gt;в документации&lt;/a&gt; BigTable она описывается очень четко и определенно, точно определяя чем эти продукты по сути являются:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A Bigtable is a sparse, distributed, persistent multidimensional sorted
map.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Предлагаю по аналогии со &lt;a href="https://www.insight-it.ru/goto/af69f471/" rel="nofollow" target="_blank" title="http://jimbojw.com/wiki/index.php?title=Understanding_HBase_and_BigTable"&gt;статьей в вики&lt;/a&gt;,
послужившей основой для данного поста, разбить это определение на
отдельные слова и последовательно пройтись по ним, попутно составляя в
голове полную картину.&lt;/p&gt;
&lt;h3 id="map"&gt;map&lt;/h3&gt;
&lt;p&gt;За этим термином нет четкого устоявшегося обозначения в русском языке
(да и в английском тоже все далеко не так однозначно), математики обычно
называют это &lt;em&gt;отображением одного множества в другое&lt;/em&gt;, в то время как
если Вы знакомы с программированием, то Вам наверняка больше знакомы
будут более знакомы такие обозначения, как &lt;em&gt;ассоциативный массив&lt;/em&gt;
(&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;), &lt;em&gt;словарь&lt;/em&gt; (&lt;a href="/tag/python/"&gt;Python&lt;/a&gt;), &lt;em&gt;хэш&lt;/em&gt;
(&lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt;) или &lt;em&gt;объект&lt;/em&gt; (&lt;a href="/tag/javascript/"&gt;JavaScript&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;По сути же имеется ввиду просто набор однозначно соответствующих пар
ключ-значение, в роли которых выступают массивы байт. Во все той же
статейке в вики все очень наглядно демонстрируется примерами в нотации
&lt;abbr title="JavaScript Object Notation"&gt;JSON&lt;/abbr&gt;, позволю себе тоже приводить аналогичные примеры. В &lt;abbr title="JavaScript Object Notation"&gt;JSON&lt;/abbr&gt; наш &lt;strong&gt;map&lt;/strong&gt; выглядел бы следующим образом:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;"qqqq"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"some"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"abc"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"sample"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"zz"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"JSON"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"123"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"map"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"mnbvcxz"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"looks like this"&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="persistent"&gt;persistent&lt;/h3&gt;
&lt;p&gt;Это прилагательное обозначает всего лишь "постоянный", то есть в данном
контексте оно говорит только о том, что данная система не зависит от
использующих ее приложений, а также хранится на устройствах постоянного
хранения данных, а не в оперативной памяти.&lt;/p&gt;
&lt;h3 id="distributed"&gt;distributed&lt;/h3&gt;
&lt;p&gt;Распределенность этих систем можно рассматривать с двух точек зрения:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Как HBase, так и BigTable сами по себе могут функционировать на
    большом количестве серверов, которые можно разделить на две большие
    категории: master и slave. Slave сервера собственно выполняют всю
    работу с данными, а master - лишь только координируют их действия и
    управляют процессом в целом. Этот факт обеспечивают высокую степень
    устойчивости к сбоям (в HBase правда количество master-серверов
    ограничено одним, что представляет собой единственную точку, сбой в
    которой приведет к отказу всей системы, но это лишь временная
    проблема, которую наверняка устранят в следующих версиях), а также
    существенно облегчает масштабируемость всей системы так как
    добавление дополнительных серверов (а значит и увеличение
    производительности и вместительности системы) достаточно тривиально,
    безболезненно и не мешает общему ее функционированию.&lt;/li&gt;
&lt;li&gt;Помимо этого каждая из этих систем обычно использует для хранения
    данных кластерную файловую систему (HBase - &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt;, а
    BigTable - &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;), которые тоже по своей природе являются
    распределенными и функционируют по схожему принципу, обеспечивая
    дополнительную сохранность данных, реплицируя их в нескольких
    экземплярах на нескольких серверах (обычно трех).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="sorted"&gt;sorted&lt;/h3&gt;
&lt;p&gt;HBase и BigTable не строят никакие индексы для ускорения процесса
извлечения данных, единственное используемое в них правило заключается в
следующем: каждый slave-сервер в системе отвечает за определенный
диапазон ключей (от и до определенных его значений), и держит все записи
в строгом лексикографическом порядке по ключам (заметьте: сортировку
значений никто не гарантирует!). Продолжая пример с &lt;abbr title="JavaScript Object Notation"&gt;JSON&lt;/abbr&gt; это выглядело
бы примерно вот так:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;"123"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"map"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"abc"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"sample"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"mnbvcxz"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"looks like this"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"qqqq"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"some"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"zz"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"JSON"&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Этим фактом можно активно пользоваться при планировании использовании
системы, например если в качестве ключей планируется использовать
доменные именные имена, то имеет смысл использовать их в "развернутом"
виде, например: "com.example.www" вместо "www.example.com". Это почти
наверняка обеспечит попадание всех поддоменов одного и того же домена на
один slave-сервер, а также группировку доменов по зонам.&lt;/p&gt;
&lt;h3 id="multidimensional"&gt;multidimensional&lt;/h3&gt;
&lt;p&gt;До сих пор ключ интерпретировался нами как нечто единое и неделимое, но
на самом деле в данной ситуации это далеко не так. На самом деле
пространство имен HBase и BigTable имеет несколько пространств, по
аналогии с трехмерным материальным пространством, где есть ширина,
высота и глубина. Если мы попытаемся представить это с помощью &lt;abbr title="JavaScript Object Notation"&gt;JSON&lt;/abbr&gt;, то
это будет выглядеть как набор вложенных простых map'ов:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;"table-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="p"&gt;{&lt;/span&gt;
     &lt;span class="s2"&gt;"column-family-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
     &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"column-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
           &lt;span class="s2"&gt;"row-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
           &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-2"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-3"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-3"&lt;/span&gt;
           &lt;span class="p"&gt;},&lt;/span&gt;
           &lt;span class="s2"&gt;"row-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
           &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-4"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-5"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-3"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-6"&lt;/span&gt;
           &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="s2"&gt;"column-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
           &lt;span class="s2"&gt;"row-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
           &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-3"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-3"&lt;/span&gt;
           &lt;span class="p"&gt;},&lt;/span&gt;
           &lt;span class="s2"&gt;"row-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
           &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-4"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-5"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-3"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-6"&lt;/span&gt;
           &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
     &lt;span class="s2"&gt;"column-family-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
     &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"column-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
           &lt;span class="c1"&gt;// ...&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="s2"&gt;""&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
           &lt;span class="s2"&gt;"row-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"some-value"&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="c1"&gt;// ...&lt;/span&gt;
     &lt;span class="p"&gt;}&lt;/span&gt;
     &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Как можно увидеть из примера, таких пространств используется пять:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;таблицы;&lt;/li&gt;
&lt;li&gt;наборы столбцов;&lt;/li&gt;
&lt;li&gt;столбцы;&lt;/li&gt;
&lt;li&gt;строки;&lt;/li&gt;
&lt;li&gt;время;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Таким образом, каждое значение в хранилище данных однозначно
соответствует ключу, состоящему из пяти компонентов, например в примере
значению "value-5" соответствует ключ, состоящий из:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;table-1;&lt;/li&gt;
&lt;li&gt;column-family-1;&lt;/li&gt;
&lt;li&gt;column-1;&lt;/li&gt;
&lt;li&gt;row-2;&lt;/li&gt;
&lt;li&gt;timestamp-2;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Принцип очень похож на используемый в более привычных базах данных, с
той лишь разницей, что добавляется еще и время (которое обычно
представляется в виде целочисленного значения, обозначающего количество
секунд с начала эпохи). Изначально оно задумывалось для предоставления
возможности отследить историю изменения данных, но этому дополнительному
измерению можно найти и массу нестандартных применений, например
используя его как самый обыкновенный стек.&lt;/p&gt;
&lt;p&gt;Хочется обратить внимание, что ассортимент наборов столбцов (column
family) указывается при создании таблиц, и изменения в них должны
производиться с помощью специального запроса, в то время как сами
столбцы являются динамическими и для его создания достаточно лишь
добавить в него данные.&lt;/p&gt;
&lt;h3 id="sparse"&gt;sparse&lt;/h3&gt;
&lt;p&gt;Развивая мысль предыдущего абзаца, можно понять, что наличие значения
столбца в каждой строке вовсе не обязательно, оно запросто может и
отсутствовать вовсе. Таким образом каждая строка может содержать
произвольное количество значений для столбцов в рамках одной column
family, ровно как может и не содержать их вовсе. Несуществующие данные
не хранятся в виде какого-либо NULL-значения, они просто отсутствуют.
Запрос на несуществующие данные просто вернет пустой результат. Если же
взглянуть с другой стороны, то тот же самый факт можно представить и как
возможность наличия пробелов в списке ключей строк.&lt;/p&gt;
&lt;h3 id="i-naposledok"&gt;И напоследок...&lt;/h3&gt;
&lt;p&gt;хочется сказать, что все это лишь дело терминологии, ровно то же самое
можно подразумевать и под краткой фразой "нереляционная база данных", не
смотря на то, что она существенно менее точна и полноценна. В данном
контексте самое главное лишь чтобы люди просто понимали друг друга.
Надеюсь после прочтения этого поста в вашем сознании сложилась четкая
картина этого продукта и предоставляемых им возможностей, которая Вам
пригодится как для "общего развития", так и для потенциального
практического применения этого продукта. Если остались неясные моменты -
смело оставляйте комментарии. &lt;a href="/feed/"&gt;Традиционная подписка на RSS&lt;/a&gt; -
приветствуется :)&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 27 Aug 2008 19:29:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-08-27:storage/2008/eshe-raz-pro-hbase/</guid><category>Hadoop</category><category>HBase</category><category>map</category><category>кластер</category></item><item><title>Hadoop возвращается</title><link>https://www.insight-it.ru//storage/2008/hadoop-vozvrashhaetsya/</link><description>&lt;p&gt;Если Вы являетесь постоянным читателем моего блога, то вполне вероятно,
что Вы помните мой &lt;a href="https://www.insight-it.ru/storage/2008/hadoop/"&gt;старый пост&lt;/a&gt; об этом
замечательном проекте от &lt;a href="https://www.insight-it.ru/goto/e3b03afc/" rel="nofollow" target="_blank" title="http://apache.org"&gt;Apache Foundation&lt;/a&gt;. С тех
пор он развивался невероятными темпами и очень многое успело измениться,
об этом я и хотел бы сегодня поделиться своими впечатлениями. В
дополнение к этому планируется небольшая инструкция по развертыванию
Hadoop на кластере из большого количества машин, который послужит
неплохим развитием темы, начатой в посте &lt;a href="https://www.insight-it.ru/storage/2008/hadoop-dlya-razrabotchika/"&gt;"Hadoop для разработчика"&lt;/a&gt;.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="chto-novogo"&gt;Что нового?&lt;/h3&gt;
&lt;p&gt;Для начала вкратце напомню что их себя представляет данный продукт,
всего в нем три компонента:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;HDFS&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;кластерная файловая система.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;MapReduce framework&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;программная основа для построения приложений, работающих по
одноименной модели.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;HBase&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;нереляционная база данных.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Повторно повторяться смысла не вижу, все уже давно &lt;a href="https://www.insight-it.ru/storage/2008/hadoop/"&gt;разложено по полочкам&lt;/a&gt;. Так что сразу перейдем к глобальным
изменениям в проекте, произошедшим с написания вышеупомянутого поста, то
есть с февраля. Сразу хочу сказать, что подробно пересказывать &lt;a href="https://www.insight-it.ru/goto/81620ac0/" rel="nofollow" target="_blank" title="http://svn.apache.org/repos/asf/hadoop/core/trunk/CHANGES.txt"&gt;release notes&lt;/a&gt; у
меня нет никакого желания, если Вам интересны все подробности о каждом
bugfix'е или изменении в API, то имеет смысл почитать их в оригинале.&lt;/p&gt;
&lt;p&gt;Наиболее значительным событием в развитии Apache Hadoop было, пожалуй,
отделение &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt; в отдельный проект. Какие же это повлекло
последствия? С точки зрения простого смертного наиболее заметен тот
факт, что HBase пропал из основного архива или репозитория Hadoop и его
теперь нужно качать отдельно :) На самом же деле такое обособление лишь
ускорило ее развитие, совсем недавно HBase отпраздновала свой релиз
версии 0.2.0, включающий в себя массу нововведений и исправленных
проблем, например язык запросов HQL был полностью заменен на jirb/jython
shell, а также было добавлено кэширование данных в памяти. Помимо этого
сильно изменилось API, очень рекомендую заглянуть в
&lt;a href="https://www.insight-it.ru/goto/f059ad5e/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/hbase/docs/current/api/index.html"&gt;javadoc&lt;/a&gt;
проекта, если Вас это интересует.&lt;/p&gt;
&lt;p&gt;На уровне файловой системы наиболее значительным изменением стало
добавление еще одного типа узлов - &lt;strong&gt;Secondary NameNode&lt;/strong&gt;. Это
нововведение является первым шагом на пути к устранению узких мест в
системе (так называемых single points of failure). Название этого типа
узлов говорит само за себя: они подстраховывают основной &lt;em&gt;NameNode&lt;/em&gt; на
случай непредвиденных сбоев. Они создают резервную копию образа
метаданных файловой системы и лога транзакций (то есть всех операций с
файлами и директориями в HDFS) и периодически ее обновляют. Полноценного
автоматического восстановления системы они в случае сбоя на сервере с
&lt;em&gt;NameNode&lt;/em&gt; они на данный момент не обеспечивают, но сохранность данных
на случай, скажем, разрушившегося RAID обеспечить могут.&lt;/p&gt;
&lt;p&gt;MapReduce framework тоже несомненно развивается и дорабатывается, но
каких-либо особо выдающихся изменений в нем не произошло: появляются
дополнительные возможности, исправляются ошибки, снимаются те или иные
ограничения. В общем все идет своим чередом.&lt;/p&gt;
&lt;h3 id="podnimaem-klaster"&gt;Поднимаем кластер&lt;/h3&gt;
&lt;div class="card blue lighten-4"&gt;
&lt;div class="card-content"&gt;
&lt;h5&gt;ВНИМАНИЕ!&lt;/h5&gt;
&lt;p&gt;Перед продолжением чтения этого раздела, настоятельно рекомендуется
прочитать &lt;a href="https://www.insight-it.ru/storage/2008/hadoop-dlya-razrabotchika/"&gt;статью о запуске псевдо-кластера из одного компьютера&lt;/a&gt;.
&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Для начала нам понадобится некоторое количество компьютеров (хотя если у
Вас серьезные намерения, то лучше все же гордо называть их серверами, а
для "побаловаться" сойдут и обычные рабочие станции с
&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;). Конкретное количество на самом деле роли не
играет, продолжать можно как с 2 серверами, так и с 20 тысячами (по
крайней мере теоретически). Хотя пару рекомендаций все же могу дать: при
использовании в "боевых" условиях стоит стараться избегать физического
совмещения мастер-узлов компонентов системы (&lt;em&gt;NameNode, JobTracker,
HMaster&lt;/em&gt;) с "рядовыми" серверами, таким образом желательно начинать с,
как минимум, 5-7 серверов.&lt;/p&gt;
&lt;p&gt;Удостоверившись, что на всем оборудовании установлен какой-нибудь
дистрибутив &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; или &lt;a href="/tag/unix/"&gt;Unix&lt;/a&gt; (любители особо
поизвращаться могут попытать счастья с "окнами" в совокупности с Cygwin)
и 5 или 6 версия JRE/JDK (желательно от Sun), можно приступать к
настройке каждого узла по тому же принципу, что и для псевдо-кластера
(да-да, предупреждение в начале раздела было написано не для мебели).
Кстати не забудьте, что &lt;a href="/tag/hbase/"&gt;HBasе&lt;/a&gt; теперь нужно скачивать
отдельно. О небольших присутствующих особенностях я расскажу чуть позже,
а пока дам маленький совет, который позволит несколько облегчить это
непростое дело.&lt;/p&gt;
&lt;p&gt;Вручную выполнять одни и те же операции на паре десятков/сотен/тысяч
серверов мало того что долго, но и чрезвычайно утомительно. Уже на
втором-третьем сервере начнет появляться желание каким-либо образом
автоматизировать процесс установки. Конечно же можно воспользоваться
специализированным программным обеспечением, скажем
&lt;a href="https://www.insight-it.ru/goto/65d64e55/" rel="nofollow" target="_blank" title="http://www.theether.org/gexec/"&gt;gexec&lt;/a&gt;, но есть и более простой способ:
существенно упростить жизнь может простой скрипт на bash в 5 строчек:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; x in &lt;span class="sb"&gt;`&lt;/span&gt;cat ~/nodes&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="k"&gt;do&lt;/span&gt;
ssh hadoop@&lt;span class="nv"&gt;$x&lt;/span&gt; &lt;span class="nv"&gt;$1&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;В файле &lt;code&gt;~/nodes&lt;/code&gt; должен располагаться список IP-адресов всех
серверов, тогда получив первым параметром произвольную консольную
команду скрипт выполнит ее на каждом сервере. С его помощью можно
существенно сократить время, требуемое на выполнение всех необходимых
действий для запуска кластера.&lt;/p&gt;
&lt;p&gt;После небольшого лирического отступления вернемся собственно к
&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt;. Как Вы уже, надеюсь, знаете, система использует
&lt;strong&gt;ssh&lt;/strong&gt; для управления всеми компонентами системы, причем очень
желателен беспарольный доступ между всеми узлами. Для этого необходимо
собрать в один файл все публичные ключи &lt;code&gt;~/.ssh/id_rsa.pub&lt;/code&gt; на
каждом из узлов (по одному на строчку) и разместить его под именем
&lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; тоже на каждом из узлов. Кстати для
упоминавшегося выше скрипта беспарольный доступ тоже очень желателен.&lt;/p&gt;
&lt;p&gt;Следующим этапом нужно подготовить конфигурационные файлы, они должны
быть идентичными на всех узлах, так что заполнив их все на одном из
узлов нужно скопировать их по всем остальным серверам (очень удобно
делать это с помощью &lt;strong&gt;rsync&lt;/strong&gt;). Теперь пройдемся по необходимым
изменениям в каждом из них:&lt;/p&gt;
&lt;h4&gt;hadoop-site.xml&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.default.name&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;hdfs://namenode:54310&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;
    The name of the default file system.  A URI whose
    scheme and authority determine the FileSystem implementation.  The
    uri's scheme determines the config property (fs.SCHEME.impl) naming
    the FileSystem implementation class.  The uri's authority is used to
    determine the host, port, etc. for a filesystem.
  &lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapred.job.tracker&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;jobtracker:54311&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;
    The host and port that the MapReduce job tracker runs
    at.  If "local", then jobs are run in-process as a single map
    and reduce task.
  &lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Каждый сервер должен знать где расположен &lt;em&gt;NameNode&lt;/em&gt;, по-этому он
  явно указывается в полном пути к файловой системе, практически
  аналогичная ситуация и с &lt;em&gt;JobTracker&lt;/em&gt;. Вместо namenode и jobtracker
  необходимо указать их IP-адреса или доменные имена (или в крайнем
  случае - имя в &lt;code&gt;/etc/hosts&lt;/code&gt;)&lt;/p&gt;
&lt;h4&gt;masters&lt;/h4&gt;
&lt;p&gt;Вопреки логике, здесь указывается список всех &lt;em&gt;SecondaryNameNode&lt;/em&gt;.
Одного-двух серверов здесь будет вполне достаточно, самое главное не
указывать здесь адрес основного &lt;em&gt;NameNode&lt;/em&gt;, лучше всего подойдет
какой-нибудь другой мастер-сервер, может быть дополненный одним из
обычных узлов кластера. Выделять под это отдельный сервер смысла не
много, так как нагрузка на них минимальна.&lt;/p&gt;
&lt;h4&gt;slaves&lt;/h4&gt;
&lt;p&gt;Список всех рядовых серверов, по одному на строку (опять же: IP или доменное имя). На них будут запущенны &lt;em&gt;DataNode&lt;/em&gt; и &lt;em&gt;TaskTracker&lt;/em&gt;.&lt;/p&gt;
&lt;h4&gt;hbase-site.xml&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;hbase.master&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;localhost:60000&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;
    the host and port that the HBase master runs at
  &lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Первое изменение достаточно очевидно: &lt;em&gt;HRegionServer&lt;/em&gt; должны знать
где находится &lt;em&gt;HMaster&lt;/em&gt;, о чем им и сообщает первое свойство
(заменяем hmaster на соответствующий адрес). А вот второе свойство
является следствием "обособления" HBase от Hadoop, о котором шла
речь ранее. Теперь имеется возможность использовать их отдельно (с
локальной файловой системой вместо HDFS), а так как появился выбор
файловой системы - ее адрес необходимо указывать полностью. В данном
случае указан адрес HDFS (такой же как в &lt;strong&gt;hadoop-site.xml&lt;/strong&gt;).&lt;/p&gt;
&lt;h4&gt;regionservers&lt;/h4&gt;
&lt;p&gt;Вполне очевидный конфигурационный файл, по аналогии со &lt;strong&gt;slaves&lt;/strong&gt;,
заполняется списком адресов для запуска &lt;em&gt;HRegionServer&lt;/em&gt;. Часто
совпадает с упомянутым &lt;strong&gt;slaves&lt;/strong&gt;, обычно достаточно просто
скопировать.&lt;/p&gt;
&lt;h3 id="zapusk"&gt;Запуск&lt;/h3&gt;
&lt;p&gt;Удостоверившись, что с конфигурационными файлами все нормально и что они
на всех серверах совпадают, можно приступать собственно к запуску. Этот
процесс практически полностью совпадает с запуском на одном узле, хотя
обычно проще желать это тоже простеньким скриптом примерно такого вида:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/bin/bash&lt;/span&gt;
ssh hadoop@namenode ~/hadoop/bin/start-dfs.sh
ssh hadoop@jobtracker ~/hadoop/bin/start-mapred.sh
ssh hadoop@hmaster ~/hbase/bin/start-hbase.sh
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Если мы нигде не ошиблись и все сделано правильно, то кластер
благополучно запустится, что легко проследить выполнив на каждом узле
команду &lt;code&gt;jps&lt;/code&gt; и проверив соответствие запущенных компонентов
запланированному (читай: указанному в конфигурационных файлах).&lt;/p&gt;
&lt;p&gt;В целом процесс достаточно прост и не занимает много времени, если Вы
все же столкнулись с какими-либо проблемами в процессе - обращайтесь,
вполне возможно, что я смогу помочь. Удостовериться, что все нормально
можно абсолютно так же, как и для псевдо-кластера - с помощью MapReduce
задач, идущих в комплекте с Hadoop. Выглядеть это может, например, вот
так:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;~/hadoop/bin/hadoop jar hadoop-*-examples.jar pi &lt;span class="m"&gt;4&lt;/span&gt; 10000
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;По-хорошему надо было бы написать подобную инструкцию сразу после
первой, но почему-то как-то не сложилось...&lt;/p&gt;
&lt;h3 id="zakliuchenie"&gt;Заключение&lt;/h3&gt;
&lt;p&gt;На данный момент &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; стал еще более работоспособным,
по сравнению с его февральским состоянием. Сообщество использующих его
разработчиков растет с каждым днем, а все ошибки и проблемы исправляются
очень и очень оперативно, многие коммерческие проекты могут позавидовать
таким темпам развития. Хоть до по-настоящему стабильного релиза еще
далеко, данный продукт уже сейчас очень активно используется в
достаточно большом количестве крупных интернет-проектов.&lt;/p&gt;
&lt;p&gt;Если Вы еще не успели подписаться на &lt;a href="/feed/"&gt;RSS&lt;/a&gt; - сейчас самое время!&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 17 Aug 2008 23:15:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-08-17:storage/2008/hadoop-vozvrashhaetsya/</guid><category>Apache</category><category>Hadoop</category><category>HBase</category><category>HDFS</category><category>MapReduce</category></item><item><title>Hypertable</title><link>https://www.insight-it.ru//storage/2008/hypertable/</link><description>&lt;p&gt;&lt;img alt="Hypertable" class="right" src="https://www.insight-it.ru/images/hypertable-logo.gif" title="Hypertable"/&gt;
&lt;a href="https://www.insight-it.ru/goto/63463036/" rel="nofollow" target="_blank" title="http://www.hypertable.org"&gt;Hypertable&lt;/a&gt; является еще одним opensource
проектом, направленным на воспроизведение функционала
&lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt; от &lt;a href="/tag/google/"&gt;Google&lt;/a&gt;. Поставленная перед
проектом цель заключается в реализации системы хранения данных на базе
распределенной файловой системы, позволяющей перейти на новый уровень
производительности при работе с гигантскими объемами данных.
&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Принцип работы &lt;a href="/tag/hypertable/"&gt;Hypertable&lt;/a&gt; прост до безобразия:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hypertable хранит данные в табличном формате, сортируя записи по
    основному ключу;&lt;/li&gt;
&lt;li&gt;для хранимых данных не используются какие-либо типы данных, любая
    ячейка интерпретируется как байтовая строка;&lt;/li&gt;
&lt;li&gt;масштабируемость достигается путем разбиения таблиц на смежные
    интервалы строк и хранения их на разных физических машинах;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;в системе используется два типа серверов:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Master Server&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;&amp;ndash; как и во многих других подобных системах мастер-сервер
выполняет обязанности скорее административного характера: он
управляет работой Range серверов, работает с метаданными
(которые хранятся просто в отдельной таблице, наравне с
остальными).&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Range Server&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;&amp;ndash; их задача стоит в собственно в хранении диапазонов строк из
различных таблиц. Каждый сервер может хранить несколько
несмежных диапазонов строк, если диапазон превышает по объему
определенный лимит (по-умолчанию - 200 MB), то он разбивается на
пополам и одна половина обычно перемещяется на другой сервер.
Если же на одном из серверов подходит к концу дисковое
пространство, то под руководством мастер-сервера часть
диапазонов с него перераспределяется на менее загруженные Range
серверы.&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Еще одним компонентом системы является Hyperspace, этот сервер
    предоставляет указатель на основную таблицу с метаданными, а также
    пространство имен. Помимо этого этот сервис выступает в роли
    lock-механизма для клиентов системы.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В качестве основы для этой системы может использоваться как входящая в
состав &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; файловая система &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt;, так и
&lt;a href="/tag/kfs/"&gt;KosmosFS&lt;/a&gt;, о которой я недавно
&lt;a href="https://www.insight-it.ru/storage/2008/fajjly-v-kosmose/"&gt;рассказывал&lt;/a&gt;. Это позволяет
Hypertable выступать в роли конкурента для &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt; в рамках
проекта &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;HBase и Hypertable выполняют достаточно похожие функции и преследуют
практически одни и те же цели, но есть некоторые ньюансы. Одним из
глобальных различий в этих системах является языки программирования, с
использованием которого они реализованы. HBase написана на
&lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, в то время как разработчики Hypertable предпочли
&lt;a href="/tag/c/"&gt;C++&lt;/a&gt;. Это повлекло за собой массу различий в инкапсулированной
реализации различных операций.&lt;/p&gt;
&lt;p&gt;Для доступа к данным каждая из систем использует язык HQL, только в
одном случае аббревиатура расшифровывается как HBase Query Language, а в
другом - Hypertable Query Language (как эгоистично :) ). По сути и то и
другое является сильно упрощенным диалектом &lt;a href="/tag/sql/"&gt;SQL&lt;/a&gt;, что
позволяет сократить знакомство с синтаксисом HQL до пары минут при
достаточном знании классического SQL. Хотелось бы отметить, что вся
простота в сравнении с классическим SQL и реляционными СУБД вполне
обоснована: обе системы хранения данных предназначены для использования
в совокупности с &lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt; программами, что делает их
просто хранилищем данных, а не средством их обработки.&lt;/p&gt;
&lt;p&gt;После небольшого лирического отступления в виде сравнения с HBase
хотелось бы все же вернуться к теме нашего разговора, а именно к
организации хранения данных в Hypertable. Данные хранятся в виде пар
ключ:значение, причем храняться все версии строк с указанием времени,
когда они были созданы. Таким образом легко проследить за процессом
изменения данных во времени, а также узнать какие именно операции
проводились над ними в прошлом. Стандартный механизм работы с версиями
данных может быть переопределен на хранения лишь фиксированного
количества версий строки, позволяя использовать удаление устаревших
записей для освобождения дополнительного дискового пространства.&lt;/p&gt;
&lt;p&gt;Для более эффективной работы с обновлением случайных ячеек таблиц
используется кэширование. Поступающие данные собираются в оперативной
памяти и при достижении определенного лимита сжимаются и записываются на
диск.&lt;/p&gt;
&lt;p&gt;Для более эффективной работы с распределенной файловой системой
используется механизм под названием &lt;em&gt;Access Groups&lt;/em&gt;. Суть заключается в
объединении колонок таблиц в группы, в которых они чаще всего
используется вместе. Такие группы данных по возможности храняться вместе
на физических носителях. Если запрос включает в себя только данные из
колонок одной группы доступа, то с дисков считывается только эти
колонки, в противном случае приходиться работать со всей строкой
целиком. Такой подход позволяет существенно оптимизировать работу
операций ввода/вывода.&lt;/p&gt;
&lt;p&gt;Проект еще находится в стадии разработки и до стабильного релиза ему еще
далеко, но тем не менее он уже вполне может себя показать в качестве
конкурента как для других систем подобного класса, так и для более
стандартных реляционных баз данных. Основными недостающими моментами в
этой системе в данной системе является отсутствие некоторого порой
необходимого функционала в HQL, а такжы некоторые проблемы с
отказоустойчивостью, вызванные единственностью в рамках системы Master и
Hyperspace серверов.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 05 Apr 2008 20:27:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-04-05:storage/2008/hypertable/</guid><category>C++</category><category>GPL</category><category>Hadoop</category><category>HDFS</category><category>HQL</category><category>Hypertable</category><category>KFS</category><category>opensource</category></item><item><title>Файлы в космосе</title><link>https://www.insight-it.ru//storage/2008/fajjly-v-kosmose/</link><description>&lt;h4&gt;...или Kosmos Distributed File System&lt;/h4&gt;
&lt;p&gt;&lt;img alt="Kosmos Distributed File System" class="right" src="https://www.insight-it.ru/images/KFS.jpg" title="KosmosFS Logo"/&gt;
Сегодня речь пойдет об еще одной распределенной файловой системе -
&lt;a href="https://www.insight-it.ru/goto/a11ac210/" rel="nofollow" target="_blank" title="http://kosmosfs.sourceforge.net/"&gt;KosmosFS&lt;/a&gt;. У русских людей название
этого проекта определенно вызывает ассоциации с космосом, но изначально
все же свою лепту в него внес изначальный разработчик -
&lt;a href="https://www.insight-it.ru/goto/636f244d/" rel="nofollow" target="_blank" title="http://www.kosmix.com/"&gt;Kosmix&lt;/a&gt;.
&lt;!--more--&gt;
По большому счету &lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; мало чем выделяется из множества
своих конкурентов, по своей структуре она состоит из сервера метаданных
и серверов блоков, доступ к системе производится средствами клиентской
библиотеки, предоставляющей соответствующий API. Список возможностей
файловой системы также вполне стандартен:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Инкрементальная масштабируемость.&lt;/em&gt; При добавлении дополнительных
    узлов в кластер, система сама адаптируется для вовлечения их в
    полноценную работу.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Стабильный доступ.&lt;/em&gt; Реплицируемость данных (по-умолчанию в трех
    экземплярах) позволяет гарантировать доступность данных вне
    зависимости от сбоев в работе отдельных узлов.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Балансировка блоков данных.&lt;/em&gt; Периодически сервер метаданных
    перераспределяет данные с целью более оптимального использования
    дискового пространства.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Целостность данных.&lt;/em&gt; Для обеспечения целостности данных вычисляются
    и сравниваются контрольные суммы блоков данных.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Кэширование.&lt;/em&gt; Для увеличения производительности используется
    кэширования на уровне клиентской библиотеки.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Прозрачная работа с недоступными узлами.&lt;/em&gt; Клиентская библиотека
    прозрачно для приложения переключается на альтернативный сервер с
    данными, если обнаруживает что один из них недоступен.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Поддержка языков программирования:&lt;/em&gt; &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;,
    &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Скрипты.&lt;/em&gt; С системой предоставляется набор скриптов для
    развертывания, запуска и остановки узлов.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Но написать этот пост меня подтолкнул вовсе не этот список. В
комментариях к &lt;a href="https://www.insight-it.ru/storage/2008/hadoop/"&gt;одной из предыдущих моих записей&lt;/a&gt;
читатели подняли тему о целесообразности использования &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;
для реализации &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt; в частности и &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; в
целом. В качестве альтернативы был предложен &lt;a href="/tag/c/"&gt;C++&lt;/a&gt; (только на
словах конечно же), аргументируя это тем, что такая реализация была бы
эффективнее. &lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; же как раз и является той самой
альтернативой &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt;, написанной на &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;.
&lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; тесно интегрируется с &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; с помощью
его интерфейсов для файловой системы. Это позволяет Hadoop-приложениям
незаметно работать с &lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; точно так же, как если бы на ее
месте была бы &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt;. Код для интеграции с Hadoop был выпущен
в виде патча к Hadoop-JIRA-1963, а начиная с Hadoop версии 0.15 этот код
входит в стандартный дистрибутив, ровно как и детальная инструкция по
интеграции.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 30 Mar 2008 23:06:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-03-30:storage/2008/fajjly-v-kosmose/</guid><category>C++</category><category>Hadoop</category><category>KFS</category><category>Kosmos Distributed File System</category><category>кластер</category><category>файловая система</category></item><item><title>Hadoop для разработчика</title><link>https://www.insight-it.ru//storage/2008/hadoop-dlya-razrabotchika/</link><description>&lt;p&gt;Для разработки приложений, работающих с использованием Hadoop, или же
алгоритмов для MapReduce framework'а совсем не нужен полномасштабный
кластер. На самом же деле для запуска всей системы, описанной мной в
&lt;a href="https://www.insight-it.ru/storage/2008/hadoop/"&gt;одном из предыдущих постов&lt;/a&gt;, вполне
достаточно одного компьютера и буквально минут 15 свободного времени,
как потратить их для решения этой задачи я Вам и поведаю.
&lt;!--more--&gt;
Рассказывать я буду на примере своего &lt;a href="/tag/gentoo-linux/"&gt;Gentoo
Linux&lt;/a&gt;, но большая часть этого повествования будет
справедлива и для других unix-like операционных систем.&lt;/p&gt;
&lt;h3 id="podgotovka"&gt;Подготовка&lt;/h3&gt;
&lt;p&gt;Перед тем, как приступить собственно говоря к установке
&lt;a href="https://www.insight-it.ru/goto/30a7481/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/"&gt;Hadoop&lt;/a&gt;, необходимо выполнить два
элементарных действия, необходимых для правильного функционирования
системы:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;открыть доступ одному из пользователей по &lt;code&gt;ssh&lt;/code&gt; к этому же
    компьютеру без пароля, можно например создать отдельного
    пользователя для этого &lt;code&gt;hadoop&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; useradd -m -n hadoop
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Далее действия выполняем от его имени:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; su hadoop
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Генерируем RSA-ключ для обеспечения аутентификации в условиях
отсутствия возможности использовать пароль:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; hadoop@localhost ~ &lt;span class="nv"&gt;$ &lt;/span&gt;ssh-keygen -t rsa -P &lt;span class="s2"&gt;""&lt;/span&gt;
Generating public/private rsa key pair.
Enter file in which to save the key &lt;span class="o"&gt;(&lt;/span&gt;/home/hadoop/.ssh/id_rsa&lt;span class="o"&gt;)&lt;/span&gt;:
Your identification has been saved in /home/hadoop/.ssh/id_rsa.
Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.
The key fingerprint is:
7b:5c:cf:79:6b:93:d6:d6:8d:41:e3:a6:9d:04:f9:85 hadoop@localhost
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;И добавляем его в список авторизованных ключей:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; cat ~/.ssh/id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Этого должно быть более чем достаточно, проверить работоспособность
соединения можно просто написав:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; ssh localhost
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Не забываем предварительно инициализировать &lt;strong&gt;sshd&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; /etc/init.d/sshd start
&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Помимо этого необходимо убедиться в наличии установленной JVM версии
    1.5.0 или выше, а также узнать директорию, где она располагается,
    вариантов сделать это множество, я нашел ее просто заглянув в самое
    логичное место - &lt;code&gt;/usr/lib&lt;/code&gt;, но при желании никто не может Вам
    помешать воспользоваться услугами, например, &lt;code&gt;slocate&lt;/code&gt;. Найденную
    директорию с JVM лучше запомнить или записать куда-нибудь, для меня
    она оказалась: &lt;code&gt;/usr/lib/jvm/sun-jdk-1.6&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="ustanovka"&gt;Установка&lt;/h3&gt;
&lt;p&gt;Установка начинается с получения копии исходного кода системы, способов
для этого существует несколько. Я перепробовал практически все, самую
адекватную версию мне удалось получить из SVN. Для ее получения
необходимо выполнить следующую команду:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;svn checkout http://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.16 ~
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;branch-0.16&lt;/strong&gt; - последняя доступная версия на данный момент, для
определения ее номера достаточно заглянуть &lt;a href="https://www.insight-it.ru/goto/99e3d37e/" rel="nofollow" target="_blank" title="http://svn.apache.org/repos/asf/hadoop/core/branches/"&gt;по тому же адресу&lt;/a&gt;
браузером. Предполагается, что Hadoop будет располагаться прямо в
&lt;code&gt;/home/hadoop&lt;/code&gt;, но запросто можно использовать и другую директорию.&lt;/p&gt;
&lt;p&gt;Сразу же стоит скомпилировать различные дополнительные компоненты
системы, особенно это актуально из-за &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt;, но и помимо
него соберется много чего интересного, например plug-in для отличной IDE
под названием &lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/b7976bc5/" rel="nofollow" target="_blank" title="http://www.eclipse.org"&gt;Eclipse&lt;/a&gt;&lt;/strong&gt; или &lt;a href="https://www.insight-it.ru/goto/9253da15/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/docs/r0.16.0/hod.html"&gt;Hadoop On
Demand&lt;/a&gt;. Задача
также элементарна:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; ~ &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; ant clean jar compile-contrib
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="nastroika"&gt;Настройка&lt;/h3&gt;
&lt;p&gt;Конфигурационные файлы можно редактировать в произвольном порядке, самое
главное ничего не забыть :)&lt;/p&gt;
&lt;h4&gt;conf/hadoop-env.sh&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;JAVA_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/usr/lib/jvm/sun-jdk-1.6
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Единственная обязательная переменная окружения - &lt;code&gt;JAVA_HOME&lt;/code&gt;,
здесь как раз пригодится заранее найденный путь до JVM, все
остальное - по желанию.&lt;/p&gt;
&lt;h4&gt;conf/hadoop-site.xml&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;hadoop.tmp.dir&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;/home/hadoop/data/${user.name}&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;A base for other temporary directories.&lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.default.name&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;hdfs://localhost:54310&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;The name of the default file system.  A URI whose
  scheme and authority determine the FileSystem implementation.  The
  uri's scheme determines the config property (fs.SCHEME.impl) naming
  the FileSystem implementation class.  The uri's authority is used to
  determine the host, port, etc. for a filesystem.&lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapred.job.tracker&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;localhost:54311&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;The host and port that the MapReduce job tracker runs
  at.  If "local", then jobs are run in-process as a single map
  and reduce task.
  &lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;dfs.replication&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;1&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  &lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Этот конфигурации файл является одним из ключевых, таким образом он
выглядит для конфигурации, состоящей из одного компьютера
(позаимствован из &lt;a href="https://www.insight-it.ru/goto/f1c9004a/" rel="nofollow" target="_blank" title="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/"&gt;англоязычного мануала&lt;/a&gt;
на ту же тему).&lt;/p&gt;
&lt;h4&gt;src/contrib/hbase/conf/hbase-site.xml&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;hbase.master&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;localhost:60000&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;The host and port that the HBase master runs at&lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;hbase.rootdir&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;/hbase&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;location of HBase instance in dfs&lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Как не сложно заметить, этот файл необходим для функционирования
&lt;strong&gt;HBase&lt;/strong&gt;, по-моему все просто и очевидно, &lt;code&gt;&amp;lt;description&amp;gt;&lt;/code&gt; говорят
сами за себя.&lt;/p&gt;
&lt;h3 id="zapusk"&gt;Запуск&lt;/h3&gt;
&lt;p&gt;Начать стоит с ознакомления с кратким описанием доступных команд Hadoop,
сделать это можно просто набрав &lt;code&gt;~/bin/hadoop&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Usage: hadoop &lt;span class="o"&gt;[&lt;/span&gt;--config confdir&lt;span class="o"&gt;]&lt;/span&gt; COMMAND
where COMMAND is one of:
  namenode -format     format the DFS filesystem
  secondarynamenode    run the DFS secondary namenode
  namenode             run the DFS namenode
  datanode             run a DFS datanode
  dfsadmin             run a DFS admin client
  fsck                 run a DFS filesystem checking utility
  fs                   run a generic filesystem user client
  balancer             run a cluster balancing utility
  jobtracker           run the MapReduce job Tracker node
  pipes                run a Pipes job
  tasktracker          run a MapReduce task Tracker node
  job                  manipulate MapReduce &lt;span class="nb"&gt;jobs&lt;/span&gt;
&lt;span class="nb"&gt;  &lt;/span&gt;version              print the version
  jar             run a jar file
  distcp   copy file or directories recursively
  daemonlog            get/set the log level &lt;span class="k"&gt;for&lt;/span&gt; each daemon
 or
  CLASSNAME            run the class named CLASSNAME
Most commands print &lt;span class="nb"&gt;help &lt;/span&gt;when invoked w/o parameters.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Первым делом необходимо отформатировать &lt;em&gt;Namenode&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;~/bin/hadoop namenode -format
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;И дело останется лишь за малым, запустить на выполнение пару
bash-скриптов, которые без вашего дальнейшего участия &lt;em&gt;инициализируют&lt;/em&gt;
всю систему, включая HBase:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;~/bin/hadoop/start-all.sh &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; ~/src/contrib/hbase/bin/start-hbase.sh
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Как только они закончат все необходимые действия, у Вас появится
возможность удостовериться, что все в порядке. Самым простым способом
является запуск клиента &lt;em&gt;Hbase Shell&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;~/bin/src/contrib/hbase/bin/hbase shell
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Если в ответ Вы получили соответствующее приглашение клиента, значит все
было сделано верно!&lt;/p&gt;
&lt;p&gt;Вот собственно говоря и все, псевдо-кластер функционирует, доступ к
HBase имеется, можно приступать к разработке :)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;P.S.:&lt;/strong&gt; Остановка системы производится по тому же принципу скриптами
&lt;code&gt;stop-all.sh&lt;/code&gt; и &lt;code&gt;stop-hbase.sh&lt;/code&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 26 Feb 2008 00:15:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-02-26:storage/2008/hadoop-dlya-razrabotchika/</guid><category>gentoo linux</category><category>Hadoop</category><category>HBase</category><category>HDFS</category><category>MapReduce</category><category>ПО</category><category>развертывание</category><category>разработка</category><category>установка</category></item><item><title>Hadoop</title><link>https://www.insight-it.ru//storage/2008/hadoop/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/30a7481/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/"&gt;Hadoop&lt;/a&gt; представляет собой платформу
для построения приложений, способных обрабатывать огромные объемы
данных. Система основывается на распределенном подходе к вычислениям и
хранению информации, основными ее особенностями являются:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Масштабируемость:&lt;/strong&gt; с помощью &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; возможно
    надежное хранение и обработка огромных объемов данных, которые могут
    измеряться петабайтами;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Экономичность:&lt;/strong&gt; информация и вычисления распределяются по
    &lt;a href="/tag/klaster/"&gt;кластеру&lt;/a&gt;, построенному на самом обыкновенном
    оборудовании. Такой кластер может состоять из тысяч узлов;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Эффективность:&lt;/strong&gt; распределение данных позволяет выполнять их
    обработку параллельно на множестве компьютеров, что существенно
    ускоряет этот процесс;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Надежность:&lt;/strong&gt; при хранении данных возможно предоставление
    избыточности, благодаря хранению нескольких копий. Такой подход
    позволяет гарантировать отсутствие потерь информации в случае сбоев
    в работе системы;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Кроссплатформенность:&lt;/strong&gt; так как основным языком программирования,
    используемым в этой системе является &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, развернуть
    ее можно на базе любой операционной системы, имеющей &lt;abbr title="Java Virtual Machine"&gt;JVM&lt;/abbr&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;h3 id="hdfs"&gt;HDFS&lt;/h3&gt;
&lt;p&gt;В основе всей системы лежит распределенная файловая система под
незамысловатым названием &lt;strong&gt;Hadoop Distributed File System&lt;/strong&gt;.
Представляет она собой вполне стандартную распределенную файловую
систему, но все же она обладает рядом особенностей:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Устойчивость к сбоям, разработчики рассматривали сбои в оборудовании
    скорее как норму, чем как исключение;&lt;/li&gt;
&lt;li&gt;Приспособленность к развертке на самом обыкновенном ненадежном
    оборудовании;&lt;/li&gt;
&lt;li&gt;Предоставление высокоскоростного потокового доступа ко всем данным;&lt;/li&gt;
&lt;li&gt;Настроена для работы с большими файлами и наборами файлов;&lt;/li&gt;
&lt;li&gt;Простая модель работы с данными: &lt;em&gt;один раз записали - много раз
    прочли&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;Следование принципу: &lt;em&gt;переместить вычисления проще, чем переместить
    данные&lt;/em&gt;;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Архитектура HDFS&lt;/h4&gt;
&lt;p&gt;Проще всего ее демонстрирует схема,
&lt;a href="https://www.insight-it.ru/goto/9c57006b/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/docs/current/images/hdfsarchitecture.gif"&gt;позаимствованная&lt;/a&gt; с официального сайта проекта и переведенная мной на руский:
&lt;img alt="Архитектура HDFS" class="responsive-img" src="https://www.insight-it.ru/images/hdfsarchitecture.jpg" title="Архитектура HDFS"/&gt;&lt;/p&gt;
&lt;p&gt;Действующие лица:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Namenode&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Этот компонент системы осуществляет всю работу с метаданными. Он
должен быть запущен только на одном компьютере в кластере. Именно он
управляет размещением информации и доступом ко всем данным,
расположенным на ресурсах кластера. Сами данные проходят с остальных
машин кластера к клиенту мимо него.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Datanode&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;На всех остальных компьютерах системы работает именно этот
компонент. Он располагает сами блоки данных в локальной файловой
системе для последующей передачи или обработки их по запросу
клиента. Группы узлов данных принято называть Rack, они
используются, например, в схемах репликации данных.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Клиент&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Просто приложение или пользователь, работающий с файловой системой.
В его роли может выступать практически что угодно.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Пространство имен &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt; имеет классическую иерархическую
структуру: пользователи и приложения имеют возможность создавать
директории и файлы. Файлы хранятся в виде блоков данных произвольной (но
одинаковой, за исключением последнего; по-умолчанию 64 mb) длины,
размещенных на &lt;strong&gt;Datanode&lt;/strong&gt;'ах. Для обеспечения отказоустойчивости блоки
хранятся в нескольких экземплярах на разных узлах, имеется возможность
настройки количества копий и алгоритма их распределения по системе.
Удаление файлов происходит не сразу, а через какое-то время после
соответствующего запроса, так как после получения запроса файл
перемещается в директорию &lt;strong&gt;/trash&lt;/strong&gt; и хранится там определенный период
времени на случай если пользователь или приложение передумают о своем
решении. В этом случае информацию можно будет восстановить, в противном
случае - физически удалить.&lt;/p&gt;
&lt;p&gt;Для обнаружения возникновения каких-либо неисправностей, &lt;strong&gt;Datanode&lt;/strong&gt;
периодически отправляют &lt;strong&gt;Namenode&lt;/strong&gt;'у сигналы о своей
работоспособности. При прекращении получения таких сигналов от одного из
узлов &lt;strong&gt;Namenode&lt;/strong&gt; помечает его как &lt;em&gt;"мертвый"&lt;/em&gt;, и прекращает какой-либо
с ним взаимодействие до возвращения его работоспособности. Данные,
хранившиеся на &lt;em&gt;"умершем"&lt;/em&gt; узле реплицируются дополнительный раз из
оставшихся &lt;em&gt;"в живых"&lt;/em&gt; копий и система продолжает свое функционирование
как ни в чем не бывало.&lt;/p&gt;
&lt;p&gt;Все коммуникации между компонентами файловой системы проходят по
специальным протоколам, основывающимся на стандартном &lt;strong&gt;TCP/IP&lt;/strong&gt;.
Клиенты работают с &lt;strong&gt;Namenode&lt;/strong&gt; с помощью так называемого
&lt;strong&gt;ClientProtocol&lt;/strong&gt;, а передача данных происходит по
&lt;strong&gt;DatanodeProtocol&lt;/strong&gt;, оба они &lt;em&gt;обернуты&lt;/em&gt; в &lt;strong&gt;Remote Procedure Call
(RPC)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Система предоставляет несколько интерфейсов, среди которых командная
оболочка &lt;strong&gt;DFSShell&lt;/strong&gt;, набор ПО для администрирования &lt;strong&gt;DFSAdmin&lt;/strong&gt;, а
также простой, но эффективный веб-интерфейс. Помимо этого существуют
несколько API для языков программирования: Java API, C pipeline, WebDAV
и так далее.&lt;/p&gt;
&lt;h3 id="mapreduce"&gt;MapReduce&lt;/h3&gt;
&lt;p&gt;Помимо файловой системы, &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; включает в себя framework
для проведения масштабных вычислений, обрабатывающих огромные объемы
данных. Каждое такое вычисление называется Job (задание) и состоит оно,
как видно из названия, из двух этапов:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Map&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Целью этого этапа является представление произвольных данных (на
практике чаще всего просто пары ключ-значение) в виде промежуточных
пар ключ-значение. Результаты сортируются и групируются по ключу и
передаются на следующий этап.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Reduce&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Полученные после &lt;strong&gt;map&lt;/strong&gt; значения используются для финального
вычисления требуемых данных. Практические любые данные могут быть
получены таким образом, все зависит от требований и функционала
приложения.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Задания выполняются, подобно файловой системе, на всех машинах в
кластере (чаще всего одних и тех же). Одна из них выполняет роль
управления работой остальных - &lt;strong&gt;JobTracker&lt;/strong&gt;, остальные же ее
бесприкословно слушаются - &lt;strong&gt;TaskTracker&lt;/strong&gt;. В задачи &lt;strong&gt;JobTracker&lt;/strong&gt;'а
входит составление расписания выполняемых работ, наблюдение за ходом
выполнения, и перераспределение в случае возникновения сбоев.&lt;/p&gt;
&lt;p&gt;В общем случае каждое приложение, работающее с этим framework'ом,
предоставляет методы для осуществления этапов &lt;strong&gt;map&lt;/strong&gt; и &lt;strong&gt;reduce&lt;/strong&gt;, а
также указывает расположения входных и выходных данных. После получения
этих данных &lt;strong&gt;JobTracker&lt;/strong&gt; распределяет задание между остальными
машинами и предоставляет клиенту полную информацию о ходе работ.&lt;/p&gt;
&lt;p&gt;Помимо основных вычислений могут выполняться вспомогательные процессы,
такие как составление отчетов о ходе работы, кэширование, сортировка и
так далее.&lt;/p&gt;
&lt;h3 id="hbase"&gt;HBase&lt;/h3&gt;
&lt;p&gt;&lt;img alt="HBase Logo" class="right" src="https://www.insight-it.ru/images/hbase-logo.png" title="HBase"/&gt;
В рамках &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; доступна еще и система хранения данных,
которую правда сложно назвать &lt;a href="/tag/subd/"&gt;СУБД&lt;/a&gt; в традиционном смысле
этого слова. Чаще проводят аналогии с проприетарной системой этого же
плана от &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; - &lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/12419d3d/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/hbase"&gt;HBase&lt;/a&gt; представляет собой
распределенную систему хранения больших объемов данных. Подобно
реляционным СУБД данные хранятся в виде таблиц, состоящих из строк и
столбцов. И даже для доступа к ним предоставляется язык запросов &lt;strong&gt;HQL&lt;/strong&gt;
(как ни странно - &lt;strong&gt;Hadoop Query Language&lt;/strong&gt;), отдаленно напоминающий
более распространенный &lt;a href="/tag/sql/"&gt;SQL&lt;/a&gt;. Помимо этого предоставляется
итерирующмй интерфейс для сканирования наборов строк.&lt;/p&gt;
&lt;p&gt;Одной из основных особенностей хранения данных в &lt;strong&gt;HBase&lt;/strong&gt; является
возможность наличия нескольких значений, соответствующих одной
комбинации таблица-строка-столбец, для их различения используется
информация о времени добавления записи. На концептуальном уровне таблицы
обычно представляют как набор строк, но физически же они хранятся по
столбцам, достаточно важный факт, который стоит учитывать при разработки
схемы хранения данных. Пустые ячейки не отображаются каким-либо образом
физически в хранимых данных, они просто отсутствуют. Существуют конечно
и другие нюансы, но я постарался упомянуть лишь основные.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HQL&lt;/strong&gt; очень прост по своей сути, если Вы уже знаете &lt;a href="/tag/sql/"&gt;SQL&lt;/a&gt;,
то для изучения его Вам понадобится лишь просмотреть по диагонали
коротенький вывод команды &lt;strong&gt;help;&lt;/strong&gt;, занимающий всего пару экранов в
консоли. Все те же &lt;strong&gt;SELECT&lt;/strong&gt;, &lt;strong&gt;INSERT&lt;/strong&gt;, &lt;strong&gt;UPDATE&lt;/strong&gt;, &lt;strong&gt;DROP&lt;/strong&gt; и так
далее, лишь со слегка измененным синтаксисом.&lt;/p&gt;
&lt;p&gt;Помимо обычно командной оболочки &lt;strong&gt;HBase Shell&lt;/strong&gt;, для работы с &lt;strong&gt;HBase&lt;/strong&gt;
также предоставлено несколько API для различных языков программирования:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/f059ad5e/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/hbase/docs/current/api/index.html"&gt;Java&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/e44fcd5/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/Jython"&gt;Jython&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8282e2e2/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/HbaseRest"&gt;REST&lt;/a&gt; и&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/185bb3f7/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/ThriftApi"&gt;Thrift&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="zakliuchenie"&gt;Заключение&lt;/h3&gt;
&lt;p&gt;&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; является отличным решением для построения
высоконагруженных приложений, которое уже активно используется
&lt;a href="https://www.insight-it.ru/goto/ab057c2a/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/PoweredBy"&gt;множеством интернет-проектов&lt;/a&gt;.
В последующих постах на эту тему я постараюсь описать процесс
развертывания этой системы и написания приложений, работающих по
принципу &lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;. Не пропустить момент их публикации
Вам может помочь подписка на &lt;a href="/feed/"&gt;RSS-ленту&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 22 Feb 2008 22:41:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-02-22:storage/2008/hadoop/</guid><category>Hadoop</category><category>HBase</category><category>HDFS</category><category>Java</category><category>MapReduce</category><category>архитектура</category><category>информационные технологии</category><category>кластер</category><category>Масштабируемость</category><category>распределенные вычисления</category><category>технология</category></item></channel></rss>