<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Insight IT</title><link>https://www.insight-it.ru/</link><description></description><atom:link href="https://www.insight-it.ru/tag/klaster/feed/index.xml" rel="self"></atom:link><lastBuildDate>Sat, 19 Feb 2011 21:23:00 +0300</lastBuildDate><item><title>Новое поколение MapReduce в Apache Hadoop</title><link>https://www.insight-it.ru//storage/2011/novoe-pokolenie-mapreduce-v-apache-hadoop/</link><description>&lt;p&gt;В большом бизнесе использование нескольких больших кластеров с
финансовой точки зрения более&amp;nbsp;эффективно, чем много маленьких. Чем
больше машин в кластере, тем большими наборами данных он может
оперировать, больше задач могут выполняться одновременно. Реализация
&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt; в &lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt;
&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; столкнулась с потолком масштабируемости на уровне
около 4000 машин в кластере. Разрабатывается следующее поколение Apaсhe
Hadoop MapReduce, &amp;nbsp;в котором появится общий планировщик ресурсов и
отдельный мастер для каждой отдельной задач, управляющий выполнением
программного кода. Так как простой оборудования по техническим причинам
обходится дорого на таком масштабе, высокий уровень доступности
проектируется с самого начала, ровно как и безопасность и
многозадачность, необходимые для поддержки одновременного использования
большого кластера многими пользователями. Новая архитектура также будет
более инновационной, гибкой и эффективной с точки зрения использования
вычислительных ресурсов.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="predistoriia"&gt;Предистория&lt;/h2&gt;
&lt;p&gt;Текущая реализация Hadoop MapReduce устаревает на глазах. Основываясь на
текущих тенденциях в размерах кластеров и нагрузок на них, JobTracker
требует кардинальных доработок, чтобы исправить его дефекты в области
масштабируемости, потребления памяти, многопоточности, надежности и
производительности. С точки зрения работы с Hadoop при каждом обновлении
кластера (даже если это просто багфикс), абсолютно все компоненты
кластера, так и приложений, которые на нем работают, должны быть
обновлены одновременно. Это так же очень неудобно, так как каждый раз
необходимо тестировать все приложения на совместимость с новой версией.&lt;/p&gt;
&lt;h2 id="trebovaniia"&gt;Требования&lt;/h2&gt;
&lt;p&gt;Прежде чем кардинально что-то менять в Hadoop mapreduce, необходимо
понять какие же основные требования предъявляются к вычислительным
кластерам на практике. Наиболее значительными требованиями к Hadoop
следующего поколения являются:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Надежность&lt;/li&gt;
&lt;li&gt;Доступность&lt;/li&gt;
&lt;li&gt;Масштабируемость - кластеры из как минимум 10 тысяч машин, 200 тысяч
    вычислительных ядер и даже больше&lt;/li&gt;
&lt;li&gt;Обратная и прямая совместимость - возможность быть уверенным, что
    приложение будет работать на новой версии так же, как оно работало
    на старой&lt;/li&gt;
&lt;li&gt;Контроль над обновлениями&lt;/li&gt;
&lt;li&gt;Предсказуемые задержки&lt;/li&gt;
&lt;li&gt;Эффективное использование ресурсов&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Среди менее значительных требований:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Поддержка альтернативных парадигм разработки (помимо MapReduce)&lt;/li&gt;
&lt;li&gt;Поддержка сервисов с коротким жизненным циклом&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Если учесть перечисленные выше требования, то становится очевидно, что
инфраструктура обработки данных в Hadoop должна быть кардинальным
образом изменена. В сообществе Hadoop люди в целом приходят к общему
мнению, что текущая архитектура MapReduce не способна решить текущие
задачи, которые перед ней ставится, и что требуется кардинальный
рефакторинг кодовой базы.&lt;/p&gt;
&lt;h2 id="mapreduce-sleduiushchego-pokoleniia"&gt;MapReduce следующего поколения&lt;/h2&gt;
&lt;p&gt;Фундаментальной идеей смены архитектуры является разделение двух
основных функций JobTracker'а на два отдельных части:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;управление ресурсами;&lt;/li&gt;
&lt;li&gt;планирования и мониторинга задач.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В итоге появляется несколько новых ролей:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ResourceManager&lt;/strong&gt; управляет глобальным распределением
    вычислительных ресурсов между приложениями;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplicationMaster&lt;/strong&gt; управляет планированием и координацией внутри
    приложения;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NodeManager&lt;/strong&gt; управляет процессами в рамках одной машины.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ApplicationMaster представляет собой библиотеку, с помощью которой можно
получить у ResourceManager квоту на вычислительные ресурсы и работать с
NodeManager(ами) для выполнения и мониторинга задач.&lt;/p&gt;
&lt;p&gt;ResourceManager поддерживает иерархическим очереди приложений, которым
может гарантированно выделяться некоторый процент ресурсов кластера. Его
функционал ограничивается планированием, никакого мониторинга и
отслеживания задач не происходит, а также нет никаких гарантий
перезапуска задач, провалившихся из-за проблем с оборудованием или
кодом. Планирование основывается на требованиях, которые выставляет
приложение с помощью ряда запросов ресурсов (среди них: запросы на
вычислительные ресурсы, память, дисковое пространство, сетевой доступ и
т.п.). Обратите внимание, что это значительное изменение по сравнению с
текущей моделью слотов фиксированного размера, которая является одной из
основных причин неэффективного использования ресурсов кластера на данный
момент.&lt;/p&gt;
&lt;p&gt;NodeManager - это агент, который работает на каждой машине и несет
ответственность за запуск контейнеров приложений, мониторинг
используемых ими ресурсов (плюс отчет планировщику).&lt;/p&gt;
&lt;p&gt;По одному ApplicationMaster запускается для каждого приложения, они
ответственны за запрос необходимых ресурсов у планировщика, запуск
задач, отслеживание статусов, мониторинг прогресса и обработку сбоев.&lt;/p&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Следующее поколение MapReduce" class="responsive-img" src="https://www.insight-it.ru/images/mapreduce-nextgen.jpg" title="Следующее поколение MapReduce"/&gt;&lt;/p&gt;
&lt;h2 id="uluchsheniia-po-sravneniiu-s-tekushchei-realizatsiei-mapreduce"&gt;Улучшения по сравнению с текущей реализацией MapReduce&lt;/h2&gt;
&lt;h3 id="masshtabiruemost"&gt;Масштабируемость&lt;/h3&gt;
&lt;p&gt;Разделение управления ресурсами и прикладными задачами позволяет
горизонтально расширять кластер более просто и эффективно. JobTracker
проводит значительную часть времени пытаясь управлять жизненным циклом
каждого приложения, что часто может приводить к различным
происшествиям - переход к отдельному менеджеру для каждого приложения
является значительным шагом вперед.&lt;/p&gt;
&lt;p&gt;Масштабируемость особенно важна в свете текущих трендов в оборудовании -
на данный момент Hadoop может быть развернут на кластере из 4000 машин.
Но 4000 средних машин 2009го года (т.е. по 8 ядер, 16Гб памяти, 4Тб
дискового пространства) только вдвое менее &amp;nbsp;ресурсоемки, чем 4000 машин
2011го года (16 ядер, 48гб памяти, 24Тб дискового пространства). Помимо
этого с точки зрения операционных издержек было выгоднее работать в еще
больших кластере от 6000 машин и выше.&lt;/p&gt;
&lt;h3 id="dostupnost"&gt;Доступность&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ResourceManager использует&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/e7095d3/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/zookeeper/"&gt;Apache ZooKeeper&lt;/a&gt; для обработки сбоев.
    Когда ResourceManager перестает работать, аналогичный процесс может
    быстро запуститься на другой машине благодаря тому, что состояние
    кластера было сохранено в ZooKeeper. При таком сценарии все
    запланированные и выполняющиеся приложения максимум лишь
    перезапустятся.&lt;/li&gt;
&lt;li&gt;ApplicationMaster - поддерживается создание точек восстановления на
    уровне приложений. ApplicationMaster может восстановить работу из
    состояния, сохраненного в HDFS, в случае сбоя.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="sovmestimost-protokola"&gt;Совместимость протокола&lt;/h3&gt;
&lt;p&gt;Это позволит различным версиям клиентов и серверов Hadoop общаться между
собой. Помимо решения многих существующих проблем с обновлением, в
будующих релизах появится возможность последовательного обновления кода
без простоя системы в целом - очень большое достижения с точки зрения
системного администрирования.&lt;/p&gt;
&lt;h3 id="innovatsionnost-i-gibkost"&gt;Инновационность и гибкость&lt;/h3&gt;
&lt;p&gt;Основным плюсом предложенной архитектуры является тот факт, что
MapReduce по сути становится просто пользовательской библиотекой.
Вычислительная же система (ResourceManager и NodeManager) становятся
полностью независимыми от специфики MapReduce.&lt;/p&gt;
&lt;p&gt;Клиенты получат возможность одновременного использования разных версий
MapReduce в одном и том же кластере. Это становится тривиальным, так как
отдельная копия ApplicationMaster'а запускается для каждого приложения.
Это дает гибкость в исправлении багов, улучшений и новых возможностей,
так как полное обновление кластер перестает быть обязательной
процедурой. Это позволяет клиентам обновлять их приложения до новых
версий MapReduce вне зависимости от обновлений кластера.&lt;/p&gt;
&lt;h3 id="effektivnost-ispolzovaniia-vychislitelnykh-resursov"&gt;Эффективность использования вычислительных ресурсов&lt;/h3&gt;
&lt;p&gt;ResourceManager использует общую концепцию для управления ресурсами и
планирования по отношению к каждому конкретному приложению. Каждая
машина в кластере на концептуальном уровне рассматривается просто как
набор ресурсов: память, процессор, ввод-вывод и др. Все машины
взаимозаменяемы и приложение может быть назначено на любую из них,
основываясь на доступных и запрашиваемых ресурсах. При этом приложения
работают в контейнерах, изолированно от других приложений, что дает
сильную поддержку многозадачности.&lt;/p&gt;
&lt;p&gt;Таким образом эта схема избавляет от текущего механизма map и reduce
слотов в Hadoop, который негативно влияет на эффективную утилизацию
вычислительных ресурсов.&lt;/p&gt;
&lt;h3 id="podderzhka-drugikh-paradigm-programmirovaniia-pomimo-mapreduce"&gt;Поддержка других парадигм программирования помимо MapReduce&lt;/h3&gt;
&lt;p&gt;В предложенной архитектуре используется общий механизм вычислений, не
привязанный конкретно к MapReduce, что позволит использовать и другие
парадигмы. Имеется возможность реализовать собственный
ApplicationMaster, способный запрашивать ресурсы у ResourceManager и
использовать их в соответствии с задачей, при этом сохраняются общие
принципы изоляции и гарантированного наличия полученных ресурсов. Среди
потенциально поддерживаемых парадигм можно назвать MapReduce, MPI,
Мaster-Worker, итеративные модели. Все они могут одновременно работать
на одном и том же кластере. Это особенно актуально для приложений
(например К-средний или Page Rank), где &lt;a href="https://www.insight-it.ru/python/2011/piccolo-postroenie-raspredelennykh-sistem-v-11-raz-bystree-hadoop/"&gt;другие подходы более чем на порядок эффективнее&lt;/a&gt; MapReduce.&lt;/p&gt;
&lt;h2 id="vyvody_1"&gt;Выводы&lt;/h2&gt;
&lt;p&gt;Apache Hadoop, и в частности Hadoop MapReduce - очень успешный
opensource проект по обработке больших объемов данных. Предложенный
Yahoo путь его переработки направлен на исправление недостатков
архитектуры текущей реализации, при этом повышая доступность,
эффективность использования ресурсов и предоставляя поддержку других
парадигм распределенных вычислений.&lt;/p&gt;
&lt;p&gt;Осталось дело за малым - собственно реализовать задуманное! :)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/336fc03c/" rel="nofollow" target="_blank" title="http://developer.yahoo.com/blogs/hadoop/posts/2011/02/mapreduce-nextgen/"&gt;Источник информации&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="/feed/"&gt;Подписаться на RSS можно здесь.&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 19 Feb 2011 21:23:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-02-19:storage/2011/novoe-pokolenie-mapreduce-v-apache-hadoop/</guid><category>Apache</category><category>Apache Hadoop</category><category>Hadoop</category><category>архитектура</category><category>кластер</category><category>кластеризация</category><category>кластерные вычисления</category><category>Масштабируемость</category><category>разработка</category><category>технологии</category></item><item><title>Еще раз про HBase</title><link>https://www.insight-it.ru//storage/2008/eshe-raz-pro-hbase/</link><description>&lt;p&gt;Некоторое время назад &lt;a href="https://www.insight-it.ru/goto/5bbb805b/" rel="nofollow" target="_blank" title="http://neuronus.blogspot.com"&gt;Neuronus&lt;/a&gt; в одном
из комментариев к посту &lt;a href="https://www.insight-it.ru/storage/2008/hadoop-vozvrashhaetsya/"&gt;"Hadoop возвращается"&lt;/a&gt; не согласился с
моим кратким определением HBase как "нереляционная база данных"
(позаимствованным, собственно говоря, откуда-то с официального портала
продукта). Этот факт подтолкнул меня попытаться найти более корректное
определение в англоязычных источниках информации, получилось вполне
успешно. Хочется прочитать более детально что к чему? Вперед!
&lt;!--more--&gt;
Если Вам уже приходилось иметь дело с этой системой,возможно Вы уже
поняли, что самым сложным этапом работы является просто-напросто
осознавание того, чем она на самом деле является. Обычно приходится
мысленно отказаться от всех привычек, доставшихся при работе с
традиционный RDBMS, и начинать постигать базовые принципы организации
хранения данных с нуля.&lt;/p&gt;
&lt;p&gt;Стоит напомнить, что проект позиционируется как opensource реализация
&lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt; от &lt;a href="/tag/google/"&gt;Google&lt;/a&gt;. Да, проекты
реализованы разными людьми на разных языках программирования, но
общие идеи и принципы функционирования у них сильно пересекаются.
Наиболее значимой общей характеристикой у них является очень схожие
модели данных (о чем
&lt;a href="https://www.insight-it.ru/goto/aef0a732/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/HbaseArchitecture#head-daee3a0ce7a6892096ffb43f3cc3e0310d047f48"&gt;упоминается&lt;/a&gt;
в вики HBase), а в свою очередь &lt;a href="https://www.insight-it.ru/goto/8667b351/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/bigtable.html"&gt;в документации&lt;/a&gt; BigTable она описывается очень четко и определенно, точно определяя чем эти продукты по сути являются:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A Bigtable is a sparse, distributed, persistent multidimensional sorted
map.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Предлагаю по аналогии со &lt;a href="https://www.insight-it.ru/goto/af69f471/" rel="nofollow" target="_blank" title="http://jimbojw.com/wiki/index.php?title=Understanding_HBase_and_BigTable"&gt;статьей в вики&lt;/a&gt;,
послужившей основой для данного поста, разбить это определение на
отдельные слова и последовательно пройтись по ним, попутно составляя в
голове полную картину.&lt;/p&gt;
&lt;h3 id="map"&gt;map&lt;/h3&gt;
&lt;p&gt;За этим термином нет четкого устоявшегося обозначения в русском языке
(да и в английском тоже все далеко не так однозначно), математики обычно
называют это &lt;em&gt;отображением одного множества в другое&lt;/em&gt;, в то время как
если Вы знакомы с программированием, то Вам наверняка больше знакомы
будут более знакомы такие обозначения, как &lt;em&gt;ассоциативный массив&lt;/em&gt;
(&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;), &lt;em&gt;словарь&lt;/em&gt; (&lt;a href="/tag/python/"&gt;Python&lt;/a&gt;), &lt;em&gt;хэш&lt;/em&gt;
(&lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt;) или &lt;em&gt;объект&lt;/em&gt; (&lt;a href="/tag/javascript/"&gt;JavaScript&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;По сути же имеется ввиду просто набор однозначно соответствующих пар
ключ-значение, в роли которых выступают массивы байт. Во все той же
статейке в вики все очень наглядно демонстрируется примерами в нотации
&lt;abbr title="JavaScript Object Notation"&gt;JSON&lt;/abbr&gt;, позволю себе тоже приводить аналогичные примеры. В &lt;abbr title="JavaScript Object Notation"&gt;JSON&lt;/abbr&gt; наш &lt;strong&gt;map&lt;/strong&gt; выглядел бы следующим образом:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;"qqqq"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"some"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"abc"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"sample"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"zz"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"JSON"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"123"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"map"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"mnbvcxz"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"looks like this"&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="persistent"&gt;persistent&lt;/h3&gt;
&lt;p&gt;Это прилагательное обозначает всего лишь "постоянный", то есть в данном
контексте оно говорит только о том, что данная система не зависит от
использующих ее приложений, а также хранится на устройствах постоянного
хранения данных, а не в оперативной памяти.&lt;/p&gt;
&lt;h3 id="distributed"&gt;distributed&lt;/h3&gt;
&lt;p&gt;Распределенность этих систем можно рассматривать с двух точек зрения:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Как HBase, так и BigTable сами по себе могут функционировать на
    большом количестве серверов, которые можно разделить на две большие
    категории: master и slave. Slave сервера собственно выполняют всю
    работу с данными, а master - лишь только координируют их действия и
    управляют процессом в целом. Этот факт обеспечивают высокую степень
    устойчивости к сбоям (в HBase правда количество master-серверов
    ограничено одним, что представляет собой единственную точку, сбой в
    которой приведет к отказу всей системы, но это лишь временная
    проблема, которую наверняка устранят в следующих версиях), а также
    существенно облегчает масштабируемость всей системы так как
    добавление дополнительных серверов (а значит и увеличение
    производительности и вместительности системы) достаточно тривиально,
    безболезненно и не мешает общему ее функционированию.&lt;/li&gt;
&lt;li&gt;Помимо этого каждая из этих систем обычно использует для хранения
    данных кластерную файловую систему (HBase - &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt;, а
    BigTable - &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;), которые тоже по своей природе являются
    распределенными и функционируют по схожему принципу, обеспечивая
    дополнительную сохранность данных, реплицируя их в нескольких
    экземплярах на нескольких серверах (обычно трех).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="sorted"&gt;sorted&lt;/h3&gt;
&lt;p&gt;HBase и BigTable не строят никакие индексы для ускорения процесса
извлечения данных, единственное используемое в них правило заключается в
следующем: каждый slave-сервер в системе отвечает за определенный
диапазон ключей (от и до определенных его значений), и держит все записи
в строгом лексикографическом порядке по ключам (заметьте: сортировку
значений никто не гарантирует!). Продолжая пример с &lt;abbr title="JavaScript Object Notation"&gt;JSON&lt;/abbr&gt; это выглядело
бы примерно вот так:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;"123"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"map"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"abc"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"sample"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"mnbvcxz"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"looks like this"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"qqqq"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"some"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"zz"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"JSON"&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Этим фактом можно активно пользоваться при планировании использовании
системы, например если в качестве ключей планируется использовать
доменные именные имена, то имеет смысл использовать их в "развернутом"
виде, например: "com.example.www" вместо "www.example.com". Это почти
наверняка обеспечит попадание всех поддоменов одного и того же домена на
один slave-сервер, а также группировку доменов по зонам.&lt;/p&gt;
&lt;h3 id="multidimensional"&gt;multidimensional&lt;/h3&gt;
&lt;p&gt;До сих пор ключ интерпретировался нами как нечто единое и неделимое, но
на самом деле в данной ситуации это далеко не так. На самом деле
пространство имен HBase и BigTable имеет несколько пространств, по
аналогии с трехмерным материальным пространством, где есть ширина,
высота и глубина. Если мы попытаемся представить это с помощью &lt;abbr title="JavaScript Object Notation"&gt;JSON&lt;/abbr&gt;, то
это будет выглядеть как набор вложенных простых map'ов:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;"table-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="p"&gt;{&lt;/span&gt;
     &lt;span class="s2"&gt;"column-family-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
     &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"column-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
           &lt;span class="s2"&gt;"row-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
           &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-2"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-3"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-3"&lt;/span&gt;
           &lt;span class="p"&gt;},&lt;/span&gt;
           &lt;span class="s2"&gt;"row-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
           &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-4"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-5"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-3"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-6"&lt;/span&gt;
           &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="s2"&gt;"column-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
           &lt;span class="s2"&gt;"row-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
           &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-3"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-3"&lt;/span&gt;
           &lt;span class="p"&gt;},&lt;/span&gt;
           &lt;span class="s2"&gt;"row-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
           &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-4"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-5"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-3"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-6"&lt;/span&gt;
           &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
     &lt;span class="s2"&gt;"column-family-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
     &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"column-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
           &lt;span class="c1"&gt;// ...&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="s2"&gt;""&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
           &lt;span class="s2"&gt;"row-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"some-value"&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="c1"&gt;// ...&lt;/span&gt;
     &lt;span class="p"&gt;}&lt;/span&gt;
     &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Как можно увидеть из примера, таких пространств используется пять:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;таблицы;&lt;/li&gt;
&lt;li&gt;наборы столбцов;&lt;/li&gt;
&lt;li&gt;столбцы;&lt;/li&gt;
&lt;li&gt;строки;&lt;/li&gt;
&lt;li&gt;время;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Таким образом, каждое значение в хранилище данных однозначно
соответствует ключу, состоящему из пяти компонентов, например в примере
значению "value-5" соответствует ключ, состоящий из:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;table-1;&lt;/li&gt;
&lt;li&gt;column-family-1;&lt;/li&gt;
&lt;li&gt;column-1;&lt;/li&gt;
&lt;li&gt;row-2;&lt;/li&gt;
&lt;li&gt;timestamp-2;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Принцип очень похож на используемый в более привычных базах данных, с
той лишь разницей, что добавляется еще и время (которое обычно
представляется в виде целочисленного значения, обозначающего количество
секунд с начала эпохи). Изначально оно задумывалось для предоставления
возможности отследить историю изменения данных, но этому дополнительному
измерению можно найти и массу нестандартных применений, например
используя его как самый обыкновенный стек.&lt;/p&gt;
&lt;p&gt;Хочется обратить внимание, что ассортимент наборов столбцов (column
family) указывается при создании таблиц, и изменения в них должны
производиться с помощью специального запроса, в то время как сами
столбцы являются динамическими и для его создания достаточно лишь
добавить в него данные.&lt;/p&gt;
&lt;h3 id="sparse"&gt;sparse&lt;/h3&gt;
&lt;p&gt;Развивая мысль предыдущего абзаца, можно понять, что наличие значения
столбца в каждой строке вовсе не обязательно, оно запросто может и
отсутствовать вовсе. Таким образом каждая строка может содержать
произвольное количество значений для столбцов в рамках одной column
family, ровно как может и не содержать их вовсе. Несуществующие данные
не хранятся в виде какого-либо NULL-значения, они просто отсутствуют.
Запрос на несуществующие данные просто вернет пустой результат. Если же
взглянуть с другой стороны, то тот же самый факт можно представить и как
возможность наличия пробелов в списке ключей строк.&lt;/p&gt;
&lt;h3 id="i-naposledok"&gt;И напоследок...&lt;/h3&gt;
&lt;p&gt;хочется сказать, что все это лишь дело терминологии, ровно то же самое
можно подразумевать и под краткой фразой "нереляционная база данных", не
смотря на то, что она существенно менее точна и полноценна. В данном
контексте самое главное лишь чтобы люди просто понимали друг друга.
Надеюсь после прочтения этого поста в вашем сознании сложилась четкая
картина этого продукта и предоставляемых им возможностей, которая Вам
пригодится как для "общего развития", так и для потенциального
практического применения этого продукта. Если остались неясные моменты -
смело оставляйте комментарии. &lt;a href="/feed/"&gt;Традиционная подписка на RSS&lt;/a&gt; -
приветствуется :)&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 27 Aug 2008 19:29:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-08-27:storage/2008/eshe-raz-pro-hbase/</guid><category>Hadoop</category><category>HBase</category><category>map</category><category>кластер</category></item><item><title>GlusterFS</title><link>https://www.insight-it.ru//storage/2008/glusterfs/</link><description>&lt;p&gt;&lt;img alt="GlusterFS Logo" class="right" src="https://www.insight-it.ru/images/glusterfs-logo.png" title="GlusterFS"/&gt;
&lt;a href="https://www.insight-it.ru/goto/12ccc1c7/" rel="nofollow" target="_blank" title="http://www.gluster.org/glusterfs.php"&gt;GlusterFS&lt;/a&gt; представляет собой
кластерную файловую систему, способную масштабироваться для хранения
далеко не одного петабайта данных. Как и многие другие кластерные
файловые системы, GlusterFS агрегирует дисковое пространство большого
количества машин в одну общую параллельную сетевую файловую систему
через Infiniband RDMA или TCP/IP соединение. Обычно в качестве
аппаратной основы для этой файловой системы используется ничем не
выдающееся недорогое серверное оборудование, в полной мере реализуя
принцип программного построения стабильности при использовании на
ненадежном оборудовании.
&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Кластерные файловые системы еще не достаточно приспособлены для
использования на крупных предприятиях: обычно процесс их развертывания и
поддержания в работающем состоянии не так уж прост. Но зато они отлично
масштабируются и достаточно дешевы, ведь для них достаточно самого
простого серверного оборудования и opensource операционных систем и
програмного обеспечения. Основной целью разработчиков
&lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; как раз и является построение кластерной
файловой системы, адаптированной для использования в рамках серьезных
компаний.&lt;/p&gt;
&lt;p&gt;Список основных ее особенностей по большей части мало чем отличается от
других кластерных файловых систем:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Состоит из клиентской и серверной частей. Клиентская часть позволяет
    монтировать файловую систему, а серверная - &lt;strong&gt;glusterfsd&lt;/strong&gt; -
    экспортировать в нее локальное дисковое пространство.&lt;/li&gt;
&lt;li&gt;Масштабируемость близка к &lt;em&gt;O(1)&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Широкий спектр возможностей за счет использования модульной
    архитектуры.&lt;/li&gt;
&lt;li&gt;Имеется возможность восстановления файлов и директорий из файловой
    системы даже без ее инициализации.&lt;/li&gt;
&lt;li&gt;Отсутствие централизованного сервера метаданных, что делает ее более
    устойчивой к потенциальным сбоям.&lt;/li&gt;
&lt;li&gt;Расширяемый интерфейс выполнения задач, с поддержкой загрузки
    модулей в зависимости от особенностей выполнения пользователями
    операций по работе с данными.&lt;/li&gt;
&lt;li&gt;Расширяющий функциональность механизм трансляторов.&lt;/li&gt;
&lt;li&gt;Поддержка &lt;em&gt;Infiniband RDMA&lt;/em&gt; и &lt;em&gt;TCP/IP&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Возможность автоматического восстановления в случае сбоев.&lt;/li&gt;
&lt;li&gt;Полностью реализована на уровне приложений, что упрощает ее
    поддержание в рабочем состоянии, портирование и дальнейшую
    разработку.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Но некоторые моменты все же заслуживают отдельного внимания:&lt;/p&gt;
&lt;h4&gt;Совместимость&lt;/h4&gt;
&lt;p&gt;Как уже упоминалось, файловая система реализована полностью на
уровне пользовательских приложений, что делает возможным ее
монтирование без каких-либо дополнительных патчей в ядре
операционной системы, единственное требование к нему: поддержка
FUSE. Серверная часть &lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; может
функционировать на любой POSIX-совместимой операционной системе и
протестирована на &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;, &lt;a href="/tag/freebsd/"&gt;FreeBSD&lt;/a&gt;,
&lt;a href="/tag/solaris/"&gt;OpenSolaris&lt;/a&gt;, в отличии от клиентской части, которая
может работать только в &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Модули&lt;/h4&gt;
&lt;p&gt;В виде модулей реализованы различные варианты выполнения
основополагающих операций: передачи данных и балансировки нагрузки в
рамках кластера. Транспортные модули обеспечивают передачу данных по
различным типам соединений:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TCP/IP&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Infiniband-verbs&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Infiniband-&lt;abbr title="Socket Direct Protocol"&gt;SDP&lt;/abbr&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Балансировка нагрузки может выполняться по следующим алгоритмам:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="Adaptive Least Usage"&gt;ALU&lt;/abbr&gt;&lt;/strong&gt; - использует
    целый ряд факторов, включающий объем свободного локального
    дискового пространства, активность операций чтения и записи,
    количество одновременно открытых файлов, скорость физического
    вращения дисков. Значимость, придаваемая каждому из показателей,
    может достаточно гибко настраиваться.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="Round Robin"&gt;RR&lt;/abbr&gt;&lt;/strong&gt; - по очереди размещает
    файлы последовательно на каждом узле, после чего начинает
    процесс заново, образуя своеобразный цикл. Этот метод эффективен
    если файлы имеют примерно одинаковый размер, а узлы кластера -
    одинаковый размер экспортированного локального дискового
    пространства.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Random&lt;/strong&gt; - распределяют файлы случайным образом.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="Non-Uniform File Access"&gt;NUFA&lt;/abbr&gt;&lt;/strong&gt; -
    приоритет отдается созданию файлов локально, а не на других
    узлах кластера.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Switch&lt;/strong&gt; - располагает файлы по определенным указанным
    особенностям имен файлов, по аналогии со &lt;strong&gt;switch(filename)&lt;/strong&gt; в
    программировании, обычно в качестве критерия распределения
    файлов имеет смысл использовать их расширение.&lt;/li&gt;
&lt;/ul&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Трансляторы&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Они представляют собой очень мощный механизм для расширения
возможностей GlusterFS, сама идея трансляторов бала позаимствована у
&lt;a href="https://www.insight-it.ru/goto/a8e9005c/" rel="nofollow" target="_blank" title="http://hurd.gnu.org"&gt;GNU/Hurd&lt;/a&gt; и заключается она в загрузке
бинарных библиотек (.so) в процессе работы системы в зависимости от
использованных настроек и использовании их в виде своеобразной
цепочки обработчиков при работе с файлами как на серверной, так и на
клиентской стороне. В &lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; практически все
дополнительные возможности реализованы именно виде трансляторов,
начиная от дополнений, увеличивающих производительность, заканчивая
средствами отладки. Вкратце перечислю основные из них:
+   &lt;strong&gt;&lt;abbr title="Automatic File Replication"&gt;AFR&lt;/abbr&gt;&lt;/strong&gt; - автоматическая репликация файлов.
+   &lt;strong&gt;Stripe&lt;/strong&gt; - разбивает файлы на блоки фиксированного размера.
+   &lt;strong&gt;Unify&lt;/strong&gt; - объединяет несколько узлов кластера в один большой
    виртуальный узел, один узел выделяется для обеспечения
    внутреннего namespace. Директории создаются на всех узлах,
    составляющих unify, а каждый файл - лишь на одном (если не
    используется &lt;abbr title="Automatic File Replication"&gt;AFR&lt;/abbr&gt;).
+   &lt;strong&gt;Trace&lt;/strong&gt; - предоставляют информацию для отладки в виде
    дополнительных записей в лог.
+   &lt;strong&gt;Filter&lt;/strong&gt; - фильтрация файлов на основании их имен и/или
    атрибутов.
+   &lt;strong&gt;Posix-locks&lt;/strong&gt; - обеспечивает POSIX блокировку записей
    независимую от используемой системы хранения.
+   &lt;strong&gt;Trash&lt;/strong&gt; - предоставляет функциональность сопоставимую с
    libtrash (или "корзиной" - если так понятнее).
+   &lt;strong&gt;Fixed-id&lt;/strong&gt; - обеспечивает доступ только для пользователей с
    определенными UID и GUID.
+   &lt;strong&gt;Posix&lt;/strong&gt; - соединяет GlusterFS с низлежащей локальной файловой
    системой.
+   &lt;strong&gt;rot-13&lt;/strong&gt; - транслятор обеспечивает возможность шифрования и
    дешифрования данных по примитивному одноименному алгоритму.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Список возможностей, обеспечиваемых широким набором модулей и
транслятором, впечатляет, большинство других opensource кластерных
файловых систем не могут похвастаться подобной функциональностью
(&lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; выпускается под GPL). Благодаря возможности
работы через Infiniband производительность передачи данных также
достаточно высока - она может достигать десятков гигабит в секунду.
Обработка сбоев в отдельных узлах также осуществляется достаточно
эффективно, так как может быть автоматизирована. Из потенциальных
недостатков можно назвать некоторое количество редко проявляющих себя
багов в коде, а также достаточно большой размер заголовков в
используемом протоколе (несколько сотен байт). В целом эта система
вполне работоспособна и полноценно выдерживает конкуренцию со стороны
своих opensource "коллег".&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 18 May 2008 21:16:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-05-18:storage/2008/glusterfs/</guid><category>GlusterFS</category><category>GPL</category><category>Infiniband</category><category>кластер</category><category>Масштабируемость</category><category>файловая система</category></item><item><title>Файлы в космосе</title><link>https://www.insight-it.ru//storage/2008/fajjly-v-kosmose/</link><description>&lt;h4&gt;...или Kosmos Distributed File System&lt;/h4&gt;
&lt;p&gt;&lt;img alt="Kosmos Distributed File System" class="right" src="https://www.insight-it.ru/images/KFS.jpg" title="KosmosFS Logo"/&gt;
Сегодня речь пойдет об еще одной распределенной файловой системе -
&lt;a href="https://www.insight-it.ru/goto/a11ac210/" rel="nofollow" target="_blank" title="http://kosmosfs.sourceforge.net/"&gt;KosmosFS&lt;/a&gt;. У русских людей название
этого проекта определенно вызывает ассоциации с космосом, но изначально
все же свою лепту в него внес изначальный разработчик -
&lt;a href="https://www.insight-it.ru/goto/636f244d/" rel="nofollow" target="_blank" title="http://www.kosmix.com/"&gt;Kosmix&lt;/a&gt;.
&lt;!--more--&gt;
По большому счету &lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; мало чем выделяется из множества
своих конкурентов, по своей структуре она состоит из сервера метаданных
и серверов блоков, доступ к системе производится средствами клиентской
библиотеки, предоставляющей соответствующий API. Список возможностей
файловой системы также вполне стандартен:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Инкрементальная масштабируемость.&lt;/em&gt; При добавлении дополнительных
    узлов в кластер, система сама адаптируется для вовлечения их в
    полноценную работу.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Стабильный доступ.&lt;/em&gt; Реплицируемость данных (по-умолчанию в трех
    экземплярах) позволяет гарантировать доступность данных вне
    зависимости от сбоев в работе отдельных узлов.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Балансировка блоков данных.&lt;/em&gt; Периодически сервер метаданных
    перераспределяет данные с целью более оптимального использования
    дискового пространства.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Целостность данных.&lt;/em&gt; Для обеспечения целостности данных вычисляются
    и сравниваются контрольные суммы блоков данных.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Кэширование.&lt;/em&gt; Для увеличения производительности используется
    кэширования на уровне клиентской библиотеки.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Прозрачная работа с недоступными узлами.&lt;/em&gt; Клиентская библиотека
    прозрачно для приложения переключается на альтернативный сервер с
    данными, если обнаруживает что один из них недоступен.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Поддержка языков программирования:&lt;/em&gt; &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;,
    &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Скрипты.&lt;/em&gt; С системой предоставляется набор скриптов для
    развертывания, запуска и остановки узлов.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Но написать этот пост меня подтолкнул вовсе не этот список. В
комментариях к &lt;a href="https://www.insight-it.ru/storage/2008/hadoop/"&gt;одной из предыдущих моих записей&lt;/a&gt;
читатели подняли тему о целесообразности использования &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;
для реализации &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt; в частности и &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; в
целом. В качестве альтернативы был предложен &lt;a href="/tag/c/"&gt;C++&lt;/a&gt; (только на
словах конечно же), аргументируя это тем, что такая реализация была бы
эффективнее. &lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; же как раз и является той самой
альтернативой &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt;, написанной на &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;.
&lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; тесно интегрируется с &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; с помощью
его интерфейсов для файловой системы. Это позволяет Hadoop-приложениям
незаметно работать с &lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; точно так же, как если бы на ее
месте была бы &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt;. Код для интеграции с Hadoop был выпущен
в виде патча к Hadoop-JIRA-1963, а начиная с Hadoop версии 0.15 этот код
входит в стандартный дистрибутив, ровно как и детальная инструкция по
интеграции.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 30 Mar 2008 23:06:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-03-30:storage/2008/fajjly-v-kosmose/</guid><category>C++</category><category>Hadoop</category><category>KFS</category><category>Kosmos Distributed File System</category><category>кластер</category><category>файловая система</category></item><item><title>Lustre</title><link>https://www.insight-it.ru//storage/2008/lustre/</link><description>&lt;p&gt;&lt;img alt="Lustre Logo" class="right" src="https://www.insight-it.ru/images/lustre-logo.png" title="Lustre"/&gt;
&lt;a href="https://www.insight-it.ru/goto/4df3f908/" rel="nofollow" target="_blank" title="http://www.lustre.org"&gt;Lustre&lt;/a&gt; представляет собой кластерную файловую
систему, основными особенностями которой являются превосходные
надежность и масштабируемость. Производительность также более чем
высока - скорость передачи данных может достигать сотен гигабит в
секунду, а теоретический максимум доступного дискового пространства
измеряется петабайтами. Эта файловая система может использоваться как на
скромных рабочих группах из нескольких компьютеров, так и на огромных
кластерах, насчитывающих десятки тысяч машин.&lt;/p&gt;
&lt;p&gt;Помимо этого поддерживаются все возможности, который должна иметь любая
уважающая себя кластерная файловая система:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;поддержка широкого ассортимента типов высокоскоростных сетевых
    соединений;&lt;/li&gt;
&lt;li&gt;надежная система "замков" для обеспечения параллельного доступа к
    файлам;&lt;/li&gt;
&lt;li&gt;возможность автоматического самовосстановления в случае падения
    любого из узлов;&lt;/li&gt;
&lt;li&gt;распределенное управление файловыми объектами для предоставления
    масштабируемого доступа к файлам.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;p&gt;Изначально архитектура этой файловой системы была разработана просто в
рамках исследовательского проекта Петера Браама в 1999, но он решил не
останавливаться на достигнутом и основал &lt;em&gt;Cluster File Systems, Inc.&lt;/em&gt;, в
которой уже и велась основная разработка самой файловой системы. Первый
релиз Lustre 1.0 был выпущен в 2003 году. Спустя четыре года компания
была приобретена Sun Microsystems в октябре 2007 года, но это лишь
способствовало дальнейшему развитию проекта. Программное обеспечение,
входящее в состав проекта, выпускается под лицензией GPL, что также
сыграло немаловажную роль в его жизни.&lt;/p&gt;
&lt;h3 id="arkhitektura"&gt;Архитектура&lt;/h3&gt;
&lt;p&gt;Каждый компьютер, входящий состав кластера Lustre, выполняет свою четко
определенную функцию:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="MetaData Server"&gt;MDS&lt;/abbr&gt;.&lt;/strong&gt; Сервер метаданных
    предназначен для хранения всей служебной информации о системе:
    названия файлов, директорий, прав доступа и так далее. Достаточно
    наличие одного такого сервера в системе, но для обеспечения
    надежности на случай каких-либо сбоев, обычно его дублируют.
    Возможно использование внешнего хранилища данных
    (&lt;abbr title="MetaData Target"&gt;MDT&lt;/abbr&gt;), которое может быть общим
    для двух дублирующих друг друга &lt;abbr title="MetaData Server"&gt;MDS&lt;/abbr&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="Object Storage Server"&gt;OSS&lt;/abbr&gt;&lt;/strong&gt; Компьютеры для
    хранения самих данных. Каждый из них работает с 2-8 &lt;abbr title="Object Storage Target"&gt;OST&lt;/abbr&gt;, в их роли могут
    выступать практически любые средства хранения данных, начиная от
    просто жестких дисков или RAID массивов внутри &lt;abbr title="Object Storage Server"&gt;OSS&lt;/abbr&gt;, заканчивая
    внешними системами хранения данных enterprise-класса. Сумма
    дискового пространства всех &lt;abbr title="Object Storage Target"&gt;OST&lt;/abbr&gt; и является размером доступного
    дискового пространства всей файловой системы Lustre.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Клиент.&lt;/strong&gt; Компьютеры, непосредственно использующие файловую
    систему. Им предоставляется полный параллельный доступ, полностью
    соответствующий стандарту POSIX.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Один и тот же компьютер теоретически может совмещать в себе несколько
функций, но в большинстве случаев это нецелесообразно (за исключением
совмещения клиентов с &lt;abbr title="Object Storage Target"&gt;OST&lt;/abbr&gt; и, возможно, случаев, когда количество узлов
кластера очень мало).&lt;/p&gt;
&lt;p&gt;Возможно более наглядно вышенаписанное сможет представить схема
архитектуры системы
(&lt;a href="https://www.insight-it.ru/goto/dd98ad05/" rel="nofollow" target="_blank" title="http://manual.lustre.org/manual/LustreManual16_HTML/figures/ClusterWithLustre-4.gif"&gt;позаимствована&lt;/a&gt; с официального сайта и переведена):
&lt;img alt="Схема архитектуры файловой системы Lustre" class="responsive-img" src="https://www.insight-it.ru/images/lustre-architecture.png" title="Схема архитектуры"/&gt;&lt;/p&gt;
&lt;p&gt;Помимо этого для функционирования системы необходим еще один компонент,
по большому счету не являющийся ее частью -
&lt;abbr title="ManaGement Server"&gt;MGS&lt;/abbr&gt;. Его роль заключается в
предоставлении конфигурационной информации всем компонентам одной или
нескольким файловым системам Lustre. Он также нуждается в отдельном
хранилище данных, но чисто теоретически он может быть и совмещен с одним
из компонентов файловой системы.&lt;/p&gt;
&lt;h3 id="funktsionirovanie"&gt;Функционирование&lt;/h3&gt;
&lt;p&gt;Основным толчком для выполнения каких-либо действий в рамках всей
файловой системы обычно является запрос с одного из клиентов.
Программное обеспечение для клиентов представляет по сути интерфейс
между виртуальной файловой системой &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; и серверами
Lustre. Каждому типу серверов соответствует своя часть клиентского ПО:
&lt;abbr title="MetaData Client"&gt;MDC&lt;/abbr&gt;, &lt;abbr title="Object Storage Client"&gt;OSC&lt;/abbr&gt;, &lt;abbr title="ManaGement Client"&gt;MGC&lt;/abbr&gt;. В отличии от &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; и &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt; файловая система Lustre должна быть примонтирована к локальной системе клиентов для полноценного их функционирования.&lt;/p&gt;
&lt;p&gt;Для осуществления коммуникации между клиентами и серверами используется
собственный API, известный как &lt;abbr title="Lustre NETworking"&gt;LNET&lt;/abbr&gt;. Он поддерживает множество
сетевых протоколов с помощью &lt;abbr title="Network Abstraction Layer"&gt;NAL&lt;/abbr&gt;.&lt;/p&gt;
&lt;p&gt;В системе отсутствуют незаменимые компоненты, это является залогом
отказоустойчивости системы. В случае возникновения каки-либо неполадок
или сбоев в работе оборудования, работу потерявших работоспособность
компонентов системы перехватят другие ее компоненты, что сделает сбой
незаметным для пользователей системы. Это достигается за счет
дублирование серверов, выполняющих одинаковые функции, а также наличие
налаженных алгоритмов действий, направленных на автоматическое
восстановление полноценного функционирования системы в случае
возникновения чрезвычайных ситуаций. Но этого конечно же не достаточно
для абсолютной надежности системы, в дополнение должна быть
предоставлена как минимум система бесперебойного питания для всех
компонентов кластера на случай проблем с электроэнергией в датацентре
(для России более чем актуально).&lt;/p&gt;
&lt;p&gt;В списке дополнительных возможностей, предоставляемых файловой системой,
можно назвать возможность выделения квот на дисковое пространство для
каждого пользователя системы, аутентификацию пользователей с помощью
механизма Kerberos, повышение физической пропускной способности сетевого
соединения путем аггрегирования физических сетевых соединений в одно
логическое виртуально сетевое соединение (достаточно интересная
возможность, способная при выполнении определенных условий существенно
повлиять на быстродействие системы). Помимо этого предоставляется целый
ряд возможностей по созданию резервных копий данных на уровне файловой
системы в целом, отдельных устройств или же файлов.&lt;/p&gt;
&lt;h3 id="zakliuchenie"&gt;Заключение&lt;/h3&gt;
&lt;p&gt;Эта файловая система нашла свое применение во множестве крупнейших
кластеров и суперкомпьютеров по всему миру, но это не мешает ей с тем же
успехом демонстрировать и на кластерах существенно меньшего масштаба.
Около половины из самых производительных суперкомпьютеров во всем мире
используют Lustre в качестве файловой системы. Помимо этого многие
компании предоставляют ее в качестве основы для Linux кластеров
(например HP StorageWorks SFS, Cray XT3, Cray XD1). Чем не показатель ее
конкурентоспособности?&lt;/p&gt;
&lt;p&gt;В качестве источников информации были использованы &lt;a href="https://www.insight-it.ru/goto/4df3f908/" rel="nofollow" target="_blank" title="http://www.lustre.org"&gt;официальный сайт проекта&lt;/a&gt; и иногда &lt;a href="https://www.insight-it.ru/goto/9328120f/" rel="nofollow" target="_blank" title="http://en.wikipedia.org/wiki/Lustre_%28file_system%29"&gt;страница английской wikipedia.org&lt;/a&gt;.
На все том же официальном сайте всегда можно найти всю необходимую
документацию, а само программное обеспечение проекта доступно на
&lt;a href="https://www.insight-it.ru/goto/7a51c11f/" rel="nofollow" target="_blank" title="http://www.sun.com/software/products/lustre/get.jsp"&gt;соответствующей странице&lt;/a&gt; сайта Sun Mictosystems.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 21 Mar 2008 21:53:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-03-21:storage/2008/lustre/</guid><category>Cluster File Systems</category><category>Linux</category><category>Lustre</category><category>Sun</category><category>кластер</category><category>Масштабируемость</category><category>файловая система</category></item><item><title>Hadoop</title><link>https://www.insight-it.ru//storage/2008/hadoop/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/30a7481/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/"&gt;Hadoop&lt;/a&gt; представляет собой платформу
для построения приложений, способных обрабатывать огромные объемы
данных. Система основывается на распределенном подходе к вычислениям и
хранению информации, основными ее особенностями являются:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Масштабируемость:&lt;/strong&gt; с помощью &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; возможно
    надежное хранение и обработка огромных объемов данных, которые могут
    измеряться петабайтами;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Экономичность:&lt;/strong&gt; информация и вычисления распределяются по
    &lt;a href="/tag/klaster/"&gt;кластеру&lt;/a&gt;, построенному на самом обыкновенном
    оборудовании. Такой кластер может состоять из тысяч узлов;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Эффективность:&lt;/strong&gt; распределение данных позволяет выполнять их
    обработку параллельно на множестве компьютеров, что существенно
    ускоряет этот процесс;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Надежность:&lt;/strong&gt; при хранении данных возможно предоставление
    избыточности, благодаря хранению нескольких копий. Такой подход
    позволяет гарантировать отсутствие потерь информации в случае сбоев
    в работе системы;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Кроссплатформенность:&lt;/strong&gt; так как основным языком программирования,
    используемым в этой системе является &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, развернуть
    ее можно на базе любой операционной системы, имеющей &lt;abbr title="Java Virtual Machine"&gt;JVM&lt;/abbr&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;h3 id="hdfs"&gt;HDFS&lt;/h3&gt;
&lt;p&gt;В основе всей системы лежит распределенная файловая система под
незамысловатым названием &lt;strong&gt;Hadoop Distributed File System&lt;/strong&gt;.
Представляет она собой вполне стандартную распределенную файловую
систему, но все же она обладает рядом особенностей:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Устойчивость к сбоям, разработчики рассматривали сбои в оборудовании
    скорее как норму, чем как исключение;&lt;/li&gt;
&lt;li&gt;Приспособленность к развертке на самом обыкновенном ненадежном
    оборудовании;&lt;/li&gt;
&lt;li&gt;Предоставление высокоскоростного потокового доступа ко всем данным;&lt;/li&gt;
&lt;li&gt;Настроена для работы с большими файлами и наборами файлов;&lt;/li&gt;
&lt;li&gt;Простая модель работы с данными: &lt;em&gt;один раз записали - много раз
    прочли&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;Следование принципу: &lt;em&gt;переместить вычисления проще, чем переместить
    данные&lt;/em&gt;;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Архитектура HDFS&lt;/h4&gt;
&lt;p&gt;Проще всего ее демонстрирует схема,
&lt;a href="https://www.insight-it.ru/goto/9c57006b/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/docs/current/images/hdfsarchitecture.gif"&gt;позаимствованная&lt;/a&gt; с официального сайта проекта и переведенная мной на руский:
&lt;img alt="Архитектура HDFS" class="responsive-img" src="https://www.insight-it.ru/images/hdfsarchitecture.jpg" title="Архитектура HDFS"/&gt;&lt;/p&gt;
&lt;p&gt;Действующие лица:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Namenode&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Этот компонент системы осуществляет всю работу с метаданными. Он
должен быть запущен только на одном компьютере в кластере. Именно он
управляет размещением информации и доступом ко всем данным,
расположенным на ресурсах кластера. Сами данные проходят с остальных
машин кластера к клиенту мимо него.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Datanode&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;На всех остальных компьютерах системы работает именно этот
компонент. Он располагает сами блоки данных в локальной файловой
системе для последующей передачи или обработки их по запросу
клиента. Группы узлов данных принято называть Rack, они
используются, например, в схемах репликации данных.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Клиент&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Просто приложение или пользователь, работающий с файловой системой.
В его роли может выступать практически что угодно.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Пространство имен &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt; имеет классическую иерархическую
структуру: пользователи и приложения имеют возможность создавать
директории и файлы. Файлы хранятся в виде блоков данных произвольной (но
одинаковой, за исключением последнего; по-умолчанию 64 mb) длины,
размещенных на &lt;strong&gt;Datanode&lt;/strong&gt;'ах. Для обеспечения отказоустойчивости блоки
хранятся в нескольких экземплярах на разных узлах, имеется возможность
настройки количества копий и алгоритма их распределения по системе.
Удаление файлов происходит не сразу, а через какое-то время после
соответствующего запроса, так как после получения запроса файл
перемещается в директорию &lt;strong&gt;/trash&lt;/strong&gt; и хранится там определенный период
времени на случай если пользователь или приложение передумают о своем
решении. В этом случае информацию можно будет восстановить, в противном
случае - физически удалить.&lt;/p&gt;
&lt;p&gt;Для обнаружения возникновения каких-либо неисправностей, &lt;strong&gt;Datanode&lt;/strong&gt;
периодически отправляют &lt;strong&gt;Namenode&lt;/strong&gt;'у сигналы о своей
работоспособности. При прекращении получения таких сигналов от одного из
узлов &lt;strong&gt;Namenode&lt;/strong&gt; помечает его как &lt;em&gt;"мертвый"&lt;/em&gt;, и прекращает какой-либо
с ним взаимодействие до возвращения его работоспособности. Данные,
хранившиеся на &lt;em&gt;"умершем"&lt;/em&gt; узле реплицируются дополнительный раз из
оставшихся &lt;em&gt;"в живых"&lt;/em&gt; копий и система продолжает свое функционирование
как ни в чем не бывало.&lt;/p&gt;
&lt;p&gt;Все коммуникации между компонентами файловой системы проходят по
специальным протоколам, основывающимся на стандартном &lt;strong&gt;TCP/IP&lt;/strong&gt;.
Клиенты работают с &lt;strong&gt;Namenode&lt;/strong&gt; с помощью так называемого
&lt;strong&gt;ClientProtocol&lt;/strong&gt;, а передача данных происходит по
&lt;strong&gt;DatanodeProtocol&lt;/strong&gt;, оба они &lt;em&gt;обернуты&lt;/em&gt; в &lt;strong&gt;Remote Procedure Call
(RPC)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Система предоставляет несколько интерфейсов, среди которых командная
оболочка &lt;strong&gt;DFSShell&lt;/strong&gt;, набор ПО для администрирования &lt;strong&gt;DFSAdmin&lt;/strong&gt;, а
также простой, но эффективный веб-интерфейс. Помимо этого существуют
несколько API для языков программирования: Java API, C pipeline, WebDAV
и так далее.&lt;/p&gt;
&lt;h3 id="mapreduce"&gt;MapReduce&lt;/h3&gt;
&lt;p&gt;Помимо файловой системы, &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; включает в себя framework
для проведения масштабных вычислений, обрабатывающих огромные объемы
данных. Каждое такое вычисление называется Job (задание) и состоит оно,
как видно из названия, из двух этапов:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Map&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Целью этого этапа является представление произвольных данных (на
практике чаще всего просто пары ключ-значение) в виде промежуточных
пар ключ-значение. Результаты сортируются и групируются по ключу и
передаются на следующий этап.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Reduce&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Полученные после &lt;strong&gt;map&lt;/strong&gt; значения используются для финального
вычисления требуемых данных. Практические любые данные могут быть
получены таким образом, все зависит от требований и функционала
приложения.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Задания выполняются, подобно файловой системе, на всех машинах в
кластере (чаще всего одних и тех же). Одна из них выполняет роль
управления работой остальных - &lt;strong&gt;JobTracker&lt;/strong&gt;, остальные же ее
бесприкословно слушаются - &lt;strong&gt;TaskTracker&lt;/strong&gt;. В задачи &lt;strong&gt;JobTracker&lt;/strong&gt;'а
входит составление расписания выполняемых работ, наблюдение за ходом
выполнения, и перераспределение в случае возникновения сбоев.&lt;/p&gt;
&lt;p&gt;В общем случае каждое приложение, работающее с этим framework'ом,
предоставляет методы для осуществления этапов &lt;strong&gt;map&lt;/strong&gt; и &lt;strong&gt;reduce&lt;/strong&gt;, а
также указывает расположения входных и выходных данных. После получения
этих данных &lt;strong&gt;JobTracker&lt;/strong&gt; распределяет задание между остальными
машинами и предоставляет клиенту полную информацию о ходе работ.&lt;/p&gt;
&lt;p&gt;Помимо основных вычислений могут выполняться вспомогательные процессы,
такие как составление отчетов о ходе работы, кэширование, сортировка и
так далее.&lt;/p&gt;
&lt;h3 id="hbase"&gt;HBase&lt;/h3&gt;
&lt;p&gt;&lt;img alt="HBase Logo" class="right" src="https://www.insight-it.ru/images/hbase-logo.png" title="HBase"/&gt;
В рамках &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; доступна еще и система хранения данных,
которую правда сложно назвать &lt;a href="/tag/subd/"&gt;СУБД&lt;/a&gt; в традиционном смысле
этого слова. Чаще проводят аналогии с проприетарной системой этого же
плана от &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; - &lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/12419d3d/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/hbase"&gt;HBase&lt;/a&gt; представляет собой
распределенную систему хранения больших объемов данных. Подобно
реляционным СУБД данные хранятся в виде таблиц, состоящих из строк и
столбцов. И даже для доступа к ним предоставляется язык запросов &lt;strong&gt;HQL&lt;/strong&gt;
(как ни странно - &lt;strong&gt;Hadoop Query Language&lt;/strong&gt;), отдаленно напоминающий
более распространенный &lt;a href="/tag/sql/"&gt;SQL&lt;/a&gt;. Помимо этого предоставляется
итерирующмй интерфейс для сканирования наборов строк.&lt;/p&gt;
&lt;p&gt;Одной из основных особенностей хранения данных в &lt;strong&gt;HBase&lt;/strong&gt; является
возможность наличия нескольких значений, соответствующих одной
комбинации таблица-строка-столбец, для их различения используется
информация о времени добавления записи. На концептуальном уровне таблицы
обычно представляют как набор строк, но физически же они хранятся по
столбцам, достаточно важный факт, который стоит учитывать при разработки
схемы хранения данных. Пустые ячейки не отображаются каким-либо образом
физически в хранимых данных, они просто отсутствуют. Существуют конечно
и другие нюансы, но я постарался упомянуть лишь основные.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HQL&lt;/strong&gt; очень прост по своей сути, если Вы уже знаете &lt;a href="/tag/sql/"&gt;SQL&lt;/a&gt;,
то для изучения его Вам понадобится лишь просмотреть по диагонали
коротенький вывод команды &lt;strong&gt;help;&lt;/strong&gt;, занимающий всего пару экранов в
консоли. Все те же &lt;strong&gt;SELECT&lt;/strong&gt;, &lt;strong&gt;INSERT&lt;/strong&gt;, &lt;strong&gt;UPDATE&lt;/strong&gt;, &lt;strong&gt;DROP&lt;/strong&gt; и так
далее, лишь со слегка измененным синтаксисом.&lt;/p&gt;
&lt;p&gt;Помимо обычно командной оболочки &lt;strong&gt;HBase Shell&lt;/strong&gt;, для работы с &lt;strong&gt;HBase&lt;/strong&gt;
также предоставлено несколько API для различных языков программирования:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/f059ad5e/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/hbase/docs/current/api/index.html"&gt;Java&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/e44fcd5/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/Jython"&gt;Jython&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8282e2e2/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/HbaseRest"&gt;REST&lt;/a&gt; и&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/185bb3f7/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/ThriftApi"&gt;Thrift&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="zakliuchenie"&gt;Заключение&lt;/h3&gt;
&lt;p&gt;&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; является отличным решением для построения
высоконагруженных приложений, которое уже активно используется
&lt;a href="https://www.insight-it.ru/goto/ab057c2a/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/PoweredBy"&gt;множеством интернет-проектов&lt;/a&gt;.
В последующих постах на эту тему я постараюсь описать процесс
развертывания этой системы и написания приложений, работающих по
принципу &lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;. Не пропустить момент их публикации
Вам может помочь подписка на &lt;a href="/feed/"&gt;RSS-ленту&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 22 Feb 2008 22:41:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-02-22:storage/2008/hadoop/</guid><category>Hadoop</category><category>HBase</category><category>HDFS</category><category>Java</category><category>MapReduce</category><category>архитектура</category><category>информационные технологии</category><category>кластер</category><category>Масштабируемость</category><category>распределенные вычисления</category><category>технология</category></item><item><title>Архитектура Flickr</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-flickr/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/f50a76e1/" rel="nofollow" target="_blank" title="http://www.flickr.com"&gt;Flickr&lt;/a&gt; является мировым лидером среди сайтов
размещения фотографий. Перед Flickr стоит впечатляющая задача, они
должны контролировать обширное море ежесекундно обновляющегося контента,
непрерывно пополняющиеся легионы пользователей, постоянный поток новых
предоставляемых пользователям возможностей, а делается все это при
постоянной поддержке отличной производительности. Как же они это
делают?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Как и предыдущий пост &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-google/"&gt;"Архитектура Google"&lt;/a&gt;, этот тоже является
переводом &lt;a href="https://www.insight-it.ru/goto/e7a0ee0d/" rel="nofollow" target="_blank" title="http://highscalability.com/flickr-architecture"&gt;статьи&lt;/a&gt; от
&lt;a href="https://www.insight-it.ru/goto/f3f1b405/" rel="nofollow" target="_blank" title="http://highscalability.com/user/todd-hoff"&gt;Todd'а Hoff'а&lt;/a&gt;. Возможно
читателям &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; был более интересен, но подход Flickr к
масштабируемости тоже более чем заслуживает внимания. Далее привожу
источники информации из оригинальной статьи:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/88014756/" rel="nofollow" target="_blank" title="http://www.niallkennedy.com/blog/uploads/flickr_php.pdf"&gt;Flickr и PHP&lt;/a&gt;
    (ранний документ)&lt;/li&gt;
&lt;li&gt;Планирование нагрузок на LAMP&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/6df1dabf/" rel="nofollow" target="_blank" title="http://www.bytebot.net/blog/archives/2007/04/25/federation-at-flickr-a-tour-of-the-flickr-architecture"&gt;Федерация Flickr: Тур по архитектуре Flickr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/9e0a13a1/" rel="nofollow" target="_blank" title="http://highscalability.com/book-building-scalable-web-sites"&gt;Построение масштабируемых веб-сайтов&lt;/a&gt;
    от Call Handerson'а из Flickr&lt;/li&gt;
&lt;li&gt;История войн баз данных #3: Tim O'Reilly о Flickr&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/d881b0d9/" rel="nofollow" target="_blank" title="http://www.iamcal.com/talks/"&gt;Cal Henderson's Talks&lt;/a&gt; - много
    полезных презентаций&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/sql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Сегментирование &lt;em&gt;(прим.: разбиение системы на части, обслуживающие
    каждая свою группу пользователей; называть можно было по-разному, но
    давайте остановимся на этом варианте перевода слова "Shards")&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt; для кэширования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt; в качестве обратной-прокси для html и
    изображений&lt;/li&gt;
&lt;li&gt;&lt;a href="/linux"&gt;Linux&lt;/a&gt; (&lt;a href="/tag/redhat/"&gt;RedHat&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/smarty/"&gt;Smarty&lt;/a&gt; в роли шаблонизатора&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/perl/"&gt;Perl&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PEAR для парсинга e-mail и XML&lt;/li&gt;
&lt;li&gt;ImageMagick для обработки изображений&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/java/"&gt;Java&lt;/a&gt; для узлового сервиса&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/systemimager/"&gt;SystemImager&lt;/a&gt; для развертывания систем&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/ganglia/"&gt;Ganglia&lt;/a&gt; для мониторинга распределенных систем&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/subcon/"&gt;Subcon&lt;/a&gt; хранит важные системные конфигурационные файлы
    в SVN-репозитории для легкого развертывания на машины в кластере.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/cvsup/"&gt;Cvsup&lt;/a&gt; для распространения и обновления коллекций
    файлов по сети&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="statistika"&gt;Статистика&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Более четырех миллиардов запросов в день&lt;/li&gt;
&lt;li&gt;Примерно 35 миллионов фотографий в кэше &lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Около двух миллионов фотографий в оперативной памяти
    &lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Всего приблизительно 470 миллионов изображений, каждое представлено
    в 4 или 5 размерах&lt;/li&gt;
&lt;li&gt;38 тысяч запросов к &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; (12 миллионов
    объектов)&lt;/li&gt;
&lt;li&gt;2 петабайта дискового пространства&lt;/li&gt;
&lt;li&gt;Более 400000 фотографий добавляются ежедневно&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="arkhitektura"&gt;Архитектура&lt;/h3&gt;
&lt;p&gt;Симпатичное изображение архитектуры Flickr можно увидеть на &lt;a href="https://www.insight-it.ru/goto/d30e097b/" rel="nofollow" target="_blank" title="http://www.slideshare.net/techdude/scalable-web-architectures-common-patterns-and-approaches/138"&gt;этом слайде&lt;/a&gt;.
Краткое ее описание выглядит следующим образом:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Два ServerIron&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt; кэши&lt;/li&gt;
&lt;li&gt;Системы хранения NetApp&lt;/li&gt;
&lt;li&gt;Серверы &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; приложений&lt;/li&gt;
&lt;li&gt;Менеджер хранения данных&lt;/li&gt;
&lt;li&gt;Master-master сегменты&lt;/li&gt;
&lt;li&gt;Центральная база данных, структурированная по принципу Dual
Tree&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt; кластер&lt;/li&gt;
&lt;li&gt;Поисковая система&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Хранение данных&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Структура Dual Tree является индивидуальным набором модификаций для
&lt;a href="/tag/sql/"&gt;MySQL&lt;/a&gt;, позволяющим масштабировать систему путем добавления
новых мастер-серверов без использования кольцевой архитектуры. Эта
система позволяет экономить на масштабировании, так как варианты
мастер-мастер требовали бы удвоенных вложений в оборудование.&lt;/li&gt;
&lt;li&gt;Центральная база данных включает в себя таблицу пользователей,
состоящую из основных ключей пользователей (несколько уникальных
идентификационных номеров) и указатель на сегмент, на котором может быть
найдена остальная информация о конкретном пользователе.&lt;/li&gt;
&lt;li&gt;Использование выделенных серверов для статического контента&lt;/li&gt;
&lt;li&gt;Все, за исключением фотографий, хранится в базе данных&lt;/li&gt;
&lt;li&gt;Отсутствие состояний заключается в том, что в случае необходимости
    они имеют возможность передать пользователей от сервера к серверу,
    что стало намного проще для них после создания своего API&lt;/li&gt;
&lt;li&gt;В основе масштабируемости лежит репликация, но этот факт помогает
    лишь при обработке операций чтения&lt;/li&gt;
&lt;li&gt;Для поиска по определенной части базы данных создается отдельная
    копия этого фрагмента&lt;/li&gt;
&lt;li&gt;Использования горизонтального масштабирования для того чтобы можно
    было проще добавлять новые машины в систему&lt;/li&gt;
&lt;li&gt;Обработка изображений, полученных от пользователей по электронной
    почте, происходит с помощью &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Раньше система страдала от задержек связанных с организацией по
    принципу мастер-слуга. При слишком большой нагрузке они имели одну
    точку, которая теоретически могла дать сбой.&lt;/li&gt;
&lt;li&gt;Им было необходимо иметь возможность проводить технические работы во
    время непрерывной работы сайта, не прекращая его функционирование.&lt;/li&gt;
&lt;li&gt;Были проведены отличные работы по планированию распределения
    дискового пространства, более подробную информацию можно найти по
    ссылкам в разделе "Источники информации".&lt;/li&gt;
&lt;li&gt;Для обеспечения возможности масштабирования в будущем, они пошли по
    федеративному пути развития:&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Сегменты системы:&lt;/em&gt; Мои данные хранятся на моем сегменте, но
запись о Вашем комментарии хранится на Вашем сегменте.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Глобальное кольцо:&lt;/em&gt; Принцип работы схож с DNS, Вам необходимо
знать куда Вы хотите пойти и кто контролирует то место, куда Вы
собираетесь пойти.&lt;/li&gt;
&lt;li&gt;Логика на &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; устанавливает соединение с сегментом и
поддерживает целостность данных (10 строк кода с комментариями!)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Сегменты:&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Срез основной базы данных&lt;/li&gt;
&lt;li&gt;Активная репликация по принципу мастер-мастер: имеет несколько
недостатков в &lt;a href="/tag/sql/"&gt;MySQL&lt;/a&gt; 4.1. Автоматическое
инкрементирование идентификационных номеров используется для
поддержания системы в режиме одновременной активности обоих серверов
в паре&lt;/li&gt;
&lt;li&gt;Привязывание новых учетных записей к сегментам системы происходит
случайным образом&lt;/li&gt;
&lt;li&gt;Миграция пользователей проводится время от времени для того, чтобы
избавиться от проблем, связанных с излишне активными пользователями.
Необходима сбалансированность в этом процессе, особенно в случаях с
большим количеством фотографий&amp;hellip; 192 тысячи фотографий, 700 тысяч
тэгов, может занять несколько минут. Миграция выполняется вручную.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Нажатие на &lt;strong&gt;Favorite&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Получается информация об учетной записи владельца из кэша для
того, чтобы узнать к какому сегменту он привязан (допустим на
shard-5)&lt;/li&gt;
&lt;li&gt;Получается информация о моей учетной записи из кэша, более
конкретно - мой сегмент (например shard-13)&lt;/li&gt;
&lt;li&gt;Начинается "распределенная транзакция" для определения ответов на
вопросы: Кто добавил эту фотографию в избранное? Как изменился
список избранных фотографий?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Подобные вопросы могут задаваться любому сегменту, информация на них
    абсолютно избыточна.&lt;/li&gt;
&lt;li&gt;Для избавления от задержек, связанных с репликацией...&lt;ul&gt;
&lt;li&gt;при каждой загрузке страницы, пользователю предоставляется список
серверов&lt;/li&gt;
&lt;li&gt;если сервер не в состоянии ответить на запрос, запрос переходит к
следующему серверу в списке; если список кончился - выводится
сообщение об ошибке. При этом не используются постоянные соединения,
каждый раз создаются и разрываются новые соединения.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Запросы на чтение и запись от каждого пользователя ограничиваются
    рамками одного сегмента. Задержки репликации исчезают из поля зрения
    пользователей.&lt;/li&gt;
&lt;li&gt;Каждый сервер в рамках одного сегмента в обычном состоянии нагружен
    ровно на половину. Выключите половину серверов в каждом сегменте и
    система продолжит функционировать без изменений. Это значит, что
    один сервер внутри сегмента может взять на себя всю нагрузку
    второго, в то время как второй сервер может по каким либо причинам
    быть отключен от системы, например для проведения технических работ.
    Обновление оборудования производится очень просто: отключается
    половина сегмента, она же обновляется, подключается обратно, процесс
    повторяется для оставшейся половины.&lt;/li&gt;
&lt;li&gt;Периоды пиковой нагрузки также нарушают правило 50% нагрузки. В
    такие моменты система получает 6-7 тысяч запросов в секунду, в то
    время как на данный момент система может работать на
    пятидесятипроцентном уровне нагрузки только при четырех тысячах
    запросов в секунду.&lt;/li&gt;
&lt;li&gt;В среднем при загрузке одной страницы выполняется 27-35
    SQL-запросов. Списки избранных фотографий обрабатываются в реальном
    времени, ровно как и доступ через API к базе данных. Все требования
    к нагрузке в реальном времени выполняются без каких-либо
    недостатков.&lt;/li&gt;
&lt;li&gt;Более 36 тысяч запросов в секунду может выполняться не выходя за
    рамки возможностей системы, даже при резком росте трафика.&lt;/li&gt;
&lt;li&gt;Каждый сегмент содержит данные о более чем 400 тысячах
    пользователей.&lt;/li&gt;
&lt;li&gt;Многие данные хранятся в двух местах одновременно. Например,
    комментарий является частью между комментатором и автором
    комментируемого контента. Где его хранить? Как насчет обоих мест?
    Транзакции используются для предотвращения рассинхронизации данных:
    открывается первая транзакция, выполняется запись, открывается
    вторая транзакция, выполняется запись, подтверждается первая
    транзакция если все нормально, после чего вторая подтверждается
    только в случае если первая прошла успешно.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Поиск&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Используется два варианта поиска: поиск в рамках сегмента,
поддерживающий до 35 тысяч запросов в секунду, а также проприетарный
веб-поиск от Yahoo!&lt;/li&gt;
&lt;li&gt;В 90% случаев используется система от Yahoo!, за исключением
поиска по тэгу фотографий одного пользователя и массовых изменений
тэгов.&lt;/li&gt;
&lt;li&gt;Эту систему стоит рассматривать как аналог Lucene.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Оборудование&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;EMT64 под управлением RHEL 4 с 16 Gb оперативной памяти.&lt;/li&gt;
&lt;li&gt;6 жестких дисков с 15000rpm, объединены в RAID-10.&lt;/li&gt;
&lt;li&gt;Размер для пользовательских метаданных достигает 12 терабайт (это
не включает фотографии, для них цифры существенно больше).&lt;/li&gt;
&lt;li&gt;Используются 2U корпуса.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Резервное копирование данных&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;ibbackup выполняется регулярно посредством cron daemon'а, на
каждом сегменте настроен на разное время.&lt;/li&gt;
&lt;li&gt;Каждую ночь делается снимок со всего кластера баз данных.&lt;/li&gt;
&lt;li&gt;Запись или удаление нескольких больших файлов с резервными копиями
одновременно на реплицирующую систему хранения может сильно
сократить производительность системы вцелом на последующие несколько
часов из-за процесса репликации. Выполнение этого на активно
работающей системе хранения фотографий было бы не самой лучшей
идеей.&lt;/li&gt;
&lt;li&gt;Содержание нескольких резервных копий всех Ваших данных требует
существенных материальных затрат, но оно того стоит. Особенно это
актуально для тех ситуаций, когда Вы понимаете, что что-то пошло не
так только спустя несколько дней после того как это случилось, в
таких случаях неплохо иметь, например, резервные копии 1, 3, 10 и
30-дневной давности.&lt;/li&gt;
&lt;li&gt;Фотографии хранятся в системе хранения данных. После загрузки
изображения система выдает различные его размеры, на чем ее работа
заканчивается. Метаданные и ссылки на файловые системы, где
расположены фотографии, хранятся в базе данных.&lt;/li&gt;
&lt;li&gt;Агрегация данных проходит очень быстро, так как она ограничена
пределами сегмента.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max_connections = 400&lt;/code&gt; соединений на каждый сегмент, неплохой запас.
Значение для кэша потоков установлено равным 45, так как не бывает
ситуаций когда более 45 пользователей одновременно выполняют
какие-либо действия с одним конкретным сегментом.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Тэги&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Тэги плохо вписываются в традиционную нормализованную схему
реляционной базы данных. Денормализация или активное кэширование -
единственные способы сгенерировать облако меток для сотен миллионов
тэгов в течении миллисекунд.&lt;/li&gt;
&lt;li&gt;Некоторые данные обрабатываются отдельными вычислительными
кластерами, которые сохраняют результаты своей работы в MySQL, так
как иначе вычисление сложных отношений заняло бы все процессорное
время основных серверов баз данных.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Направления для развития&lt;/h4&gt;
&lt;p&gt;Ускорение работы с помощью создания
организационного плана для непрерывной работы всей системы на уровне
нескольких датацентров, таким образом чтобы все датацентры имели
возможность получать запросы на общий уровень данных (как сами БД,
так и memcache и прочее) все вместе одновременно. Если все части
системы постоянно активны - время простоя оборудования будет сведено
к минимуму.&lt;/p&gt;
&lt;h3 id="podvodim-itogi"&gt;Подводим итоги&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Старайтесь думать о своем приложении как о чем-то большем, чем просто
    веб-приложении, тогда у Вас возможно появятся поддержка различных
    API, RSS и Atom ленты и многие другие возможности.&lt;/li&gt;
&lt;li&gt;Отсутствие состояний системы позволяет более легко выполнять
    модернизации не моргнув и глазом.&lt;/li&gt;
&lt;li&gt;Реструктуризация базы данных - не самое лучшее занятие.&lt;/li&gt;
&lt;li&gt;Планирование нагрузок должно проводиться уже на ранних этапах
    развития проекта&lt;/li&gt;
&lt;li&gt;Начинайте медленно. Не покупайте сразу много оборудования просто
    из-за того, что Вы рады/боитесь, что ваш сайт взорвется.&lt;/li&gt;
&lt;li&gt;Измеряйте реально, планирование нагрузок должно базироваться на
    реальных вещах, а не абстрактных.&lt;/li&gt;
&lt;li&gt;Внедряйте ведение логов и индивидуальные измерения для оценки
    реальных показателей на основе серверной статистики, статистика
    использования не менее важна чем серверная.&lt;/li&gt;
&lt;li&gt;Кэширование и оперативная память может стать ответом на все вопросы.&lt;/li&gt;
&lt;li&gt;Создавайте четкие уровни абстракции между работой базы данных,
    бизнес-логикой, логикой страниц, разметкой страниц и презентационным
    уровнем. Это позволяет ускорить циклы итеративной разработки.&lt;/li&gt;
&lt;li&gt;Разделение приложения на уровни позволяет каждому заниматься своим
    делом: разработчики могут строить логику страниц, в то время как
    дизайнеры работают с удобством работы для пользователей.&lt;/li&gt;
&lt;li&gt;Делайте релизы как можно чаще, пускай даже это будет происходить
    каждые полчаса.&lt;/li&gt;
&lt;li&gt;Забудьте о всех небольших эффективных вещах, предварительная
    оптимизация является корнем всего зла в примерно 97% всех случаев.&lt;/li&gt;
&lt;li&gt;Тестируйте в работе. Постройте архитектурные механизмы (флаги
    конфигурации, балансировку нагрузки, и так далее), которые позволят
    Вам разворачивать новое оборудование в (и из) работу.&lt;/li&gt;
&lt;li&gt;Забудьте об искусственных тестах, они годятся только для получения
    общего представления о нагрузках, но не для планирования.
    Искуственные тесты дают искусственные результаты, для настоящих
    тестов все же стоит пользоваться реальным временем выполнения задач.&lt;/li&gt;
&lt;li&gt;Найдите максимальное значения для всех показателей:&lt;ul&gt;
&lt;li&gt;Какой максимум чего-то, что может выполнять каждый сервер?&lt;/li&gt;
&lt;li&gt;Как близко параметр находится к максимуму и каковы тенденции?&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/sql/"&gt;MySQL&lt;/a&gt; (дисковый ввод/вывод?)&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/squid/"&gt;Squid&lt;/a&gt; (дисковый ввод/вывод? или процессорное время?)&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt; (процессорное время? или пропускная способность?)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Старайтесь учесть особенности использования Вашего приложения.&lt;ul&gt;
&lt;li&gt;Возможен ли резкий рост нагрузки, связанный с каким-либо событием?
Например: какое-либо бедствие, или может быть новость?&lt;/li&gt;
&lt;li&gt;Flickr получает на 20-40% больше новых фотографий в первый рабочий
день нового года, чем в любой пик в предыдущем году.&lt;/li&gt;
&lt;li&gt;По воскресеньям нагрузка в среднем на 40-50% выше, чем в любой
другой день недели.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Учтите возможность экспоненциального роста. Больше пользователей
    означает больше контента, больше контента означает больше
    соединений, больше соединений означает более активное использование.&lt;/li&gt;
&lt;li&gt;Планируйте возможные варианты управления работой системы в периоды
    пиковых нагрузок.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 08 Feb 2008 22:41:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-02-08:highload/2008/arkhitektura-flickr/</guid><category>Apache</category><category>Cvsup</category><category>flickr</category><category>Ganglia</category><category>Java</category><category>Linux</category><category>Memcached</category><category>MySQL</category><category>online</category><category>Perl</category><category>PHP</category><category>RedHat</category><category>shard</category><category>Smarty</category><category>Squid</category><category>Subcon</category><category>SystemImager</category><category>архитектура</category><category>архитектура Flickr</category><category>интернет</category><category>кластер</category><category>Масштабируемость</category><category>сервер</category></item><item><title>Архитектура Google</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-google/</link><description>&lt;p&gt;&lt;em&gt;Эта статья датируется 2008 годом, новая версия: &lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-google-2011/"&gt;Архитектура Google 2011&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="/tag/google/"&gt;Google&lt;/a&gt;&lt;/strong&gt; - Король масштабируемости.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Каждый хоть раз слышал о &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; благодаря их
всеобъемлющему, "умному" и быстрому поисковому сервису, но ни для кого
не секрет, что они не ограничиваются только им. Их платформа для
построения масштабируемых приложений позволяет выпускать множество
удивительно конкурентноспособных интернет-приложений, работающих на
уровне всего Интернета вцелом. Они ставят перед собой цель постоянно
строить все более и более производительную и масштабируемую архитектуру
для поддержки своих продуктов. Как же им это удается?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Сразу хочу сказать, что эта запись является переводом с английского,
автор &lt;a href="https://www.insight-it.ru/goto/31bfd110/" rel="nofollow" target="_blank" title="http://highscalability.com/google-architecture"&gt;оригинальной версии&lt;/a&gt; - &lt;a href="https://www.insight-it.ru/goto/f3f1b405/" rel="nofollow" target="_blank" title="http://highscalability.com/user/todd-hoff"&gt;Todd Hoff&lt;/a&gt;. Оригинал написан приблизительно в середине 2007 года, но по-моему до сих пор очень даже актуально.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Далее следует перечисление источников информации из оригинала:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/741bec4c/" rel="nofollow" target="_blank" title="http://video.google.com/videoplay?docid=-5699448884004201579"&gt;Video: Построение больших систем в Google&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/fae0d413/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/gfs.html"&gt;Google Lab: Файловая система Google (GFS)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/39138d08/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/mapreduce.html"&gt;Google Lab: MapReduce: упрощенная обработка данных на больших кластерах&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8667b351/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/bigtable.html"&gt;Google Lab: BigTable.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/dab5470e/" rel="nofollow" target="_blank" title="http://video.google.com/videoplay?docid=7278544055668715642"&gt;Video: BigTable: система распределенного хранения данных.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/87fff9b2/" rel="nofollow" target="_blank" title="http://www.baselinemag.com/article2/0,1540,1985514,00.asp"&gt;Как работает Google&lt;/a&gt;
    от David Carr в Baseline Magazine.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a426f3de/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/sawzall.html"&gt;Google Lab: интерпретирование данных. Параллельный анализ с помощью Sawzall.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/ed8bca67/" rel="nofollow" target="_blank" title="http://www.25hoursaday.com/weblog/2007/06/25/GoogleScalabilityConferenceTripReportMapReduceBigTableAndOtherDistributedSystemAbstractionsForHandlingLargeDatasets.aspx"&gt;Записи с конференции по масштабированию от Dare Obasonjo.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Большое разнообразие языков программирования: &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;,
    &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="chto-vnutri"&gt;Что внутри?&lt;/h3&gt;
&lt;h4&gt;Статистика&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;На 2006 год система включала в себя 450000 недорогих серверов&lt;/li&gt;
&lt;li&gt;За 2005 год было проиндексировано 8 миллиардов страниц. На данный
    момент&amp;hellip; кто знает?&lt;/li&gt;
&lt;li&gt;На момент написания оригинала Google включает в себя более 200
    &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt; кластеров. Один кластер может состоять из 1000 или
    даже 5000 компьютеров&lt;/li&gt;
&lt;li&gt;Десятки и сотни тысяч компьютеров получают данные из &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;
    кластеров, которые насчитывают более 5 петабайт дискового
    пространства. Суммарные пропускная способность операций записи и
    чтения между дата центрами может достигать 40 гигабайт в секунду&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt; позволяет хранить миллиарды ссылок (URL),
    сотни терабайт снимков со спутников, а также настройки миллионов
    пользователей&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;// Цифры не первой свежести конечно, но тоже неплохо.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;Стек&lt;/h4&gt;
&lt;p&gt;&lt;a href="/tag/google/"&gt;Google&lt;/a&gt; визуализирует свою инфраструктуру в виде
трехслойного стека:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Продукты:&lt;/em&gt; поиск, реклама, электронная почта, карты, видео, чат,
    блоги&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Распределенная инфраструктура системы:&lt;/em&gt; &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;,
    &lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt; и &lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Вычислительные платформы:&lt;/em&gt; множество компьютеров во множестве
    датацентров&lt;/li&gt;
&lt;li&gt;Легкое развертывание для компании при низком уровне издержек&lt;/li&gt;
&lt;li&gt;Больше денег вкладывается в оборудование для исключения возможности
    потерь данных&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Надежное хранение данных с помощью &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Надежное масштабируемое хранение данных крайне необходимо для любого
    приложения. &lt;strong&gt;GFS&lt;/strong&gt; является основой их платформы хранения
    информации&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;&lt;/strong&gt; - большая распределенная файловая система, способная хранить и обрабатывать огромные объемы информации&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Зачем строить что-либо самим вместо того, чтобы просто взять это с полки?&lt;/em&gt; Они контролируют абсолютно всю систему и именно эта платформа отличает их от всех остальных.&lt;/p&gt;
&lt;p&gt;Она предоставляет:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;высокую надежность дата центров&lt;/li&gt;
&lt;li&gt;масштабируемость до тысяч сетевых узлов
&amp;ndash; высокую пропускную способность операций чтения и записи&lt;/li&gt;
&lt;li&gt;поддержку больших блоков данных, размер которых может измеряться в
гигабайтах&lt;/li&gt;
&lt;li&gt;эффективное распределение операций между датацентрами для
избежания возникновения "узких мест" в системе&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;В системе существуют мастер-сервера и сервера, собственно хранящие
    информацию:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Мастер-сервера хранят метаданные для всех файлов. Сами данные
    хранятся блоками по 64 мегабайта на остальных серверах. Клиенты
    могут выполнять операции с метаданными на мастер-серверах, чтобы
    узнать на каком именно сервере расположены необходимые данные.&lt;/li&gt;
&lt;li&gt;Для обеспечения надежности один и тот же блок данных хранится
    в трех экземплярах на разных серверах, что обеспечивает
    избыточность на случай сбоев в работе какого-либо сервера.&lt;/li&gt;
&lt;li&gt;Новые приложения могут пользоваться как существующими
    кластерами, так и новыми, созданными специально для них.&lt;/li&gt;
&lt;li&gt;Ключ успеха заключается в том, чтобы быть уверенными в том,
    что у людей есть достаточно вариантов выбора для реализации их
    приложений. &lt;strong&gt;&lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;&lt;/strong&gt; может быть настроена для
    удовлетворения нужд любого конкретного приложения.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Работаем с данными при помощи MapReduce&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Теперь, когда у нас есть отличная система хранения, что же делать с
    такими объемами данных? Допустим, у нас есть много терабайт данных,
    равномерно распределенных между 1000 компьютерами. Коммерческие базы
    данных не могут эффективно масштабироваться до такого уровня, именно
    в такой ситуации в дело вступает технология
    &lt;strong&gt;&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;&lt;/strong&gt; является программной моделью и
    соответствующей реализацией обработки и генерации больших наборов
    данных. Пользователи могут задавать функцию, обрабатывающую пары
    ключ/значение для генерации промежуточных аналогичных пар, и
    сокращающую функцию, которая объединяет все промежуточные значения,
    соответствующие одному и тому же ключу. Многие реальные задачи могут
    быть выражены с помощью этой модели. Программы, написанные в таком
    функциональном стиле автоматически распараллеливаются и адаптируются
    для выполнения на обширных кластерах. Система берет на себя детали
    разбиения входных данных на части, составления расписания выполнения
    программ на различных компьютерах, управления ошибками, и
    организации необходимой коммуникации между компьютерами. Это
    позволяет программистам, не обладающим опытом работы с параллельными
    и распределенными системами, легко использовать все ресурсы больших
    распределенных систем.&lt;/li&gt;
&lt;li&gt;Зачем использовать &lt;strong&gt;&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;&lt;/strong&gt;?
    &amp;ndash; Отличный способ распределения задач между множеством компьютеров
    &amp;ndash; Обработка сбоев в работе
    &amp;ndash; Работа с различными типами смежных приложений, таких как поиск или
    реклама. Возможно предварительное вычисление и обработка данных,
    подсчет количества слов, сортировка терабайт данных и так далее
    &amp;ndash; Вычисления автоматически приближаются к источнику ввода-вывода&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;&lt;/strong&gt; использует три типа серверов:&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Master:&lt;/em&gt; назначают задания остальным типам серверов, а также
следят за процессом их выполнения&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Map:&lt;/em&gt; принимают входные данные от пользователей и обрабатывают
их, результаты записываются в промежуточные файлы&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Reduce:&lt;/em&gt; принимают промежуточные файлы от Map-серверов и
сокращают их указанным выше способом&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Например, мы хотим посчитать количество слов на всех страницах. Для
    этого нам необходимо передать все страницы, хранимые в &lt;strong&gt;GFS&lt;/strong&gt;, на
    обработку в &lt;strong&gt;MapReduce&lt;/strong&gt;. Этот процесс будет происходить на тысячах
    машин одновременно с полной координацией действий, в соответствии с
    автоматически составленным расписанием выполняемых работ, обработкой
    потенциальных ошибок, и передачей данных выполняемыми автоматически.&lt;ul&gt;
&lt;li&gt;Последовательность выполняемых действий выглядела бы следующим
образом: &lt;code&gt;GFS &amp;rarr; Map &amp;rarr; перемешивание &amp;rarr; Reduce &amp;rarr; запись результатов обратно в GFS&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Технология &lt;strong&gt;MapReduce&lt;/strong&gt; состоит из двух компонентов:
соответственно &lt;em&gt;map&lt;/em&gt; и &lt;em&gt;reduce&lt;/em&gt;. Map отображает один набор данных в
другой, создавая тем самым пары ключ/значение, которпыми в нашем
случае являются слова и их количества.&lt;/li&gt;
&lt;li&gt;В процессе перемешивания происходит агрегирование типов ключей.&lt;/li&gt;
&lt;li&gt;Reduction в нашем случае просто суммирует все результаты и
возвращает финальный результат.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;В процессе индексирования &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; подвергает поток
    данных обработке около 20 разных механизмов сокращения. Сначала идет
    работа над всеми записями и агрегированными ключами, после чего
    результат передается следующему механизму и второй механизм уже
    работает с результатами работы первого, и так далее.&lt;/li&gt;
&lt;li&gt;Программы могут быть очень маленькими, всего лишь от 20 до 50 строк
    кода.&lt;/li&gt;
&lt;li&gt;Единственной проблемой могут быть "отстающие компьютеры". Если один
    компьютер работает существенно медленнее, чем все остальные, это
    будет задерживать работу всей системы в целом.&lt;/li&gt;
&lt;li&gt;Транспортировка данных между серверами происходит в сжатом виде.
    Идея заключается в том, что ограничивающим фактором является
    пропускная способность канала и ввода-вывода, что делает резонным
    потратить часть процессорного времени на компрессию и декомпрессию
    данных.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Хранение структурированных данных в BigTable&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;BigTable&lt;/strong&gt; является крупномасштабной, устойчивой к потенциальным
    ошибкам, самоуправляемой системой, которая может включать в себя
    терабайты памяти и петабайты данных, а также управлять миллионами
    операций чтения и записи в секунду.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BigTable&lt;/strong&gt; представляет собой распределенный механизм хэширования,
    построенный поверх &lt;strong&gt;&lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;&lt;/strong&gt;, а вовсе не реляционную базу
    данных и, как следствие, не поддерживает &lt;a href="/tag/sql/"&gt;SQL&lt;/a&gt;-запросы и
    операции типа Join.&lt;/li&gt;
&lt;li&gt;Она предоставляет механизм просмотра данных для получения доступа к
    структурированным данным по имеющемуся ключу. &lt;strong&gt;&lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;&lt;/strong&gt;
    хранит данные не поддающиеся пониманию, хотя многим приложениям
    необходимы структурированные данные.&lt;/li&gt;
&lt;li&gt;Коммерческие базы данных попросту не могут масштабироваться до
    такого уровня и, соответственно, не могут работать с тысячами машин
    одновременно.&lt;/li&gt;
&lt;li&gt;С помощью контролирования своих низкоуровневых систем хранения
    данных, &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; получает больше возможностей по
    управлению и модификации их системой. Например, если им понадобится
    функция, упрощающая координацию работы между датацентрами, они
    просто могут написать ее и внедрить в систему.&lt;/li&gt;
&lt;li&gt;Подключение и отключение компьютеров к функционирующей системе никак
    не мешает ей просто работать.&lt;/li&gt;
&lt;li&gt;Каждый блок данных хранится в ячейке, доступ к которой может быть
    предоставлен как по ключу строки или столбца, так и по временной
    метке.&lt;/li&gt;
&lt;li&gt;Каждая строка может храниться в одной или нескольких таблицах.
    Таблицы реализуются в виде последовательности блоков по 64
    килобайта, организованных в формате данных под названием
    &lt;strong&gt;SSTable&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;В &lt;strong&gt;&lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt;&lt;/strong&gt; тоже используется три типа серверов:&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Master:&lt;/em&gt; распределяют таблицы по Tablet-серверам, а также следят
за расположением таблиц и перераспределяют задания в случае
необходимости.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Tablet:&lt;/em&gt; обрабатывают запросы чтения/записи для таблиц. Они
разделяют таблицы, когда те превышают лимит размера (обычно 100-200
мегабайт). Когда такой сервер прекращает функционирование по
каким-либо причинам, 100 других серверов берут на себя по одной
таблице и система продолжает работать как-будто ничего не произошло.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Lock:&lt;/em&gt; формируют распределенный сервис ограничения одновременного
доступа. Операции открытия таблицы для записи, анализа
Master-сервером или проверки доступа должны быть
взаимоисключающими.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Локальная группировка может быть использована для физического
    хранения связанных данных вместе, чтобы обеспечить лучшую
    локализацию ссылок на данные.&lt;/li&gt;
&lt;li&gt;Таблицы по возможности кэшируются в оперативной памяти серверов.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="oborudovanie"&gt;Оборудование&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Как эффективно организовать большую группу компьютеров с точки
    зрения издержек и производительности?&lt;/li&gt;
&lt;li&gt;Используется самое обыкновенное ультра-дешевое оборудование и поверх
    него строится программное обеспечение, способное спокойно пережить
    смерть любой части оборудования.&lt;/li&gt;
&lt;li&gt;Тысячекратный рост вычислительной мощности может быть достигнут с
    издержками в 33 раза меньшими, если воспользоваться толерантной к
    сбоям инфраструктурой, по сравнению с инфраструктурой, построенной
    на высоконадежных компонентах. Надежность строится поверх ненадежных
    компонентов.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;, домашнее размещение серверов, материнские платы
    предназначенные для персональных компьютеров, дешевые средства
    хранения данных.&lt;/li&gt;
&lt;li&gt;Цена за каждый ватт энергии в расчете на производительность не
    становится меньше, что ведет к большим проблемам связанным с
    энергообеспечением и охлаждением.&lt;/li&gt;
&lt;li&gt;Использование совместного размещения в своих и арендуемых
    датацентрах.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="raznoe"&gt;Разное&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Быстрый выпуск изменений более предпочтителен, чем ожидание.&lt;/li&gt;
&lt;li&gt;Библиотеки - превалирующий метод построения программ.&lt;/li&gt;
&lt;li&gt;Некоторые приложения предоставляются в виде сервисов.&lt;/li&gt;
&lt;li&gt;Инфраструктура управляет определением версий приложений таким
    образом, что они могут выпускать новые продукты, не боясь сломать
    работу какого-либо компонента системы.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="puti-razvitiia"&gt;Пути развития&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Поддержка географически распределенных кластеров.&lt;/li&gt;
&lt;li&gt;Создание единого глобального пространства имен для всех данных. На
    данный момент данные распределены по кластерам.&lt;/li&gt;
&lt;li&gt;Более автоматизированные передача и обработка данных&lt;/li&gt;
&lt;li&gt;Решение вопросов, связанных с поддержанием работоспособности
    сервисов даже в тех случаях, когда целый кластер отключается от
    системы в связи с техническими работами или каким-либо сбоем в
    работе.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="podvodim-itogi"&gt;Подводим итоги&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Инфраструктура может быть конкурентным преимуществом.&lt;/strong&gt; Это
    определенно так для Google. Они могут выпускать новые интернет
    сервисы быстрее, с меньшими издержками, на таком уровне, что мало
    кто сможет составить им конкуренцию. Подход многих компаний сильно
    отличается от подхода &lt;a href="/tag/google/"&gt;Google&lt;/a&gt;, эти компании
    рассматривают инфраструктуру как статью расходов, они обычно
    используют совсем другие технологии и совсем не задумываются о
    планировании и организации своей системы. Google позиционирует себя
    как компанию по построению систем, что является очень современным
    подходом к разработке программного обеспечения.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Охватывание нескольких дата центров до сих пор является нерешенной проблемой.&lt;/strong&gt; Большинство сайтов базируется в одном или двух дата
    центрах. Полное распределение сайта между несколькими датацентрами
    является хитрой задачей.&lt;/li&gt;
&lt;li&gt;Взгляните на &lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/30a7481/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/"&gt;Hadoop&lt;/a&gt;&lt;/em&gt;, если у Вас
    нет времени на собственноручное построение всей архитектуры с нуля.
    &lt;em&gt;Hadoop&lt;/em&gt; является opensource воплощением в жизнь многих идей здесь
    представленных.&lt;/li&gt;
&lt;li&gt;Часто недооцениваемым преимуществом платформенного подхода является
    тот факт, что даже неопытные разработчики могут быстро и качественно
    реализовывать трудоемкие приложения на базе платформы. Но если бы
    каждый проект требовал одинаково распределенной архитектуры, то это
    создало бы много проблем, так как люди, которые понимают как это
    делается, являются достаточно большой редкостью.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Совместная деятельность не всегда является таким уж плохим занятием.&lt;/strong&gt; Если все части системы работают взаимосвязанно, то
    улучшение в одной из них сразу и абсолютно прозрачно отразится
    положительным образом и на остальных компонентах системы. В
    противном случае такой эффект наблюдаться не будет.&lt;/li&gt;
&lt;li&gt;Построение самоуправляемых систем позволяет более легко
    перераспределять ресурсы между серверами, расширять систему,
    отключать некоторые компьютеры и элегантно проводить обновления.&lt;/li&gt;
&lt;li&gt;Производить длительные операции стоит параллельно.&lt;/li&gt;
&lt;li&gt;Всему, что было сделано Google, предшествовало искусство, а не
    только крупномасштабное развертывание системы.&lt;/li&gt;
&lt;li&gt;Учитывайте возможность &lt;strong&gt;компрессии данных&lt;/strong&gt;, она является очень
    неплохим решением, если остается лишнее процессорное время, но
    присутствует нехватка пропускной способности.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 31 Jan 2008 18:05:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-01-31:highload/2008/arkhitektura-google/</guid><category>BigTable</category><category>featured</category><category>GFS</category><category>Google</category><category>MapReduce</category><category>online</category><category>Sawzall</category><category>архитектура</category><category>архитектура Google</category><category>интернет</category><category>кластер</category><category>Масштабируемость</category><category>поиск</category><category>сервер</category><category>хранение данных</category></item></channel></rss>