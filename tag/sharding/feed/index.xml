<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Insight IT</title><link>https://www.insight-it.ru/</link><description></description><atom:link href="https://www.insight-it.ru/tag/sharding/feed/index.xml" rel="self"></atom:link><lastBuildDate>Thu, 22 May 2008 16:39:00 +0400</lastBuildDate><item><title>Архитектура Google Talk</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-googletalk/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/8bce017/" rel="nofollow" target="_blank" title="http://www.google.com/talk"&gt;Google Talk&lt;/a&gt; представляет собой сервис
мгновенного обмена сообщениями от Google. В основе этого сервиса лежит
&lt;abbr title="eXtensible Messaging and Presence Protocol"&gt;XMPP&lt;/abbr&gt; протокол, более известный как &lt;em&gt;Jabber&lt;/em&gt;. В России среди &lt;abbr title="Instant Messaging"&gt;IM&lt;/abbr&gt;-сервисов
несомненно наиболее широко распространен &lt;em&gt;ICQ&lt;/em&gt;, но количество русских
пользователей &lt;em&gt;Jabber&lt;/em&gt; тоже неуклонно растет.&lt;/p&gt;
&lt;p&gt;Вам когда-нибудь доводилось задумываться какое количество сообщений
приходится обрабатывать такого рода сервисам? Допустим есть абстрактный
&lt;abbr title="Instant Messaging"&gt;IM&lt;/abbr&gt;-сервис, которым пользуется миллион пользователей, в среднем каждый из
них отправляет сто текстовых сообщений. Сколько всего сообщений
обработал и доставил сервис? Сто миллионов? Наивно!
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="vvedenie"&gt;Введение&lt;/h3&gt;
&lt;p&gt;Сервисы мгновенного обмена на самом деле подвергаются существенно
большей нагрузке, чем это может показаться на первый взгляд. Давайте
взглянем на расшифровку аббревиатуры &lt;abbr title="eXtensible Messaging and Presence Protocol"&gt;XMPP&lt;/abbr&gt;: eXtensible Messaging and
Presence Protocol. Обмен сообщениями - лишь одна из его функций,
наиболее важная же его часть остается "за сценой" - отображение
присутствия пользователей &lt;em&gt;online&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Давайте посмотрим на наш абстрактный пример с точки зрения присутствия:
пускай им пользуется все тот же миллион пользователей, когда один из них
включил компьютер и появился online - он должен уведомить весь свой
список контактов об этом событии, а также узнать кто из них находится
online. Если этот список велик, то такое элементарное событие может
обернуться для сервиса далеко не одной сотней обработанных и
доставленных сообщений. Помимо простого изменения статуса online/offline
подобную цепочку сообщений может генерировать и любое другое изменение
статуса: связанное с отсутствием пользователя около компьютера или с
изменением небольшого текстового сообщения, которое обычно отображается
в контакт листе рядом с ником пользователя и призвано отображать текущее
его состояние, занятие или чего там только не пишут (эта функция не
всегда предоставляется &lt;abbr title="Instant Messaging"&gt;IM&lt;/abbr&gt;-сервисами, но наверняка многим знакома по
&lt;em&gt;ICQ&lt;/em&gt;, если не по &lt;em&gt;Jabber&lt;/em&gt;). Все эти сообщения как раз и стоят за
"presence" в аббревиатуре &lt;abbr title="eXtensible Messaging and Presence Protocol"&gt;XMPP&lt;/abbr&gt;, суммарный траффик, ими генерируемый,
может в несколько раз превышать траффик от собственно самих текстовых
сообщений.&lt;/p&gt;
&lt;p&gt;Если учесть факты, описанные в предыдущем абзаце, не трудно догадаться,
что зависимость суммарного количества presence-сообщений от количества
пользователей &lt;abbr title="Instant Messaging"&gt;IM&lt;/abbr&gt;-сервиса далеко не линейна. Их количество за какой-то
период времени можно очень приблизительно посчитать как произведение
трех параметров: количества пользователей online, средней длины списка
контактов среди них и количества изменений статуса каждым пользователем.
А каждый дополнительный пользователь в системе так или иначе увеличивает
как минимум два из этих трех параметров.&lt;/p&gt;
&lt;p&gt;Введение несколько затянулась, а проблема масштабируемости &lt;abbr title="eXtensible Messaging and Presence Protocol"&gt;XMPP&lt;/abbr&gt;-сервисов
я думаю теперь стала очевидна, так что сейчас очень подходящий момент,
чтобы вернуться к основной теме разговора - сервису Google Talk и том,
как команда его разработчиков решает эту проблему.&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;Наверное уже стало заметно, что это не очередной перевод, а лично мной
написанный текстик. Так что сразу выдам
&lt;a href="https://www.insight-it.ru/goto/bd0c001f/" rel="nofollow" target="_blank" title="http://video.google.com/videoplay?docid=6202268628085731280"&gt;видео&lt;/a&gt;,
являющееся основным источником информации, и продолжу.&lt;/p&gt;
&lt;h3 id="arkhitektura"&gt;Архитектура&lt;/h3&gt;
&lt;p&gt;Со стороны Google (о котором я, кстати говоря, &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-google/"&gt;уже писал&lt;/a&gt;) было бы глупо строить
сервис мгновенного обмена сообщениями в стороне от остальных
коммуникационных сервисов, предоставляемых этой компанией. Еще до своего
публичного старта Google Talk был интегрирован в почтовый сервис
&lt;a href="https://www.insight-it.ru/goto/af1829ed/" rel="nofollow" target="_blank" title="http://www.gmail.com"&gt;GMail&lt;/a&gt; и социальную сеть
&lt;a href="https://www.insight-it.ru/goto/31afe99f/" rel="nofollow" target="_blank" title="http://www.orkut.com"&gt;Orkut&lt;/a&gt;: эти сервисы просто запрашивали у Google
Talk присутствие online пользователей из своего списка контактов при
возникновении соответствующих событий, но при этом не отображали
результаты в своих страницах. Таким образом разработчики получили
возможность оценить предстоящие нагрузки и готовность сервиса к
публичному запуску намного более точно, чем они могли бы это сделать
средствами синтетических тестов.&lt;/p&gt;
&lt;p&gt;В отношении распределения нагрузок, сразу же был выбран и реализован
подход, связанный с разбиением пользователей на группы и распределением
работы с каждой отдельной группой по разным серверам. Это позволило
избежать всей той эволюции серверной части приложения от одного сервера
до большого кластера, что впрочем вполне оправданно, так как сразу же
после запуска сервису предстояло столкнуться с огромным количеством
пользователей и не ничуть не меньшей нагрузкой. Разработчики не забыли и
сразу же предусмотреть безболезненный перенос пользователей с одного
сервера на другой без видимых для него изменений, это позволило очень
гибко изменять количество серверов в системе.&lt;/p&gt;
&lt;p&gt;С точки зрения интеграции сервиса с другими проектами
&lt;a href="/tag/google/"&gt;Google&lt;/a&gt;, очень важно было предоставить определенный
уровень абстракции для взаимодействия в виде API и набора адресов, по
которым необходимо обращаться к сервису. Придерживаясь одного API можно
производить практически любые архитектурные или программные изменения в
рамках проекта таким образом, что все его пользователи и проекты, в
которые он интегрирован, просто не заметят что что-то изменилось.
Адреса, к которым происходит обращение при обмене данных, так же
являются своеобразной абстракцией - можно переместить сервис в новый
датацентр и благодаря DNS трафик будет направляться в нужное место.&lt;/p&gt;
&lt;p&gt;С другой стороны необходимо учитывать и программное обеспечение
работающие ниже уровнем, чем собственно код приложения: особенно ядро
операционной системы и используемые библиотеки. В данном случае большую
роль играет количество открытых TCP соединений, так как &lt;abbr title="Instant Messaging"&gt;IM&lt;/abbr&gt; требует
большое их количество, но активность в них не велика.&lt;/p&gt;
&lt;p&gt;Разработчики Google Talk постарались как можно больше внимания уделить
возможным сбоям и связанным с ними ситуациям. Любое даже запланированное
временное прекращение функционирования какой-то части системы может
резко увеличить нагрузку на остальную часть, даже если это просто
перезагрузка части системы - из-за очистившегося кэша серверы снова
начнут полноценно функционировать далеко не сразу, не говоря уже о
непредвиденных сбоях, когда последствия намного более глобальны. Для
своевременного устранения потенциальных проблем как с общем
функционированием системы, так и с недостаточной производительностью,
ведутся логи для всех этапов обработки запросов, а также предусмотрена
возможность профайлинга прямо на работающих в системе серверах.&lt;/p&gt;
&lt;p&gt;Но не стоит забывать и о клиентской части программного обеспечения:
какая-нибудь глупая ошибка в коде клиента сервиса запросто может
устроить DDoS атаку на сервис, что и случилось с одной из ранних версий
клиента Google Talk. Помимо этого необходимо поддерживать совместимость
разных версий клиентских приложений.&lt;/p&gt;
&lt;h3 id="zakliuchenie"&gt;Заключение&lt;/h3&gt;
&lt;p&gt;Благодаря описанным выше принципам Google Talk удается обрабатывать
каждое из миллиардов сообщений в день менее чем за 100 миллисекунд.
Тесная интеграция с другими сервисами &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; позволила
проекту сразу же получить невероятную популярность, а продуманный подход
к разработке сервиса позволил справиться с огромной нагрузкой.&lt;/p&gt;
&lt;p&gt;На этот раз статья получилась скорее о специфике сервиса, чем о его
реализации. Технической информации найти практически не удалось, так что
очень кратко все, но надеюсь и в таком варианте было достаточно
интересно почитать. Напоследок хочу порекомендовать &lt;a href="/feed/"&gt;подписаться на RSS&lt;/a&gt;, если не хотите пропустить публикацию новых постов.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 22 May 2008 16:39:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-05-22:highload/2008/arkhitektura-googletalk/</guid><category>Google</category><category>Google Talk</category><category>IM</category><category>Jabber</category><category>Java</category><category>Linux</category><category>online</category><category>sharding</category><category>XMPP</category><category>архитектура</category><category>архитектура Google Talk</category><category>присутствие</category></item><item><title>Сегментирование базы данных</title><link>https://www.insight-it.ru//theory/2008/segmentirovanie-bazy-dannykh/</link><description>&lt;p&gt;&lt;img alt="Сегментирование базы данных" class="left" src="https://www.insight-it.ru/images/partitions.png"/&gt;
В процессе чтения моего блога у вас наверняка возникал вопрос: а что же
имеется в виду под фразой &lt;em&gt;сегментирование базы данных&lt;/em&gt;?
На самом деле это просто приглянувшийся мне вариант перевода термина
&lt;em&gt;sharding&lt;/em&gt; (или он же - &lt;em&gt;partitioning&lt;/em&gt;), в качестве альтернатив можно
было бы использовать &lt;em&gt;партиционирование&lt;/em&gt;, &lt;em&gt;секционирование&lt;/em&gt; или
что-нибудь еще менее звучное, суть от этого не меняется.&lt;/p&gt;
&lt;p&gt;Сильно сомневаюсь, что предыдущий абзац предоставил Вам полную
информацию по данному вопросу, так что позволю себе перейти к более
детальным ответам...
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="chto-eto-takoe"&gt;Что это такое?&lt;/h3&gt;
&lt;p&gt;Особенно в условиях Сети данные накапливаются и запрашиваются с
невероятной скоростью, рано или поздно даже самый мощный сервер
перестанет справляться с задачей хранения и предоставления данных.
Количество запросов в секунду, которое способен обеспечивать один сервер
ограничено ничуть не меньше, чем его дисковое пространство. Когда
запросы данных начинают поступать слишком интенсивно, чаще всего
прибегают к наиболее тривиальному решению: реплицирование данных на
несколько серверов и обработка запросов на чтение данных параллельно, а
записи - лишь на одном, классическая схема master-slave. Но и у такого
подхода есть предел, с ростом системы затраты вычислительных мощностей
на реплицирование данных рано или поздно начнут потреблять большую часть
процессорного времени, что сделает прирост производительности от
простого добавления в систему дополнительных серверов минимальным.
Единственным выходом из такой ситуации становится пересмотр архитектуры
всей системы хранения данным, одним из возможных исходов которого и
сожет стать сегментирование базы данных.&lt;/p&gt;
&lt;p&gt;Сама идея сегментирования проста: разбить все данные на части по
какому-либо признаку и хранить каждую часть на отдельном сервере или
кластере, такую часть данных в совокупности с системой хранения данных,
в которой она находится, и называют сегментом или shard'ом.&lt;/p&gt;
&lt;p&gt;Признак по которому разделяются данные должен быть максимально прост и
очевиден, ведь в случае необходимости получения конкретных данных именно
он будет служить путеводителем для определения в каком именно сегменте
эти данные необходимо искать. В большинстве ситуаций такая необходимость
возникает очень часто, а значит и проверка признака должна выполняться
очень быстро. Сами эти признаки можно распределить на несколько групп по
принципу их проверки:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Диапазон&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Самый простой и тривиальный вариант, выбирается один из параметров
какой-либо записи, например идентификационный номер, и проверяется
его принадлежность определенному диапазону, например все записи с ID
от 0 до 999 хранятся на одном сервере, от 1000 до 1999 - на другом,
и так далее.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Список&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Принцип остается такой же как и при использовании диапазонов, с той
лишь разницей что проверяется принадлежность параметра какому-либо
списку значений (который может состоять и из одного значения), а не
диапазону, например: людей можно разбить по районам проживания или
их имени.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Хэш-функция&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;О принципах работы хэширующих функций &lt;a href="https://www.insight-it.ru/security/2008/obratnogo-puti-net/"&gt;я уже рассказывал&lt;/a&gt;, так что
лишь вкратце опишу принцип такого подхода: для определения в каком
именно сегменте хранится та или иная запись, один из ее заранее
известных параметров передается хэширующей функции, возвращающей в
качестве результата номер сегмента от 0 до (n-1), где - n общее
количество сегментов.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Композиция&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;При дальнейшем росте объемов данных и нагрузке на систему можно
несколько усложнить ее работу, реализовав сегментирование на основе
композиции из нескольких упомянутых выше признаков.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;В условиях интернет-проектов данные обычно разбиваются по принадлежности
к пользователю, автором которых он является (или просто они как-либо
взаимосвязаны), с использованием хэш-функций.&lt;/p&gt;
&lt;h3 id="dlia-chego-eto-vse-nuzhno"&gt;Для чего это все нужно?&lt;/h3&gt;
&lt;p&gt;Возможно описание получилось несколько устрашающим, но на самом деле вся
эта история с распределением данных того стоит. Для начала стоит
отметить, что такой подход позволяет оперировать огромными объемами
данными с невероятной скоростью, благодаря параллельной обработке
запросов в абсолютно разных частях системы. Количество запросов,
обрабатываемых каждым узлом одновременно невелико, что не может не
сказаться на скорости обработки каждого запроса.&lt;/p&gt;
&lt;p&gt;Использование в качестве системы хранения данных для каждого сегмента
кластера, а не просто одного сервера, может существенно повысить
надежность системы в случае программных или аппаратных сбоев. Тем более
в случае использования master-slave репликации в рамках каждого
сегмента, у системы в целом все равно не будет единственного сервера для
обработке операций записи позволит минимизировать издержки
реплицирования данных.&lt;/p&gt;
&lt;h3 id="a-kak-zhe"&gt;А как же...&lt;/h3&gt;
&lt;h4&gt;...перераспределять данные?&lt;/h4&gt;
&lt;p&gt;При необходимости по тем или иным причинам изменить количество сегментов
возникновение ситуации, когда система нагружена неравномерно, очень
вероятно. Наиболее простым решением было бы простое перераспределение
записей, но как же его реализовать?&lt;/p&gt;
&lt;p&gt;Над этой проблемой стоит задуматься сразу же при переходе к
сегментированной архитектуре. Она может сначала показаться нерешаемой
без одновременного перемещения огромных массивов данных и временной
практически полной потери производительности, но это не так. Наиболее
элегантным решением является организация для системы некоторого сервиса
определения местоположения данных, в обязанности входит не только
определение номера сегмента по заранее определенному алгоритму, но и
постепенное перемещение данных в случае необходимости. Например, в
случае появления необходимости разбить сегмент на две части в связи с
приближающимся переполнением дискового простронства, данный сервис
начнет постепенно последовательно создавать копии записей на новом или
существующем слабо загруженном сегменте. Пока копирование каждой
конкретной записи не будет завершено операции чтения перенаправляются на
исходный сегмент. Как только процесс завершился - перенаправление данных
переключается на копию, а оригинал уничтожается, после чего система
переходит к перемещению следующей записи.&lt;/p&gt;
&lt;h4&gt;...выполнить операцию, затрагивающию разные сегменты?&lt;/h4&gt;
&lt;p&gt;Когда все данные хранились в одном месте - можно было бы выполнить один
сложноструктурированный запрос и получить все нужные данные, но при
таком распределении данных это не возможно. Для достижения такого же
результата необходимо выполнение нескольких запросов к различным
сегментам и аггрегация полученных данных на программном уровне.&lt;/p&gt;
&lt;h4&gt;...реализовать все это?&lt;/h4&gt;
&lt;p&gt;Один из самых интересных вопросов, которые можно было бы задать по
данной теме. Однозначно лучшего решения для реализации этого подхода не
существует. В большинстве случаев приходится реализовывать теорию на
практике своими силами, но готовые решения все же тоже существуют:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/2b245ae9/" rel="nofollow" target="_blank" title="http://www.enterprisedb.com/community/projects/gridsql.do"&gt;GridSQL&lt;/a&gt;
    от EnterpriseDB предоставляет систему сегментирования на базе
    &lt;a href="/tag/postgresql/"&gt;PostgreSQL&lt;/a&gt; (которую, кстати, не так давно под GPL
    опубликовали);&lt;/li&gt;
&lt;li&gt;Многие реляционные системы управления базами данных, такие как MySQL
    и Oracle, имеют собственную встроенную систему разбиения данных на
    партиции;&lt;/li&gt;
&lt;li&gt;В рамках проекта &lt;a href="https://www.insight-it.ru/goto/cacbd918/" rel="nofollow" target="_blank" title="http://www.hibernate.org"&gt;Hibernate&lt;/a&gt;
    разрабатывается библиотека, инкапсулирующая сегментирование данных;&lt;/li&gt;
&lt;li&gt;В &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-livejournal/"&gt;статье об архитектуре LiveJournal&lt;/a&gt; я рассказывал о спектре opensource-решений от разработчиков этого проекта для схожих задач.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Так или иначе ни один из них не является средством из серии "установил и
все сразу заработало", они лишь могут упростить реализацию такой
системы, существенную часть работы придется проделать самостоятельно.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ещё остались вопросы по теме?&lt;/em&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 01 May 2008 20:12:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-05-01:theory/2008/segmentirovanie-bazy-dannykh/</guid><category>partitioning</category><category>shard</category><category>sharding</category><category>архитектура</category><category>Масштабируемость</category><category>партиционирование</category><category>сегментирование</category><category>сегментирование баз данных</category><category>секционирование</category><category>СУБД</category></item></channel></rss>