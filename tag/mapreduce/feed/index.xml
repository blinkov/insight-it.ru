<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Insight IT</title><link>https://www.insight-it.ru/</link><description></description><atom:link href="https://www.insight-it.ru/tag/mapreduce/feed/index.xml" rel="self"></atom:link><lastBuildDate>Sun, 17 Aug 2008 23:15:00 +0400</lastBuildDate><item><title>Hadoop возвращается</title><link>https://www.insight-it.ru//storage/2008/hadoop-vozvrashhaetsya/</link><description>&lt;p&gt;Если Вы являетесь постоянным читателем моего блога, то вполне вероятно,
что Вы помните мой &lt;a href="https://www.insight-it.ru/storage/2008/hadoop/"&gt;старый пост&lt;/a&gt; об этом
замечательном проекте от &lt;a href="https://www.insight-it.ru/goto/e3b03afc/" rel="nofollow" target="_blank" title="http://apache.org"&gt;Apache Foundation&lt;/a&gt;. С тех
пор он развивался невероятными темпами и очень многое успело измениться,
об этом я и хотел бы сегодня поделиться своими впечатлениями. В
дополнение к этому планируется небольшая инструкция по развертыванию
Hadoop на кластере из большого количества машин, который послужит
неплохим развитием темы, начатой в посте &lt;a href="https://www.insight-it.ru/storage/2008/hadoop-dlya-razrabotchika/"&gt;"Hadoop для разработчика"&lt;/a&gt;.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="chto-novogo"&gt;Что нового?&lt;/h3&gt;
&lt;p&gt;Для начала вкратце напомню что их себя представляет данный продукт,
всего в нем три компонента:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;HDFS&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;кластерная файловая система.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;MapReduce framework&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;программная основа для построения приложений, работающих по
одноименной модели.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;HBase&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;нереляционная база данных.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Повторно повторяться смысла не вижу, все уже давно &lt;a href="https://www.insight-it.ru/storage/2008/hadoop/"&gt;разложено по полочкам&lt;/a&gt;. Так что сразу перейдем к глобальным
изменениям в проекте, произошедшим с написания вышеупомянутого поста, то
есть с февраля. Сразу хочу сказать, что подробно пересказывать &lt;a href="https://www.insight-it.ru/goto/81620ac0/" rel="nofollow" target="_blank" title="http://svn.apache.org/repos/asf/hadoop/core/trunk/CHANGES.txt"&gt;release notes&lt;/a&gt; у
меня нет никакого желания, если Вам интересны все подробности о каждом
bugfix'е или изменении в API, то имеет смысл почитать их в оригинале.&lt;/p&gt;
&lt;p&gt;Наиболее значительным событием в развитии Apache Hadoop было, пожалуй,
отделение &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt; в отдельный проект. Какие же это повлекло
последствия? С точки зрения простого смертного наиболее заметен тот
факт, что HBase пропал из основного архива или репозитория Hadoop и его
теперь нужно качать отдельно :) На самом же деле такое обособление лишь
ускорило ее развитие, совсем недавно HBase отпраздновала свой релиз
версии 0.2.0, включающий в себя массу нововведений и исправленных
проблем, например язык запросов HQL был полностью заменен на jirb/jython
shell, а также было добавлено кэширование данных в памяти. Помимо этого
сильно изменилось API, очень рекомендую заглянуть в
&lt;a href="https://www.insight-it.ru/goto/f059ad5e/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/hbase/docs/current/api/index.html"&gt;javadoc&lt;/a&gt;
проекта, если Вас это интересует.&lt;/p&gt;
&lt;p&gt;На уровне файловой системы наиболее значительным изменением стало
добавление еще одного типа узлов - &lt;strong&gt;Secondary NameNode&lt;/strong&gt;. Это
нововведение является первым шагом на пути к устранению узких мест в
системе (так называемых single points of failure). Название этого типа
узлов говорит само за себя: они подстраховывают основной &lt;em&gt;NameNode&lt;/em&gt; на
случай непредвиденных сбоев. Они создают резервную копию образа
метаданных файловой системы и лога транзакций (то есть всех операций с
файлами и директориями в HDFS) и периодически ее обновляют. Полноценного
автоматического восстановления системы они в случае сбоя на сервере с
&lt;em&gt;NameNode&lt;/em&gt; они на данный момент не обеспечивают, но сохранность данных
на случай, скажем, разрушившегося RAID обеспечить могут.&lt;/p&gt;
&lt;p&gt;MapReduce framework тоже несомненно развивается и дорабатывается, но
каких-либо особо выдающихся изменений в нем не произошло: появляются
дополнительные возможности, исправляются ошибки, снимаются те или иные
ограничения. В общем все идет своим чередом.&lt;/p&gt;
&lt;h3 id="podnimaem-klaster"&gt;Поднимаем кластер&lt;/h3&gt;
&lt;div class="card blue lighten-4"&gt;
&lt;div class="card-content"&gt;
&lt;h5&gt;ВНИМАНИЕ!&lt;/h5&gt;
&lt;p&gt;Перед продолжением чтения этого раздела, настоятельно рекомендуется
прочитать &lt;a href="https://www.insight-it.ru/storage/2008/hadoop-dlya-razrabotchika/"&gt;статью о запуске псевдо-кластера из одного компьютера&lt;/a&gt;.
&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Для начала нам понадобится некоторое количество компьютеров (хотя если у
Вас серьезные намерения, то лучше все же гордо называть их серверами, а
для "побаловаться" сойдут и обычные рабочие станции с
&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;). Конкретное количество на самом деле роли не
играет, продолжать можно как с 2 серверами, так и с 20 тысячами (по
крайней мере теоретически). Хотя пару рекомендаций все же могу дать: при
использовании в "боевых" условиях стоит стараться избегать физического
совмещения мастер-узлов компонентов системы (&lt;em&gt;NameNode, JobTracker,
HMaster&lt;/em&gt;) с "рядовыми" серверами, таким образом желательно начинать с,
как минимум, 5-7 серверов.&lt;/p&gt;
&lt;p&gt;Удостоверившись, что на всем оборудовании установлен какой-нибудь
дистрибутив &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; или &lt;a href="/tag/unix/"&gt;Unix&lt;/a&gt; (любители особо
поизвращаться могут попытать счастья с "окнами" в совокупности с Cygwin)
и 5 или 6 версия JRE/JDK (желательно от Sun), можно приступать к
настройке каждого узла по тому же принципу, что и для псевдо-кластера
(да-да, предупреждение в начале раздела было написано не для мебели).
Кстати не забудьте, что &lt;a href="/tag/hbase/"&gt;HBasе&lt;/a&gt; теперь нужно скачивать
отдельно. О небольших присутствующих особенностях я расскажу чуть позже,
а пока дам маленький совет, который позволит несколько облегчить это
непростое дело.&lt;/p&gt;
&lt;p&gt;Вручную выполнять одни и те же операции на паре десятков/сотен/тысяч
серверов мало того что долго, но и чрезвычайно утомительно. Уже на
втором-третьем сервере начнет появляться желание каким-либо образом
автоматизировать процесс установки. Конечно же можно воспользоваться
специализированным программным обеспечением, скажем
&lt;a href="https://www.insight-it.ru/goto/65d64e55/" rel="nofollow" target="_blank" title="http://www.theether.org/gexec/"&gt;gexec&lt;/a&gt;, но есть и более простой способ:
существенно упростить жизнь может простой скрипт на bash в 5 строчек:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; x in &lt;span class="sb"&gt;`&lt;/span&gt;cat ~/nodes&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="k"&gt;do&lt;/span&gt;
ssh hadoop@&lt;span class="nv"&gt;$x&lt;/span&gt; &lt;span class="nv"&gt;$1&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;В файле &lt;code&gt;~/nodes&lt;/code&gt; должен располагаться список IP-адресов всех
серверов, тогда получив первым параметром произвольную консольную
команду скрипт выполнит ее на каждом сервере. С его помощью можно
существенно сократить время, требуемое на выполнение всех необходимых
действий для запуска кластера.&lt;/p&gt;
&lt;p&gt;После небольшого лирического отступления вернемся собственно к
&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt;. Как Вы уже, надеюсь, знаете, система использует
&lt;strong&gt;ssh&lt;/strong&gt; для управления всеми компонентами системы, причем очень
желателен беспарольный доступ между всеми узлами. Для этого необходимо
собрать в один файл все публичные ключи &lt;code&gt;~/.ssh/id_rsa.pub&lt;/code&gt; на
каждом из узлов (по одному на строчку) и разместить его под именем
&lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; тоже на каждом из узлов. Кстати для
упоминавшегося выше скрипта беспарольный доступ тоже очень желателен.&lt;/p&gt;
&lt;p&gt;Следующим этапом нужно подготовить конфигурационные файлы, они должны
быть идентичными на всех узлах, так что заполнив их все на одном из
узлов нужно скопировать их по всем остальным серверам (очень удобно
делать это с помощью &lt;strong&gt;rsync&lt;/strong&gt;). Теперь пройдемся по необходимым
изменениям в каждом из них:&lt;/p&gt;
&lt;h4&gt;hadoop-site.xml&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.default.name&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;hdfs://namenode:54310&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;
    The name of the default file system.  A URI whose
    scheme and authority determine the FileSystem implementation.  The
    uri's scheme determines the config property (fs.SCHEME.impl) naming
    the FileSystem implementation class.  The uri's authority is used to
    determine the host, port, etc. for a filesystem.
  &lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapred.job.tracker&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;jobtracker:54311&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;
    The host and port that the MapReduce job tracker runs
    at.  If "local", then jobs are run in-process as a single map
    and reduce task.
  &lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Каждый сервер должен знать где расположен &lt;em&gt;NameNode&lt;/em&gt;, по-этому он
  явно указывается в полном пути к файловой системе, практически
  аналогичная ситуация и с &lt;em&gt;JobTracker&lt;/em&gt;. Вместо namenode и jobtracker
  необходимо указать их IP-адреса или доменные имена (или в крайнем
  случае - имя в &lt;code&gt;/etc/hosts&lt;/code&gt;)&lt;/p&gt;
&lt;h4&gt;masters&lt;/h4&gt;
&lt;p&gt;Вопреки логике, здесь указывается список всех &lt;em&gt;SecondaryNameNode&lt;/em&gt;.
Одного-двух серверов здесь будет вполне достаточно, самое главное не
указывать здесь адрес основного &lt;em&gt;NameNode&lt;/em&gt;, лучше всего подойдет
какой-нибудь другой мастер-сервер, может быть дополненный одним из
обычных узлов кластера. Выделять под это отдельный сервер смысла не
много, так как нагрузка на них минимальна.&lt;/p&gt;
&lt;h4&gt;slaves&lt;/h4&gt;
&lt;p&gt;Список всех рядовых серверов, по одному на строку (опять же: IP или доменное имя). На них будут запущенны &lt;em&gt;DataNode&lt;/em&gt; и &lt;em&gt;TaskTracker&lt;/em&gt;.&lt;/p&gt;
&lt;h4&gt;hbase-site.xml&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;hbase.master&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;localhost:60000&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;
    the host and port that the HBase master runs at
  &lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Первое изменение достаточно очевидно: &lt;em&gt;HRegionServer&lt;/em&gt; должны знать
где находится &lt;em&gt;HMaster&lt;/em&gt;, о чем им и сообщает первое свойство
(заменяем hmaster на соответствующий адрес). А вот второе свойство
является следствием "обособления" HBase от Hadoop, о котором шла
речь ранее. Теперь имеется возможность использовать их отдельно (с
локальной файловой системой вместо HDFS), а так как появился выбор
файловой системы - ее адрес необходимо указывать полностью. В данном
случае указан адрес HDFS (такой же как в &lt;strong&gt;hadoop-site.xml&lt;/strong&gt;).&lt;/p&gt;
&lt;h4&gt;regionservers&lt;/h4&gt;
&lt;p&gt;Вполне очевидный конфигурационный файл, по аналогии со &lt;strong&gt;slaves&lt;/strong&gt;,
заполняется списком адресов для запуска &lt;em&gt;HRegionServer&lt;/em&gt;. Часто
совпадает с упомянутым &lt;strong&gt;slaves&lt;/strong&gt;, обычно достаточно просто
скопировать.&lt;/p&gt;
&lt;h3 id="zapusk"&gt;Запуск&lt;/h3&gt;
&lt;p&gt;Удостоверившись, что с конфигурационными файлами все нормально и что они
на всех серверах совпадают, можно приступать собственно к запуску. Этот
процесс практически полностью совпадает с запуском на одном узле, хотя
обычно проще желать это тоже простеньким скриптом примерно такого вида:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/bin/bash&lt;/span&gt;
ssh hadoop@namenode ~/hadoop/bin/start-dfs.sh
ssh hadoop@jobtracker ~/hadoop/bin/start-mapred.sh
ssh hadoop@hmaster ~/hbase/bin/start-hbase.sh
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Если мы нигде не ошиблись и все сделано правильно, то кластер
благополучно запустится, что легко проследить выполнив на каждом узле
команду &lt;code&gt;jps&lt;/code&gt; и проверив соответствие запущенных компонентов
запланированному (читай: указанному в конфигурационных файлах).&lt;/p&gt;
&lt;p&gt;В целом процесс достаточно прост и не занимает много времени, если Вы
все же столкнулись с какими-либо проблемами в процессе - обращайтесь,
вполне возможно, что я смогу помочь. Удостовериться, что все нормально
можно абсолютно так же, как и для псевдо-кластера - с помощью MapReduce
задач, идущих в комплекте с Hadoop. Выглядеть это может, например, вот
так:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;~/hadoop/bin/hadoop jar hadoop-*-examples.jar pi &lt;span class="m"&gt;4&lt;/span&gt; 10000
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;По-хорошему надо было бы написать подобную инструкцию сразу после
первой, но почему-то как-то не сложилось...&lt;/p&gt;
&lt;h3 id="zakliuchenie"&gt;Заключение&lt;/h3&gt;
&lt;p&gt;На данный момент &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; стал еще более работоспособным,
по сравнению с его февральским состоянием. Сообщество использующих его
разработчиков растет с каждым днем, а все ошибки и проблемы исправляются
очень и очень оперативно, многие коммерческие проекты могут позавидовать
таким темпам развития. Хоть до по-настоящему стабильного релиза еще
далеко, данный продукт уже сейчас очень активно используется в
достаточно большом количестве крупных интернет-проектов.&lt;/p&gt;
&lt;p&gt;Если Вы еще не успели подписаться на &lt;a href="/feed/"&gt;RSS&lt;/a&gt; - сейчас самое время!&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 17 Aug 2008 23:15:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-08-17:storage/2008/hadoop-vozvrashhaetsya/</guid><category>Apache</category><category>Hadoop</category><category>HBase</category><category>HDFS</category><category>MapReduce</category></item><item><title>Hadoop для разработчика</title><link>https://www.insight-it.ru//storage/2008/hadoop-dlya-razrabotchika/</link><description>&lt;p&gt;Для разработки приложений, работающих с использованием Hadoop, или же
алгоритмов для MapReduce framework'а совсем не нужен полномасштабный
кластер. На самом же деле для запуска всей системы, описанной мной в
&lt;a href="https://www.insight-it.ru/storage/2008/hadoop/"&gt;одном из предыдущих постов&lt;/a&gt;, вполне
достаточно одного компьютера и буквально минут 15 свободного времени,
как потратить их для решения этой задачи я Вам и поведаю.
&lt;!--more--&gt;
Рассказывать я буду на примере своего &lt;a href="/tag/gentoo-linux/"&gt;Gentoo
Linux&lt;/a&gt;, но большая часть этого повествования будет
справедлива и для других unix-like операционных систем.&lt;/p&gt;
&lt;h3 id="podgotovka"&gt;Подготовка&lt;/h3&gt;
&lt;p&gt;Перед тем, как приступить собственно говоря к установке
&lt;a href="https://www.insight-it.ru/goto/30a7481/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/"&gt;Hadoop&lt;/a&gt;, необходимо выполнить два
элементарных действия, необходимых для правильного функционирования
системы:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;открыть доступ одному из пользователей по &lt;code&gt;ssh&lt;/code&gt; к этому же
    компьютеру без пароля, можно например создать отдельного
    пользователя для этого &lt;code&gt;hadoop&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; useradd -m -n hadoop
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Далее действия выполняем от его имени:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; su hadoop
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Генерируем RSA-ключ для обеспечения аутентификации в условиях
отсутствия возможности использовать пароль:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; hadoop@localhost ~ &lt;span class="nv"&gt;$ &lt;/span&gt;ssh-keygen -t rsa -P &lt;span class="s2"&gt;""&lt;/span&gt;
Generating public/private rsa key pair.
Enter file in which to save the key &lt;span class="o"&gt;(&lt;/span&gt;/home/hadoop/.ssh/id_rsa&lt;span class="o"&gt;)&lt;/span&gt;:
Your identification has been saved in /home/hadoop/.ssh/id_rsa.
Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.
The key fingerprint is:
7b:5c:cf:79:6b:93:d6:d6:8d:41:e3:a6:9d:04:f9:85 hadoop@localhost
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;И добавляем его в список авторизованных ключей:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; cat ~/.ssh/id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Этого должно быть более чем достаточно, проверить работоспособность
соединения можно просто написав:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; ssh localhost
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Не забываем предварительно инициализировать &lt;strong&gt;sshd&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; /etc/init.d/sshd start
&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Помимо этого необходимо убедиться в наличии установленной JVM версии
    1.5.0 или выше, а также узнать директорию, где она располагается,
    вариантов сделать это множество, я нашел ее просто заглянув в самое
    логичное место - &lt;code&gt;/usr/lib&lt;/code&gt;, но при желании никто не может Вам
    помешать воспользоваться услугами, например, &lt;code&gt;slocate&lt;/code&gt;. Найденную
    директорию с JVM лучше запомнить или записать куда-нибудь, для меня
    она оказалась: &lt;code&gt;/usr/lib/jvm/sun-jdk-1.6&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="ustanovka"&gt;Установка&lt;/h3&gt;
&lt;p&gt;Установка начинается с получения копии исходного кода системы, способов
для этого существует несколько. Я перепробовал практически все, самую
адекватную версию мне удалось получить из SVN. Для ее получения
необходимо выполнить следующую команду:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;svn checkout http://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.16 ~
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;branch-0.16&lt;/strong&gt; - последняя доступная версия на данный момент, для
определения ее номера достаточно заглянуть &lt;a href="https://www.insight-it.ru/goto/99e3d37e/" rel="nofollow" target="_blank" title="http://svn.apache.org/repos/asf/hadoop/core/branches/"&gt;по тому же адресу&lt;/a&gt;
браузером. Предполагается, что Hadoop будет располагаться прямо в
&lt;code&gt;/home/hadoop&lt;/code&gt;, но запросто можно использовать и другую директорию.&lt;/p&gt;
&lt;p&gt;Сразу же стоит скомпилировать различные дополнительные компоненты
системы, особенно это актуально из-за &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt;, но и помимо
него соберется много чего интересного, например plug-in для отличной IDE
под названием &lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/b7976bc5/" rel="nofollow" target="_blank" title="http://www.eclipse.org"&gt;Eclipse&lt;/a&gt;&lt;/strong&gt; или &lt;a href="https://www.insight-it.ru/goto/9253da15/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/docs/r0.16.0/hod.html"&gt;Hadoop On
Demand&lt;/a&gt;. Задача
также элементарна:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; ~ &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; ant clean jar compile-contrib
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="nastroika"&gt;Настройка&lt;/h3&gt;
&lt;p&gt;Конфигурационные файлы можно редактировать в произвольном порядке, самое
главное ничего не забыть :)&lt;/p&gt;
&lt;h4&gt;conf/hadoop-env.sh&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;JAVA_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/usr/lib/jvm/sun-jdk-1.6
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Единственная обязательная переменная окружения - &lt;code&gt;JAVA_HOME&lt;/code&gt;,
здесь как раз пригодится заранее найденный путь до JVM, все
остальное - по желанию.&lt;/p&gt;
&lt;h4&gt;conf/hadoop-site.xml&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;hadoop.tmp.dir&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;/home/hadoop/data/${user.name}&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;A base for other temporary directories.&lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.default.name&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;hdfs://localhost:54310&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;The name of the default file system.  A URI whose
  scheme and authority determine the FileSystem implementation.  The
  uri's scheme determines the config property (fs.SCHEME.impl) naming
  the FileSystem implementation class.  The uri's authority is used to
  determine the host, port, etc. for a filesystem.&lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapred.job.tracker&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;localhost:54311&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;The host and port that the MapReduce job tracker runs
  at.  If "local", then jobs are run in-process as a single map
  and reduce task.
  &lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;dfs.replication&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;1&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  &lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Этот конфигурации файл является одним из ключевых, таким образом он
выглядит для конфигурации, состоящей из одного компьютера
(позаимствован из &lt;a href="https://www.insight-it.ru/goto/f1c9004a/" rel="nofollow" target="_blank" title="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/"&gt;англоязычного мануала&lt;/a&gt;
на ту же тему).&lt;/p&gt;
&lt;h4&gt;src/contrib/hbase/conf/hbase-site.xml&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;hbase.master&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;localhost:60000&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;The host and port that the HBase master runs at&lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;hbase.rootdir&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;/hbase&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;location of HBase instance in dfs&lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Как не сложно заметить, этот файл необходим для функционирования
&lt;strong&gt;HBase&lt;/strong&gt;, по-моему все просто и очевидно, &lt;code&gt;&amp;lt;description&amp;gt;&lt;/code&gt; говорят
сами за себя.&lt;/p&gt;
&lt;h3 id="zapusk"&gt;Запуск&lt;/h3&gt;
&lt;p&gt;Начать стоит с ознакомления с кратким описанием доступных команд Hadoop,
сделать это можно просто набрав &lt;code&gt;~/bin/hadoop&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Usage: hadoop &lt;span class="o"&gt;[&lt;/span&gt;--config confdir&lt;span class="o"&gt;]&lt;/span&gt; COMMAND
where COMMAND is one of:
  namenode -format     format the DFS filesystem
  secondarynamenode    run the DFS secondary namenode
  namenode             run the DFS namenode
  datanode             run a DFS datanode
  dfsadmin             run a DFS admin client
  fsck                 run a DFS filesystem checking utility
  fs                   run a generic filesystem user client
  balancer             run a cluster balancing utility
  jobtracker           run the MapReduce job Tracker node
  pipes                run a Pipes job
  tasktracker          run a MapReduce task Tracker node
  job                  manipulate MapReduce &lt;span class="nb"&gt;jobs&lt;/span&gt;
&lt;span class="nb"&gt;  &lt;/span&gt;version              print the version
  jar             run a jar file
  distcp   copy file or directories recursively
  daemonlog            get/set the log level &lt;span class="k"&gt;for&lt;/span&gt; each daemon
 or
  CLASSNAME            run the class named CLASSNAME
Most commands print &lt;span class="nb"&gt;help &lt;/span&gt;when invoked w/o parameters.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Первым делом необходимо отформатировать &lt;em&gt;Namenode&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;~/bin/hadoop namenode -format
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;И дело останется лишь за малым, запустить на выполнение пару
bash-скриптов, которые без вашего дальнейшего участия &lt;em&gt;инициализируют&lt;/em&gt;
всю систему, включая HBase:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;~/bin/hadoop/start-all.sh &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; ~/src/contrib/hbase/bin/start-hbase.sh
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Как только они закончат все необходимые действия, у Вас появится
возможность удостовериться, что все в порядке. Самым простым способом
является запуск клиента &lt;em&gt;Hbase Shell&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;~/bin/src/contrib/hbase/bin/hbase shell
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Если в ответ Вы получили соответствующее приглашение клиента, значит все
было сделано верно!&lt;/p&gt;
&lt;p&gt;Вот собственно говоря и все, псевдо-кластер функционирует, доступ к
HBase имеется, можно приступать к разработке :)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;P.S.:&lt;/strong&gt; Остановка системы производится по тому же принципу скриптами
&lt;code&gt;stop-all.sh&lt;/code&gt; и &lt;code&gt;stop-hbase.sh&lt;/code&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 26 Feb 2008 00:15:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-02-26:storage/2008/hadoop-dlya-razrabotchika/</guid><category>gentoo linux</category><category>Hadoop</category><category>HBase</category><category>HDFS</category><category>MapReduce</category><category>ПО</category><category>развертывание</category><category>разработка</category><category>установка</category></item><item><title>Hadoop</title><link>https://www.insight-it.ru//storage/2008/hadoop/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/30a7481/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/"&gt;Hadoop&lt;/a&gt; представляет собой платформу
для построения приложений, способных обрабатывать огромные объемы
данных. Система основывается на распределенном подходе к вычислениям и
хранению информации, основными ее особенностями являются:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Масштабируемость:&lt;/strong&gt; с помощью &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; возможно
    надежное хранение и обработка огромных объемов данных, которые могут
    измеряться петабайтами;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Экономичность:&lt;/strong&gt; информация и вычисления распределяются по
    &lt;a href="/tag/klaster/"&gt;кластеру&lt;/a&gt;, построенному на самом обыкновенном
    оборудовании. Такой кластер может состоять из тысяч узлов;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Эффективность:&lt;/strong&gt; распределение данных позволяет выполнять их
    обработку параллельно на множестве компьютеров, что существенно
    ускоряет этот процесс;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Надежность:&lt;/strong&gt; при хранении данных возможно предоставление
    избыточности, благодаря хранению нескольких копий. Такой подход
    позволяет гарантировать отсутствие потерь информации в случае сбоев
    в работе системы;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Кроссплатформенность:&lt;/strong&gt; так как основным языком программирования,
    используемым в этой системе является &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, развернуть
    ее можно на базе любой операционной системы, имеющей &lt;abbr title="Java Virtual Machine"&gt;JVM&lt;/abbr&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;h3 id="hdfs"&gt;HDFS&lt;/h3&gt;
&lt;p&gt;В основе всей системы лежит распределенная файловая система под
незамысловатым названием &lt;strong&gt;Hadoop Distributed File System&lt;/strong&gt;.
Представляет она собой вполне стандартную распределенную файловую
систему, но все же она обладает рядом особенностей:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Устойчивость к сбоям, разработчики рассматривали сбои в оборудовании
    скорее как норму, чем как исключение;&lt;/li&gt;
&lt;li&gt;Приспособленность к развертке на самом обыкновенном ненадежном
    оборудовании;&lt;/li&gt;
&lt;li&gt;Предоставление высокоскоростного потокового доступа ко всем данным;&lt;/li&gt;
&lt;li&gt;Настроена для работы с большими файлами и наборами файлов;&lt;/li&gt;
&lt;li&gt;Простая модель работы с данными: &lt;em&gt;один раз записали - много раз
    прочли&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;Следование принципу: &lt;em&gt;переместить вычисления проще, чем переместить
    данные&lt;/em&gt;;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Архитектура HDFS&lt;/h4&gt;
&lt;p&gt;Проще всего ее демонстрирует схема,
&lt;a href="https://www.insight-it.ru/goto/9c57006b/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/docs/current/images/hdfsarchitecture.gif"&gt;позаимствованная&lt;/a&gt; с официального сайта проекта и переведенная мной на руский:
&lt;img alt="Архитектура HDFS" class="responsive-img" src="https://www.insight-it.ru/images/hdfsarchitecture.jpg" title="Архитектура HDFS"/&gt;&lt;/p&gt;
&lt;p&gt;Действующие лица:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Namenode&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Этот компонент системы осуществляет всю работу с метаданными. Он
должен быть запущен только на одном компьютере в кластере. Именно он
управляет размещением информации и доступом ко всем данным,
расположенным на ресурсах кластера. Сами данные проходят с остальных
машин кластера к клиенту мимо него.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Datanode&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;На всех остальных компьютерах системы работает именно этот
компонент. Он располагает сами блоки данных в локальной файловой
системе для последующей передачи или обработки их по запросу
клиента. Группы узлов данных принято называть Rack, они
используются, например, в схемах репликации данных.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Клиент&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Просто приложение или пользователь, работающий с файловой системой.
В его роли может выступать практически что угодно.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Пространство имен &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt; имеет классическую иерархическую
структуру: пользователи и приложения имеют возможность создавать
директории и файлы. Файлы хранятся в виде блоков данных произвольной (но
одинаковой, за исключением последнего; по-умолчанию 64 mb) длины,
размещенных на &lt;strong&gt;Datanode&lt;/strong&gt;'ах. Для обеспечения отказоустойчивости блоки
хранятся в нескольких экземплярах на разных узлах, имеется возможность
настройки количества копий и алгоритма их распределения по системе.
Удаление файлов происходит не сразу, а через какое-то время после
соответствующего запроса, так как после получения запроса файл
перемещается в директорию &lt;strong&gt;/trash&lt;/strong&gt; и хранится там определенный период
времени на случай если пользователь или приложение передумают о своем
решении. В этом случае информацию можно будет восстановить, в противном
случае - физически удалить.&lt;/p&gt;
&lt;p&gt;Для обнаружения возникновения каких-либо неисправностей, &lt;strong&gt;Datanode&lt;/strong&gt;
периодически отправляют &lt;strong&gt;Namenode&lt;/strong&gt;'у сигналы о своей
работоспособности. При прекращении получения таких сигналов от одного из
узлов &lt;strong&gt;Namenode&lt;/strong&gt; помечает его как &lt;em&gt;"мертвый"&lt;/em&gt;, и прекращает какой-либо
с ним взаимодействие до возвращения его работоспособности. Данные,
хранившиеся на &lt;em&gt;"умершем"&lt;/em&gt; узле реплицируются дополнительный раз из
оставшихся &lt;em&gt;"в живых"&lt;/em&gt; копий и система продолжает свое функционирование
как ни в чем не бывало.&lt;/p&gt;
&lt;p&gt;Все коммуникации между компонентами файловой системы проходят по
специальным протоколам, основывающимся на стандартном &lt;strong&gt;TCP/IP&lt;/strong&gt;.
Клиенты работают с &lt;strong&gt;Namenode&lt;/strong&gt; с помощью так называемого
&lt;strong&gt;ClientProtocol&lt;/strong&gt;, а передача данных происходит по
&lt;strong&gt;DatanodeProtocol&lt;/strong&gt;, оба они &lt;em&gt;обернуты&lt;/em&gt; в &lt;strong&gt;Remote Procedure Call
(RPC)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Система предоставляет несколько интерфейсов, среди которых командная
оболочка &lt;strong&gt;DFSShell&lt;/strong&gt;, набор ПО для администрирования &lt;strong&gt;DFSAdmin&lt;/strong&gt;, а
также простой, но эффективный веб-интерфейс. Помимо этого существуют
несколько API для языков программирования: Java API, C pipeline, WebDAV
и так далее.&lt;/p&gt;
&lt;h3 id="mapreduce"&gt;MapReduce&lt;/h3&gt;
&lt;p&gt;Помимо файловой системы, &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; включает в себя framework
для проведения масштабных вычислений, обрабатывающих огромные объемы
данных. Каждое такое вычисление называется Job (задание) и состоит оно,
как видно из названия, из двух этапов:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Map&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Целью этого этапа является представление произвольных данных (на
практике чаще всего просто пары ключ-значение) в виде промежуточных
пар ключ-значение. Результаты сортируются и групируются по ключу и
передаются на следующий этап.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Reduce&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Полученные после &lt;strong&gt;map&lt;/strong&gt; значения используются для финального
вычисления требуемых данных. Практические любые данные могут быть
получены таким образом, все зависит от требований и функционала
приложения.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Задания выполняются, подобно файловой системе, на всех машинах в
кластере (чаще всего одних и тех же). Одна из них выполняет роль
управления работой остальных - &lt;strong&gt;JobTracker&lt;/strong&gt;, остальные же ее
бесприкословно слушаются - &lt;strong&gt;TaskTracker&lt;/strong&gt;. В задачи &lt;strong&gt;JobTracker&lt;/strong&gt;'а
входит составление расписания выполняемых работ, наблюдение за ходом
выполнения, и перераспределение в случае возникновения сбоев.&lt;/p&gt;
&lt;p&gt;В общем случае каждое приложение, работающее с этим framework'ом,
предоставляет методы для осуществления этапов &lt;strong&gt;map&lt;/strong&gt; и &lt;strong&gt;reduce&lt;/strong&gt;, а
также указывает расположения входных и выходных данных. После получения
этих данных &lt;strong&gt;JobTracker&lt;/strong&gt; распределяет задание между остальными
машинами и предоставляет клиенту полную информацию о ходе работ.&lt;/p&gt;
&lt;p&gt;Помимо основных вычислений могут выполняться вспомогательные процессы,
такие как составление отчетов о ходе работы, кэширование, сортировка и
так далее.&lt;/p&gt;
&lt;h3 id="hbase"&gt;HBase&lt;/h3&gt;
&lt;p&gt;&lt;img alt="HBase Logo" class="right" src="https://www.insight-it.ru/images/hbase-logo.png" title="HBase"/&gt;
В рамках &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; доступна еще и система хранения данных,
которую правда сложно назвать &lt;a href="/tag/subd/"&gt;СУБД&lt;/a&gt; в традиционном смысле
этого слова. Чаще проводят аналогии с проприетарной системой этого же
плана от &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; - &lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/12419d3d/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/hbase"&gt;HBase&lt;/a&gt; представляет собой
распределенную систему хранения больших объемов данных. Подобно
реляционным СУБД данные хранятся в виде таблиц, состоящих из строк и
столбцов. И даже для доступа к ним предоставляется язык запросов &lt;strong&gt;HQL&lt;/strong&gt;
(как ни странно - &lt;strong&gt;Hadoop Query Language&lt;/strong&gt;), отдаленно напоминающий
более распространенный &lt;a href="/tag/sql/"&gt;SQL&lt;/a&gt;. Помимо этого предоставляется
итерирующмй интерфейс для сканирования наборов строк.&lt;/p&gt;
&lt;p&gt;Одной из основных особенностей хранения данных в &lt;strong&gt;HBase&lt;/strong&gt; является
возможность наличия нескольких значений, соответствующих одной
комбинации таблица-строка-столбец, для их различения используется
информация о времени добавления записи. На концептуальном уровне таблицы
обычно представляют как набор строк, но физически же они хранятся по
столбцам, достаточно важный факт, который стоит учитывать при разработки
схемы хранения данных. Пустые ячейки не отображаются каким-либо образом
физически в хранимых данных, они просто отсутствуют. Существуют конечно
и другие нюансы, но я постарался упомянуть лишь основные.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HQL&lt;/strong&gt; очень прост по своей сути, если Вы уже знаете &lt;a href="/tag/sql/"&gt;SQL&lt;/a&gt;,
то для изучения его Вам понадобится лишь просмотреть по диагонали
коротенький вывод команды &lt;strong&gt;help;&lt;/strong&gt;, занимающий всего пару экранов в
консоли. Все те же &lt;strong&gt;SELECT&lt;/strong&gt;, &lt;strong&gt;INSERT&lt;/strong&gt;, &lt;strong&gt;UPDATE&lt;/strong&gt;, &lt;strong&gt;DROP&lt;/strong&gt; и так
далее, лишь со слегка измененным синтаксисом.&lt;/p&gt;
&lt;p&gt;Помимо обычно командной оболочки &lt;strong&gt;HBase Shell&lt;/strong&gt;, для работы с &lt;strong&gt;HBase&lt;/strong&gt;
также предоставлено несколько API для различных языков программирования:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/f059ad5e/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/hbase/docs/current/api/index.html"&gt;Java&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/e44fcd5/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/Jython"&gt;Jython&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8282e2e2/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/HbaseRest"&gt;REST&lt;/a&gt; и&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/185bb3f7/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/ThriftApi"&gt;Thrift&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="zakliuchenie"&gt;Заключение&lt;/h3&gt;
&lt;p&gt;&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; является отличным решением для построения
высоконагруженных приложений, которое уже активно используется
&lt;a href="https://www.insight-it.ru/goto/ab057c2a/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/PoweredBy"&gt;множеством интернет-проектов&lt;/a&gt;.
В последующих постах на эту тему я постараюсь описать процесс
развертывания этой системы и написания приложений, работающих по
принципу &lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;. Не пропустить момент их публикации
Вам может помочь подписка на &lt;a href="/feed/"&gt;RSS-ленту&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 22 Feb 2008 22:41:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-02-22:storage/2008/hadoop/</guid><category>Hadoop</category><category>HBase</category><category>HDFS</category><category>Java</category><category>MapReduce</category><category>архитектура</category><category>информационные технологии</category><category>кластер</category><category>Масштабируемость</category><category>распределенные вычисления</category><category>технология</category></item><item><title>Архитектура Google</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-google/</link><description>&lt;p&gt;&lt;em&gt;Эта статья датируется 2008 годом, новая версия: &lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-google-2011/"&gt;Архитектура Google 2011&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="/tag/google/"&gt;Google&lt;/a&gt;&lt;/strong&gt; - Король масштабируемости.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Каждый хоть раз слышал о &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; благодаря их
всеобъемлющему, "умному" и быстрому поисковому сервису, но ни для кого
не секрет, что они не ограничиваются только им. Их платформа для
построения масштабируемых приложений позволяет выпускать множество
удивительно конкурентноспособных интернет-приложений, работающих на
уровне всего Интернета вцелом. Они ставят перед собой цель постоянно
строить все более и более производительную и масштабируемую архитектуру
для поддержки своих продуктов. Как же им это удается?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Сразу хочу сказать, что эта запись является переводом с английского,
автор &lt;a href="https://www.insight-it.ru/goto/31bfd110/" rel="nofollow" target="_blank" title="http://highscalability.com/google-architecture"&gt;оригинальной версии&lt;/a&gt; - &lt;a href="https://www.insight-it.ru/goto/f3f1b405/" rel="nofollow" target="_blank" title="http://highscalability.com/user/todd-hoff"&gt;Todd Hoff&lt;/a&gt;. Оригинал написан приблизительно в середине 2007 года, но по-моему до сих пор очень даже актуально.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Далее следует перечисление источников информации из оригинала:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/741bec4c/" rel="nofollow" target="_blank" title="http://video.google.com/videoplay?docid=-5699448884004201579"&gt;Video: Построение больших систем в Google&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/fae0d413/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/gfs.html"&gt;Google Lab: Файловая система Google (GFS)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/39138d08/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/mapreduce.html"&gt;Google Lab: MapReduce: упрощенная обработка данных на больших кластерах&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8667b351/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/bigtable.html"&gt;Google Lab: BigTable.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/dab5470e/" rel="nofollow" target="_blank" title="http://video.google.com/videoplay?docid=7278544055668715642"&gt;Video: BigTable: система распределенного хранения данных.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/87fff9b2/" rel="nofollow" target="_blank" title="http://www.baselinemag.com/article2/0,1540,1985514,00.asp"&gt;Как работает Google&lt;/a&gt;
    от David Carr в Baseline Magazine.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a426f3de/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/sawzall.html"&gt;Google Lab: интерпретирование данных. Параллельный анализ с помощью Sawzall.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/ed8bca67/" rel="nofollow" target="_blank" title="http://www.25hoursaday.com/weblog/2007/06/25/GoogleScalabilityConferenceTripReportMapReduceBigTableAndOtherDistributedSystemAbstractionsForHandlingLargeDatasets.aspx"&gt;Записи с конференции по масштабированию от Dare Obasonjo.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Большое разнообразие языков программирования: &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;,
    &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="chto-vnutri"&gt;Что внутри?&lt;/h3&gt;
&lt;h4&gt;Статистика&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;На 2006 год система включала в себя 450000 недорогих серверов&lt;/li&gt;
&lt;li&gt;За 2005 год было проиндексировано 8 миллиардов страниц. На данный
    момент&amp;hellip; кто знает?&lt;/li&gt;
&lt;li&gt;На момент написания оригинала Google включает в себя более 200
    &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt; кластеров. Один кластер может состоять из 1000 или
    даже 5000 компьютеров&lt;/li&gt;
&lt;li&gt;Десятки и сотни тысяч компьютеров получают данные из &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;
    кластеров, которые насчитывают более 5 петабайт дискового
    пространства. Суммарные пропускная способность операций записи и
    чтения между дата центрами может достигать 40 гигабайт в секунду&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt; позволяет хранить миллиарды ссылок (URL),
    сотни терабайт снимков со спутников, а также настройки миллионов
    пользователей&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;// Цифры не первой свежести конечно, но тоже неплохо.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;Стек&lt;/h4&gt;
&lt;p&gt;&lt;a href="/tag/google/"&gt;Google&lt;/a&gt; визуализирует свою инфраструктуру в виде
трехслойного стека:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Продукты:&lt;/em&gt; поиск, реклама, электронная почта, карты, видео, чат,
    блоги&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Распределенная инфраструктура системы:&lt;/em&gt; &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;,
    &lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt; и &lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Вычислительные платформы:&lt;/em&gt; множество компьютеров во множестве
    датацентров&lt;/li&gt;
&lt;li&gt;Легкое развертывание для компании при низком уровне издержек&lt;/li&gt;
&lt;li&gt;Больше денег вкладывается в оборудование для исключения возможности
    потерь данных&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Надежное хранение данных с помощью &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Надежное масштабируемое хранение данных крайне необходимо для любого
    приложения. &lt;strong&gt;GFS&lt;/strong&gt; является основой их платформы хранения
    информации&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;&lt;/strong&gt; - большая распределенная файловая система, способная хранить и обрабатывать огромные объемы информации&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Зачем строить что-либо самим вместо того, чтобы просто взять это с полки?&lt;/em&gt; Они контролируют абсолютно всю систему и именно эта платформа отличает их от всех остальных.&lt;/p&gt;
&lt;p&gt;Она предоставляет:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;высокую надежность дата центров&lt;/li&gt;
&lt;li&gt;масштабируемость до тысяч сетевых узлов
&amp;ndash; высокую пропускную способность операций чтения и записи&lt;/li&gt;
&lt;li&gt;поддержку больших блоков данных, размер которых может измеряться в
гигабайтах&lt;/li&gt;
&lt;li&gt;эффективное распределение операций между датацентрами для
избежания возникновения "узких мест" в системе&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;В системе существуют мастер-сервера и сервера, собственно хранящие
    информацию:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Мастер-сервера хранят метаданные для всех файлов. Сами данные
    хранятся блоками по 64 мегабайта на остальных серверах. Клиенты
    могут выполнять операции с метаданными на мастер-серверах, чтобы
    узнать на каком именно сервере расположены необходимые данные.&lt;/li&gt;
&lt;li&gt;Для обеспечения надежности один и тот же блок данных хранится
    в трех экземплярах на разных серверах, что обеспечивает
    избыточность на случай сбоев в работе какого-либо сервера.&lt;/li&gt;
&lt;li&gt;Новые приложения могут пользоваться как существующими
    кластерами, так и новыми, созданными специально для них.&lt;/li&gt;
&lt;li&gt;Ключ успеха заключается в том, чтобы быть уверенными в том,
    что у людей есть достаточно вариантов выбора для реализации их
    приложений. &lt;strong&gt;&lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;&lt;/strong&gt; может быть настроена для
    удовлетворения нужд любого конкретного приложения.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Работаем с данными при помощи MapReduce&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Теперь, когда у нас есть отличная система хранения, что же делать с
    такими объемами данных? Допустим, у нас есть много терабайт данных,
    равномерно распределенных между 1000 компьютерами. Коммерческие базы
    данных не могут эффективно масштабироваться до такого уровня, именно
    в такой ситуации в дело вступает технология
    &lt;strong&gt;&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;&lt;/strong&gt; является программной моделью и
    соответствующей реализацией обработки и генерации больших наборов
    данных. Пользователи могут задавать функцию, обрабатывающую пары
    ключ/значение для генерации промежуточных аналогичных пар, и
    сокращающую функцию, которая объединяет все промежуточные значения,
    соответствующие одному и тому же ключу. Многие реальные задачи могут
    быть выражены с помощью этой модели. Программы, написанные в таком
    функциональном стиле автоматически распараллеливаются и адаптируются
    для выполнения на обширных кластерах. Система берет на себя детали
    разбиения входных данных на части, составления расписания выполнения
    программ на различных компьютерах, управления ошибками, и
    организации необходимой коммуникации между компьютерами. Это
    позволяет программистам, не обладающим опытом работы с параллельными
    и распределенными системами, легко использовать все ресурсы больших
    распределенных систем.&lt;/li&gt;
&lt;li&gt;Зачем использовать &lt;strong&gt;&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;&lt;/strong&gt;?
    &amp;ndash; Отличный способ распределения задач между множеством компьютеров
    &amp;ndash; Обработка сбоев в работе
    &amp;ndash; Работа с различными типами смежных приложений, таких как поиск или
    реклама. Возможно предварительное вычисление и обработка данных,
    подсчет количества слов, сортировка терабайт данных и так далее
    &amp;ndash; Вычисления автоматически приближаются к источнику ввода-вывода&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;&lt;/strong&gt; использует три типа серверов:&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Master:&lt;/em&gt; назначают задания остальным типам серверов, а также
следят за процессом их выполнения&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Map:&lt;/em&gt; принимают входные данные от пользователей и обрабатывают
их, результаты записываются в промежуточные файлы&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Reduce:&lt;/em&gt; принимают промежуточные файлы от Map-серверов и
сокращают их указанным выше способом&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Например, мы хотим посчитать количество слов на всех страницах. Для
    этого нам необходимо передать все страницы, хранимые в &lt;strong&gt;GFS&lt;/strong&gt;, на
    обработку в &lt;strong&gt;MapReduce&lt;/strong&gt;. Этот процесс будет происходить на тысячах
    машин одновременно с полной координацией действий, в соответствии с
    автоматически составленным расписанием выполняемых работ, обработкой
    потенциальных ошибок, и передачей данных выполняемыми автоматически.&lt;ul&gt;
&lt;li&gt;Последовательность выполняемых действий выглядела бы следующим
образом: &lt;code&gt;GFS &amp;rarr; Map &amp;rarr; перемешивание &amp;rarr; Reduce &amp;rarr; запись результатов обратно в GFS&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Технология &lt;strong&gt;MapReduce&lt;/strong&gt; состоит из двух компонентов:
соответственно &lt;em&gt;map&lt;/em&gt; и &lt;em&gt;reduce&lt;/em&gt;. Map отображает один набор данных в
другой, создавая тем самым пары ключ/значение, которпыми в нашем
случае являются слова и их количества.&lt;/li&gt;
&lt;li&gt;В процессе перемешивания происходит агрегирование типов ключей.&lt;/li&gt;
&lt;li&gt;Reduction в нашем случае просто суммирует все результаты и
возвращает финальный результат.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;В процессе индексирования &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; подвергает поток
    данных обработке около 20 разных механизмов сокращения. Сначала идет
    работа над всеми записями и агрегированными ключами, после чего
    результат передается следующему механизму и второй механизм уже
    работает с результатами работы первого, и так далее.&lt;/li&gt;
&lt;li&gt;Программы могут быть очень маленькими, всего лишь от 20 до 50 строк
    кода.&lt;/li&gt;
&lt;li&gt;Единственной проблемой могут быть "отстающие компьютеры". Если один
    компьютер работает существенно медленнее, чем все остальные, это
    будет задерживать работу всей системы в целом.&lt;/li&gt;
&lt;li&gt;Транспортировка данных между серверами происходит в сжатом виде.
    Идея заключается в том, что ограничивающим фактором является
    пропускная способность канала и ввода-вывода, что делает резонным
    потратить часть процессорного времени на компрессию и декомпрессию
    данных.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Хранение структурированных данных в BigTable&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;BigTable&lt;/strong&gt; является крупномасштабной, устойчивой к потенциальным
    ошибкам, самоуправляемой системой, которая может включать в себя
    терабайты памяти и петабайты данных, а также управлять миллионами
    операций чтения и записи в секунду.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;BigTable&lt;/strong&gt; представляет собой распределенный механизм хэширования,
    построенный поверх &lt;strong&gt;&lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;&lt;/strong&gt;, а вовсе не реляционную базу
    данных и, как следствие, не поддерживает &lt;a href="/tag/sql/"&gt;SQL&lt;/a&gt;-запросы и
    операции типа Join.&lt;/li&gt;
&lt;li&gt;Она предоставляет механизм просмотра данных для получения доступа к
    структурированным данным по имеющемуся ключу. &lt;strong&gt;&lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;&lt;/strong&gt;
    хранит данные не поддающиеся пониманию, хотя многим приложениям
    необходимы структурированные данные.&lt;/li&gt;
&lt;li&gt;Коммерческие базы данных попросту не могут масштабироваться до
    такого уровня и, соответственно, не могут работать с тысячами машин
    одновременно.&lt;/li&gt;
&lt;li&gt;С помощью контролирования своих низкоуровневых систем хранения
    данных, &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; получает больше возможностей по
    управлению и модификации их системой. Например, если им понадобится
    функция, упрощающая координацию работы между датацентрами, они
    просто могут написать ее и внедрить в систему.&lt;/li&gt;
&lt;li&gt;Подключение и отключение компьютеров к функционирующей системе никак
    не мешает ей просто работать.&lt;/li&gt;
&lt;li&gt;Каждый блок данных хранится в ячейке, доступ к которой может быть
    предоставлен как по ключу строки или столбца, так и по временной
    метке.&lt;/li&gt;
&lt;li&gt;Каждая строка может храниться в одной или нескольких таблицах.
    Таблицы реализуются в виде последовательности блоков по 64
    килобайта, организованных в формате данных под названием
    &lt;strong&gt;SSTable&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;В &lt;strong&gt;&lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt;&lt;/strong&gt; тоже используется три типа серверов:&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Master:&lt;/em&gt; распределяют таблицы по Tablet-серверам, а также следят
за расположением таблиц и перераспределяют задания в случае
необходимости.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Tablet:&lt;/em&gt; обрабатывают запросы чтения/записи для таблиц. Они
разделяют таблицы, когда те превышают лимит размера (обычно 100-200
мегабайт). Когда такой сервер прекращает функционирование по
каким-либо причинам, 100 других серверов берут на себя по одной
таблице и система продолжает работать как-будто ничего не произошло.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Lock:&lt;/em&gt; формируют распределенный сервис ограничения одновременного
доступа. Операции открытия таблицы для записи, анализа
Master-сервером или проверки доступа должны быть
взаимоисключающими.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Локальная группировка может быть использована для физического
    хранения связанных данных вместе, чтобы обеспечить лучшую
    локализацию ссылок на данные.&lt;/li&gt;
&lt;li&gt;Таблицы по возможности кэшируются в оперативной памяти серверов.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="oborudovanie"&gt;Оборудование&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Как эффективно организовать большую группу компьютеров с точки
    зрения издержек и производительности?&lt;/li&gt;
&lt;li&gt;Используется самое обыкновенное ультра-дешевое оборудование и поверх
    него строится программное обеспечение, способное спокойно пережить
    смерть любой части оборудования.&lt;/li&gt;
&lt;li&gt;Тысячекратный рост вычислительной мощности может быть достигнут с
    издержками в 33 раза меньшими, если воспользоваться толерантной к
    сбоям инфраструктурой, по сравнению с инфраструктурой, построенной
    на высоконадежных компонентах. Надежность строится поверх ненадежных
    компонентов.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;, домашнее размещение серверов, материнские платы
    предназначенные для персональных компьютеров, дешевые средства
    хранения данных.&lt;/li&gt;
&lt;li&gt;Цена за каждый ватт энергии в расчете на производительность не
    становится меньше, что ведет к большим проблемам связанным с
    энергообеспечением и охлаждением.&lt;/li&gt;
&lt;li&gt;Использование совместного размещения в своих и арендуемых
    датацентрах.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="raznoe"&gt;Разное&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Быстрый выпуск изменений более предпочтителен, чем ожидание.&lt;/li&gt;
&lt;li&gt;Библиотеки - превалирующий метод построения программ.&lt;/li&gt;
&lt;li&gt;Некоторые приложения предоставляются в виде сервисов.&lt;/li&gt;
&lt;li&gt;Инфраструктура управляет определением версий приложений таким
    образом, что они могут выпускать новые продукты, не боясь сломать
    работу какого-либо компонента системы.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="puti-razvitiia"&gt;Пути развития&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Поддержка географически распределенных кластеров.&lt;/li&gt;
&lt;li&gt;Создание единого глобального пространства имен для всех данных. На
    данный момент данные распределены по кластерам.&lt;/li&gt;
&lt;li&gt;Более автоматизированные передача и обработка данных&lt;/li&gt;
&lt;li&gt;Решение вопросов, связанных с поддержанием работоспособности
    сервисов даже в тех случаях, когда целый кластер отключается от
    системы в связи с техническими работами или каким-либо сбоем в
    работе.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="podvodim-itogi"&gt;Подводим итоги&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Инфраструктура может быть конкурентным преимуществом.&lt;/strong&gt; Это
    определенно так для Google. Они могут выпускать новые интернет
    сервисы быстрее, с меньшими издержками, на таком уровне, что мало
    кто сможет составить им конкуренцию. Подход многих компаний сильно
    отличается от подхода &lt;a href="/tag/google/"&gt;Google&lt;/a&gt;, эти компании
    рассматривают инфраструктуру как статью расходов, они обычно
    используют совсем другие технологии и совсем не задумываются о
    планировании и организации своей системы. Google позиционирует себя
    как компанию по построению систем, что является очень современным
    подходом к разработке программного обеспечения.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Охватывание нескольких дата центров до сих пор является нерешенной проблемой.&lt;/strong&gt; Большинство сайтов базируется в одном или двух дата
    центрах. Полное распределение сайта между несколькими датацентрами
    является хитрой задачей.&lt;/li&gt;
&lt;li&gt;Взгляните на &lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/30a7481/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/"&gt;Hadoop&lt;/a&gt;&lt;/em&gt;, если у Вас
    нет времени на собственноручное построение всей архитектуры с нуля.
    &lt;em&gt;Hadoop&lt;/em&gt; является opensource воплощением в жизнь многих идей здесь
    представленных.&lt;/li&gt;
&lt;li&gt;Часто недооцениваемым преимуществом платформенного подхода является
    тот факт, что даже неопытные разработчики могут быстро и качественно
    реализовывать трудоемкие приложения на базе платформы. Но если бы
    каждый проект требовал одинаково распределенной архитектуры, то это
    создало бы много проблем, так как люди, которые понимают как это
    делается, являются достаточно большой редкостью.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Совместная деятельность не всегда является таким уж плохим занятием.&lt;/strong&gt; Если все части системы работают взаимосвязанно, то
    улучшение в одной из них сразу и абсолютно прозрачно отразится
    положительным образом и на остальных компонентах системы. В
    противном случае такой эффект наблюдаться не будет.&lt;/li&gt;
&lt;li&gt;Построение самоуправляемых систем позволяет более легко
    перераспределять ресурсы между серверами, расширять систему,
    отключать некоторые компьютеры и элегантно проводить обновления.&lt;/li&gt;
&lt;li&gt;Производить длительные операции стоит параллельно.&lt;/li&gt;
&lt;li&gt;Всему, что было сделано Google, предшествовало искусство, а не
    только крупномасштабное развертывание системы.&lt;/li&gt;
&lt;li&gt;Учитывайте возможность &lt;strong&gt;компрессии данных&lt;/strong&gt;, она является очень
    неплохим решением, если остается лишнее процессорное время, но
    присутствует нехватка пропускной способности.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 31 Jan 2008 18:05:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-01-31:highload/2008/arkhitektura-google/</guid><category>BigTable</category><category>featured</category><category>GFS</category><category>Google</category><category>MapReduce</category><category>online</category><category>Sawzall</category><category>архитектура</category><category>архитектура Google</category><category>интернет</category><category>кластер</category><category>Масштабируемость</category><category>поиск</category><category>сервер</category><category>хранение данных</category></item></channel></rss>