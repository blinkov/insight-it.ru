<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Insight IT</title><link>https://www.insight-it.ru/</link><description></description><atom:link href="https://www.insight-it.ru/tag/cassandra/feed/index.xml" rel="self"></atom:link><lastBuildDate>Mon, 04 Jun 2012 05:38:00 +0400</lastBuildDate><item><title>Серверная часть интерактивного сайта и потоки сообщений</title><link>https://www.insight-it.ru//interactive/2012/servernaya-chast-interaktivnogo-sajjta-i-potoki-soobshhenijj/</link><description>&lt;p&gt;Вернемся к теме &lt;a href="https://www.insight-it.ru/interactive/"&gt;интерактивных сайтов&lt;/a&gt; с обратной стороны, серверной. В ней есть огромный простор для творчества, так как
в отличии от клиентской части отсутствуют ограничения, накладываемыми
браузерами. С "простором" же приходит и неоднозначность/неопределенность, вариантов как реализовать одно и то же множество, так что возможно приводимые мной примеры Вам окажутся не по душе &amp;nbsp;- и это нормально, правильный путь не единственный, их много :)&lt;/p&gt;
&lt;p&gt;Приступим!&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="vnutrennie-servisy"&gt;Внутренние сервисы&lt;/h2&gt;
&lt;p&gt;Напомню, что обычно на внутренние сервисы ложится реализация всей или
большей части бизнес-логики приложения. Они получают пользовательские
запросы в стандартизированном виде через прослойки в виде внешних
интерфейсов и, при необходимости взаимодействуя друг с другом и
остальными компонентами системы, определяют какой ответ необходимо
отправить и какие другие действия предпринять.&lt;/p&gt;
&lt;p&gt;Я не буду здесь особо вдаваться в возможные детали реализации самой
бизнес-логики - она практически всегда уникальна, скорее заслуживает
внимания её "обертка" - сам процесс, принимающий и создающий внутренние
запросы.&lt;/p&gt;
&lt;p&gt;Вообще создание внутренних сервисов очень хорошо ложится на так
называемую &lt;a href="https://www.insight-it.ru/goto/7e699ecd/" rel="nofollow" target="_blank" title="http://ru.wikipedia.org/wiki/%D0%9C%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C_%D0%B0%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2"&gt;модель "акторов"&lt;/a&gt;,
система разбивается на некие логические примитивы, общающиеся между
собой исключительно передачей сообщений. По сути процессы с
определенными разработчиками наборами входящих и исходящих сообщений и
алгоритмом преобразования одних в другие. При таком подходе группа
одинаково функционирующих акторов (вероятно распределенная по нескольким
серверам для отказоустойчивости и возможности масштабирования) и
образует внутренний сервис.&lt;/p&gt;
&lt;p&gt;На практике есть масса способов воплотить эту модель в жизнь,
перечислю&amp;nbsp;с пояснениями наиболее заслуживающие внимания&amp;nbsp;на мой взгляд:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Функциональные языки программирования,&lt;/strong&gt;&amp;nbsp;в &lt;a href="/tag/erlang/"&gt;Erlang&lt;/a&gt;
    и &lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt; модель акторов является практически "сердцем"
    всего языка и связанной платформы; у обоих есть библиотеки для
    реализации надежных, высокопроизводительных и масштабируемых акторов
    (&lt;strong&gt;OTP&lt;/strong&gt; и &lt;strong&gt;Akka&lt;/strong&gt;, соответственно). Если не боитесь кардинально
    отличающейся от нынче модного ООП парадигмы разработки, этот вариант
    наиболее жизнеспособный, &lt;em&gt;рекомендую&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Асинхронный HTTP-сервер&lt;/strong&gt;, в частности &lt;a href="/tag/tornado/"&gt;Tornado&lt;/a&gt; и
    &lt;a href="/tag/node-js/"&gt;node.js&lt;/a&gt;&amp;nbsp;- они основаны на &lt;a href="https://www.insight-it.ru/linux/2012/kak-rabotaet-epoll/"&gt;epoll&lt;/a&gt; и помимо эффективной обработки HTTP-запросов умеют и эффективно их отправлять посредством идущего в комплекте асинхронного же клиента.
    При таком подходе по сути получается несколько "уровней"
    HTTP-серверов, первый из которых публично доступен для общения с
    внешним миром и в ответ на каждый входящий запрос обращается сразу к
    нескольким внутренним HTTP-сервисам (вероятно параллельно) и на их
    основе составляет ответ пользователю. Этот подход одно время активно
    пропагандировали на конференциях ребята из одного крупного
    отечественного сайта с вакансиями. Особенным бонусом этого варианта
    является возможность использовать в роли внутреннего сервиса
    какую-то старую, доставшуюся по наследству &lt;em&gt;(legacy)&lt;/em&gt;, систему,
    которая с одной стороны по-прежнему нужна, а с другой - человек,
    который в ней&amp;nbsp;разбирался уже давно уволился.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/c/"&gt;С++&lt;/a&gt; и &lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt;&lt;/strong&gt; - хоть одного из
    участников этой пары можно легко заменить на альтернативу, вместе
    они смотрятся наиболее органично: потенциально
    высокопроизводительная реализация бизнес-логики на С++ плюс
    проверенная в деле многими крупными и очень крупными проектами
    обертка для создания серверов и клиентов, легко общающихся из разных
    языков программирования (речь о Thrift, если не очевидно). Если в
    команде проекта есть гуру C++ - этот вариант Ваш, в противном случае
    не рекомендую, т.к. &lt;em&gt;очень&lt;/em&gt; легко накосячить.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Иногда внутренние сервисы возможно сделать совсем изолированными, то
есть без взаимодействия с другими компонентами системы. Но в большинстве
случаев это не так, зачастую для принятия решения им необходимы внешние
данные.&lt;/p&gt;
&lt;h2 id="baza-dannykh-i-keshirovanie"&gt;База данных и кэширование&lt;/h2&gt;
&lt;p&gt;По большому счету интерактивные сайты не особо сильно отличаются от
статичных с точки зрения организации хранения данных.&lt;/p&gt;
&lt;p&gt;Из особенностей хочу отметить более-менее четкое разграничение
&lt;strong&gt;стабильной&lt;/strong&gt; информации и &lt;strong&gt;свежей&lt;/strong&gt;, актуальной лишь короткое время.
Для социальной сети это могут быть, например, профили пользователей
(стабильная) и сообщения (свежая).&lt;/p&gt;
&lt;p&gt;В соответствии с этим стоит выбирать хранилище данных и политику
кэширования:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Стабильная информация, которая редко обновляется и в тысячи раз чаще
    читается, прекрасно поддается кэшированию и возможно даже прекрасно
    будет себя чувствовать в реляционной СУБД.&lt;/li&gt;
&lt;li&gt;Свежую информацию вероятно вообще важнее доставить в кратчайшие
    сроки получателю, а сохранять в персистентном виде можно вообще
    постфактум для архива, на маловероятный случай когда она повторно
    понадобится. Про кэширование лучше вообще забыть. Для этого самого
    "архива" часто используют нереляционные распределенные базы данных
    вроде &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt;, &lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt; или
    &lt;a href="/tag/riak/"&gt;Riak&lt;/a&gt;. А про оперативную доставку получателю поговорим
    в следующем разделе.&lt;/li&gt;
&lt;li&gt;Хранилища данных в памяти вроде &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; или
    &lt;a href="/tag/redis/"&gt;Redis&lt;/a&gt; с отключенной персистентностью можно
    использовать независимо для временного хранения каких-то побочных
    данных (восстановимых производных данных или просто чего-то не особо
    важного, вроде счетчиков пользователей онлайн).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="potoki-soobshchenii"&gt;Потоки сообщений&lt;/h2&gt;
&lt;p&gt;Одной из ключевых задач интерактивного сайта является доставка сообщений
пользователем в реальном времени, причем их источник может быть как
внешний, так и внутренний, зачастую это просто другие пользователи.&lt;/p&gt;
&lt;p&gt;Часть системы, отвечающую за маршрутизацию таких сообщений, обычно
назвают &lt;strong&gt;брокером сообщений&lt;/strong&gt;&amp;ensp;&lt;em&gt;(message broker)&lt;/em&gt;. Для доставки
сообщений в браузер чаще всего используют &lt;strong&gt;интерфейс сериализованных
данных&lt;/strong&gt;, подробно обсуждавшийся в &lt;a href="https://www.insight-it.ru/interactive/2012/postoyannoe-soedinenie-mezhdu-brauzerom-i-serverom/"&gt;одной из предыдущих статей серии&lt;/a&gt;. Когда пользователь устанавливает соединение с этим интерфейсом, он, в
свою очередь, напрямую или через внутренний сервис регистрируется в
брокере сообщений для оперативного получения сообщений, предназначенных
соответствующему пользователю.&lt;/p&gt;
&lt;p&gt;Предлагаю рассмотреть типичные сценарии маршрутизации сообщений, они
довольно просты:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Конкретный получатель&lt;/strong&gt;, к сообщению (которое обычно никак не
    анализируется брокером) прикрепляется метка-идентификатор,
    обозначающий кому именно оно предназначено. Такое сообщение получит
    только процесс, зарегистрировавшийся с аналогичным идентификатором.
    Типичный пример использования - личные сообщения от пользователя к
    пользователю.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Группа получателей&lt;/strong&gt;, актуально для проектов, где пользователи
    взаимодействуют не на глобальном пространстве, а разбиты на части по
    какому-то признаку. Скажем это может быть какой-то B2B сервис и
    сообщения ходят только между сотрудниками одной компании-клиента.
    Обычно используется такие же метки, как и при конкретном получателе,
    только с одной из сторон (обычно принимающей) вместо конкретного
    идентификатора указывается какой-то паттерн, вроде &lt;code&gt;CompanyA.*&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Публичные сообщения&lt;/strong&gt; - получают все пользователи, метки не
    используются. Обычно это уведомления о глобальных для сайта событиях
    или публикации каких-то материалов.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Реализаций брокеров сообщений есть много разных, общий принцип работы у
всех примерно одинаковый и соответствует трем изложенным выше пунктам.
Для интернет-проектов очень рекомендую &lt;a href="/tag/rabbitmq/"&gt;RabbitMQ&lt;/a&gt;, в нем
эти стратегии маршрутизации называются &lt;em&gt;direct&lt;/em&gt;, &lt;em&gt;topic&lt;/em&gt; и &lt;em&gt;fanout&lt;/em&gt;
exchange, соответственно.&lt;/p&gt;
&lt;p&gt;Отправлять сообщения через брокер в большинстве случаев будут различные
внутренние сервисы в случае возникновения определенных событий &lt;em&gt;(читай:
получения ими определенных входящих сообщений и попадания в определенную
ветвь алгоритма их обработки)&lt;/em&gt;. Какую стратегию маршрутизации
использовать - тоже на их совести.&lt;/p&gt;
&lt;p&gt;К слову, внутренние сервисы также могут подписываться на получение части
сообщений из брокера, например для асинхронного создания "архива"
событий, отправки почтовых уведомлений или выполнения ресурсоемких задач
вроде конвертации медиа-файлов.&lt;/p&gt;
&lt;p&gt;При получении сообщения клиентская часть меняет соответствующим образом
текущую версию открытой страницы. От открытия дополнительного
всплывающего окна до просто смены цифры в количестве чего-нибудь.&lt;/p&gt;
&lt;p&gt;Будьте аккуратны с публичными сообщениями - их количество в единицу
времени может рости очень быстро с увеличением размеров аудитории.
Горизонтально масштабируемый брокер сообщений очень важен, если в Вашем
проекте в основном используются именно публичные сообщения.&lt;/p&gt;
&lt;h2 id="zakliuchenie"&gt;Заключение&lt;/h2&gt;
&lt;p&gt;Таким образом наша цепь замыкается - между браузерами любых
пользователей можно в "мягком" реальном времени пересылать любые
сообщения, пропуская их через бизнес-логику для регулирования данного
процесса, и, при необходимости, использовать постоянные и временные
хранилища данных.&lt;/p&gt;
&lt;p&gt;Как я уже упоминал&amp;nbsp;&lt;a href="https://www.insight-it.ru/interactive/2012/arkhitektura-interaktivnykh-sajjtov/"&gt;в первой статье серии&lt;/a&gt;, серверная часть у интерактивного сайта не так уж и кардинально отличается от любого другого - примерно те же компоненты, примерно так же работают и взаимодействуют. Разница в деталях.&lt;/p&gt;
&lt;p&gt;В следующей, заключительной, статье серии мы по второму кругу пройдемся
по ключевым моментам и попробуем рассмотреть наиболее перспективные
моменты для улучшений и оптимизации, хотя, как говорится, заранее
оптимизировать - плохая примета :)&lt;/p&gt;
&lt;div class="card green"&gt;
&lt;p&gt;&lt;div class="card-content white-text"&gt;
Эта статья - пятая в &lt;a class="green-text text-lighten-4" href="https://www.insight-it.ru/interactive/"&gt;серии про Интерактивные сайты&lt;/a&gt;, автор - &lt;a class="green-text text-lighten-4" href="https://www.insight-it.ru/goto/b03d9116/" rel="nofollow" target="_blank" title="http://blinkov.ru"&gt;Иван&amp;nbsp;Блинков&lt;/a&gt;, основано на личном опыте.
До встречи &lt;a class="green-text text-lighten-4" href="/feed/"&gt;на страницах Insight IT&lt;/a&gt;!
&lt;/div&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 04 Jun 2012 05:38:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-06-04:interactive/2012/servernaya-chast-interaktivnogo-sajjta-i-potoki-soobshhenijj/</guid><category>Akka</category><category>C++</category><category>Cassandra</category><category>Erlang</category><category>HBase</category><category>Memcached</category><category>OTP</category><category>RabbitMQ</category><category>Redis</category><category>Riak</category><category>Scala</category><category>Thrift</category></item><item><title>Архитектура Twitter. Два года спустя.</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-twitter-dva-goda-spustya/</link><description>&lt;p&gt;В далеком 2008м я уже публиковал статью про &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-twitter/"&gt;архитектуру Twitter&lt;/a&gt;, но время летит
стремительно и она уже абсолютно устарела. За это время аудитория
Twitter росла просто фантастическими темпами и многое поменялось и с
технической точки зрения. Интересно что новенького у одного из самых
популярных социальных интернет-проектов?&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;3 год, 2 месяца и 1 день потребовалось Twitter, чтобы набрать 1
    миллиард твитов&lt;/li&gt;
&lt;li&gt;На сегодняшний день, чтобы отправить миллиард твитов пользователям
    нужна всего одна неделя&lt;/li&gt;
&lt;li&gt;752% рост аудитории за 2008 год&lt;/li&gt;
&lt;li&gt;1358% рост аудитории за 2009 год&amp;nbsp;(без учета API, по данным comScore)&lt;/li&gt;
&lt;li&gt;175 миллионов зарегистрированных пользователей на сентябрь 2010 года&lt;/li&gt;
&lt;li&gt;460 тысяч регистраций пользователей в день&lt;/li&gt;
&lt;li&gt;9й сайт в мире по популярности (по данным Alexa, год назад был на 12
    месте)&lt;/li&gt;
&lt;li&gt;50 миллионов твитов в день год назад, 140 миллионов твитов в день
    месяц назад, 177 миллионов твитов в день на 11 марта 2011г.&lt;/li&gt;
&lt;li&gt;Рекорд по количеству твитов за секунду 6939, установлен через минуту
    после того, как Новый Год 2011 наступил в Японии&lt;/li&gt;
&lt;li&gt;600 миллионов поисков в день&lt;/li&gt;
&lt;li&gt;Лишь 25% трафика приходится на веб сайт, остальное идет через API&lt;/li&gt;
&lt;li&gt;Росто числа мобильных пользователей за последний год 182%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;6 миллиардов&lt;/strong&gt; запросов к API в день, около 70 тысяч в секунду&lt;/li&gt;
&lt;li&gt;8, 29, 130, 350, 400 - это количество сотрудников Twitter на январь
    2008, январь 2009, январь 2010, январь и март 2011, соответственно&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Самая свежая &lt;a href="https://www.insight-it.ru/goto/682783c0/" rel="nofollow" target="_blank" title="http://blog.twitter.com/2011/03/numbers.html"&gt;статистика про Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; + &lt;code&gt;mod_proxy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/unicorn/"&gt;Unicorn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt; +&amp;nbsp;&lt;a href="/tag/ror/"&gt;Ruby on Rails&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/flock/"&gt;Flock&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/kestrel/"&gt;Kestrel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/scribe/"&gt;Scribe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt;, &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt; и &lt;a href="/tag/pig/"&gt;Pig&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Сравните с аналогичным разделом предыдущей статьи о Twitter - увидите
много новых лиц, подробнее ниже.&lt;/p&gt;
&lt;h2 id="oborudovanie"&gt;Оборудование&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Сервера расположены в NTT America&lt;/li&gt;
&lt;li&gt;Никаких облаков и виртуализации, существующие решения страдают
    слишком высокими задержками&lt;/li&gt;
&lt;li&gt;Более тысячи серверов&lt;/li&gt;
&lt;li&gt;Планируется переезд в собственный датацентр&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="chto-takoe-tvit"&gt;Что такое твит?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Сообщение длиной до 140 символов + метаданные&lt;/li&gt;
&lt;li&gt;Типичные запросы:&lt;ul&gt;
&lt;li&gt;по идентификатору&lt;/li&gt;
&lt;li&gt;по автору&lt;/li&gt;
&lt;li&gt;по @упоминаниям пользователей&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Процесс обработки запроса в Twitter" class="responsive-img" src="https://www.insight-it.ru/images/twitter-request-flow.jpeg" title="Процесс обработки запроса в Twitter"/&gt;&lt;/p&gt;
&lt;h3 id="unicorn"&gt;Unicorn&lt;/h3&gt;
&lt;p&gt;Сервер приложений для Rails:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Развертывание новых версий кода &lt;strong&gt;без простоя&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;На 30% меньше расход вычислительных ресурсов и оперативной памяти,
    по сравнению с другими решениями&lt;/li&gt;
&lt;li&gt;Перешли с &lt;code&gt;mod_proxy_balancer&lt;/code&gt; на &lt;code&gt;mod_proxy_pass&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="rails"&gt;Rails&lt;/h3&gt;
&lt;p&gt;Используется в основном для генерации страниц, работа за сценой
реализована на чистом Ruby или Scala.&lt;/p&gt;
&lt;p&gt;Столкнулись со следующими проблемами:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Проблемы с кэшированием, особенно по части инвалидации&lt;/li&gt;
&lt;li&gt;ActiveRecord генерирует не самые удачные SQL-запросы, что замедляло
    время отклика&lt;/li&gt;
&lt;li&gt;Высокие задержки в очереди и при репликации&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="memcached"&gt;memcached&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;memcached не идеален. Twitter начал сталкиваться с Segmentation
    Fault в нем очень рано.&lt;/li&gt;
&lt;li&gt;Большинство стратегий кэширования основываются на длинных TTL
    (более минуты).&lt;/li&gt;
&lt;li&gt;Вытеснение данных делает его непригодным для важных конфигурационных
    данных (например флагов "темного режима", о котором пойдет речь
    ниже).&lt;/li&gt;
&lt;li&gt;Разбивается на несколько пулов для улучшения производительности и
    снижения риска вытеснения.&lt;/li&gt;
&lt;li&gt;Оптимизированная библиотека для доступа к memcached из Ruby на
    основе libmemcached + FNV hash, вместо чистого Ruby и md5.&lt;/li&gt;
&lt;li&gt;Twitter является одним их наиболее активных проектов, участвующих в
    разработке libmemcached.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="mysql"&gt;MySQL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Разбиение больших объемов данных является тяжелой задачей.&lt;/li&gt;
&lt;li&gt;Задержки в репликации и вытеснение данных из кэша является причиной
    нарушения целостности данных с точки зрения конечного пользователя.&lt;/li&gt;
&lt;li&gt;Блокировки создают борьбу за ресурсы для популярных данных.&lt;/li&gt;
&lt;li&gt;Репликация однопоточна и происходит недостаточно быстро.&lt;/li&gt;
&lt;li&gt;Данные социальных сетей плохо подходят для реляционных СУБД:&lt;ul&gt;
&lt;li&gt;NxN отношения, социальный граф и обход деревьев - не самые
    подходящие задачи для таких баз данных&lt;/li&gt;
&lt;li&gt;Проблемы с дисковой подсистемой (выбор файловой системы,
    noatime, алгоритм планирования)&lt;/li&gt;
&lt;li&gt;ACID практически не требуется&lt;/li&gt;
&lt;li&gt;Для очередей также практически непригодны&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Twitter сталкивался с большими проблемами касательно таблиц
    пользователей и их статусов&lt;/li&gt;
&lt;li&gt;Читать данные с мастера при Master/Slave репликации = медленная
    смерть&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="flockdb"&gt;FlockDB&lt;/h3&gt;
&lt;p&gt;Масштабируемое хранилище для данных социального графа:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Разбиение данных через Gizzard&lt;/li&gt;
&lt;li&gt;Множество серверов MySQL в качестве низлежащей системы хранения&lt;/li&gt;
&lt;li&gt;В Twitter содержит 13 миллиардов ребер графа и обеспечивает 20 тысяч
    операций записи и 100 тысяч операций чтения в секунду&lt;/li&gt;
&lt;li&gt;Грани хранятся и индексируются в обоих направлениях&lt;/li&gt;
&lt;li&gt;Поддерживает распределенный подсчет количества строк&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/4fe0530b/" rel="nofollow" target="_blank" title="https://github.com/twitter/flockdb"&gt;Open source!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Среднее время на выполнение операций:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Подсчет количества строк: 1мс&lt;/li&gt;
&lt;li&gt;Временные запросы: 2мс&lt;/li&gt;
&lt;li&gt;Запись: 1мс для журнала, 16мс для надежной записи&lt;/li&gt;
&lt;li&gt;Обход дерева: 100 граней/мс&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Подробнее про эволюцию систем хранения данных в Twitter &lt;a href="https://www.insight-it.ru/goto/32077a90/" rel="nofollow" target="_blank" title="http://www.slideshare.net/nkallen/q-con-3770885"&gt;в презентации
Nick Kallen&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="cassandra"&gt;Cassandra&lt;/h3&gt;
&lt;p&gt;Распределенная система хранения данных, ориентированная на работу в
реальном времени:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Изначально разработана в &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Очень высокая производительность на запись&lt;/li&gt;
&lt;li&gt;Из слабых сторон: высокая задержка при случайном доступе&lt;/li&gt;
&lt;li&gt;Децентрализованная, способна переносить сбои оборудования&lt;/li&gt;
&lt;li&gt;Гибкая схема данных&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Планируется полный переход на нее по
    следующему алгоритму:&lt;/del&gt;&lt;ul&gt;
&lt;li&gt;&lt;del&gt;Все твиты пишутся и в Cassandra
    и в MySQL&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Динамически часть операций
    чтения переводится на Cassandra&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Анализируется реакция системы,
    что сломалось&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Полностью отключаем чтение из
    Cassandra, чиним неисправности&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Начинаем сначала&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/e83e4e8e/" rel="nofollow" target="_blank" title="http://engineering.twitter.com/2010/07/cassandra-at-twitter-today.html"&gt;Обновление:&lt;/a&gt;&lt;/strong&gt; стратегия по поводу использования Cassandra изменилась, попытки
    использовать её в роли основного хранилища для твитов прекратились,
    но она продолжает использоваться для аналитики и географической
    информации.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Подробнее почему Twitter пришел к решению использовать Cassandra можно
прочитать &lt;a href="https://www.insight-it.ru/goto/ffc31d1/" rel="nofollow" target="_blank" title="http://www.slideshare.net/ryansking/scaling-twitter-with-cassandra"&gt;в отдельной презентации&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Помимо всего прочего Cassandra&amp;nbsp;&lt;del&gt;планируется использовать&lt;/del&gt; используется для аналитики в реальном времени.&lt;/p&gt;
&lt;h3 id="scribe"&gt;Scribe&lt;/h3&gt;
&lt;p&gt;Пользователи Twitter генерируют огромное количество данных, около 15-25
Гб в минуту, более 12 Тб в день, и эта цифра удваивается несколько раз
в год.&lt;/p&gt;
&lt;p&gt;Изначально для сбора логов использовали &lt;code&gt;syslog-ng&lt;/code&gt;, но он очень быстро
перестал справляться с нагрузкой.&lt;/p&gt;
&lt;p&gt;Решение нашлось очень просто: &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt; столкнулся с
аналогичной проблемой и разработал проект Scribe, который был
опубликован в opensource.&lt;/p&gt;
&lt;p&gt;По сути это фреймворк для сбора и агрегации логов, основанный на
&lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt;. Вы пишете текст для логов и указываете
категорию, остальное он берет на себя.&lt;/p&gt;
&lt;p&gt;Работает локально, надежен даже в случае потери сетевого соединения,
каждый узел знает только на какой сервер передавать логи, что позволяет
создавать масштабируемую иерархию для сбора логов.&lt;/p&gt;
&lt;p&gt;Поддерживаются различные системы для записи в данным, &amp;nbsp;в том числе
обычные файлы и HDFS (о ней ниже).&lt;/p&gt;
&lt;p&gt;Этот продукт полностью решил проблему Twitter со сбором логов,
используется около 30 различных категорий. В процессе использования была
создана и опубликована масса доработок. Активно сотрудничают с командой
Facebook в развитии проекта.&lt;/p&gt;
&lt;h3 id="hadoop"&gt;Hadoop&lt;/h3&gt;
&lt;p&gt;Как Вы обычно сохраняете 12Тб новых данных, поступающих каждый день?&lt;/p&gt;
&lt;p&gt;Если считать, что средняя скорость записи современного жесткого диска
составляет 80Мбайт в секунду, запись 12Тб данных заняла бы почти 48
часов.&lt;/p&gt;
&lt;p&gt;На одном даже очень большом сервере данную задачу не решить, логичным
решением задачи стало использование кластера для хранения и анализа
таких объемов данных.&lt;/p&gt;
&lt;p&gt;Использование кластерной файловой системы добавляет сложности, но
позволяет меньше заботиться о деталях.&lt;/p&gt;
&lt;p&gt;Hadoop Distributed File System (HDFS) предоставляет возможность
автоматической репликации и помогает справляться со сбоями оборудования.&lt;/p&gt;
&lt;p&gt;MapReduce framework позволяет обрабатывать огромные объемы данных,
анализируя пары ключ-значение.&lt;/p&gt;
&lt;p&gt;Типичные вычислительные задачи, которые решаются с помощью Hadoop в
Twitter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Вычисление связей дружбы в социальном графе (&lt;code&gt;grep&lt;/code&gt; и &lt;code&gt;awk&lt;/code&gt; не
    справились бы, self join в MySQL на таблицах с миллиардами строк -
    тоже)&lt;/li&gt;
&lt;li&gt;Подсчет статистики (количество пользователей и твитов, например
    подсчет количества твитов занимает 5 минут при 12 миллиардах
    записей)&lt;/li&gt;
&lt;li&gt;Подсчет PageRank между пользователями для вычисления репутации.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В твиттер используется бесплатный дистрибутив от Cloudera, версия Hadoop
0.20.1, данные храняться &lt;a href="https://www.insight-it.ru/goto/1ac5bba3/" rel="nofollow" target="_blank" title="https://github.com/kevinweil/hadoop-lzo"&gt;в сжатом по алгоритму LZO виде&lt;/a&gt;, библиотеки для работы с
данными опубликованы под названием
&lt;a href="https://www.insight-it.ru/goto/a1b5430e/" rel="nofollow" target="_blank" title="https://github.com/kevinweil/elephant-bird"&gt;elephant-bird&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="pig"&gt;Pig&lt;/h3&gt;
&lt;p&gt;Для того чтобы анализировать данные с помощью MapReduce обычно
необходимо разрабатывать код на Java, что далеко не все умеют делать, да
и трудоемко это.&lt;/p&gt;
&lt;p&gt;Pig представляет собой высокоуровневый язык, позволяющий
трансформировать огромные наборы данных шаг за шагом.&lt;/p&gt;
&lt;p&gt;Немного напоминает SQL, но намного проще. Это позволяет писать в 20 раз
меньше кода, чем при анализе данных с помощью обычных MapReduce работ.
Большая часть работы по анализу данных в Twitter осуществляется с
помощью Pig.&lt;/p&gt;
&lt;h3 id="dannye"&gt;Данные&lt;/h3&gt;
&lt;p&gt;Полу-структурированные данные:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;логи Apache, RoR, MySQL, A/B тестирования, процесса регистрации&lt;/li&gt;
&lt;li&gt;поисковые запросы&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Структурированные данные:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Твиты&lt;/li&gt;
&lt;li&gt;Пользователи&lt;/li&gt;
&lt;li&gt;Блок-листы&lt;/li&gt;
&lt;li&gt;Номера телефонов&lt;/li&gt;
&lt;li&gt;Любимые твиты&lt;/li&gt;
&lt;li&gt;Сохраненные поиски&lt;/li&gt;
&lt;li&gt;Ретвиты&lt;/li&gt;
&lt;li&gt;Авторизации&lt;/li&gt;
&lt;li&gt;Подписки&lt;/li&gt;
&lt;li&gt;Сторонние клиенты&lt;/li&gt;
&lt;li&gt;География&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Запутанные данные:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Социальный граф&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Что же они делают с этим всем?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Подсчет математического ожидания, минимума, максимума и дисперсии
    следующих показателей:&lt;ul&gt;
&lt;li&gt;Количество запросов за сутки&lt;/li&gt;
&lt;li&gt;Средняя задержка, 95% задержка&lt;/li&gt;
&lt;li&gt;Распределение кодов HTTP-ответов (по часам)&lt;/li&gt;
&lt;li&gt;Количество поисков осуществляется каждый день&lt;/li&gt;
&lt;li&gt;Количество уникальных запросов и пользователей&lt;/li&gt;
&lt;li&gt;Географическое распределение запросов и пользователей&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Подсчет вероятности, ковариации, влияния:&lt;ul&gt;
&lt;li&gt;Как отличается использование через мобильные устройства?&lt;/li&gt;
&lt;li&gt;Как влияет использование клиентов сторонних разработчиков?&lt;/li&gt;
&lt;li&gt;Когортный анализ&lt;/li&gt;
&lt;li&gt;Проблемы с сайтом (киты и роботы, подробнее ниже)&lt;/li&gt;
&lt;li&gt;Какие функциональные возможности цепляют пользователей?&lt;/li&gt;
&lt;li&gt;Какие функциональные возможности чаще используются популярными
    пользователями?&lt;/li&gt;
&lt;li&gt;Корректировка и предложение поисковых запросов&lt;/li&gt;
&lt;li&gt;A/B тестирование&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Предсказания, анализ графов, естественные языки:&lt;ul&gt;
&lt;li&gt;Анализ пользователей по их твитам, твитов, на которые они
    подписаны, твитам их фоловеров&lt;/li&gt;
&lt;li&gt;Какая структура графа ведет к успешным популярным сетям&lt;/li&gt;
&lt;li&gt;Пользовательская репутация&lt;/li&gt;
&lt;li&gt;Анализ эмоциональной окраски&lt;/li&gt;
&lt;li&gt;Какие особенности заставляют людей ретвитнуть твит?&lt;/li&gt;
&lt;li&gt;Что влияет на глубину дерева ретвитов ?&lt;/li&gt;
&lt;li&gt;Долгосрочное обнаружение дубликатов&lt;/li&gt;
&lt;li&gt;Машинное обучение&lt;/li&gt;
&lt;li&gt;Обнаружения языка&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Подробнее про обработку данных &lt;a href="https://www.insight-it.ru/goto/3d4649ef/" rel="nofollow" target="_blank" title="http://www.slideshare.net/kevinweil/nosql-at-twitter-nosql-eu-2010"&gt;в презентации Kevin Weil&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="hbase"&gt;HBase&lt;/h3&gt;
&lt;p&gt;Twitter начинают строить настоящие сервисы на основе Hadoop, например
поиск людей:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HBase используется как изменяемая прослойка над HDFS&lt;/li&gt;
&lt;li&gt;Данные экспортируются из HBase c помощью периодической MapReduce
    работы:&lt;ul&gt;
&lt;li&gt;На этапе Map используются также данные из FlockDB и нескольких
    внутренних сервисов&lt;/li&gt;
&lt;li&gt;Собственная схема разбиения данных&lt;/li&gt;
&lt;li&gt;Данные подтягиваются через высокопроизводительный, горизонтально
    масштабируемый сервис на Scala (&lt;a href="https://www.insight-it.ru/goto/917f8c95/" rel="nofollow" target="_blank" title="http://www.slideshare.net/al3x/building-distributed-systems-in-scala"&gt;подробнее о построении распределенных сервисов на Scala&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;На основе HBase разрабатываются и другие продукты внутри Twitter.&lt;/p&gt;
&lt;p&gt;Основными её достоинствами являются гибкость и легкая интеграция с
Hadoop и Pig.&lt;/p&gt;
&lt;p&gt;По сравнению с Cassandra:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"Их происхождение объясняет их сильные и слабые стороны"&lt;/li&gt;
&lt;li&gt;HBase построен на основе системы по пакетной обработке данных,
    высокие задержки, работает далеко не в реальном времени&lt;/li&gt;
&lt;li&gt;Cassandra построена с нуля для работы с низкими задержками&lt;/li&gt;
&lt;li&gt;HBase легко использовать при анализе данных как источник или место
    сохранения результатов, Cassandra для этого подходит меньше, но они
    работают над этим&lt;/li&gt;
&lt;li&gt;HBase на данный момент единственную точку отказа в виде мастер-узла&lt;/li&gt;
&lt;li&gt;В твиттере HBase используется для аналитики, анализа и создания
    наборов данных, а Cassandra - для онлайн систем&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="loony"&gt;Loony&lt;/h3&gt;
&lt;p&gt;Централизованная система управления оборудованием.&lt;/p&gt;
&lt;p&gt;Реализована с использованием:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/django/"&gt;Django&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8927633f/" rel="nofollow" target="_blank" title="http://www.lag.net/paramiko"&gt;Paraminko&lt;/a&gt; (реализация протокола SSH
    на Python, разработана и опубликована в opensource в Twitter)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Интегрирована с LDAP, анализирует входящую почту от датацентра и
автоматически вносит изменения в базу.&lt;/p&gt;
&lt;h3 id="murder"&gt;Murder&lt;/h3&gt;
&lt;p&gt;Система развертывания кода и ПО, основанная на протоколе BitTorrent.&lt;/p&gt;
&lt;p&gt;Благодаря своей P2P природе позволяет обновить более тысячи серверов за
30-60 секунд.&lt;/p&gt;
&lt;h3 id="kestrel"&gt;Kestrel&lt;/h3&gt;
&lt;p&gt;Распределенная очередь, работающая по протоколу memcache:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;set&lt;/code&gt; - поставить в очередь&lt;/li&gt;
&lt;li&gt;&lt;code&gt;get&lt;/code&gt; - взять из очереди&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Особенности:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Отсутствие строгого порядка выполнения заданий&lt;/li&gt;
&lt;li&gt;Отсутствие общего состояния между серверами&lt;/li&gt;
&lt;li&gt;Разработана на &lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="daemony"&gt;Daemon'ы&lt;/h3&gt;
&lt;p&gt;Каждый твит обрабатывается с помощью daemon'ов.&lt;/p&gt;
&lt;p&gt;В unicorn обрабатываются только HTTP запросы, вся работа за сценой
реализована в виде отдельных daemon'ов.&lt;/p&gt;
&lt;p&gt;Раньше использовалось много разных демонов, по одному на каждую задачу
(Rails), но перешли к меньшему их количеству, способному решать
несколько задач одновременно.&lt;/p&gt;
&lt;h3 id="kak-oni-spravliaiutsia-s-takimi-tempami-rosta"&gt;Как они справляются с такими темпами роста?&lt;/h3&gt;
&lt;p&gt;Рецепт прост, но эффективен, подходит практически для любого
интернет-проекта:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;обнаружить самое слабое место в системе;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;принять меры по его устранению;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;перейти к следующему самому слабому месту.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;На словах звучит и правда примитивно, но на практике нужно предпринять
ряд мер, чтобы такой подход был бы реализуем:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Автоматический сбор метрик (причем в агрегированном виде)&lt;/li&gt;
&lt;li&gt;Построение графиков (RRD, Ganglia)&lt;/li&gt;
&lt;li&gt;Сбор и анализ&amp;nbsp;логов&lt;/li&gt;
&lt;li&gt;Все данные должны получаться с минимальной задержкой, как можно
    более близко к реальному времени&lt;/li&gt;
&lt;li&gt;Анализ:&lt;ul&gt;
&lt;li&gt;Из данных необходимо получать &lt;em&gt;информацию&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Следить за динамикой показателей: стало лучше или хуже?&lt;/li&gt;
&lt;li&gt;Особенно при развертывании новых версий кода&lt;/li&gt;
&lt;li&gt;Планирование использования ресурсов намного проще, чем решение
    экстренных ситуаций, когда они на исходу&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Примерами агрегированных метрик в Twitter являются "киты" и "роботы",
вернее их количество в единицу времени.&lt;/p&gt;
&lt;h5&gt;Что такое "робот"?&lt;/h5&gt;
&lt;p&gt;&lt;img alt="Twitter Робот" class="responsive-img" src="https://www.insight-it.ru/images/twitter-bot.jpg" title="Twitter Робот"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ошибка внутри Rails (HTTP 500)&lt;/li&gt;
&lt;li&gt;Непойманное исключение&lt;/li&gt;
&lt;li&gt;Проблема в коде или нулевой результат&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Что такое "кит"?&lt;/h5&gt;
&lt;p&gt;&lt;img alt="Twitter Кит" class="responsive-img" src="https://www.insight-it.ru/images/twitter-whale.jpg" title="Twitter Кит"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HTTP ошибка 502 или 503&lt;/li&gt;
&lt;li&gt;В твиттер используется фиксированный таймаут в 5 секунд (лучше
    кому-то показать ошибку, чем захлебнуться в запросах)&lt;/li&gt;
&lt;li&gt;Убитый слишком длинный запрос к базе данных (mkill)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Значительное превышение нормального количества китов или роботов в
минуту является поводом для беспокойством.&lt;/p&gt;
&lt;p&gt;Реализован этот механизм простым bash-скриптом, который просматривает
агрегированные логи за последние 60 секунд, подсчитывает количество
китов/роботов и рассылает уведомления, если значение оказалось выше
порогового значения. Подробнее про работу команды оперативного
реагирования &lt;a href="https://www.insight-it.ru/goto/30562be/" rel="nofollow" target="_blank" title="http://www.slideshare.net/netik/billions-of-hits-scaling-twitter"&gt;в презентации John Adams&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="temnyi-rezhim"&gt;"Темный режим"&lt;/h3&gt;
&lt;p&gt;Для экстренных ситуаций в Twitter предусмотрен так называемый "темный
режим", который представляет собой набор механизмов для отключения
тяжелых по вычислительным ресурсам или вводу-выводу функциональных
частей сайта. Что-то вроде стоп-крана для сайта.&lt;/p&gt;
&lt;p&gt;Имеется около 60 выключателей, в том числе и полный режим "только для
чтения".&lt;/p&gt;
&lt;p&gt;Все изменения в настройках этого режима фиксируются в логах и сообщаются
руководству, чтобы никто не баловался.&lt;/p&gt;
&lt;h2 id="podvodim-itogi_1"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Не бросайте систему на самотек, начинайте собирать метрики и их
    визуализировать как можно раньше&lt;/li&gt;
&lt;li&gt;Заранее планируйте рост требуемых ресурсов и свои действия в случае
    экстренных ситуаций&lt;/li&gt;
&lt;li&gt;Кэшируйте по максимуму все, что возможно&lt;/li&gt;
&lt;li&gt;Все инженерные решения не вечны, ни одно из решений не идеально, но
    многие будут нормально работать в течение какого-то периода времени&lt;/li&gt;
&lt;li&gt;Заранее начинайте задумываться о плане масштабирования&lt;/li&gt;
&lt;li&gt;Не полагайтесь полностью на memcached и базу данных - они могут Вас
    подвести в самый неподходящий момент&lt;/li&gt;
&lt;li&gt;Все данные для запросов в реальном времени должны находиться в
    памяти, диски в основном для записи&lt;/li&gt;
&lt;li&gt;Убивайте медленные запросы (mkill) прежде, чем они убьют всю систему&lt;/li&gt;
&lt;li&gt;Некоторые задачи могут решаться путем предварительного подсчета и
    анализа, но далеко не все&lt;/li&gt;
&lt;li&gt;Приближайте вычисления к данным по возможности&lt;/li&gt;
&lt;li&gt;Используйте не mongrel, а unicorn для RoR&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Спасибо за внимание, &lt;a href="/feed/"&gt;жду Вас снова&lt;/a&gt;! Буду рад, если Вы
&lt;a href="https://www.insight-it.ru/goto/26b8fa1/" rel="nofollow" target="_blank" title="http://twitter.com/blinkov"&gt;подпишитесь на меня в Twitter&lt;/a&gt;, с
удовольствием пообщаюсь со всеми читателями :)&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 05 Mar 2011 20:47:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-03-05:highload/2011/arkhitektura-twitter-dva-goda-spustya/</guid><category>Apache</category><category>Cassandra</category><category>featured</category><category>Flock</category><category>FlockDB</category><category>Hadoop</category><category>HBase</category><category>Kestrel</category><category>Memcached</category><category>MySQL</category><category>Pig</category><category>Ruby</category><category>Ruby on Rails</category><category>Scala</category><category>Scribe</category><category>Twitter</category><category>Unicorn</category><category>архитектура Twitter</category><category>интернет-проекты</category><category>Масштабируемость</category><category>социальная сеть</category></item><item><title>Архитектура Mollom</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-mollom/</link><description>&lt;p&gt;&lt;img alt="Mollom" class="right" src="https://www.insight-it.ru/images/mollom-logo.jpg" title="Mollom"/&gt;
&lt;a href="https://www.insight-it.ru/goto/aa3c2fd2/" rel="nofollow" target="_blank" title="http://mollom.com/"&gt;Mollom&lt;/a&gt; - это прибыльный SaaS сервис по фильтрации различных форм спама из
контента, сгенерированного пользователями: комментариев, постов на
форумах и блогах, опросов, контактных и регистрационных форм.
Определение спама основано не только на контенте, но и репутации и
прошлой активности разместившего его пользователя. Алгоритм машинного
обучения Mollom выполняет роль цифрового модератора 24х7 для более 40
тысяч сайтов, в том числе и очень крупных компаний.&lt;/p&gt;
&lt;p&gt;С того момента, как Mollom запустили систему анализа цифрового контента,
они выявили более 373 миллионов спам сообщений, обнаружив в процессе что
впечатляющие 90% всех прошедших через них сообщений оказались спамом.
Весь этот поток спама в 100 сообщений в секунду обрабатывается всего
двумя географически распределенными серверами. На каждом из них работает
сервер Java-приложений и Cassandra. Так мало ресурсов требуется лишь
из-за того, что они создали очень эффективную систему машинного
обучения. Разве не круто? Так как же они это делают?&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Обслуживаются 40000 активных веб-сайтов, многие их которых
    принадлежат крупным клиентам, таким как Adobe, Sony BMG, Warner
    Brothers, Fox News и The Economist. Много крупных брендов, с
    крупными сайтами, масса комментариев.&lt;/li&gt;
&lt;li&gt;Обнаруживают пол-миллиона спам-сообщений ежедневно.&lt;/li&gt;
&lt;li&gt;Обрабатывается около 100 запросов к API в секунду.&lt;/li&gt;
&lt;li&gt;Проверка сообщения на спам занимает очень мало времени, обычно около
    30-50 миллисекунд, 95% запросов укладывается в 250 миллисекунд,
    когда самые медленные обрабатываются пол секунды.&lt;/li&gt;
&lt;li&gt;Эффективность определения спама составляет 99.95%. Это означает, что
    из 10000 спам-сообщений Mollom пропустит только 5.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/82e0a09b/" rel="nofollow" target="_blank" title="http://mollom.com/blog/netlog-using-mollom"&gt;Netlog&lt;/a&gt;, европейская
    социальная сеть, имеет отдельный Mollom-сервер в своем датацентре.
    Netlog проверяют на спам около 4 миллионов сообщений каждый день на
    классификаторах, специально натренированных на их данных.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/java/"&gt;Java&lt;/a&gt; - исторически сложилось, что Mollom был с самого
    начала был разработан на Java.&lt;/li&gt;
&lt;li&gt;Два сервера обслуживают основную часть клиентов:&lt;ul&gt;
&lt;li&gt;Один сервер на восточном побережье США, другой - на западном&lt;/li&gt;
&lt;li&gt;В случае сбоя один сервер может полностью подменить другой&lt;/li&gt;
&lt;li&gt;Конфигурация обоих: Intel Xeon Quad core, 2.8GHz, 16GB RAM, 4
    диска по 300 GB, RAID 10.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/ad66f2bf/" rel="nofollow" target="_blank" title="http://www.softlayer.com/"&gt;SoftLayer&lt;/a&gt; - хостинг-провайдер.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/baf11ec8/" rel="nofollow" target="_blank" title="http://cassandra.apache.org/"&gt;Cassandra&lt;/a&gt; - NoSQL база данных,
    выбранная из-за высокой производительности на запись и способности
    работать на серверах, располагающихся в разных датацентрах (была
    разработана в&amp;nbsp;&lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-facebook/"&gt;Facebook&lt;/a&gt;,
    но там практически не используется).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/dce103e6/" rel="nofollow" target="_blank" title="http://www.mysql.com/"&gt;MySQL&lt;/a&gt; - Java Persistence API используется
    для обычных наборов данных, когда Cassandra используется для больших
    объемов данных.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/2b056aff/" rel="nofollow" target="_blank" title="http://en.wikipedia.org/wiki/GlassFish"&gt;Glassfish&lt;/a&gt; - open source
    сервер приложений для платформы Java EE. Они выбрали именно
    Glassfish за его возможности корпоративного уровня, такие как
    репликация и обработка сбоев.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/cc84f430/" rel="nofollow" target="_blank" title="http://hudson-ci.org/"&gt;Hudson&lt;/a&gt; - предоставляет непрерывное
    тестирование и развертывание кода серверной части на всех
    используемых машинах.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/f94e85c8/" rel="nofollow" target="_blank" title="http://munin-monitoring.org/"&gt;Munin&lt;/a&gt; - измерение и построение
    графиков, касающихся здоровья серверов.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/cfd5a6c7/" rel="nofollow" target="_blank" title="http://www.pingdom.com/"&gt;Pingdom&lt;/a&gt; - внешний мониторинг.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/acf8f59d/" rel="nofollow" target="_blank" title="http://www.zendesk.com/"&gt;Zendesk&lt;/a&gt; - используется для оказания
    поддержки клиентам.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/17f204ac/" rel="nofollow" target="_blank" title="http://drupal.org/"&gt;Drupal&lt;/a&gt; - используется для основного сайта со
    специализированным модулем интернет-магазина.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a81d9bb9/" rel="nofollow" target="_blank" title="http://unfuddle.com/"&gt;Unfuddle&lt;/a&gt; - хостинг Subversion для
    взаимодействия удаленной команды разработчиков.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="kak-eto-rabotaet"&gt;Как это работает?&lt;/h2&gt;
&lt;p&gt;Процесс выглядит следующим образом:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Когда пользователь отправляет комментарий на сайт, происходит запрос
    к API Mollom.&lt;/li&gt;
&lt;li&gt;Контент анализируется, если он оказывается спамом, то сайту
    сообщается, что необходимо его заблокировать, если же алгоритм не
    уверен на 100% - сайту советуют показать CAPTCHA, которую сервис
    также предоставляет.&lt;/li&gt;
&lt;li&gt;После того, как CAPTCHA будет успешно заполнена, контент
    принимается. В большинстве случаев пользователи не будут ее видеть и
    контент будет приниматься сразу же.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Обнаружение спама является сложным балансом между отказом нормальному
контенту и принятию спама.&lt;/p&gt;
&lt;h2 id="biznes-model"&gt;Бизнес-модель&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Основным залогом популярности Mollom является бесплатная возможность
    попробовать сервис, ограничение составляет 100 нормальных (не спам)
    сообщений в день. Небольшие сайты могут никогда и не достичь этого
    ограничения.&lt;/li&gt;
&lt;li&gt;Далее есть два тарифа: 1 евро в день и 3600 евро с возможностями
    вполне соответствующими этим суммам&lt;/li&gt;
&lt;li&gt;Сайты, использующие бесплатный тариф, вовсе не зря тратят ресурсы
    системы, как кажется на первый взгляд, а являются жизненно-важным
    источником данных для тренировки системы. Без этих данных алгоритмы
    были бы существенно менее точны.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Разработчики Mollom уделяют максимум внимания времени отклика,
    эффективности кода и использования серверных ресурсов.&lt;/li&gt;
&lt;li&gt;Физически каждый сервер может справиться со всеми запросами, два
    сервера нужны для избежания перерывов в работе системы. Когда оба
    сервера в строю - работа распределяется между ними, когда один
    падает - второй перехватывает его запросы.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mollom прошел через несколько этапов развития:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Изначально маленькая команда из двух человек работала вечерами
    над основными алгоритмами, классификаторами и реальными
    бизнес-задачами, которые они пытались решить. Для построения
    инфраструктуры серверной части они использовали свои реализации
    базовых механизмов по управлению ресурсами, соединениями и
    потоками. В итоге они обнаружили, что тратят слишком много
    времени на эти вещи. После этого они переключились на Glassfish,
    что позволило им намного меньше беспокоится об управлении
    памятью, REST-запросах, парсинге XML и поддержании пула
    соединений с базой данных.&lt;/li&gt;
&lt;li&gt;В прошлом основной проблемой была пропускная способность
    дисковой подсистемы. Они должны хранить информацию о репутации
    всех IP-адресов и URL по всему Интернету, что привело к
    массивному набору данных с большим количеством случайных
    обращений.&lt;/li&gt;
&lt;li&gt;Поначалу они использовали MySQL на недорогой виртуальной машине,
    что в итоге не смогло масштабироваться.&lt;/li&gt;
&lt;li&gt;Они перенесли данные на твердотельные жесткие диски (SSD) и
    стали все хранить в файлах. Этот шаг решил проблемы с записью,
    но возникли новые проблемы:&lt;ol&gt;
&lt;li&gt;Это правда дорого.&lt;/li&gt;
&lt;li&gt;Очень чувствительно к типу используемой файловой системы&lt;/li&gt;
&lt;li&gt;Запись стала происходить быстрой, но итерация по большим
    наборам данным (что они делали довольно часто для очистки
    данных и обучения классификаторов) по-прежнему была очень
    медленным процессом.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;В итоге они отказались от твердотельных накопителей и стали
    использовать Cassandra.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cassandra сейчас используется для обработки интенсивного потока
    запросов на запись и в роли кэша:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Работает на RAID10, что хорошо подходит для высоких смешанных
    нагрузок на запись/чтение.&lt;/li&gt;
&lt;li&gt;Cassandra оптимизирована для записи, а в Mollom запись как раз
    происходит намного чаще, чем чтение.&lt;/li&gt;
&lt;li&gt;Разработана для распределенной работы как внутри датацентра, так
    и между датацентрами.&lt;/li&gt;
&lt;li&gt;Обратной стороной медали является отсутствие стандартного NoSQL
    интерфейса, что усложняет реализацию приложений.&lt;/li&gt;
&lt;li&gt;Механизм кэширования строк в Cassandra позволяет им не
    использовать отдельную систему для кэширования, что существенно
    упростило код приложения.&lt;/li&gt;
&lt;li&gt;Cassandra имеет функцию удаления устаревшей информации после
    определенного периода времени. В Европе существуют строгие
    законы о приватности личных данных, согласно которым они должны
    храниться не более определенного срока (штаб-квартира Mollom
    находится в Бельгии). В этом плане эта функция очень удобна.
    Эта функция опять же избавляет от необходимости реализовывать
    данный функционал вручную.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Типичный путь одного комментария внутри системы:&lt;ul&gt;
&lt;li&gt;Балансировка нагрузки между серверами лежит на клиентской
    библиотеке, в роли типичного клиента может выступать сайт на
    Drupal, осуществляющий запрос к API через XML-RPC или REST.&lt;/li&gt;
&lt;li&gt;Запросы обрабатываются сервером приложений Glassfish и проходят
    стандартный процесс обработки с помощью сервлетов и специфичных
    классов.&lt;/li&gt;
&lt;li&gt;Платящие клиенты обслуживаются в первую очередь, что приводит к
    тому, что клиенты на бесплатном тарифе могут ожидать результата
    несколько дольше.&lt;/li&gt;
&lt;li&gt;Запрос анализируется и оценка вероятности спама возвращается
    пользователю. Помимо этого отдельная часть кода Mollom отвечает
    за генерацию, выдачу и проверку CAPTCHA.&lt;/li&gt;
&lt;li&gt;Классификаторы полностью располагаются в оперативной памяти.
    Небольшой кусок контента разбивается на тысячи и тысячи
    крошечных частей, которые могут быть идентифицированы как спам.
    Такие классификаторы хранят в памяти до нескольких миллионов
    признаков, характерных для спама. Анализ должен выполняться
    очень быстро, так что никаких других вариантов кроме
    расположения всех требуемых данных в оперативной памяти просто
    не было.&lt;/li&gt;
&lt;li&gt;В Cassandra хранятся очки репутации, частоты, URL и IP-адреса.&lt;/li&gt;
&lt;li&gt;Струтуры данных в памяти не реплицируются напрямую. Они
    записываются в Cassandra, которая и передает их на второй
    сервер. Промежуток времени, когда данные не консистентны, очень
    невелик, так что это не сказывается негативно на алгоритмах.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Балансировка нагрузки с помощью клиента:&lt;ul&gt;
&lt;li&gt;Mollom использует такой подход к балансировке, так как стартап
    не может себе позволить дорогой железный балансировщик нагрузки.
    Если учесть, что им нужна балансировка между датацентрами,
    решение от любого из вендоров было бы комплексным и дорогим.&lt;/li&gt;
&lt;li&gt;У каждого клиента есть индивидуальный список серверов, которыми
    он может воспользоваться. Этот список изменяется через API.&lt;/li&gt;
&lt;li&gt;Каждый клиент может использовать разный список, платящим
    клиентам могут предоставлять отдельные сервера для уменьшения
    задержек.&lt;/li&gt;
&lt;li&gt;Если сервер упал - клиент пытается подключиться к следующему
    серверу в списке.&lt;/li&gt;
&lt;li&gt;С другой стороны такой подход усложняет разработку неофициальных
    клиентов: авторам проекта приходится тесно работать с
    разработчиками сторонних клиентов для обеспечения правильной
    реализации в них балансировки нагрузки.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Машинное обучение:&lt;ul&gt;
&lt;li&gt;Mollom - это набор самообучающихся систем. Отдельные
    CAPTCHA-решения, не учитывают ни пользовательское поведение, ни
    источник контента, заставляя каждого пользователя вводить
    проверочный код при каждом сообщении. В случае с Mollom это
    происходит только когда система анализа контента не уверена в
    конкретном решении.&lt;/li&gt;
&lt;li&gt;Средняя длина сообщения - 500 символов, обычно оно разбивается
    на 3000 характеристик. Принадлежность контента к спаму
    определяется путем оценки репутации IP адреса или Open ID,
    пользовательского идентификатора, эмоциональной окраски, языка,
    профанации, проверки на наличие специфичных слов и фраз, также
    учитывается качество написания текста и многие другие факторы.
    Все эти данные основываются на классификаторах. Некоторые из них
    статистические по природе, так что обучение происходит
    автоматически. Другие же основываются на правилах для того,
    чтобы быть уверенными, что они никогда не могут быть настроены
    неверно. Комбинация результатов всех тестов после нормализации и
    образует финальный рейтинг принадлежности к спаму каждого
    конкретного сообщения.&lt;/li&gt;
&lt;li&gt;Классификаторы и внутренние метрики обучаются с каждым новым
    сообщением и обновляются в реальном времени.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Glassfish берет на себя планировку нагрузки, учитывая многоядерность
    системы:&lt;ul&gt;
&lt;li&gt;Ключ к дизайну системы в многопроцессорном окружении заключается
    в максимальном параллелизации работы при минимальном простое
    из-за блокировок.&lt;/li&gt;
&lt;li&gt;Они используют 16 thread'ов на сервер.&lt;/li&gt;
&lt;li&gt;Большинство запросов обрабатываются сессионными объектами (Java
    Bean), не имеющими состояний. Они хорошо подходят для управления
    параллельными запросами.&lt;/li&gt;
&lt;li&gt;Они держат пул из нескольких сессионных объектов, но определение
    их количества делегируется Glassfish. В пиковую нагрузку это
    число увеличивается для более эффективной обработки запросов,
    порой оно достигает 32.&lt;/li&gt;
&lt;li&gt;Все классификаторы реализованы как раз как такие объекты,
    повторно использующиеся различными thread'ами.&lt;/li&gt;
&lt;li&gt;У каждого объекта есть свое клиентское соединение с Cassandra,
    чтобы гарантировать отсутствие блокировок.&lt;/li&gt;
&lt;li&gt;Когда пользователь не отвечает на CAPTCHA сессия очищается и
    Mollom узнает что это скорее всего был спам.&lt;/li&gt;
&lt;li&gt;На каждом сервере запущено по одной копии каждого
    классификатора.&lt;/li&gt;
&lt;li&gt;В момент очистки сессии происходит небольшая блокировка, когда
    происходит обновление классификаторов.&lt;/li&gt;
&lt;li&gt;Обновленные классификаторы записываются в Cassandra каждые пол
    часа.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Интеграция приложений:&lt;ul&gt;
&lt;li&gt;Mollom использует открытый API, который может быть интегрирован
    в любую систему.&lt;/li&gt;
&lt;li&gt;Библиотеки: Java, PHP, Ruby и другие.&lt;/li&gt;
&lt;li&gt;Готовые модули: Drupal, Joomla, Wordpress и прочие системы
    управления контентом.&lt;/li&gt;
&lt;li&gt;Решения от сторонних разработчиков, основанные на примерах кода
    от &amp;nbsp;Mollom.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Для мониторинга здоровья серверов они используют Munin:&lt;ul&gt;
&lt;li&gt;Каков размер heap памяти после сбора мусора?&lt;/li&gt;
&lt;li&gt;Каково количество доступных соединений?&lt;/li&gt;
&lt;li&gt;Каково количество thread'ов в пуле?&lt;/li&gt;
&lt;li&gt;Оценка времени блокировок в каждом thread'е.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Если взглянуть в целом на архитектуру Mollom, можно увидеть, что они
    стараются построить систему, способную прозрачно работать в
    нескольких датацентрах, чтобы позволить горизонтально расширить
    систему, когда они перерастут текущую двухсерверную конфигурацию:&lt;ul&gt;
&lt;li&gt;Балансировка нагрузки на клиенте позволяет выбирать оптимальный
    сервер и справляться со сбоями одного из них.&lt;/li&gt;
&lt;li&gt;Кластеризация Glassfish облегчает добавление/удаление новых
    машин и позволяет перехватывать запросы, когда один из серверов
    выходит из строя.&lt;/li&gt;
&lt;li&gt;Cassandra используется для управления данными между серверами в
    нескольких датацентрах.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Инсталляция Mollom в Netlog обладает некоторыми интересными
    характеристиками. Она обрабатывает больше сообщений, чем основные
    сервера Mollom, но распределение спама в ней совершенно другое, так
    как люди в ней общаются в рамках социальной сети. Внутри Netlog лишь
    10% сообщений является спамом, когда в суровом мире информационных
    порталов распределение обратно. Интересным следствием является тот
    факт, что обработка нормальных сообщений требует меньше
    вычислительных ресурсов, так что на аналогичном оборудовании удается
    обрабатывать больший поток сообщений.&lt;/li&gt;
&lt;li&gt;Изначально они думали о виртуализированных серверах, в частности об
    Amazon EC2, но в итоге обнаружилось, что наиболее узким местом
    являются операции ввода-вывода - низкая производительность дисковой
    подсистемы в виртуальных машинах создавали реальные проблемы, так
    что они решили воспользоваться вертикальным масштабированием и
    переехали на более дорогие физические машины с большим объемом
    дискового пространства:&lt;ul&gt;
&lt;li&gt;На удивление они не упираются в вычислительные ресурсы: лишь два
    ядра из 8 занимаются вычислениями, когда остальные же работают
    над операциями ввода-вывода.&lt;/li&gt;
&lt;li&gt;Трафик Mollom практически постоянен, так что физические сервера
    более эффективны с финансовой точки зрения. Они рассматривают
    Amazon лишь как запасной вариант для обработки непредвиденных
    пиков нагрузки.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Процесс разработки:&lt;ul&gt;
&lt;li&gt;Команда распределена: трое в Бельгии, остальные в Техасе,
    Бостоне и Германии.&lt;/li&gt;
&lt;li&gt;Scrum используется в процессе разработки и они довольны этой
    методологией. Scrum-собрание проходит через Skype в два часа дня
    по Бельгии.&lt;/li&gt;
&lt;li&gt;Разработчики работают локально и отправляют код на Unfuddle.&lt;/li&gt;
&lt;li&gt;Hudson используется для непрерывного интеграционного
    тестирования.&amp;nbsp;Hudson позволил облегчить миграцию, так как перед
    развертыванием все тесты должны быть пройдены. Они не теряли
    лишнего времени на проблемах, обнаруженных уже в развернутом
    приложении.&lt;/li&gt;
&lt;li&gt;Они активно используют автоматическое тестирование: юнит-тесты,
    системные тесты, тесты Drupal.&lt;/li&gt;
&lt;li&gt;Развертывание по-прежнему делается вручную для минимизации риска
    простоя (что правда спорный момент).&lt;/li&gt;
&lt;li&gt;Для обнаружения утечек памяти они используют анализ дампов
    оперативной памяти. Анализ дампа сервера с 16Гб памяти - дело
    непростое, практически невозможное на обычном компьютере, так
    что они арендуют большую виртуальную машину на Amazon для
    проведения анализа. Весь процесс занимает всего около 2 часов.
    Они сравнивают два дампа: через 10 и 20 часов после запуска
    сервера. Если обнаруживаются значительные отличия, то скорее
    всего дело в утечке памяти.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="puti-razvitiia"&gt;Пути развития&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Mollom API основано на XML-RPC, REST-интерфейс находится на стадии
    тестирования для облегчения интеграции других сервисов.&lt;/li&gt;
&lt;li&gt;Они мигрировали на Cassandra, чтобы облегчить процесс
    горизонтального масштабирования, когда нагрузка достигнет
    соответствующего уровня.&lt;/li&gt;
&lt;li&gt;Скоро будут выпущены корпоративные возможности, которые позволят
    работать с сотнями сайтов как с единым целым. Появится возможность
    легко модерировать несколько сайтов одновременно по эмоциональной
    окраске сообщений, рейтингу спама или удалить все сообщения с
    определенного IP-адреса.&lt;/li&gt;
&lt;li&gt;Они думали над участием в бизнесе потоковых данных вроде Twitter, но
    они сильно ограничены европейскими более строгими требованиями по
    приватности.&lt;/li&gt;
&lt;li&gt;Планируются эксперименты по использованию Glassfish для балансировки
    нагрузки в рамках каждого датацентра.&lt;/li&gt;
&lt;li&gt;Если нагрузка увеличится десятикратно им придется добавить больше
    серверов в Cassandra. Дисковый ввод-вывод является узким местом.
    Дополнительные сервера приложения понадобятся только если нагрузка
    вырастет более, чем на порядок.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mollom очень серьезно относится к
    разработке&amp;nbsp;высокопроизводительной системы.&lt;/strong&gt; Они гордятся тем, что
    Mollom очень эффективно использует вычислительные и финансовые
    ресурсы. Множество запросов может обрабатываться одним сервером с
    низкой задержкой, что очень радует как клиентов, так и владельцев
    проекта, так как издержки очень низки. Этот вопрос был выбран
    приоритетным с самого начала и они выбрали подходящие технологии для
    реализации своих целей. Это позволило им вкладывать средства в
    маркетинг, построить базу клиентов и создавать новые продукты на
    основе Mollom.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Машинное обучение требует много исходных данных&amp;nbsp;для успешного
    обнаружение спама.&lt;/strong&gt; Для сбора этих данных предлагает бесплатные
    услуги. Крупные клиенты обеспечивают доход и получают выгоду от
    данных, полученных от более мелких клиентов. Эта модель очень хорошо
    себя проявила в машинном обучении, за которым как известно будущее.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Старайтесь избавиться от проблем, не связанных напрямую с
    продуктом.&lt;/strong&gt; Большие системы требуют серьезных усилий на разработку
    инфраструктуры. Можно убить все время на построение&amp;nbsp;инфраструктуры,
    вместо создания по-настоящему ценного продукта (классификаторов,
    системы репутации, клиентских библиотек). Mollom постоянно пытались
    максимально избавляться от лишних проблем, именно по-этому они
    выбрали Cassandra и Glassfish.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Будьте осторожны с клиентским кодом.&lt;/strong&gt; Выполнение кода на
    клиентской части привлекательно тем, что он тратит чужие ресурсы, а
    не серверные. Проблемы начинаются когда сторонние библиотеки
    разрабатываются некачественно, что заставляет систему в целом
    работать плохо. Плотно работайте с разработчиками клиентских
    библиотек для повышения качества их продукции.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Отдавайте приоритет платящим клиентам.&lt;/strong&gt; Платящие клиенты получают
    более высокое качество услуг, обрабатываются вне очереди, получают
    меньше задержек и получают доступ к запасному серверу когда основной
    дал сбой. Этого вполне достаточно, чтобы подтолкнуть клиентов
    платить.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Уменьшайте объем кода, позволяя используемым сторонним продуктам
    брать на себя грязную работу&lt;/strong&gt;.&amp;nbsp;Поначалу код Mollom был существенно
    большим по объему, чем сейчас. Использование Cassandra и Glassfish
    позволило убрать массу кода, связанного с кэшированием,
    кластеризацией, репликацией и обработкой сбоев. Упрощайте систему со
    временем.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Минимизируйте блокировки.&lt;/strong&gt; Mollom потратили много времени на
    устранение блокировок внутри Glassfish, так как это начинало
    становиться узким местом. Минимизируйте простой от блокировок для
    достижения полного параллелизма.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="istochniki-informatsii-i-dopolnitelnye-materialy"&gt;Источники информации и дополнительные материалы&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/796d6b53/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2011/2/8/mollom-architecture-killing-over-373-million-spams-at-100-re.html"&gt;Mollom Architecture - Killing Over 373 Million Spams At 100 Request
    Per
    Second&lt;/a&gt;
    (основной источник информации)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a695ef0e/" rel="nofollow" target="_blank" title="http://mollom.com/files/mollom-technical-whitepaper.pdf"&gt;Mollom Technical
    Whitepaper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/23d06e29/" rel="nofollow" target="_blank" title="http://blogs.sun.com/glassfishpodcast/entry/episode_072_mollom_com_s"&gt;Episode #072 - Mollom.com's GlassFish backend with Dries and
    Johan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/c1eeadac/" rel="nofollow" target="_blank" title="http://buytaert.net/mollom-gets-a-new-backend"&gt;Mollom gets a new
    backend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/fd87538b/" rel="nofollow" target="_blank" title="http://blogs.lodgon.com/johan/"&gt;Fighting spam with Mollom on
    Glassfish&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/9c3dc3c9/" rel="nofollow" target="_blank" title="http://mollom.com/api"&gt;Mollom API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Если Вам понравилась данная статья, можете ознакомиться с другими
&lt;a href="https://www.insight-it.ru/highload/"&gt;материалами по архитектуре высоконагруженных систем&lt;/a&gt; и &lt;a href="/feed/"&gt;подписаться на RSS&lt;/a&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 15 Feb 2011 19:19:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-02-15:highload/2011/arkhitektura-mollom/</guid><category>Cassandra</category><category>Drupal</category><category>Glassfish</category><category>Hudson</category><category>Intel</category><category>Java</category><category>Mollom</category><category>Munin</category><category>MySQL</category><category>Pingdom</category><category>saas</category><category>SoftLayer</category><category>Unfuddle</category><category>Xeon</category><category>Zendesk</category><category>Архитектура Mollom</category></item></channel></rss>