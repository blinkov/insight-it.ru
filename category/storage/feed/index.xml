<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Insight IT</title><link>https://www.insight-it.ru/</link><description></description><atom:link href="https://www.insight-it.ru/category/storage/feed/index.xml" rel="self"></atom:link><lastBuildDate>Tue, 13 Nov 2012 02:09:00 +0400</lastBuildDate><item><title>Обзор Riak</title><link>https://www.insight-it.ru//storage/2012/obzor-riak/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/243e6801/" rel="nofollow" target="_blank" title="http://basho.com/products/riak-kv/"&gt;Riak&lt;/a&gt; - распределенная
&lt;em&gt;opensource&lt;/em&gt;&amp;nbsp;база данных, разработанная на &lt;a href="/tag/erlang/"&gt;Erlang&lt;/a&gt; и
спроектированная в расчете на:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Высокую доступность и устойчивость к сбоям;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Масштабируемость и простоту обслуживания;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Универсальность.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;У проекта отличная&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/e20af489/" rel="nofollow" target="_blank" title="https://docs.basho.com/riak/latest/"&gt;официальная документация&lt;/a&gt;&amp;nbsp;на английском, далее же в этой статье я расскажу об основных её особенностях чуть подробнее, а также хитростях и подводных камнях, выявленных в процессе применения на практике (с перспективы веб-разработки).&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="vysokaia-dostupnost-i-ustoichivost-k-sboiam"&gt;Высокая доступность и устойчивость к сбоям&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Все данные в кластере&amp;nbsp;&lt;em&gt;реплицируются&lt;/em&gt; по принципу соседей на хэш
    кольце (см. логотип для иллюстрации) и даже в случае сбоев
    &lt;em&gt;доступны&lt;/em&gt; посредством интеллектуального перенаправления запросов
    внутри кластера.&lt;/li&gt;
&lt;li&gt;В случае возникновения коллизий из-за разрыва сетевого соединения
    или просто одновременной записи, на запрос получения данных &lt;em&gt;может
    вернуться несколько версий&lt;/em&gt; и приложение само может решить как их
    объединить или какую версию использовать.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="masshtabiruemost-i-prostota-obsluzhivaniia"&gt;Масштабируемость и простота обслуживания&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Добавление нового сервера тривиально путем копирования конфига и
    одной команды.&lt;/li&gt;
&lt;li&gt;Перераспределение данных и все остальное прозрачно происходит за
    сценой.&lt;/li&gt;
&lt;li&gt;Минимальный рекомендуемый размер Riak кластера - 5 серверов, меньшее
    количество не дает раскрыть весь потенциал.&lt;/li&gt;
&lt;li&gt;Одинаково легко обслуживать как маленький, так и большой кластер.&lt;/li&gt;
&lt;li&gt;Есть коммерческая &lt;em&gt;Enterprise&lt;/em&gt; версия с поддержкой от &lt;strong&gt;Basho&lt;/strong&gt;,
    компании-разработчика Riak (изначально выходцы из Akamai),
    равноправной зашифрованной репликацией между датацентрами и
    поддержкой &lt;em&gt;SNMP&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Есть встроенный веб-интерфейс для мониторинга и управления
    кластером, у меня правда так и не дошли руки его освоить:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="video-container"&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="480" src="//www.youtube.com/embed/R0_PLMCrtZw?rel=0" width="853"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;h2 id="universalnost"&gt;Универсальность&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Схема отсутствует, ключи и данные - произвольные бинарные строки.
    Ключи располагаются в пространствах имен &lt;em&gt;(bucket)&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Сериализация - на усмотрение разработчика, популярные варианты -
    Erlang'овский &lt;em&gt;BERT&lt;/em&gt;, &lt;em&gt;JSON&lt;/em&gt; для других платформ, можно использовать
    просто как &lt;em&gt;файловую систему&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Модульная система хранилищ данных, альтернатив много, основная -
    &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; &lt;a href="/tag/leveldb/"&gt;LevelDB&lt;/a&gt;;&amp;nbsp;еще интересный
    вариант с хранением полностью в оперативной памяти - получается
    продвинутый распределенный кэш с репликацией, поиском и пр.&lt;/li&gt;
&lt;li&gt;Гибко настраиваемое количество узлов кластера, которые должны
    подтвердить успешность операции, чтобы она считалась успешной: можно
    указывать для всего кластера, пространства имен и даже конкретного
    запроса. Riak в любом случае остается eventually consistent базой
    данных (AP из CAP теоремы), но с возможностью управлять балансом
    производительности операций и надежностью выполнения запросов.&lt;/li&gt;
&lt;li&gt;Три интерфейса доступа &lt;em&gt;(API)&lt;/em&gt;:&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/protocol-buffers/"&gt;Google ProtocolBuffers&lt;/a&gt; - для основного
    использования в боевых условиях.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/http/"&gt;HTTP&lt;/a&gt; &lt;a href="/tag/rest/"&gt;REST&lt;/a&gt; - для использования в
    языках, где нет готового клиента на ProtocolBuffers и для того,
    чтобы по-быстрому что-то посмотреть из консоли через curl. Хотя
    по факту клиенты для большинства языков программирования есть и
    проще делать запросы через интерпретатор.&lt;/li&gt;
&lt;li&gt;Еще есть прямой интерфейс Erlang-сообщений, но даже из самого
    Erlang им пользоваться не рекомендуют, не говоря уже о
    реализациях Erlang node (BERT) на других платформах.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Вместе с данными хранятся метаданные для разных целей, которые
    используются в соответствующих типах запросов:&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/3d61cc81/" rel="nofollow" target="_blank" title="http://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D1%8B%D0%B5_%D1%87%D0%B0%D1%81%D1%8B"&gt;Векторные часы&lt;/a&gt;
    для разрешения конфликтов версий данных&amp;nbsp;&lt;em&gt;(обязательно, есть
    автоматическое разрешение);&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Индекс для полнотекстного поиска &lt;em&gt;(концептуально позаимствован у
    &lt;a href="/tag/lucene/"&gt;Lucene&lt;/a&gt;/&lt;a href="/tag/solr/"&gt;Solr&lt;/a&gt;, опционально);&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Индекс для простых выборок &lt;em&gt;(по бинарным и числовым полям,
    опционально);&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Связанные ключи &lt;em&gt;(отдаленный аналог внешних ключей,
    опционально).&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Встроенная поддержка &lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;, фазы можно
    реализовывать на &lt;a href="/tag/erlang/"&gt;Erlang&lt;/a&gt; или
    &lt;a href="/tag/javascript/"&gt;JavaScript&lt;/a&gt;;&amp;nbsp;для обоих языков есть библиотека с
    наиболее популярными случаями, которые можно использовать для
    образца.&lt;/li&gt;
&lt;li&gt;Есть поддержка выполнения операций до/после операций записи/чтения
    &lt;em&gt;(hooks)&lt;/em&gt;, чаще всего используются для построения полнотекстного
    индекса, но можно реализовать и свои, специфичные для приложения.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="nedokumentirovannye-vozmozhnosti"&gt;Недокументированные возможности&lt;/h2&gt;
&lt;p&gt;Пока я их нашел всего две:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Счетчики:&lt;/strong&gt;&amp;nbsp;как такового API в для&amp;nbsp;увеличения/уменьшения числовых
    значений &lt;em&gt;(increment/decrement)&lt;/em&gt; в Riak нет, так как он не лезет
    внутрь хранящихся данных. Зато есть векторные часы, которые растут с
    каждой операцией записи по ключу. Чтобы реализовать увеличение
    &lt;em&gt;(increment)&lt;/em&gt; необходимо записать в Riak пустую бинарную строку с
    опцией &lt;em&gt;return_body,&amp;nbsp;&lt;/em&gt;и у вернувшегося значения сложить все поля в
    векторных часах. &lt;a href="https://www.insight-it.ru/goto/6a894d09/" rel="nofollow" target="_blank" title="https://gist.github.com/4061705"&gt;Пример на
    Erlang&lt;/a&gt;. Если нужно еще и
    уменьшение (decrement) этого можно добиться с помощью пары счетчиков
    "плюс и минус" и вычитать второе значение из первого. Для&amp;nbsp;авто
    инкремента&amp;nbsp;основных ключей не самый лучший вариант, но для не особо
    критичных случаев вполне себе работает.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Выборка по списку ключей &lt;em&gt;(multiget)&lt;/em&gt;:&amp;nbsp;&lt;/strong&gt;такого API тоже нет, но
    здесь на выручку приходит &lt;em&gt;MapReduce&lt;/em&gt;. Это, пожалуй, наиболее
    популярное его применение. На вход подаем имеющийся список ключей и
    используем фазы из готовой библиотеки: &lt;em&gt;reduce_set_union&lt;/em&gt; и
    &lt;em&gt;map_identity&lt;/em&gt;. Данные возвращаются&amp;nbsp;неотсортированные &amp;nbsp;и требуют
    небольшой обертки на выходе, но все равно это намного быстрее, чем
    последовательно проходить по списку ключей и делать для каждого
    обычный &lt;em&gt;get&lt;/em&gt;. &lt;a href="https://www.insight-it.ru/goto/d557f797/" rel="nofollow" target="_blank" title="https://gist.github.com/4061784"&gt;Пример на Erlang&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="card blue lighten-4"&gt;
&lt;p&gt;&lt;div class="card-content"&gt;
Буду рад, если Вы поможете мне дополнить этот список, оставив известные
Вам подобные трюки в комментариях.
&lt;/div&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="podvodnye-kamni"&gt;Подводные камни&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Если в Вашем приложении необходима функциональность &lt;strong&gt;постраничного
    просмотра отсортированных данных&lt;/strong&gt; &lt;em&gt;(pagination)&lt;/em&gt;, то будьте готовы
    реализовать её на клиенте. То есть Riak быстро сделал нужную выборку
    всех "страниц" и уже на клиенте её придется отсортировать и выкинуть
    лишнее. Вообще в большинстве случаев результаты запросов к Riak
    приходят в произвольном порядке из-за его распределенной природы.&lt;/li&gt;
&lt;li&gt;В продолжение к предыдущему: в &lt;a href="https://www.insight-it.ru/goto/1093e454/" rel="nofollow" target="_blank" title="https://docs.basho.com/riak/latest/dev/using/search/"&gt;REST Solr интерфейсе&lt;/a&gt;&amp;nbsp;есть
    аргументы (в ProtoBuf это тоже добавили в одной из последних
    версий), которые, казалось бы, достаточны для реализации
    постраничного просмотра: &lt;strong&gt;sort&lt;/strong&gt;, &lt;strong&gt;start&lt;/strong&gt;, &lt;strong&gt;rows&lt;/strong&gt; - что еще
    нужно? На практике оно работает не так, как было бы логично.
    Сортировка по значению (заданная в sort) применяется ПОСЛЕ того, как
    была отсчитана страница по start и rows. Они отмеряются по ключам
    или рейтингу значения в полнотекстном поиске и никак иначе. С тем же
    успехом эти 5-10 значений можно очень быстро отсортировать и на
    клиенте. Зачем-то это может быть и нужно, но в моем случае оказалось
    совершенно бесполезно.&lt;/li&gt;
&lt;li&gt;У Riak есть 4 основных типа запросов: простой
    get/set,&amp;nbsp;полнотекстовый&amp;nbsp;поиск, вторичные ключи &lt;em&gt;(secondary
    indices)&lt;/em&gt;, МapReduce и проход по связанным ключам &lt;em&gt;(link walking)&lt;/em&gt;.&lt;ul&gt;
&lt;li&gt;Если Ваши данные являются сериализованным JSON, BERT или XML, то
    в большинстве случаев Вам нужны лишь первые два из них,
    исключение - упомянутая выше выборка по списку ключей через
    MapReduce.&lt;/li&gt;
&lt;li&gt;Основной сценарий использования вторичных индексов - метаданные
    к произвольным неструктурированным бинарным данным, например в
    случае с аналогом файловой системы. Либо совсем примитивные
    случаи, когда правда нужно сделать простую выборку по одному
    целочисленному полю, что бывает редко.&lt;/li&gt;
&lt;li&gt;Если данные сериализованы, то связанные ключи проще хранить
    внутри данных, а не средствами СУБД. Разницы в
    производительности нет, в итоге делается тот же MapReduce с теми
    же фазами.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Хоть Riak "из коробки" и правда надежнее многих других СУБД и 1-2
    упавших/отключенных сервера в кластере внешне практически не
    заметны, есть одно но. Если один узел упал - соединения всех
    подключенных к нему клиентов теряются. Два основных
    пути&amp;nbsp;преодоления&amp;nbsp;этого момента:&lt;ul&gt;
&lt;li&gt;Если кластер клиентов и кластер Riak расположены на разных
    серверах, то между ними можно поставить отказоустойчивый TCP
    балансировщик нагрузки, в частности &lt;a href="/tag/haproxy/"&gt;HAProxy&lt;/a&gt; или
    &lt;a href="/tag/ipvs/"&gt;IPVS&lt;/a&gt; здесь наиболее органично вписываются.&lt;/li&gt;
&lt;li&gt;Если на одних и тех же, то есть вариант поставить балансировщик
    нагрузки перед клиентами (для веба возможно и в HTTP/HTTPS
    режиме), а каждый клиент подключается к своему локальному
    серверу Riak и если один, другой или оба сразу упали, то
    отрубать весь физический сервер целиком.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="vyvody"&gt;Выводы&lt;/h2&gt;
&lt;p&gt;Riak отлично подходит для многих вариантов использования, как в Интернет
среде, так и в смежных вроде телекома. Обладает отличным набором
положительных "черт характера", о которых шла речь в начале статьи.
Прекрасно справляется с большим потоком как операций записи, так и
операций чтения.&lt;/p&gt;
&lt;p&gt;Как уже упоминалось, практически единственный сценарий, где Riak совсем
не справляется, это выборки по большим объемам данных с сортировкой и
постраничным выводом. Но даже в этом случае никто не мешает использовать
отдельный сервис, который будет индексировать нужным образом данные и
подготавливать список идентификаторов для последующей multiget выборки
из Riak. К слову, проекты по этой части уже появляются, например
&lt;a href="https://www.insight-it.ru/goto/1232aa05/" rel="nofollow" target="_blank" title="https://github.com/rzezeski/yokozuna"&gt;Yokozuna&lt;/a&gt; - интеграция
полноценного Solr с Riak &lt;em&gt;(Riak Search - лишь частичный порт Solr+Lucene
на Erlang)&lt;/em&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 13 Nov 2012 02:09:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-11-13:storage/2012/obzor-riak/</guid><category>Basho</category><category>Erlang</category><category>LevelDB</category><category>Protocol Buffers</category><category>REST</category><category>Riak</category><category>БД</category><category>обзор</category><category>СУБД</category></item><item><title>HBase в Facebook: 135 миллиардов сообщений в месяц</title><link>https://www.insight-it.ru//storage/2011/hbase-v-facebook-135-milliardov-soobshhenijj-v-mesyac/</link><description>&lt;p&gt;С тех пор, как я написал пост про &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-facebook/"&gt;Архитектуру Facebook&lt;/a&gt;, я как-то перестал
активно следить за развитием событий и, как оказалось, зря. &amp;nbsp;В
&lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt; ввели новый функционал &lt;a href="https://www.insight-it.ru/goto/6068befc/" rel="nofollow" target="_blank" title="http://blog.facebook.com/blog.php?post=452288242130"&gt;"социального почтового ящика"&lt;/a&gt;,
агрегирующий входящие сообщения из электронной почты, мессенджеров, SMS
и сообщений на сайте Facebook. Изначально они разрабатывали
&lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt; именно для использования в этом проекте, но
в итоге этот пост заняла достаточно противоречивая технология:
&lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt;. HBase одержала вверх над Cassandra, MySQL и
многими другими решениями. Как так получилось?&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;Используемая в Cassandra логика целостности данных оказалась не
достаточно строгой для использования в этом продукте. В Facebook широко
используется &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;, но производительность существенно
снижается с ростом массива данных и увеличением размеров индексов.
Возможно они взялись бы за разработку нового решения для этой задачи, но
их выбор пал на HBase.&lt;/p&gt;
&lt;p&gt;HBase представляет собой горизонтально масштабируемую систему хранения
таблиц, поддерживающую высокую частоту обновления строк в массивных
наборах данных. Звучит как то что надо, для построения новой системы
сообщений в Facebook. В основе модели данных HBase лежит концепция
BigTable от &lt;a href="/tag/google/"&gt;Google&lt;/a&gt;, которая хорошо подходит для поиска
строк по идентификаторам, фильтрации и сканированию наборов строк. Из
слабых сторон можно назвать отсутствию поддержки сложных запросов, но
этот факт компенсируется широким спектром инструментов по аналитике, в
том числе и &lt;a href="/tag/hive/"&gt;Hive&lt;/a&gt;, разработанном в самом Facebook для
работы с их многопетабайтным хранилищем данных.&amp;nbsp;Помимо прочего HBase
основана на &lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; и
&lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt;, с которыми Facebook и так активно работает для
анализа данных.&lt;/p&gt;
&lt;p&gt;Это решение было принято не на пустом месте, а так как они &lt;em&gt;отслеживали&lt;/em&gt;
как используется данный функционал и пришли к выводу, что это то, что им
нужно. Им нужна была система, справляющаяся с двумя типичными
ситуациями:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Небольшой нестабильный набор временных данных&lt;/li&gt;
&lt;li&gt;Постоянно растущий архив информации, который очень редко
    используется&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Звучит правдоподобно. Обычно входящие сообщения читаются один раз, и
очень редко к ним возвращаются снова. Напрашивается использование двух
разных систем для каждого случая, но на практике оказалось, что HBase
отлично справляется с обоими. Полнотекстный поиск же, скорее всего,
переложен на одну из сторонних систем вроде Lucene.&lt;/p&gt;
&lt;p&gt;HBase:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;имеет более подходящую модель консистентности, чем Cassandra;&lt;/li&gt;
&lt;li&gt;отлично масштабируется и показывает неплохую производительность при
    паттернах использования в FB;&lt;/li&gt;
&lt;li&gt;имеет ряд преимуществ благодаря использованию HDFS для хранения
    данных: репликация, проверка целостности, автоматическая
    перебалансировка;&lt;/li&gt;
&lt;li&gt;легко поддерживать в Facebook, так как их системные администраторы
    уже имеют большой опыт со смежными проектами - Hadoop и HDFS.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Для хранения прикрепленных файлов используется Haystack, их собственная
система, изначально разработанная для хранения изображений.&lt;/p&gt;
&lt;p&gt;Для сбора сообщений из различных источников используется собственный
сервер приложений.&lt;/p&gt;
&lt;p&gt;Поиск новых пользователей основан на &lt;a href="/tag/zookeper/"&gt;ZooKeeper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Продукт тесно с другими сервисами Facebook для:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Проверка адресов электронной почты&lt;/li&gt;
&lt;li&gt;Определения отношений дружбы&lt;/li&gt;
&lt;li&gt;Настроек приватности&lt;/li&gt;
&lt;li&gt;Решений о транспорте для отправки сообщения (e-mail, SMS, внутреннее
    сообщение)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Использование смежных проектов Hadoop и Hive стало одним из ключевых
факторов, повлиявших на то, что эта технология прижилась как часть
экосистемы в таком крупном проекте, как Facebook. Это идеальный сценарий
для практически любого продукта: стать партнером успешного популярного
продукта в надежде на то, что пользователь воспользуется обоими за
компанию - именно по такому пути развивается HBase.&lt;/p&gt;
&lt;p&gt;Хочется добавить пару слов от себя: я имел довольно приличный опыт
работы с HBase в (уже) далеком 2008 году, на тот момент HBase был самым
нестабильным из всех проектов, составляющих экосистему Hadoop. На бумаге
HBase и правда выглядит идеальным решением для многих задач, но малейший
сбой в метаданных делал всю базу данных неработоспособной, а таковое
случалось достаточно часто, обычно по вине HDFS. В том проекте было
убито массу времени на попытки "нормализовать" работу связки
Hadoop+HBase, но в итоге от последней пришлось отказаться. Очень рад,
что этот проект развивается такими семимильными шагами, задумка у
проекта изначально была и правда очень стоящая.&amp;nbsp;HBase буквально за пару
лет стал пригоден для production использования, да еще и на таком
масштабе. &amp;nbsp;Пройдет еще год-другой, кардинально изменится архитектура
Hadoop в лучшую сторону, и HBase наверняка станет лучшей из
распределенных систем хранения структурированных данных, доступных на
рынке. Если, конечно, Google к тому времени не успеет опубликовать в
opensource её прародителя, BigTable :)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;А Вы как относитесь к HBase? Использовали ли на практике? Какие
впечатления?&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/239ce3b6/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2010/11/16/facebooks-new-real-time-messaging-system-hbase-to-store-135.html"&gt;Facebook's New Real-Time Messaging System: HBase To Store 135+ Billion Messages A&amp;nbsp;Month&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/9b50927a/" rel="nofollow" target="_blank" title="http://www.facebook.com/notes/facebook-engineering/the-underlying-technology-of-messages/454991608919"&gt;The Underlying Technology of Messages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/6068befc/" rel="nofollow" target="_blank" title="http://blog.facebook.com/blog.php?post=452288242130"&gt;See The Messages That Matter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8f497448/" rel="nofollow" target="_blank" title="http://facility9.com/2010/11/18/facebook-messaging-hbase-comes-of-age"&gt;Facebook Messaging - HBase Comes of Age&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 10 Mar 2011 20:49:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-03-10:storage/2011/hbase-v-facebook-135-milliardov-soobshhenijj-v-mesyac/</guid><category>Facebook</category><category>HBase</category></item><item><title>Google Megastore</title><link>https://www.insight-it.ru//storage/2011/google-megastore/</link><description>&lt;p&gt;Гигантский шаг в сторону распределенного будущего был предпринят
командой &lt;a href="https://www.insight-it.ru/goto/536fbea0/" rel="nofollow" target="_blank" title="http://www.appspot.com"&gt;Google App Engine&lt;/a&gt; в момент их релиза
системы хранения данных с повышенным уровнем репликации. Она направленна
на критичные для бизнеса приложения, которые требуют расположения копий
данных как минимум в трех датацентрах, полной семантики ACID для групп
сущностей и ограниченных гарантий консистентности между группами
сущностей.
&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Это было большим достижением, ведь всего несколько компаний во всем мире
способны на реализацию по-настоящему меж-датацентровой системы хранения
данных. Помимо SimpleDB, как много других публично-доступных сервисов
баз данных могут хранить информацию в нескольких датацентрах
одновременно? Теперь эта возможность доступна каждому. Но всему есть
цена: так как Megastore использует втрое больше ресурсов, чем обычное
Master-Slave хранилище в GAE, стоимость так же увеличивается в три раза.
Помимо этого стоит учитывать, что с ростом надежности и издержек,
понижается производительность. Именно из-за этого, новая система
хранения является альтернативой обычному хранилищу GAE для критичных
задач, а не полной заменой.&lt;/p&gt;
&lt;h2 id="osnovnye-osobennosti-megastore"&gt;Основные особенности Megastore&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Megastore совмещает масштабируемость NoSQL систем хранения данных с
    удобством традиционных СУБД. Оно использовалось для внутренних
    проектов Google на протяжении нескольких лет. Более 100 приложений,
    3 миллиардов транзакций на запись и 20 миллиардов на чтение, более
    петабайта данных распределены по множеству датацентров по всему
    миру.&lt;/li&gt;
&lt;li&gt;Megastore - хранилище, разработанное для удовлетворения требований
    современных интерактивных онлайн сервисов. Используется синхронная
    репликация для достижения высокого уровня доступности и
    консистентности. Вкратце, оно предоставляет полную ACID семантику
    для удаленных реплик с достаточно низкой задержкой, чтобы
    использоваться в интерактивных приложениях. Хранилище
    партиционируется и каждая часть реплицируется отдельно, позволяя
    достичь полного соответствия ACID внутри партиции, но
    консистентность между ними гарантируется лишь ограниченно.
    Предоставляются некоторый функционал традиционных СУБД, такой как
    вторичные индексы, но только те из них, которые масштабируются без
    сильного негативного влияния на задержки и которые укладываются в
    семантику используемой схемы партиционирования.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/9003689b/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/paxos_made_live.html"&gt;Paxos&lt;/a&gt;
    используется для управлением синхронной репликацией между
    датацентрами. Это позволяет достичь очень высокого уровня надежности
    ценой повышения времени выполнения операций записи. Обычно Paxos
    используется только для координации, но в Megastore он также
    используются и для управления записью данных.&lt;/li&gt;
&lt;li&gt;Поддерживается три уровня консистентности при чтении: текущий,
    снимок и неконсистентный.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/e52d8d3e/" rel="nofollow" target="_blank" title="http://code.google.com/appengine/docs/python/datastore/entities.html"&gt;Группы сущностей&lt;/a&gt;
    являются&amp;nbsp;единицей&amp;nbsp;консистентности и транзакционности. Они
    рассматриваются как маленькие независимые базы данных. Сами же
    данные внутри каждого датацентра хранятся в масштабируемом NoSQL
    хранилище.&lt;/li&gt;
&lt;li&gt;Megastore, как и обычное хранилище в GAE, не поддерживает транзакции
    с использованием нескольких групп сущностей, это существенно
    повысило бы время их выполнения.&lt;/li&gt;
&lt;li&gt;Группы сущностей - основной механизм группировки данных для быстрого
    осуществления операций. Их размер и композиция должны быть
    сбалансированы. В каждом приложении должен найтись способ
    естественным образом очертить границы групп сущностей. При
    оптимальном выборе групп сущностей ресурсоемкие кросс-групповые
    операции будут сведены к минимуму. По сути этот процесс чем-то
    напоминает нормализацию в реляционных СУБД.&lt;/li&gt;
&lt;li&gt;Запросы с высокими требованиями по консистентности должны быть
    ограничены одной группой сущностей. Кросс-групповые запросу могут
    вернуть устаревшие результаты. Это является серьезным отличием в
    поведении от обычного хранилища GAE, где по-умолчанию используется
    высокий уровень консистентности для всех запросов, так как операции
    записи и чтения по-умолчанию происходят с мастера.&lt;/li&gt;
&lt;li&gt;В обычном хранилище GAE иногда отключается в связи с
    запланированными техническими работами, а также вовремя
    непредвиденных проблем с инфраструктурой. Megastore в большинстве
    случаев не страдает этими проблемами.&lt;/li&gt;
&lt;li&gt;Резервирование данных и избыточность достигаются посредством
    синхронной репликации, снимков и инкрементального лога транзакций.&lt;/li&gt;
&lt;li&gt;API для доступа к данным остался прежним.&lt;/li&gt;
&lt;li&gt;Операции записи могут достигать секунды для каждой группы сущностей,
    так что для приложений с высокой нагрузкой на запись оно подходит не
    так хорошо.&lt;/li&gt;
&lt;li&gt;Только новые приложения могут воспользоваться опцией Megastore,
    существующие приложения необходимо пересоздать, чтобы использовать
    эту возможность. Впоследствии изменить тип хранилища невозможно.&lt;/li&gt;
&lt;li&gt;Одно приложение не может использовать одновременно обычное хранилище
    и Megastore. Напрашивается использование одного приложения с Google
    Megastore для критически важных данных, а другое приложение с
    обычным хранилищем для всего остального, но такая схема противоречит
    правилам использования сервиса.&lt;/li&gt;
&lt;li&gt;Автоматической миграции данных между Master/Slave хранилищем и
    Megastore не существует, разработчики приложения должны сами
    позаботиться об этом. Google предоставляют лишь набор инструментов и
    примеров кода, чтобы облегчить процесс миграции.&lt;/li&gt;
&lt;li&gt;В приложениях, использующих Megastore, еще большее &amp;nbsp;значение
    приобретает эффективное кэширование данных.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="materialy-po-teme"&gt;Материалы по теме&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7b4d766b/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2011/1/11/google-megastore-3-billion-writes-and-20-billion-read-transa.html"&gt;Google Megastore - 3 Billion Writes And 20 Billion Read
    Transactions
    Daily&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/aceb53b5/" rel="nofollow" target="_blank" title="http://www.cidrdb.org/cidr2011/Papers/CIDR11_Paper32.pdf"&gt;Megastore: Providing Scalable, Highly Available Storage for
    Interactive
    Services&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/79ab1d81/" rel="nofollow" target="_blank" title="https://groups.google.com/group/google-appengine-downtime-notify/msg/e9414ee6493da6fb?pli=1"&gt;May 25th Datastore Outage
    Post-mortem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/9003689b/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/paxos_made_live.html"&gt;Paxos Made Live &amp;ndash; An Engineering
    Perspective&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/3e2c1159/" rel="nofollow" target="_blank" title="http://code.google.com/appengine/docs/python/datastore/hr/"&gt;Choosing a
    Datastore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a24a0860/" rel="nofollow" target="_blank" title="http://code.google.com/appengine/docs/python/datastore/hr/overview.html"&gt;Using the High Replication
    Datastore&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8667b351/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/bigtable.html"&gt;Bigtable: A Distributed Storage System for Structured
    Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/28e767f5/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2009/8/24/how-google-serves-data-from-multiple-datacenters.html"&gt;How Google Serves Data From Multiple
    Datacenters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/6b995f76/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2009/3/10/paper-consensus-protocols-paxos.html"&gt;Consensus Protocols:
    Paxos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/83657c8b/" rel="nofollow" target="_blank" title="http://groups.google.com/group/google-appengine/browse_thread/thread/5fc3b6a4366de62f/4b4d23e924b7b136?lnk=gst&amp;amp;q=High+Replication+Datastore+for+App+Engine#"&gt;Performance
    comparison&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr/&gt;
&lt;p&gt;Еще не все возможности Google Megastore полностью доступны пользователям
App Engine в виде High Replication Storage, но я думаю это вопрос
времени. Хотелось бы пообсуждать в комментариях области применения
новинки на практике: какие приложения, критичные к доступности и
сохранности данных, можно позволить себе отдать в PaaS, пускай даже от
Google?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;P.S.: По традиции хочу напомнить, что читать Insight IT удобнее всего
&lt;a href="/feed/"&gt;через RSS-reader&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 22 Feb 2011 20:18:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-02-22:storage/2011/google-megastore/</guid><category>ACID</category><category>Google</category><category>google app engine</category><category>Google Megastore</category><category>консистентность</category><category>репликация</category></item><item><title>Новое поколение MapReduce в Apache Hadoop</title><link>https://www.insight-it.ru//storage/2011/novoe-pokolenie-mapreduce-v-apache-hadoop/</link><description>&lt;p&gt;В большом бизнесе использование нескольких больших кластеров с
финансовой точки зрения более&amp;nbsp;эффективно, чем много маленьких. Чем
больше машин в кластере, тем большими наборами данных он может
оперировать, больше задач могут выполняться одновременно. Реализация
&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt; в &lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt;
&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; столкнулась с потолком масштабируемости на уровне
около 4000 машин в кластере. Разрабатывается следующее поколение Apaсhe
Hadoop MapReduce, &amp;nbsp;в котором появится общий планировщик ресурсов и
отдельный мастер для каждой отдельной задач, управляющий выполнением
программного кода. Так как простой оборудования по техническим причинам
обходится дорого на таком масштабе, высокий уровень доступности
проектируется с самого начала, ровно как и безопасность и
многозадачность, необходимые для поддержки одновременного использования
большого кластера многими пользователями. Новая архитектура также будет
более инновационной, гибкой и эффективной с точки зрения использования
вычислительных ресурсов.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="predistoriia"&gt;Предистория&lt;/h2&gt;
&lt;p&gt;Текущая реализация Hadoop MapReduce устаревает на глазах. Основываясь на
текущих тенденциях в размерах кластеров и нагрузок на них, JobTracker
требует кардинальных доработок, чтобы исправить его дефекты в области
масштабируемости, потребления памяти, многопоточности, надежности и
производительности. С точки зрения работы с Hadoop при каждом обновлении
кластера (даже если это просто багфикс), абсолютно все компоненты
кластера, так и приложений, которые на нем работают, должны быть
обновлены одновременно. Это так же очень неудобно, так как каждый раз
необходимо тестировать все приложения на совместимость с новой версией.&lt;/p&gt;
&lt;h2 id="trebovaniia"&gt;Требования&lt;/h2&gt;
&lt;p&gt;Прежде чем кардинально что-то менять в Hadoop mapreduce, необходимо
понять какие же основные требования предъявляются к вычислительным
кластерам на практике. Наиболее значительными требованиями к Hadoop
следующего поколения являются:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Надежность&lt;/li&gt;
&lt;li&gt;Доступность&lt;/li&gt;
&lt;li&gt;Масштабируемость - кластеры из как минимум 10 тысяч машин, 200 тысяч
    вычислительных ядер и даже больше&lt;/li&gt;
&lt;li&gt;Обратная и прямая совместимость - возможность быть уверенным, что
    приложение будет работать на новой версии так же, как оно работало
    на старой&lt;/li&gt;
&lt;li&gt;Контроль над обновлениями&lt;/li&gt;
&lt;li&gt;Предсказуемые задержки&lt;/li&gt;
&lt;li&gt;Эффективное использование ресурсов&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Среди менее значительных требований:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Поддержка альтернативных парадигм разработки (помимо MapReduce)&lt;/li&gt;
&lt;li&gt;Поддержка сервисов с коротким жизненным циклом&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Если учесть перечисленные выше требования, то становится очевидно, что
инфраструктура обработки данных в Hadoop должна быть кардинальным
образом изменена. В сообществе Hadoop люди в целом приходят к общему
мнению, что текущая архитектура MapReduce не способна решить текущие
задачи, которые перед ней ставится, и что требуется кардинальный
рефакторинг кодовой базы.&lt;/p&gt;
&lt;h2 id="mapreduce-sleduiushchego-pokoleniia"&gt;MapReduce следующего поколения&lt;/h2&gt;
&lt;p&gt;Фундаментальной идеей смены архитектуры является разделение двух
основных функций JobTracker'а на два отдельных части:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;управление ресурсами;&lt;/li&gt;
&lt;li&gt;планирования и мониторинга задач.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В итоге появляется несколько новых ролей:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ResourceManager&lt;/strong&gt; управляет глобальным распределением
    вычислительных ресурсов между приложениями;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplicationMaster&lt;/strong&gt; управляет планированием и координацией внутри
    приложения;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NodeManager&lt;/strong&gt; управляет процессами в рамках одной машины.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ApplicationMaster представляет собой библиотеку, с помощью которой можно
получить у ResourceManager квоту на вычислительные ресурсы и работать с
NodeManager(ами) для выполнения и мониторинга задач.&lt;/p&gt;
&lt;p&gt;ResourceManager поддерживает иерархическим очереди приложений, которым
может гарантированно выделяться некоторый процент ресурсов кластера. Его
функционал ограничивается планированием, никакого мониторинга и
отслеживания задач не происходит, а также нет никаких гарантий
перезапуска задач, провалившихся из-за проблем с оборудованием или
кодом. Планирование основывается на требованиях, которые выставляет
приложение с помощью ряда запросов ресурсов (среди них: запросы на
вычислительные ресурсы, память, дисковое пространство, сетевой доступ и
т.п.). Обратите внимание, что это значительное изменение по сравнению с
текущей моделью слотов фиксированного размера, которая является одной из
основных причин неэффективного использования ресурсов кластера на данный
момент.&lt;/p&gt;
&lt;p&gt;NodeManager - это агент, который работает на каждой машине и несет
ответственность за запуск контейнеров приложений, мониторинг
используемых ими ресурсов (плюс отчет планировщику).&lt;/p&gt;
&lt;p&gt;По одному ApplicationMaster запускается для каждого приложения, они
ответственны за запрос необходимых ресурсов у планировщика, запуск
задач, отслеживание статусов, мониторинг прогресса и обработку сбоев.&lt;/p&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Следующее поколение MapReduce" class="responsive-img" src="https://www.insight-it.ru/images/mapreduce-nextgen.jpg" title="Следующее поколение MapReduce"/&gt;&lt;/p&gt;
&lt;h2 id="uluchsheniia-po-sravneniiu-s-tekushchei-realizatsiei-mapreduce"&gt;Улучшения по сравнению с текущей реализацией MapReduce&lt;/h2&gt;
&lt;h3 id="masshtabiruemost"&gt;Масштабируемость&lt;/h3&gt;
&lt;p&gt;Разделение управления ресурсами и прикладными задачами позволяет
горизонтально расширять кластер более просто и эффективно. JobTracker
проводит значительную часть времени пытаясь управлять жизненным циклом
каждого приложения, что часто может приводить к различным
происшествиям - переход к отдельному менеджеру для каждого приложения
является значительным шагом вперед.&lt;/p&gt;
&lt;p&gt;Масштабируемость особенно важна в свете текущих трендов в оборудовании -
на данный момент Hadoop может быть развернут на кластере из 4000 машин.
Но 4000 средних машин 2009го года (т.е. по 8 ядер, 16Гб памяти, 4Тб
дискового пространства) только вдвое менее &amp;nbsp;ресурсоемки, чем 4000 машин
2011го года (16 ядер, 48гб памяти, 24Тб дискового пространства). Помимо
этого с точки зрения операционных издержек было выгоднее работать в еще
больших кластере от 6000 машин и выше.&lt;/p&gt;
&lt;h3 id="dostupnost"&gt;Доступность&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ResourceManager использует&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/e7095d3/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/zookeeper/"&gt;Apache ZooKeeper&lt;/a&gt; для обработки сбоев.
    Когда ResourceManager перестает работать, аналогичный процесс может
    быстро запуститься на другой машине благодаря тому, что состояние
    кластера было сохранено в ZooKeeper. При таком сценарии все
    запланированные и выполняющиеся приложения максимум лишь
    перезапустятся.&lt;/li&gt;
&lt;li&gt;ApplicationMaster - поддерживается создание точек восстановления на
    уровне приложений. ApplicationMaster может восстановить работу из
    состояния, сохраненного в HDFS, в случае сбоя.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="sovmestimost-protokola"&gt;Совместимость протокола&lt;/h3&gt;
&lt;p&gt;Это позволит различным версиям клиентов и серверов Hadoop общаться между
собой. Помимо решения многих существующих проблем с обновлением, в
будующих релизах появится возможность последовательного обновления кода
без простоя системы в целом - очень большое достижения с точки зрения
системного администрирования.&lt;/p&gt;
&lt;h3 id="innovatsionnost-i-gibkost"&gt;Инновационность и гибкость&lt;/h3&gt;
&lt;p&gt;Основным плюсом предложенной архитектуры является тот факт, что
MapReduce по сути становится просто пользовательской библиотекой.
Вычислительная же система (ResourceManager и NodeManager) становятся
полностью независимыми от специфики MapReduce.&lt;/p&gt;
&lt;p&gt;Клиенты получат возможность одновременного использования разных версий
MapReduce в одном и том же кластере. Это становится тривиальным, так как
отдельная копия ApplicationMaster'а запускается для каждого приложения.
Это дает гибкость в исправлении багов, улучшений и новых возможностей,
так как полное обновление кластер перестает быть обязательной
процедурой. Это позволяет клиентам обновлять их приложения до новых
версий MapReduce вне зависимости от обновлений кластера.&lt;/p&gt;
&lt;h3 id="effektivnost-ispolzovaniia-vychislitelnykh-resursov"&gt;Эффективность использования вычислительных ресурсов&lt;/h3&gt;
&lt;p&gt;ResourceManager использует общую концепцию для управления ресурсами и
планирования по отношению к каждому конкретному приложению. Каждая
машина в кластере на концептуальном уровне рассматривается просто как
набор ресурсов: память, процессор, ввод-вывод и др. Все машины
взаимозаменяемы и приложение может быть назначено на любую из них,
основываясь на доступных и запрашиваемых ресурсах. При этом приложения
работают в контейнерах, изолированно от других приложений, что дает
сильную поддержку многозадачности.&lt;/p&gt;
&lt;p&gt;Таким образом эта схема избавляет от текущего механизма map и reduce
слотов в Hadoop, который негативно влияет на эффективную утилизацию
вычислительных ресурсов.&lt;/p&gt;
&lt;h3 id="podderzhka-drugikh-paradigm-programmirovaniia-pomimo-mapreduce"&gt;Поддержка других парадигм программирования помимо MapReduce&lt;/h3&gt;
&lt;p&gt;В предложенной архитектуре используется общий механизм вычислений, не
привязанный конкретно к MapReduce, что позволит использовать и другие
парадигмы. Имеется возможность реализовать собственный
ApplicationMaster, способный запрашивать ресурсы у ResourceManager и
использовать их в соответствии с задачей, при этом сохраняются общие
принципы изоляции и гарантированного наличия полученных ресурсов. Среди
потенциально поддерживаемых парадигм можно назвать MapReduce, MPI,
Мaster-Worker, итеративные модели. Все они могут одновременно работать
на одном и том же кластере. Это особенно актуально для приложений
(например К-средний или Page Rank), где &lt;a href="https://www.insight-it.ru/python/2011/piccolo-postroenie-raspredelennykh-sistem-v-11-raz-bystree-hadoop/"&gt;другие подходы более чем на порядок эффективнее&lt;/a&gt; MapReduce.&lt;/p&gt;
&lt;h2 id="vyvody_1"&gt;Выводы&lt;/h2&gt;
&lt;p&gt;Apache Hadoop, и в частности Hadoop MapReduce - очень успешный
opensource проект по обработке больших объемов данных. Предложенный
Yahoo путь его переработки направлен на исправление недостатков
архитектуры текущей реализации, при этом повышая доступность,
эффективность использования ресурсов и предоставляя поддержку других
парадигм распределенных вычислений.&lt;/p&gt;
&lt;p&gt;Осталось дело за малым - собственно реализовать задуманное! :)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/336fc03c/" rel="nofollow" target="_blank" title="http://developer.yahoo.com/blogs/hadoop/posts/2011/02/mapreduce-nextgen/"&gt;Источник информации&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="/feed/"&gt;Подписаться на RSS можно здесь.&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 19 Feb 2011 21:23:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-02-19:storage/2011/novoe-pokolenie-mapreduce-v-apache-hadoop/</guid><category>Apache</category><category>Apache Hadoop</category><category>Hadoop</category><category>архитектура</category><category>кластер</category><category>кластеризация</category><category>кластерные вычисления</category><category>Масштабируемость</category><category>разработка</category><category>технологии</category></item><item><title>Sun Unified Storage</title><link>https://www.insight-it.ru//storage/2010/sun-unified-storage/</link><description>&lt;p&gt;По работе мне доводилось активно "иметь дело" с железкой от Sun под
названием &lt;a href="https://www.insight-it.ru/goto/683a31cf/" rel="nofollow" target="_blank" title="http://www.sun.com/storage/disk_systems/unified_storage/7410/"&gt;Sun Unified Storage 7410&lt;/a&gt;.
Представляет собой достаточно мощную систему хранения данных с
установленным Solaris, но доступом и управлением исключительно через
веб-интерфейс. Основной "фишкой" системы является модульность: дисковый
массив наращивается подключаемыми внешне дисковыми модулями по примерно
20-50ТБ, сетевой интерфейс также модульный - на выбор начиная от
нескольких обычных Ethernet по 1GBps и заканчивая оптоволокном, CX4 или
InfiniBand. Две таких машины можно легко объединить в одну
виртуальную&amp;nbsp;для повышения надежности доступа к данным, подключив к ним
общий дисковый массив. RAID используется софтверный средствами ZFS,
вполне стандартный набор опций из зеркалирования, stripe, RAID5/6 и их
комбинаций.&lt;/p&gt;
&lt;p&gt;С точки зрения производительности тоже достаточно интересная штука: при
подключении через 4x 1GBps Ethernet (с использованием LACP, но это тема
для отдельного поста) определенно упирается в сеть, но все равно отлично
подходит для использования в решении многих прикладных задач. Из
интересных опций можно отметить прозрачное использование нескольких
SSD-дисков в каждом дисковом массиве в роли кэша.&lt;/p&gt;
&lt;p&gt;Все функции системы абсолютно прозрачны и настраиваются в несколько
кликов через веб-интерфейс, командная строка хоть при желании и
доступна, но практически не нужна. Там же можно увидеть статистику
использования подсистем и прочую полезную информацию. В целом отличная
система хранения данных: простая, надежная, быстрая, удобная,
вместительная и масштабируемая, правда с одним большим НО - цена просто
зашкаливает, прицениться можно, сходив по ссылке в начале записи, но
вообще есть и более дешевые модели в этой серии.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;К чему я это&amp;nbsp;все вспомнил?&lt;/em&gt; На почту пришел очередной рекламный буклет
от Sun с &lt;a href="https://www.insight-it.ru/goto/78e197bb/" rel="nofollow" target="_blank" title="https://dct.sun.com/dct/forms/reg_us_1308_670_0.jsp"&gt;предложением попробовать Sun Unified Storage в виртуальной машине VirtualBox или VMWare&lt;/a&gt;, сам еще не
установил - времени не нашлось, но возможно Вам покажется интересным.
Конечно это не совсем то же самое, что и физическая железка -
производительность дисковых и сетевых подсиситем не померять, но
веб-интерфейс заценить можно.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 13 Jan 2010 20:34:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-01-13:storage/2010/sun-unified-storage/</guid><category>7410</category><category>Solaris</category><category>Sun</category><category>Sun Unified Storage</category><category>VirtualBox</category><category>VMWare</category></item><item><title>Terrastore</title><link>https://www.insight-it.ru//storage/2010/terrastore/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/76e0ccd6/" rel="nofollow" target="_blank" title="http://code.google.com/p/terrastore/"&gt;Terrastore&lt;/a&gt; является
свежеиспеченной системой хранения документов, с отличными возможностями
по масштабируемости и эластичной настройке, при этом без жертв со
стороны консистентности данных.&lt;/p&gt;
&lt;p&gt;Вместо подробного описания несколько ключевых характеристик продукта:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Легкодоступность:&lt;/strong&gt; данные доступны посредством повсеместно
    используемого протокола HTTP.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Распреденность:&lt;/strong&gt; узлы могут работать и существовать на любых
    доступных серверах.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Эластичность:&lt;/strong&gt; имеется возможность динамического добавления и
    удаления узлов кластера на лету, без малейшего простоя системы и
    каких-либо изменений в конфигурации.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Масштабируемость на уровне данных:&lt;/strong&gt; документы разбиваются на
    группы и распределяются между доступными узлами с автоматической
    прозрачной балансировкой, в том числе и при добавлении и исключении
    узлов в кластере.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Масштабируемость на вычислительном уровне:&lt;/strong&gt; запросы и обновление
    данных распределяются по узлам, которые физически хранят
    используемые данные, тем самым минимизируется трафик и
    распределяется вычислительная нагрузка.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Консистентность:&lt;/strong&gt; система обеспечивает по-документную
    консистентность данных, таким образом гарантируя тот факт, что
    пользователь всегда получает самую свежую версию документа,
    обеспечивая изоляцию для параллельных модификаций документов.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Отсутствие схемы:&lt;/strong&gt; предоставляет JSON интерфейс, основанный на
    коллекциях; пользователям предоставляется возможность просто создать
    свою коллекцию и положить туда что угодно.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Простота в работе:&lt;/strong&gt; установка полностью работоспособного кластера
    заключается в вводе всего нескольких команд и не требует какого-либо
    редактирование XML-конфигов.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Богатый функционал:&lt;/strong&gt; поддерживаются push-down предикаты, запросы
    по диапазонам и серверные функции обновления.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Если Вам показалось интересным, у Вас есть возможность &lt;a href="https://www.insight-it.ru/goto/76e0ccd6/" rel="nofollow" target="_blank" title="http://code.google.com/p/terrastore/"&gt;получить более подробную информацию&lt;/a&gt;,&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/ff8d489f/" rel="nofollow" target="_blank" title="http://groups.google.com/group/terrastore-discussions"&gt;принять участие в проекте&lt;/a&gt;,&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/b14ec8b0/" rel="nofollow" target="_blank" title="http://code.google.com/p/terrastore/downloads/list"&gt;скачать дистрибутив&lt;/a&gt;
или&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/caed0d40/" rel="nofollow" target="_blank" title="http://code.google.com/p/terrastore/source/createClone"&gt;получить копию исходного кода&lt;/a&gt;!
﻿&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;В очередной раз спасибо &lt;a href="https://www.insight-it.ru/goto/7b0eba82/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2009/12/30/terrastore-scalable-elastic-consistent-document-store.html"&gt;highscalability.com за источник
информации&lt;/a&gt;,
за одно хотелось бы услышать мнения о таком формате постов. Я тут уже
почти неделю копаюсь над постом-долгостроем про Baidu, а такой можно
сочинить за полчаса.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Кстати про &lt;a href="https://www.insight-it.ru/goto/65f34522/" rel="nofollow" target="_blank" title="http://www.terracotta.org"&gt;Terracotta&lt;/a&gt;, на основе
которой работает данный продукт, тоже давно пора было уже написать, в
ближайшее время займусь :)&lt;/em&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 07 Jan 2010 01:22:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-01-07:storage/2010/terrastore/</guid><category>HTTP</category><category>Terracotta</category><category>Terrastore</category><category>документы</category><category>Масштабируемость</category><category>хранение</category></item><item><title>memcached на пальцах</title><link>https://www.insight-it.ru//storage/2009/memcached-na-palcakh/</link><description>&lt;p&gt;Ранее уже была сделана публикация с &lt;a href="https://www.insight-it.ru/storage/2008/obzor-memcached/"&gt;обзором memcached&lt;/a&gt;. Давайте вернемся к данной теме и рассмотрим практику работы с memcached на примерах.
&lt;!--more--&gt;&lt;/p&gt;
&lt;div class="card blue lighten-4"&gt;
&lt;p&gt;&lt;div class="card-content justify"&gt;
К сожалению, у меня по прежнему не доходят руки активно заниматься
блогом, но наконец-то появился появился первый человек, откликнувшийся
на &lt;a href="https://www.insight-it.ru/guest-posts/"&gt;мое предложение стать гостевым автором данного блога&lt;/a&gt;.
Его имя &lt;em&gt;Владислав Клименко&lt;/em&gt; и именно он является автором данного поста,
а я лишь выступаю в роли редактора. Может быть данный пример подтолкнет
и других читателей поучаствовать в возвращении &lt;strong class="trebuchet"&gt;Insight IT&lt;/strong&gt; к жизни.
&lt;div class="right"&gt;С уважением,&lt;br&gt;Иван Блинков&lt;/br&gt;&lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Итак, пара слов о предмете разговора. memcached - это распределенная
система кэширования объектов в оперативной памяти. Разрабатывается
фирмой &lt;a href="https://www.insight-it.ru/goto/4742ee7f/" rel="nofollow" target="_blank" title="http://danga.com/"&gt;Danga Interactive&lt;/a&gt; (кстати, они являются
авторами не только memcached, но и других интересных проектов). Но о
них, возможно, в следующий раз. Обычно memcached используется
приложениями для временного хранения данных, которые надо часто читать.
Приложения не взаимодействуют (обычно) напрямую с сервером memcached, а
работают при помощи клиентских библиотек. На настоящее время созданы
библиотеки для многих языков программирования (а для некоторых еще и по
нескольку альтернативных)&amp;nbsp; - полный список клиентских библиотек доступен
на &lt;a href="https://www.insight-it.ru/goto/39c492fb/" rel="nofollow" target="_blank" title="http://code.google.com/p/memcached/wiki/Clients"&gt;wiki проекта&lt;/a&gt;. В
целом, данная схема похожа на работу с БД, знакомую многим
разработчикам.&lt;/p&gt;
&lt;p&gt;Будем рассматривать установку и использование memcached для Linux. Так
же при рассмотрении примеров на PHP и обзоре кэширования сессий
потребуются PHP и Apache. Возможно, их придется установить, но мы не
будем заострять внимание на вопросах установки.&lt;/p&gt;
&lt;h2 id="server-memcached"&gt;Сервер memcached&lt;/h2&gt;
&lt;p&gt;Давайте приступим к установке memcached. Практически во всех
дистрибутивах Linux memcached можно установить из репозитариев. Если
есть желание собрать самую свежую версию, то можно заглянуть на &lt;a href="https://www.insight-it.ru/goto/df55df38/" rel="nofollow" target="_blank" title="http://danga.com/memcached/"&gt;сайт
разработчика&lt;/a&gt; (на момент написания этих
строк последняя версия -
&lt;a href="https://www.insight-it.ru/goto/5fa8e800/" rel="nofollow" target="_blank" title="http://memcached.googlecode.com/files/memcached-1.4.0.tar.gz"&gt;1.4.0&lt;/a&gt;).
Также, возможно, понадобится установить libevent. Последняя стабильная
версия -
&lt;a href="https://www.insight-it.ru/goto/a42e2966/" rel="nofollow" target="_blank" title="http://www.monkey.org/~provos/libevent-1.4.11-stable.tar.gz"&gt;1.4.11&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Собираем, устанавливаем и запускаем memcached в режиме вывода сообщений.
Интересно же посмотреть, что с ним происходит:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;memcached -vv
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Процесс запускается и ждет подключений (по умолчанию на порту 11211).
Серверная часть готова обрабатывать подключения клиентов и кэшировать
полученные данные.&lt;/p&gt;
&lt;p&gt;Но для разработчика приложений это только полпути. Необходимо поддержать
работу с memcached в своем приложении. Для этого, рассмотрим некоторые
существующие клиентские библиотеки memcached.&lt;/p&gt;
&lt;h2 id="klienty-memcached"&gt;Клиенты memcached&lt;/h2&gt;
&lt;p&gt;Из всего многообразия клиентских библиотек рассмотрим две:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;libmemcached (для Си);&lt;/li&gt;
&lt;li&gt;PECL extension для PHP (построенный на базе предыдущей библиотеки).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="si"&gt;Си&lt;/h2&gt;
&lt;p&gt;Библиотека libmemcached на данный момент активно развивается и
представляется наиболее подходящим выбором при работе с Си и PHP. Также,
в комплекте с самой клиентской библиотекой поставляются дополнительные
утилиты для работы с memcached, позволяющие просматривать,
устанавливать, удалять значения в кэше memcached. Кстати, удивляет, что
набор утилит идет не с серверной частью, а с клиентской библиотекой.&lt;/p&gt;
&lt;p&gt;Итак, приступим к установке libmemcached. На момент написания этих строк
текущая версия libmemcached -
&lt;a href="https://www.insight-it.ru/goto/44752735/" rel="nofollow" target="_blank" title="http://download.tangent.org/libmemcached-0.31.tar.gz"&gt;0.31&lt;/a&gt;.
Компилируем, устанавливаем. Для начала, наслаждаемся чтением страниц
man:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;man libmemcached
man libmemcached_examples
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;C библиотекой поставляются описание несложных примеров использования. За
более интересными же способами применения имеет смысл заглянуть в
исходные тексты утилит, благо все идет вместе.&lt;/p&gt;
&lt;p&gt;Рекомендую обратить внимание на собранные утилиты. Наверняка многие из
них станут верными помощниками при разработке приложений.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;memstat&lt;/code&gt; - выдает информацию о сервере memcached&lt;/li&gt;
&lt;li&gt;&lt;code&gt;memcat&lt;/code&gt; - выдает значение по ключу&lt;/li&gt;
&lt;li&gt;&lt;code&gt;memrm&lt;/code&gt; - удаляет значение по ключу&lt;/li&gt;
&lt;li&gt;&lt;code&gt;memdump&lt;/code&gt; - выдает список ключей&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Для начала посмотрим, что скажет сервер memcached, запущенный нами
немного ранее в режиме выдачи сообщений. Запросим статистику сервера при
помощи утилиты memstat:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;memstat --servers localhost

 Listing &lt;span class="m"&gt;1&lt;/span&gt; Server
 Server: localhost &lt;span class="o"&gt;(&lt;/span&gt;11211&lt;span class="o"&gt;)&lt;/span&gt;
 pid: 14534
  uptime: 1950
 &lt;span class="nb"&gt;time&lt;/span&gt;: 1247390264
 version: 1.4.0
 pointer_size: 32
 rusage_user: 0.0
 rusage_system: 0.0
 curr_items: 0
 total_items: 0
 bytes: 0
 curr_connections: 10
 total_connections: 11
 connection_structures: 11
 cmd_get: 0
 cmd_set: 0
 get_hits: 0
 get_misses: 0
 evictions: 0
 bytes_read: 0
 bytes_written: 0
 limit_maxbytes: 67108864
 threads: 5
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Получили статистику - следовательно memcached функционирует и
откликается на запросы.&lt;/p&gt;
&lt;p&gt;Итак, на настоящий момент готовы к использованию сервер memcached и
клиентская библиотека. Осталось дело за малым - внедрить использование
memcached в разрабатываемое приложение. Что касается приложения - все в
руках разработчиков, а мы рассмотрим небольшой пример работы с базовыми
функциями.&lt;/p&gt;
&lt;p&gt;memcached предоставляет следующий набор основных функций (их, конечно,
больше, но здесь приведены основные):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;set&lt;/strong&gt;&amp;nbsp;- занести в кэш пару ключ-значение&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;add&lt;/strong&gt;&amp;nbsp;- занести в кэш значение при условии, что значения с таким
    ключом в кэше еще нет&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;replace&lt;/strong&gt; - обновляет кэш при условии, что значение с таким ключом
    в кэше уже есть&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;get&lt;/strong&gt; - получает значение из кэша по указанному ключу&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="primer-programmy-na-c"&gt;Пример программы на C&lt;/h3&gt;
&lt;p&gt;Файл mc.c:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cp"&gt;#include "stdio.h"&lt;/span&gt;
&lt;span class="cp"&gt;#include "string.h"&lt;/span&gt;
&lt;span class="cp"&gt;#include "memcached.h"&lt;/span&gt;

&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="kt"&gt;char&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"key"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;char&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"value"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;uint32_t&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;char&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;value2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;memcached_return&lt;/span&gt; &lt;span class="n"&gt;rc&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="c1"&gt;// 1. создать структуру для работы с кэшем&lt;/span&gt;
    &lt;span class="n"&gt;memcached_st&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;memc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;memcached_create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

    &lt;span class="c1"&gt;// 2. указать сервер с которым будем работать&lt;/span&gt;
    &lt;span class="n"&gt;memcached_server_add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;memc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;"localhost"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;11211&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

    &lt;span class="c1"&gt;// 3. занести пару ключ-значение в кэш&lt;/span&gt;
    &lt;span class="n"&gt;rc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;memcached_set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;memc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strlen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strlen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;time_t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rc&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;MEMCACHED_SUCCESS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;// обработать ошибку&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="c1"&gt;// 4. получить значение&lt;/span&gt;
    &lt;span class="n"&gt;value2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;memcached_get&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;memc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strlen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;     &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;flags&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;rc&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rc&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;MEMCACHED_SUCCESS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"%s&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;// обработать ошибку&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="c1"&gt;// 5. высвободить структуру&lt;/span&gt;
    &lt;span class="n"&gt;memcached_free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;memc&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Программа состоит из 5 основных операций и в особых комментариях не
нуждается. Разве что можно отметить, что в пункте 2 можно добавлять
много серверов, в случае использования распределенной системы.&lt;/p&gt;
&lt;p&gt;Компилируем, возможно придется явно указать пути к библиотекам:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;gcc -Wall -o mc mc.c -I/usr/local/include/libmemcached/ -lmemcached
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Запускаем:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;./mc
 value
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Видим требуемое значение - должно быть, &lt;em&gt;заработало&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;Для уточнения деталей, смотрим сообщения на сервере memcached:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&amp;lt;&lt;span class="m"&gt;32&lt;/span&gt; new auto-negotiating client connection
32: Client using the ascii protocol
&lt;span class="m"&gt;32&lt;/span&gt; STORED
&lt;span class="m"&gt;32&lt;/span&gt; sending key key
&amp;gt;32 END
&amp;lt;&lt;span class="m"&gt;32&lt;/span&gt; quit
&amp;lt;&lt;span class="m"&gt;32&lt;/span&gt; connection closed.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;В данном примере представлены следующие события: подключение клиента,
установка пары ключ-значение, чтение данных по ключу и отключение
клиента.&lt;/p&gt;
&lt;p&gt;Посмотрим статистику на сервере:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;memstat --servers localhost
 Listing &lt;span class="m"&gt;1&lt;/span&gt; Server
 Server: localhost &lt;span class="o"&gt;(&lt;/span&gt;11211&lt;span class="o"&gt;)&lt;/span&gt;
 pid: 14534
 uptime: 4659
 &lt;span class="nb"&gt;time&lt;/span&gt;: 1247392973
 version: 1.4.0
 pointer_size: 32
 rusage_user: 0.0
 rusage_system: 0.0
 curr_items: 1
 total_items: 1
 bytes: 58
 curr_connections: 10
 total_connections: 13
 connection_structures: 11
 cmd_get: 1
 cmd_set: 1
 get_hits: 1
 get_misses: 0
 evictions: 0
 bytes_read: 58
 bytes_written: 58
 limit_maxbytes: 67108864
 threads: 5
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Следующие две строчки показывают, что в кэше появилось значение:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;curr_items: 1
total_items: 1
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Посмотрим на данное значение:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;memcat --servers localhost key
 value
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Итак, приложение, использующее memcached - готово.&lt;/p&gt;
&lt;h2 id="php_1"&gt;PHP&lt;/h2&gt;
&lt;p&gt;Для начала установим PECL extension для PHP - memcached&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pecl install memcached
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;На этом этапе возможно появление сообщения об ошибке вида:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;ERROR: &lt;span class="s1"&gt;'phpize'&lt;/span&gt; failed
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Это означает, что не установлен пакет php-dev или его аналог.
Устанавливаем его и можно пробовать снова:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pecl install memcached
 install ok: channel://pecl.php.net/memcached-1.0.0
 You should add &lt;span class="s2"&gt;"extension=memcached.so"&lt;/span&gt; to php.ini
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Как нам и советуют, дописываем &lt;code&gt;extension=memcached.so&lt;/code&gt; в php.ini и
перезапускаем Apache.&lt;/p&gt;
&lt;p&gt;Смотрим информацию об используемом PHP:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;memcached support&amp;nbsp; enabled
Version  1.0.0
libmemcached version &amp;nbsp;&amp;nbsp; 0.31
Session support &amp;nbsp;&amp;nbsp; yes
igbinary support &amp;nbsp; no
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="primer-programmy-na-php"&gt;Пример программы на PHP&lt;/h3&gt;
&lt;p&gt;Можно смело использовать обращения к memcached из PHP. Как обычно,
рассмотрим пример:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cp"&gt;&amp;lt;?php&lt;/span&gt;
&lt;span class="nv"&gt;$m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nx"&gt;Memcached&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;

&lt;span class="nv"&gt;$m&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="na"&gt;addServer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'localhost'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11211&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="nv"&gt;$m&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="na"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'phpkey'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'phpvalue'&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="nb"&gt;var_dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="nv"&gt;$m&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="na"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'phpkey'&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;span class="cp"&gt;?&amp;gt;&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Результат работы данного скрипта:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="x"&gt;string(8)&amp;nbsp; "phpvalue"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Итак, PHP-приложение, использующее memcached - готово.&lt;/p&gt;
&lt;h2 id="keshirovanie-dannykh-sessii_1"&gt;Кэширование данных сессий&lt;/h2&gt;
&lt;p&gt;Memcached можно использовать и как хранилище данных сессий для PHP.
Такой подход часто используется в реальных приложениях. Давайте
рассмотрим, что для этого надо сделать.&lt;/p&gt;
&lt;p&gt;Вносим изменения в php.ini&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;;session.save_handler = files&lt;/span&gt;
&lt;span class="na"&gt;session.save_handler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;memcached&lt;/span&gt;

&lt;span class="c1"&gt;;session.save_path = /var/lib/php5&lt;/span&gt;
&lt;span class="na"&gt;session.save_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;localhost:11211&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Параметр &lt;code&gt;session.save_handler&lt;/code&gt; указывает, что теперь данные будут
храниться в memcached. Второй параметр - &lt;code&gt;session.save_path&lt;/code&gt; указывает
сервер memcached (их может быть указано несколько, через запятую) на
котором будут сохранятся данные.&lt;/p&gt;
&lt;p&gt;Перезапускаем Apache - и готово!&lt;/p&gt;
&lt;p&gt;Теперь надо проверить, что теперь данные сессии реально хранятся не на
диске, а в memcached.&lt;/p&gt;
&lt;p&gt;Рассмотрим работу несложного скрипта, заносящего что-нибудь в сессию:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cp"&gt;&amp;lt;?php&lt;/span&gt;
&lt;span class="nb"&gt;session_start&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="nv"&gt;$_SESSION&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'intval'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;123&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nv"&gt;$_SESSION&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'strval'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"qwe"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="cp"&gt;?&amp;gt;&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Запускаем скрипт, он заносит данные в сессию, после чего смотрим на кэш&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;memdump --servers localhost
 key
 keyphp
 memc.sess.key.3ff8ccab14424082ff83a6dfbcf0941f
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Итак - к нашим знакомым по предыдущим примерам ключам, добавился ключ с
характерным именем &lt;code&gt;memc.sess.key.3ff8ccab14424082ff83a6dfbcf0941f&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Хранение данных сессии перенесено в систему кэширования. Более подробную
информацию по работе с memcached из PHP можно почитать &lt;a href="https://www.insight-it.ru/goto/6146f92c/" rel="nofollow" target="_blank" title="http://ru2.php.net/manual/ru/book.memcached.php"&gt;на сайте PHP&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="zakliuchenie"&gt;Заключение&lt;/h2&gt;
&lt;p&gt;Мы рассмотрели установку и примеры использования memcached. Следует
особо подчеркнуть, что memcached - это не система хранения данных,
поэтому на практике memcached почти всегда используется в паре с БД.
Также следовало бы уделить внимание своевременной инвалидации данных в
кэше и вопросам безопасности. В общем, тема интересная, и еще далека от
закрытия.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Владислав Клименко</dc:creator><pubDate>Wed, 15 Jul 2009 15:09:00 +0400</pubDate><guid>tag:www.insight-it.ru,2009-07-15:storage/2009/memcached-na-palcakh/</guid><category>C++</category><category>Memcached</category><category>PHP</category><category>Программирование</category></item><item><title>Еще раз про HBase</title><link>https://www.insight-it.ru//storage/2008/eshe-raz-pro-hbase/</link><description>&lt;p&gt;Некоторое время назад &lt;a href="https://www.insight-it.ru/goto/5bbb805b/" rel="nofollow" target="_blank" title="http://neuronus.blogspot.com"&gt;Neuronus&lt;/a&gt; в одном
из комментариев к посту &lt;a href="https://www.insight-it.ru/storage/2008/hadoop-vozvrashhaetsya/"&gt;"Hadoop возвращается"&lt;/a&gt; не согласился с
моим кратким определением HBase как "нереляционная база данных"
(позаимствованным, собственно говоря, откуда-то с официального портала
продукта). Этот факт подтолкнул меня попытаться найти более корректное
определение в англоязычных источниках информации, получилось вполне
успешно. Хочется прочитать более детально что к чему? Вперед!
&lt;!--more--&gt;
Если Вам уже приходилось иметь дело с этой системой,возможно Вы уже
поняли, что самым сложным этапом работы является просто-напросто
осознавание того, чем она на самом деле является. Обычно приходится
мысленно отказаться от всех привычек, доставшихся при работе с
традиционный RDBMS, и начинать постигать базовые принципы организации
хранения данных с нуля.&lt;/p&gt;
&lt;p&gt;Стоит напомнить, что проект позиционируется как opensource реализация
&lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt; от &lt;a href="/tag/google/"&gt;Google&lt;/a&gt;. Да, проекты
реализованы разными людьми на разных языках программирования, но
общие идеи и принципы функционирования у них сильно пересекаются.
Наиболее значимой общей характеристикой у них является очень схожие
модели данных (о чем
&lt;a href="https://www.insight-it.ru/goto/aef0a732/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/HbaseArchitecture#head-daee3a0ce7a6892096ffb43f3cc3e0310d047f48"&gt;упоминается&lt;/a&gt;
в вики HBase), а в свою очередь &lt;a href="https://www.insight-it.ru/goto/8667b351/" rel="nofollow" target="_blank" title="http://labs.google.com/papers/bigtable.html"&gt;в документации&lt;/a&gt; BigTable она описывается очень четко и определенно, точно определяя чем эти продукты по сути являются:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A Bigtable is a sparse, distributed, persistent multidimensional sorted
map.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Предлагаю по аналогии со &lt;a href="https://www.insight-it.ru/goto/af69f471/" rel="nofollow" target="_blank" title="http://jimbojw.com/wiki/index.php?title=Understanding_HBase_and_BigTable"&gt;статьей в вики&lt;/a&gt;,
послужившей основой для данного поста, разбить это определение на
отдельные слова и последовательно пройтись по ним, попутно составляя в
голове полную картину.&lt;/p&gt;
&lt;h3 id="map"&gt;map&lt;/h3&gt;
&lt;p&gt;За этим термином нет четкого устоявшегося обозначения в русском языке
(да и в английском тоже все далеко не так однозначно), математики обычно
называют это &lt;em&gt;отображением одного множества в другое&lt;/em&gt;, в то время как
если Вы знакомы с программированием, то Вам наверняка больше знакомы
будут более знакомы такие обозначения, как &lt;em&gt;ассоциативный массив&lt;/em&gt;
(&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;), &lt;em&gt;словарь&lt;/em&gt; (&lt;a href="/tag/python/"&gt;Python&lt;/a&gt;), &lt;em&gt;хэш&lt;/em&gt;
(&lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt;) или &lt;em&gt;объект&lt;/em&gt; (&lt;a href="/tag/javascript/"&gt;JavaScript&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;По сути же имеется ввиду просто набор однозначно соответствующих пар
ключ-значение, в роли которых выступают массивы байт. Во все той же
статейке в вики все очень наглядно демонстрируется примерами в нотации
&lt;abbr title="JavaScript Object Notation"&gt;JSON&lt;/abbr&gt;, позволю себе тоже приводить аналогичные примеры. В &lt;abbr title="JavaScript Object Notation"&gt;JSON&lt;/abbr&gt; наш &lt;strong&gt;map&lt;/strong&gt; выглядел бы следующим образом:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;"qqqq"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"some"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"abc"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"sample"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"zz"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"JSON"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"123"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"map"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"mnbvcxz"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"looks like this"&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="persistent"&gt;persistent&lt;/h3&gt;
&lt;p&gt;Это прилагательное обозначает всего лишь "постоянный", то есть в данном
контексте оно говорит только о том, что данная система не зависит от
использующих ее приложений, а также хранится на устройствах постоянного
хранения данных, а не в оперативной памяти.&lt;/p&gt;
&lt;h3 id="distributed"&gt;distributed&lt;/h3&gt;
&lt;p&gt;Распределенность этих систем можно рассматривать с двух точек зрения:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Как HBase, так и BigTable сами по себе могут функционировать на
    большом количестве серверов, которые можно разделить на две большие
    категории: master и slave. Slave сервера собственно выполняют всю
    работу с данными, а master - лишь только координируют их действия и
    управляют процессом в целом. Этот факт обеспечивают высокую степень
    устойчивости к сбоям (в HBase правда количество master-серверов
    ограничено одним, что представляет собой единственную точку, сбой в
    которой приведет к отказу всей системы, но это лишь временная
    проблема, которую наверняка устранят в следующих версиях), а также
    существенно облегчает масштабируемость всей системы так как
    добавление дополнительных серверов (а значит и увеличение
    производительности и вместительности системы) достаточно тривиально,
    безболезненно и не мешает общему ее функционированию.&lt;/li&gt;
&lt;li&gt;Помимо этого каждая из этих систем обычно использует для хранения
    данных кластерную файловую систему (HBase - &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt;, а
    BigTable - &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt;), которые тоже по своей природе являются
    распределенными и функционируют по схожему принципу, обеспечивая
    дополнительную сохранность данных, реплицируя их в нескольких
    экземплярах на нескольких серверах (обычно трех).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="sorted"&gt;sorted&lt;/h3&gt;
&lt;p&gt;HBase и BigTable не строят никакие индексы для ускорения процесса
извлечения данных, единственное используемое в них правило заключается в
следующем: каждый slave-сервер в системе отвечает за определенный
диапазон ключей (от и до определенных его значений), и держит все записи
в строгом лексикографическом порядке по ключам (заметьте: сортировку
значений никто не гарантирует!). Продолжая пример с &lt;abbr title="JavaScript Object Notation"&gt;JSON&lt;/abbr&gt; это выглядело
бы примерно вот так:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;"123"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"map"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"abc"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"sample"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"mnbvcxz"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"looks like this"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"qqqq"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"some"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;"zz"&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"JSON"&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Этим фактом можно активно пользоваться при планировании использовании
системы, например если в качестве ключей планируется использовать
доменные именные имена, то имеет смысл использовать их в "развернутом"
виде, например: "com.example.www" вместо "www.example.com". Это почти
наверняка обеспечит попадание всех поддоменов одного и того же домена на
один slave-сервер, а также группировку доменов по зонам.&lt;/p&gt;
&lt;h3 id="multidimensional"&gt;multidimensional&lt;/h3&gt;
&lt;p&gt;До сих пор ключ интерпретировался нами как нечто единое и неделимое, но
на самом деле в данной ситуации это далеко не так. На самом деле
пространство имен HBase и BigTable имеет несколько пространств, по
аналогии с трехмерным материальным пространством, где есть ширина,
высота и глубина. Если мы попытаемся представить это с помощью &lt;abbr title="JavaScript Object Notation"&gt;JSON&lt;/abbr&gt;, то
это будет выглядеть как набор вложенных простых map'ов:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;"table-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
  &lt;span class="p"&gt;{&lt;/span&gt;
     &lt;span class="s2"&gt;"column-family-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
     &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"column-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
           &lt;span class="s2"&gt;"row-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
           &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-2"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-3"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-3"&lt;/span&gt;
           &lt;span class="p"&gt;},&lt;/span&gt;
           &lt;span class="s2"&gt;"row-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
           &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-4"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-5"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-3"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-6"&lt;/span&gt;
           &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="s2"&gt;"column-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
           &lt;span class="s2"&gt;"row-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
           &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-1"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-3"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-3"&lt;/span&gt;
           &lt;span class="p"&gt;},&lt;/span&gt;
           &lt;span class="s2"&gt;"row-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
           &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-4"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-5"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="s2"&gt;"timestamp-3"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"value-6"&lt;/span&gt;
           &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
     &lt;span class="s2"&gt;"column-family-2"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
     &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;"column-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
           &lt;span class="c1"&gt;// ...&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="s2"&gt;""&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
           &lt;span class="s2"&gt;"row-1"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"some-value"&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="c1"&gt;// ...&lt;/span&gt;
     &lt;span class="p"&gt;}&lt;/span&gt;
     &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Как можно увидеть из примера, таких пространств используется пять:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;таблицы;&lt;/li&gt;
&lt;li&gt;наборы столбцов;&lt;/li&gt;
&lt;li&gt;столбцы;&lt;/li&gt;
&lt;li&gt;строки;&lt;/li&gt;
&lt;li&gt;время;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Таким образом, каждое значение в хранилище данных однозначно
соответствует ключу, состоящему из пяти компонентов, например в примере
значению "value-5" соответствует ключ, состоящий из:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;table-1;&lt;/li&gt;
&lt;li&gt;column-family-1;&lt;/li&gt;
&lt;li&gt;column-1;&lt;/li&gt;
&lt;li&gt;row-2;&lt;/li&gt;
&lt;li&gt;timestamp-2;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Принцип очень похож на используемый в более привычных базах данных, с
той лишь разницей, что добавляется еще и время (которое обычно
представляется в виде целочисленного значения, обозначающего количество
секунд с начала эпохи). Изначально оно задумывалось для предоставления
возможности отследить историю изменения данных, но этому дополнительному
измерению можно найти и массу нестандартных применений, например
используя его как самый обыкновенный стек.&lt;/p&gt;
&lt;p&gt;Хочется обратить внимание, что ассортимент наборов столбцов (column
family) указывается при создании таблиц, и изменения в них должны
производиться с помощью специального запроса, в то время как сами
столбцы являются динамическими и для его создания достаточно лишь
добавить в него данные.&lt;/p&gt;
&lt;h3 id="sparse"&gt;sparse&lt;/h3&gt;
&lt;p&gt;Развивая мысль предыдущего абзаца, можно понять, что наличие значения
столбца в каждой строке вовсе не обязательно, оно запросто может и
отсутствовать вовсе. Таким образом каждая строка может содержать
произвольное количество значений для столбцов в рамках одной column
family, ровно как может и не содержать их вовсе. Несуществующие данные
не хранятся в виде какого-либо NULL-значения, они просто отсутствуют.
Запрос на несуществующие данные просто вернет пустой результат. Если же
взглянуть с другой стороны, то тот же самый факт можно представить и как
возможность наличия пробелов в списке ключей строк.&lt;/p&gt;
&lt;h3 id="i-naposledok"&gt;И напоследок...&lt;/h3&gt;
&lt;p&gt;хочется сказать, что все это лишь дело терминологии, ровно то же самое
можно подразумевать и под краткой фразой "нереляционная база данных", не
смотря на то, что она существенно менее точна и полноценна. В данном
контексте самое главное лишь чтобы люди просто понимали друг друга.
Надеюсь после прочтения этого поста в вашем сознании сложилась четкая
картина этого продукта и предоставляемых им возможностей, которая Вам
пригодится как для "общего развития", так и для потенциального
практического применения этого продукта. Если остались неясные моменты -
смело оставляйте комментарии. &lt;a href="/feed/"&gt;Традиционная подписка на RSS&lt;/a&gt; -
приветствуется :)&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 27 Aug 2008 19:29:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-08-27:storage/2008/eshe-raz-pro-hbase/</guid><category>Hadoop</category><category>HBase</category><category>map</category><category>кластер</category></item><item><title>Hadoop возвращается</title><link>https://www.insight-it.ru//storage/2008/hadoop-vozvrashhaetsya/</link><description>&lt;p&gt;Если Вы являетесь постоянным читателем моего блога, то вполне вероятно,
что Вы помните мой &lt;a href="https://www.insight-it.ru/storage/2008/hadoop/"&gt;старый пост&lt;/a&gt; об этом
замечательном проекте от &lt;a href="https://www.insight-it.ru/goto/e3b03afc/" rel="nofollow" target="_blank" title="http://apache.org"&gt;Apache Foundation&lt;/a&gt;. С тех
пор он развивался невероятными темпами и очень многое успело измениться,
об этом я и хотел бы сегодня поделиться своими впечатлениями. В
дополнение к этому планируется небольшая инструкция по развертыванию
Hadoop на кластере из большого количества машин, который послужит
неплохим развитием темы, начатой в посте &lt;a href="https://www.insight-it.ru/storage/2008/hadoop-dlya-razrabotchika/"&gt;"Hadoop для разработчика"&lt;/a&gt;.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="chto-novogo"&gt;Что нового?&lt;/h3&gt;
&lt;p&gt;Для начала вкратце напомню что их себя представляет данный продукт,
всего в нем три компонента:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;HDFS&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;кластерная файловая система.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;MapReduce framework&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;программная основа для построения приложений, работающих по
одноименной модели.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;HBase&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;нереляционная база данных.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Повторно повторяться смысла не вижу, все уже давно &lt;a href="https://www.insight-it.ru/storage/2008/hadoop/"&gt;разложено по полочкам&lt;/a&gt;. Так что сразу перейдем к глобальным
изменениям в проекте, произошедшим с написания вышеупомянутого поста, то
есть с февраля. Сразу хочу сказать, что подробно пересказывать &lt;a href="https://www.insight-it.ru/goto/81620ac0/" rel="nofollow" target="_blank" title="http://svn.apache.org/repos/asf/hadoop/core/trunk/CHANGES.txt"&gt;release notes&lt;/a&gt; у
меня нет никакого желания, если Вам интересны все подробности о каждом
bugfix'е или изменении в API, то имеет смысл почитать их в оригинале.&lt;/p&gt;
&lt;p&gt;Наиболее значительным событием в развитии Apache Hadoop было, пожалуй,
отделение &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt; в отдельный проект. Какие же это повлекло
последствия? С точки зрения простого смертного наиболее заметен тот
факт, что HBase пропал из основного архива или репозитория Hadoop и его
теперь нужно качать отдельно :) На самом же деле такое обособление лишь
ускорило ее развитие, совсем недавно HBase отпраздновала свой релиз
версии 0.2.0, включающий в себя массу нововведений и исправленных
проблем, например язык запросов HQL был полностью заменен на jirb/jython
shell, а также было добавлено кэширование данных в памяти. Помимо этого
сильно изменилось API, очень рекомендую заглянуть в
&lt;a href="https://www.insight-it.ru/goto/f059ad5e/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/hbase/docs/current/api/index.html"&gt;javadoc&lt;/a&gt;
проекта, если Вас это интересует.&lt;/p&gt;
&lt;p&gt;На уровне файловой системы наиболее значительным изменением стало
добавление еще одного типа узлов - &lt;strong&gt;Secondary NameNode&lt;/strong&gt;. Это
нововведение является первым шагом на пути к устранению узких мест в
системе (так называемых single points of failure). Название этого типа
узлов говорит само за себя: они подстраховывают основной &lt;em&gt;NameNode&lt;/em&gt; на
случай непредвиденных сбоев. Они создают резервную копию образа
метаданных файловой системы и лога транзакций (то есть всех операций с
файлами и директориями в HDFS) и периодически ее обновляют. Полноценного
автоматического восстановления системы они в случае сбоя на сервере с
&lt;em&gt;NameNode&lt;/em&gt; они на данный момент не обеспечивают, но сохранность данных
на случай, скажем, разрушившегося RAID обеспечить могут.&lt;/p&gt;
&lt;p&gt;MapReduce framework тоже несомненно развивается и дорабатывается, но
каких-либо особо выдающихся изменений в нем не произошло: появляются
дополнительные возможности, исправляются ошибки, снимаются те или иные
ограничения. В общем все идет своим чередом.&lt;/p&gt;
&lt;h3 id="podnimaem-klaster"&gt;Поднимаем кластер&lt;/h3&gt;
&lt;div class="card blue lighten-4"&gt;
&lt;div class="card-content"&gt;
&lt;h5&gt;ВНИМАНИЕ!&lt;/h5&gt;
&lt;p&gt;Перед продолжением чтения этого раздела, настоятельно рекомендуется
прочитать &lt;a href="https://www.insight-it.ru/storage/2008/hadoop-dlya-razrabotchika/"&gt;статью о запуске псевдо-кластера из одного компьютера&lt;/a&gt;.
&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Для начала нам понадобится некоторое количество компьютеров (хотя если у
Вас серьезные намерения, то лучше все же гордо называть их серверами, а
для "побаловаться" сойдут и обычные рабочие станции с
&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;). Конкретное количество на самом деле роли не
играет, продолжать можно как с 2 серверами, так и с 20 тысячами (по
крайней мере теоретически). Хотя пару рекомендаций все же могу дать: при
использовании в "боевых" условиях стоит стараться избегать физического
совмещения мастер-узлов компонентов системы (&lt;em&gt;NameNode, JobTracker,
HMaster&lt;/em&gt;) с "рядовыми" серверами, таким образом желательно начинать с,
как минимум, 5-7 серверов.&lt;/p&gt;
&lt;p&gt;Удостоверившись, что на всем оборудовании установлен какой-нибудь
дистрибутив &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; или &lt;a href="/tag/unix/"&gt;Unix&lt;/a&gt; (любители особо
поизвращаться могут попытать счастья с "окнами" в совокупности с Cygwin)
и 5 или 6 версия JRE/JDK (желательно от Sun), можно приступать к
настройке каждого узла по тому же принципу, что и для псевдо-кластера
(да-да, предупреждение в начале раздела было написано не для мебели).
Кстати не забудьте, что &lt;a href="/tag/hbase/"&gt;HBasе&lt;/a&gt; теперь нужно скачивать
отдельно. О небольших присутствующих особенностях я расскажу чуть позже,
а пока дам маленький совет, который позволит несколько облегчить это
непростое дело.&lt;/p&gt;
&lt;p&gt;Вручную выполнять одни и те же операции на паре десятков/сотен/тысяч
серверов мало того что долго, но и чрезвычайно утомительно. Уже на
втором-третьем сервере начнет появляться желание каким-либо образом
автоматизировать процесс установки. Конечно же можно воспользоваться
специализированным программным обеспечением, скажем
&lt;a href="https://www.insight-it.ru/goto/65d64e55/" rel="nofollow" target="_blank" title="http://www.theether.org/gexec/"&gt;gexec&lt;/a&gt;, но есть и более простой способ:
существенно упростить жизнь может простой скрипт на bash в 5 строчек:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; x in &lt;span class="sb"&gt;`&lt;/span&gt;cat ~/nodes&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="k"&gt;do&lt;/span&gt;
ssh hadoop@&lt;span class="nv"&gt;$x&lt;/span&gt; &lt;span class="nv"&gt;$1&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;В файле &lt;code&gt;~/nodes&lt;/code&gt; должен располагаться список IP-адресов всех
серверов, тогда получив первым параметром произвольную консольную
команду скрипт выполнит ее на каждом сервере. С его помощью можно
существенно сократить время, требуемое на выполнение всех необходимых
действий для запуска кластера.&lt;/p&gt;
&lt;p&gt;После небольшого лирического отступления вернемся собственно к
&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt;. Как Вы уже, надеюсь, знаете, система использует
&lt;strong&gt;ssh&lt;/strong&gt; для управления всеми компонентами системы, причем очень
желателен беспарольный доступ между всеми узлами. Для этого необходимо
собрать в один файл все публичные ключи &lt;code&gt;~/.ssh/id_rsa.pub&lt;/code&gt; на
каждом из узлов (по одному на строчку) и разместить его под именем
&lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; тоже на каждом из узлов. Кстати для
упоминавшегося выше скрипта беспарольный доступ тоже очень желателен.&lt;/p&gt;
&lt;p&gt;Следующим этапом нужно подготовить конфигурационные файлы, они должны
быть идентичными на всех узлах, так что заполнив их все на одном из
узлов нужно скопировать их по всем остальным серверам (очень удобно
делать это с помощью &lt;strong&gt;rsync&lt;/strong&gt;). Теперь пройдемся по необходимым
изменениям в каждом из них:&lt;/p&gt;
&lt;h4&gt;hadoop-site.xml&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.default.name&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;hdfs://namenode:54310&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;
    The name of the default file system.  A URI whose
    scheme and authority determine the FileSystem implementation.  The
    uri's scheme determines the config property (fs.SCHEME.impl) naming
    the FileSystem implementation class.  The uri's authority is used to
    determine the host, port, etc. for a filesystem.
  &lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapred.job.tracker&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;jobtracker:54311&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;
    The host and port that the MapReduce job tracker runs
    at.  If "local", then jobs are run in-process as a single map
    and reduce task.
  &lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Каждый сервер должен знать где расположен &lt;em&gt;NameNode&lt;/em&gt;, по-этому он
  явно указывается в полном пути к файловой системе, практически
  аналогичная ситуация и с &lt;em&gt;JobTracker&lt;/em&gt;. Вместо namenode и jobtracker
  необходимо указать их IP-адреса или доменные имена (или в крайнем
  случае - имя в &lt;code&gt;/etc/hosts&lt;/code&gt;)&lt;/p&gt;
&lt;h4&gt;masters&lt;/h4&gt;
&lt;p&gt;Вопреки логике, здесь указывается список всех &lt;em&gt;SecondaryNameNode&lt;/em&gt;.
Одного-двух серверов здесь будет вполне достаточно, самое главное не
указывать здесь адрес основного &lt;em&gt;NameNode&lt;/em&gt;, лучше всего подойдет
какой-нибудь другой мастер-сервер, может быть дополненный одним из
обычных узлов кластера. Выделять под это отдельный сервер смысла не
много, так как нагрузка на них минимальна.&lt;/p&gt;
&lt;h4&gt;slaves&lt;/h4&gt;
&lt;p&gt;Список всех рядовых серверов, по одному на строку (опять же: IP или доменное имя). На них будут запущенны &lt;em&gt;DataNode&lt;/em&gt; и &lt;em&gt;TaskTracker&lt;/em&gt;.&lt;/p&gt;
&lt;h4&gt;hbase-site.xml&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;hbase.master&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;localhost:60000&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;
    the host and port that the HBase master runs at
  &lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Первое изменение достаточно очевидно: &lt;em&gt;HRegionServer&lt;/em&gt; должны знать
где находится &lt;em&gt;HMaster&lt;/em&gt;, о чем им и сообщает первое свойство
(заменяем hmaster на соответствующий адрес). А вот второе свойство
является следствием "обособления" HBase от Hadoop, о котором шла
речь ранее. Теперь имеется возможность использовать их отдельно (с
локальной файловой системой вместо HDFS), а так как появился выбор
файловой системы - ее адрес необходимо указывать полностью. В данном
случае указан адрес HDFS (такой же как в &lt;strong&gt;hadoop-site.xml&lt;/strong&gt;).&lt;/p&gt;
&lt;h4&gt;regionservers&lt;/h4&gt;
&lt;p&gt;Вполне очевидный конфигурационный файл, по аналогии со &lt;strong&gt;slaves&lt;/strong&gt;,
заполняется списком адресов для запуска &lt;em&gt;HRegionServer&lt;/em&gt;. Часто
совпадает с упомянутым &lt;strong&gt;slaves&lt;/strong&gt;, обычно достаточно просто
скопировать.&lt;/p&gt;
&lt;h3 id="zapusk"&gt;Запуск&lt;/h3&gt;
&lt;p&gt;Удостоверившись, что с конфигурационными файлами все нормально и что они
на всех серверах совпадают, можно приступать собственно к запуску. Этот
процесс практически полностью совпадает с запуском на одном узле, хотя
обычно проще желать это тоже простеньким скриптом примерно такого вида:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/bin/bash&lt;/span&gt;
ssh hadoop@namenode ~/hadoop/bin/start-dfs.sh
ssh hadoop@jobtracker ~/hadoop/bin/start-mapred.sh
ssh hadoop@hmaster ~/hbase/bin/start-hbase.sh
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Если мы нигде не ошиблись и все сделано правильно, то кластер
благополучно запустится, что легко проследить выполнив на каждом узле
команду &lt;code&gt;jps&lt;/code&gt; и проверив соответствие запущенных компонентов
запланированному (читай: указанному в конфигурационных файлах).&lt;/p&gt;
&lt;p&gt;В целом процесс достаточно прост и не занимает много времени, если Вы
все же столкнулись с какими-либо проблемами в процессе - обращайтесь,
вполне возможно, что я смогу помочь. Удостовериться, что все нормально
можно абсолютно так же, как и для псевдо-кластера - с помощью MapReduce
задач, идущих в комплекте с Hadoop. Выглядеть это может, например, вот
так:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;~/hadoop/bin/hadoop jar hadoop-*-examples.jar pi &lt;span class="m"&gt;4&lt;/span&gt; 10000
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;По-хорошему надо было бы написать подобную инструкцию сразу после
первой, но почему-то как-то не сложилось...&lt;/p&gt;
&lt;h3 id="zakliuchenie"&gt;Заключение&lt;/h3&gt;
&lt;p&gt;На данный момент &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; стал еще более работоспособным,
по сравнению с его февральским состоянием. Сообщество использующих его
разработчиков растет с каждым днем, а все ошибки и проблемы исправляются
очень и очень оперативно, многие коммерческие проекты могут позавидовать
таким темпам развития. Хоть до по-настоящему стабильного релиза еще
далеко, данный продукт уже сейчас очень активно используется в
достаточно большом количестве крупных интернет-проектов.&lt;/p&gt;
&lt;p&gt;Если Вы еще не успели подписаться на &lt;a href="/feed/"&gt;RSS&lt;/a&gt; - сейчас самое время!&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 17 Aug 2008 23:15:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-08-17:storage/2008/hadoop-vozvrashhaetsya/</guid><category>Apache</category><category>Hadoop</category><category>HBase</category><category>HDFS</category><category>MapReduce</category></item><item><title>GlusterFS</title><link>https://www.insight-it.ru//storage/2008/glusterfs/</link><description>&lt;p&gt;&lt;img alt="GlusterFS Logo" class="right" src="https://www.insight-it.ru/images/glusterfs-logo.png" title="GlusterFS"/&gt;
&lt;a href="https://www.insight-it.ru/goto/12ccc1c7/" rel="nofollow" target="_blank" title="http://www.gluster.org/glusterfs.php"&gt;GlusterFS&lt;/a&gt; представляет собой
кластерную файловую систему, способную масштабироваться для хранения
далеко не одного петабайта данных. Как и многие другие кластерные
файловые системы, GlusterFS агрегирует дисковое пространство большого
количества машин в одну общую параллельную сетевую файловую систему
через Infiniband RDMA или TCP/IP соединение. Обычно в качестве
аппаратной основы для этой файловой системы используется ничем не
выдающееся недорогое серверное оборудование, в полной мере реализуя
принцип программного построения стабильности при использовании на
ненадежном оборудовании.
&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Кластерные файловые системы еще не достаточно приспособлены для
использования на крупных предприятиях: обычно процесс их развертывания и
поддержания в работающем состоянии не так уж прост. Но зато они отлично
масштабируются и достаточно дешевы, ведь для них достаточно самого
простого серверного оборудования и opensource операционных систем и
програмного обеспечения. Основной целью разработчиков
&lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; как раз и является построение кластерной
файловой системы, адаптированной для использования в рамках серьезных
компаний.&lt;/p&gt;
&lt;p&gt;Список основных ее особенностей по большей части мало чем отличается от
других кластерных файловых систем:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Состоит из клиентской и серверной частей. Клиентская часть позволяет
    монтировать файловую систему, а серверная - &lt;strong&gt;glusterfsd&lt;/strong&gt; -
    экспортировать в нее локальное дисковое пространство.&lt;/li&gt;
&lt;li&gt;Масштабируемость близка к &lt;em&gt;O(1)&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Широкий спектр возможностей за счет использования модульной
    архитектуры.&lt;/li&gt;
&lt;li&gt;Имеется возможность восстановления файлов и директорий из файловой
    системы даже без ее инициализации.&lt;/li&gt;
&lt;li&gt;Отсутствие централизованного сервера метаданных, что делает ее более
    устойчивой к потенциальным сбоям.&lt;/li&gt;
&lt;li&gt;Расширяемый интерфейс выполнения задач, с поддержкой загрузки
    модулей в зависимости от особенностей выполнения пользователями
    операций по работе с данными.&lt;/li&gt;
&lt;li&gt;Расширяющий функциональность механизм трансляторов.&lt;/li&gt;
&lt;li&gt;Поддержка &lt;em&gt;Infiniband RDMA&lt;/em&gt; и &lt;em&gt;TCP/IP&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Возможность автоматического восстановления в случае сбоев.&lt;/li&gt;
&lt;li&gt;Полностью реализована на уровне приложений, что упрощает ее
    поддержание в рабочем состоянии, портирование и дальнейшую
    разработку.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Но некоторые моменты все же заслуживают отдельного внимания:&lt;/p&gt;
&lt;h4&gt;Совместимость&lt;/h4&gt;
&lt;p&gt;Как уже упоминалось, файловая система реализована полностью на
уровне пользовательских приложений, что делает возможным ее
монтирование без каких-либо дополнительных патчей в ядре
операционной системы, единственное требование к нему: поддержка
FUSE. Серверная часть &lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; может
функционировать на любой POSIX-совместимой операционной системе и
протестирована на &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;, &lt;a href="/tag/freebsd/"&gt;FreeBSD&lt;/a&gt;,
&lt;a href="/tag/solaris/"&gt;OpenSolaris&lt;/a&gt;, в отличии от клиентской части, которая
может работать только в &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Модули&lt;/h4&gt;
&lt;p&gt;В виде модулей реализованы различные варианты выполнения
основополагающих операций: передачи данных и балансировки нагрузки в
рамках кластера. Транспортные модули обеспечивают передачу данных по
различным типам соединений:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TCP/IP&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Infiniband-verbs&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Infiniband-&lt;abbr title="Socket Direct Protocol"&gt;SDP&lt;/abbr&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Балансировка нагрузки может выполняться по следующим алгоритмам:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="Adaptive Least Usage"&gt;ALU&lt;/abbr&gt;&lt;/strong&gt; - использует
    целый ряд факторов, включающий объем свободного локального
    дискового пространства, активность операций чтения и записи,
    количество одновременно открытых файлов, скорость физического
    вращения дисков. Значимость, придаваемая каждому из показателей,
    может достаточно гибко настраиваться.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="Round Robin"&gt;RR&lt;/abbr&gt;&lt;/strong&gt; - по очереди размещает
    файлы последовательно на каждом узле, после чего начинает
    процесс заново, образуя своеобразный цикл. Этот метод эффективен
    если файлы имеют примерно одинаковый размер, а узлы кластера -
    одинаковый размер экспортированного локального дискового
    пространства.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Random&lt;/strong&gt; - распределяют файлы случайным образом.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="Non-Uniform File Access"&gt;NUFA&lt;/abbr&gt;&lt;/strong&gt; -
    приоритет отдается созданию файлов локально, а не на других
    узлах кластера.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Switch&lt;/strong&gt; - располагает файлы по определенным указанным
    особенностям имен файлов, по аналогии со &lt;strong&gt;switch(filename)&lt;/strong&gt; в
    программировании, обычно в качестве критерия распределения
    файлов имеет смысл использовать их расширение.&lt;/li&gt;
&lt;/ul&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Трансляторы&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Они представляют собой очень мощный механизм для расширения
возможностей GlusterFS, сама идея трансляторов бала позаимствована у
&lt;a href="https://www.insight-it.ru/goto/a8e9005c/" rel="nofollow" target="_blank" title="http://hurd.gnu.org"&gt;GNU/Hurd&lt;/a&gt; и заключается она в загрузке
бинарных библиотек (.so) в процессе работы системы в зависимости от
использованных настроек и использовании их в виде своеобразной
цепочки обработчиков при работе с файлами как на серверной, так и на
клиентской стороне. В &lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; практически все
дополнительные возможности реализованы именно виде трансляторов,
начиная от дополнений, увеличивающих производительность, заканчивая
средствами отладки. Вкратце перечислю основные из них:
+   &lt;strong&gt;&lt;abbr title="Automatic File Replication"&gt;AFR&lt;/abbr&gt;&lt;/strong&gt; - автоматическая репликация файлов.
+   &lt;strong&gt;Stripe&lt;/strong&gt; - разбивает файлы на блоки фиксированного размера.
+   &lt;strong&gt;Unify&lt;/strong&gt; - объединяет несколько узлов кластера в один большой
    виртуальный узел, один узел выделяется для обеспечения
    внутреннего namespace. Директории создаются на всех узлах,
    составляющих unify, а каждый файл - лишь на одном (если не
    используется &lt;abbr title="Automatic File Replication"&gt;AFR&lt;/abbr&gt;).
+   &lt;strong&gt;Trace&lt;/strong&gt; - предоставляют информацию для отладки в виде
    дополнительных записей в лог.
+   &lt;strong&gt;Filter&lt;/strong&gt; - фильтрация файлов на основании их имен и/или
    атрибутов.
+   &lt;strong&gt;Posix-locks&lt;/strong&gt; - обеспечивает POSIX блокировку записей
    независимую от используемой системы хранения.
+   &lt;strong&gt;Trash&lt;/strong&gt; - предоставляет функциональность сопоставимую с
    libtrash (или "корзиной" - если так понятнее).
+   &lt;strong&gt;Fixed-id&lt;/strong&gt; - обеспечивает доступ только для пользователей с
    определенными UID и GUID.
+   &lt;strong&gt;Posix&lt;/strong&gt; - соединяет GlusterFS с низлежащей локальной файловой
    системой.
+   &lt;strong&gt;rot-13&lt;/strong&gt; - транслятор обеспечивает возможность шифрования и
    дешифрования данных по примитивному одноименному алгоритму.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Список возможностей, обеспечиваемых широким набором модулей и
транслятором, впечатляет, большинство других opensource кластерных
файловых систем не могут похвастаться подобной функциональностью
(&lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; выпускается под GPL). Благодаря возможности
работы через Infiniband производительность передачи данных также
достаточно высока - она может достигать десятков гигабит в секунду.
Обработка сбоев в отдельных узлах также осуществляется достаточно
эффективно, так как может быть автоматизирована. Из потенциальных
недостатков можно назвать некоторое количество редко проявляющих себя
багов в коде, а также достаточно большой размер заголовков в
используемом протоколе (несколько сотен байт). В целом эта система
вполне работоспособна и полноценно выдерживает конкуренцию со стороны
своих opensource "коллег".&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 18 May 2008 21:16:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-05-18:storage/2008/glusterfs/</guid><category>GlusterFS</category><category>GPL</category><category>Infiniband</category><category>кластер</category><category>Масштабируемость</category><category>файловая система</category></item><item><title>Hypertable</title><link>https://www.insight-it.ru//storage/2008/hypertable/</link><description>&lt;p&gt;&lt;img alt="Hypertable" class="right" src="https://www.insight-it.ru/images/hypertable-logo.gif" title="Hypertable"/&gt;
&lt;a href="https://www.insight-it.ru/goto/63463036/" rel="nofollow" target="_blank" title="http://www.hypertable.org"&gt;Hypertable&lt;/a&gt; является еще одним opensource
проектом, направленным на воспроизведение функционала
&lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt; от &lt;a href="/tag/google/"&gt;Google&lt;/a&gt;. Поставленная перед
проектом цель заключается в реализации системы хранения данных на базе
распределенной файловой системы, позволяющей перейти на новый уровень
производительности при работе с гигантскими объемами данных.
&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Принцип работы &lt;a href="/tag/hypertable/"&gt;Hypertable&lt;/a&gt; прост до безобразия:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hypertable хранит данные в табличном формате, сортируя записи по
    основному ключу;&lt;/li&gt;
&lt;li&gt;для хранимых данных не используются какие-либо типы данных, любая
    ячейка интерпретируется как байтовая строка;&lt;/li&gt;
&lt;li&gt;масштабируемость достигается путем разбиения таблиц на смежные
    интервалы строк и хранения их на разных физических машинах;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;в системе используется два типа серверов:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Master Server&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;&amp;ndash; как и во многих других подобных системах мастер-сервер
выполняет обязанности скорее административного характера: он
управляет работой Range серверов, работает с метаданными
(которые хранятся просто в отдельной таблице, наравне с
остальными).&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Range Server&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;&amp;ndash; их задача стоит в собственно в хранении диапазонов строк из
различных таблиц. Каждый сервер может хранить несколько
несмежных диапазонов строк, если диапазон превышает по объему
определенный лимит (по-умолчанию - 200 MB), то он разбивается на
пополам и одна половина обычно перемещяется на другой сервер.
Если же на одном из серверов подходит к концу дисковое
пространство, то под руководством мастер-сервера часть
диапазонов с него перераспределяется на менее загруженные Range
серверы.&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Еще одним компонентом системы является Hyperspace, этот сервер
    предоставляет указатель на основную таблицу с метаданными, а также
    пространство имен. Помимо этого этот сервис выступает в роли
    lock-механизма для клиентов системы.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В качестве основы для этой системы может использоваться как входящая в
состав &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; файловая система &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt;, так и
&lt;a href="/tag/kfs/"&gt;KosmosFS&lt;/a&gt;, о которой я недавно
&lt;a href="https://www.insight-it.ru/storage/2008/fajjly-v-kosmose/"&gt;рассказывал&lt;/a&gt;. Это позволяет
Hypertable выступать в роли конкурента для &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt; в рамках
проекта &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;HBase и Hypertable выполняют достаточно похожие функции и преследуют
практически одни и те же цели, но есть некоторые ньюансы. Одним из
глобальных различий в этих системах является языки программирования, с
использованием которого они реализованы. HBase написана на
&lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, в то время как разработчики Hypertable предпочли
&lt;a href="/tag/c/"&gt;C++&lt;/a&gt;. Это повлекло за собой массу различий в инкапсулированной
реализации различных операций.&lt;/p&gt;
&lt;p&gt;Для доступа к данным каждая из систем использует язык HQL, только в
одном случае аббревиатура расшифровывается как HBase Query Language, а в
другом - Hypertable Query Language (как эгоистично :) ). По сути и то и
другое является сильно упрощенным диалектом &lt;a href="/tag/sql/"&gt;SQL&lt;/a&gt;, что
позволяет сократить знакомство с синтаксисом HQL до пары минут при
достаточном знании классического SQL. Хотелось бы отметить, что вся
простота в сравнении с классическим SQL и реляционными СУБД вполне
обоснована: обе системы хранения данных предназначены для использования
в совокупности с &lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt; программами, что делает их
просто хранилищем данных, а не средством их обработки.&lt;/p&gt;
&lt;p&gt;После небольшого лирического отступления в виде сравнения с HBase
хотелось бы все же вернуться к теме нашего разговора, а именно к
организации хранения данных в Hypertable. Данные хранятся в виде пар
ключ:значение, причем храняться все версии строк с указанием времени,
когда они были созданы. Таким образом легко проследить за процессом
изменения данных во времени, а также узнать какие именно операции
проводились над ними в прошлом. Стандартный механизм работы с версиями
данных может быть переопределен на хранения лишь фиксированного
количества версий строки, позволяя использовать удаление устаревших
записей для освобождения дополнительного дискового пространства.&lt;/p&gt;
&lt;p&gt;Для более эффективной работы с обновлением случайных ячеек таблиц
используется кэширование. Поступающие данные собираются в оперативной
памяти и при достижении определенного лимита сжимаются и записываются на
диск.&lt;/p&gt;
&lt;p&gt;Для более эффективной работы с распределенной файловой системой
используется механизм под названием &lt;em&gt;Access Groups&lt;/em&gt;. Суть заключается в
объединении колонок таблиц в группы, в которых они чаще всего
используется вместе. Такие группы данных по возможности храняться вместе
на физических носителях. Если запрос включает в себя только данные из
колонок одной группы доступа, то с дисков считывается только эти
колонки, в противном случае приходиться работать со всей строкой
целиком. Такой подход позволяет существенно оптимизировать работу
операций ввода/вывода.&lt;/p&gt;
&lt;p&gt;Проект еще находится в стадии разработки и до стабильного релиза ему еще
далеко, но тем не менее он уже вполне может себя показать в качестве
конкурента как для других систем подобного класса, так и для более
стандартных реляционных баз данных. Основными недостающими моментами в
этой системе в данной системе является отсутствие некоторого порой
необходимого функционала в HQL, а такжы некоторые проблемы с
отказоустойчивостью, вызванные единственностью в рамках системы Master и
Hyperspace серверов.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 05 Apr 2008 20:27:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-04-05:storage/2008/hypertable/</guid><category>C++</category><category>GPL</category><category>Hadoop</category><category>HDFS</category><category>HQL</category><category>Hypertable</category><category>KFS</category><category>opensource</category></item><item><title>Файлы в космосе</title><link>https://www.insight-it.ru//storage/2008/fajjly-v-kosmose/</link><description>&lt;h4&gt;...или Kosmos Distributed File System&lt;/h4&gt;
&lt;p&gt;&lt;img alt="Kosmos Distributed File System" class="right" src="https://www.insight-it.ru/images/KFS.jpg" title="KosmosFS Logo"/&gt;
Сегодня речь пойдет об еще одной распределенной файловой системе -
&lt;a href="https://www.insight-it.ru/goto/a11ac210/" rel="nofollow" target="_blank" title="http://kosmosfs.sourceforge.net/"&gt;KosmosFS&lt;/a&gt;. У русских людей название
этого проекта определенно вызывает ассоциации с космосом, но изначально
все же свою лепту в него внес изначальный разработчик -
&lt;a href="https://www.insight-it.ru/goto/636f244d/" rel="nofollow" target="_blank" title="http://www.kosmix.com/"&gt;Kosmix&lt;/a&gt;.
&lt;!--more--&gt;
По большому счету &lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; мало чем выделяется из множества
своих конкурентов, по своей структуре она состоит из сервера метаданных
и серверов блоков, доступ к системе производится средствами клиентской
библиотеки, предоставляющей соответствующий API. Список возможностей
файловой системы также вполне стандартен:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Инкрементальная масштабируемость.&lt;/em&gt; При добавлении дополнительных
    узлов в кластер, система сама адаптируется для вовлечения их в
    полноценную работу.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Стабильный доступ.&lt;/em&gt; Реплицируемость данных (по-умолчанию в трех
    экземплярах) позволяет гарантировать доступность данных вне
    зависимости от сбоев в работе отдельных узлов.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Балансировка блоков данных.&lt;/em&gt; Периодически сервер метаданных
    перераспределяет данные с целью более оптимального использования
    дискового пространства.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Целостность данных.&lt;/em&gt; Для обеспечения целостности данных вычисляются
    и сравниваются контрольные суммы блоков данных.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Кэширование.&lt;/em&gt; Для увеличения производительности используется
    кэширования на уровне клиентской библиотеки.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Прозрачная работа с недоступными узлами.&lt;/em&gt; Клиентская библиотека
    прозрачно для приложения переключается на альтернативный сервер с
    данными, если обнаруживает что один из них недоступен.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Поддержка языков программирования:&lt;/em&gt; &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;,
    &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Скрипты.&lt;/em&gt; С системой предоставляется набор скриптов для
    развертывания, запуска и остановки узлов.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Но написать этот пост меня подтолкнул вовсе не этот список. В
комментариях к &lt;a href="https://www.insight-it.ru/storage/2008/hadoop/"&gt;одной из предыдущих моих записей&lt;/a&gt;
читатели подняли тему о целесообразности использования &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;
для реализации &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt; в частности и &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; в
целом. В качестве альтернативы был предложен &lt;a href="/tag/c/"&gt;C++&lt;/a&gt; (только на
словах конечно же), аргументируя это тем, что такая реализация была бы
эффективнее. &lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; же как раз и является той самой
альтернативой &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt;, написанной на &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;.
&lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; тесно интегрируется с &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; с помощью
его интерфейсов для файловой системы. Это позволяет Hadoop-приложениям
незаметно работать с &lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; точно так же, как если бы на ее
месте была бы &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt;. Код для интеграции с Hadoop был выпущен
в виде патча к Hadoop-JIRA-1963, а начиная с Hadoop версии 0.15 этот код
входит в стандартный дистрибутив, ровно как и детальная инструкция по
интеграции.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 30 Mar 2008 23:06:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-03-30:storage/2008/fajjly-v-kosmose/</guid><category>C++</category><category>Hadoop</category><category>KFS</category><category>Kosmos Distributed File System</category><category>кластер</category><category>файловая система</category></item><item><title>Lustre</title><link>https://www.insight-it.ru//storage/2008/lustre/</link><description>&lt;p&gt;&lt;img alt="Lustre Logo" class="right" src="https://www.insight-it.ru/images/lustre-logo.png" title="Lustre"/&gt;
&lt;a href="https://www.insight-it.ru/goto/4df3f908/" rel="nofollow" target="_blank" title="http://www.lustre.org"&gt;Lustre&lt;/a&gt; представляет собой кластерную файловую
систему, основными особенностями которой являются превосходные
надежность и масштабируемость. Производительность также более чем
высока - скорость передачи данных может достигать сотен гигабит в
секунду, а теоретический максимум доступного дискового пространства
измеряется петабайтами. Эта файловая система может использоваться как на
скромных рабочих группах из нескольких компьютеров, так и на огромных
кластерах, насчитывающих десятки тысяч машин.&lt;/p&gt;
&lt;p&gt;Помимо этого поддерживаются все возможности, который должна иметь любая
уважающая себя кластерная файловая система:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;поддержка широкого ассортимента типов высокоскоростных сетевых
    соединений;&lt;/li&gt;
&lt;li&gt;надежная система "замков" для обеспечения параллельного доступа к
    файлам;&lt;/li&gt;
&lt;li&gt;возможность автоматического самовосстановления в случае падения
    любого из узлов;&lt;/li&gt;
&lt;li&gt;распределенное управление файловыми объектами для предоставления
    масштабируемого доступа к файлам.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;p&gt;Изначально архитектура этой файловой системы была разработана просто в
рамках исследовательского проекта Петера Браама в 1999, но он решил не
останавливаться на достигнутом и основал &lt;em&gt;Cluster File Systems, Inc.&lt;/em&gt;, в
которой уже и велась основная разработка самой файловой системы. Первый
релиз Lustre 1.0 был выпущен в 2003 году. Спустя четыре года компания
была приобретена Sun Microsystems в октябре 2007 года, но это лишь
способствовало дальнейшему развитию проекта. Программное обеспечение,
входящее в состав проекта, выпускается под лицензией GPL, что также
сыграло немаловажную роль в его жизни.&lt;/p&gt;
&lt;h3 id="arkhitektura"&gt;Архитектура&lt;/h3&gt;
&lt;p&gt;Каждый компьютер, входящий состав кластера Lustre, выполняет свою четко
определенную функцию:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="MetaData Server"&gt;MDS&lt;/abbr&gt;.&lt;/strong&gt; Сервер метаданных
    предназначен для хранения всей служебной информации о системе:
    названия файлов, директорий, прав доступа и так далее. Достаточно
    наличие одного такого сервера в системе, но для обеспечения
    надежности на случай каких-либо сбоев, обычно его дублируют.
    Возможно использование внешнего хранилища данных
    (&lt;abbr title="MetaData Target"&gt;MDT&lt;/abbr&gt;), которое может быть общим
    для двух дублирующих друг друга &lt;abbr title="MetaData Server"&gt;MDS&lt;/abbr&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="Object Storage Server"&gt;OSS&lt;/abbr&gt;&lt;/strong&gt; Компьютеры для
    хранения самих данных. Каждый из них работает с 2-8 &lt;abbr title="Object Storage Target"&gt;OST&lt;/abbr&gt;, в их роли могут
    выступать практически любые средства хранения данных, начиная от
    просто жестких дисков или RAID массивов внутри &lt;abbr title="Object Storage Server"&gt;OSS&lt;/abbr&gt;, заканчивая
    внешними системами хранения данных enterprise-класса. Сумма
    дискового пространства всех &lt;abbr title="Object Storage Target"&gt;OST&lt;/abbr&gt; и является размером доступного
    дискового пространства всей файловой системы Lustre.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Клиент.&lt;/strong&gt; Компьютеры, непосредственно использующие файловую
    систему. Им предоставляется полный параллельный доступ, полностью
    соответствующий стандарту POSIX.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Один и тот же компьютер теоретически может совмещать в себе несколько
функций, но в большинстве случаев это нецелесообразно (за исключением
совмещения клиентов с &lt;abbr title="Object Storage Target"&gt;OST&lt;/abbr&gt; и, возможно, случаев, когда количество узлов
кластера очень мало).&lt;/p&gt;
&lt;p&gt;Возможно более наглядно вышенаписанное сможет представить схема
архитектуры системы
(&lt;a href="https://www.insight-it.ru/goto/dd98ad05/" rel="nofollow" target="_blank" title="http://manual.lustre.org/manual/LustreManual16_HTML/figures/ClusterWithLustre-4.gif"&gt;позаимствована&lt;/a&gt; с официального сайта и переведена):
&lt;img alt="Схема архитектуры файловой системы Lustre" class="responsive-img" src="https://www.insight-it.ru/images/lustre-architecture.png" title="Схема архитектуры"/&gt;&lt;/p&gt;
&lt;p&gt;Помимо этого для функционирования системы необходим еще один компонент,
по большому счету не являющийся ее частью -
&lt;abbr title="ManaGement Server"&gt;MGS&lt;/abbr&gt;. Его роль заключается в
предоставлении конфигурационной информации всем компонентам одной или
нескольким файловым системам Lustre. Он также нуждается в отдельном
хранилище данных, но чисто теоретически он может быть и совмещен с одним
из компонентов файловой системы.&lt;/p&gt;
&lt;h3 id="funktsionirovanie"&gt;Функционирование&lt;/h3&gt;
&lt;p&gt;Основным толчком для выполнения каких-либо действий в рамках всей
файловой системы обычно является запрос с одного из клиентов.
Программное обеспечение для клиентов представляет по сути интерфейс
между виртуальной файловой системой &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; и серверами
Lustre. Каждому типу серверов соответствует своя часть клиентского ПО:
&lt;abbr title="MetaData Client"&gt;MDC&lt;/abbr&gt;, &lt;abbr title="Object Storage Client"&gt;OSC&lt;/abbr&gt;, &lt;abbr title="ManaGement Client"&gt;MGC&lt;/abbr&gt;. В отличии от &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; и &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt; файловая система Lustre должна быть примонтирована к локальной системе клиентов для полноценного их функционирования.&lt;/p&gt;
&lt;p&gt;Для осуществления коммуникации между клиентами и серверами используется
собственный API, известный как &lt;abbr title="Lustre NETworking"&gt;LNET&lt;/abbr&gt;. Он поддерживает множество
сетевых протоколов с помощью &lt;abbr title="Network Abstraction Layer"&gt;NAL&lt;/abbr&gt;.&lt;/p&gt;
&lt;p&gt;В системе отсутствуют незаменимые компоненты, это является залогом
отказоустойчивости системы. В случае возникновения каки-либо неполадок
или сбоев в работе оборудования, работу потерявших работоспособность
компонентов системы перехватят другие ее компоненты, что сделает сбой
незаметным для пользователей системы. Это достигается за счет
дублирование серверов, выполняющих одинаковые функции, а также наличие
налаженных алгоритмов действий, направленных на автоматическое
восстановление полноценного функционирования системы в случае
возникновения чрезвычайных ситуаций. Но этого конечно же не достаточно
для абсолютной надежности системы, в дополнение должна быть
предоставлена как минимум система бесперебойного питания для всех
компонентов кластера на случай проблем с электроэнергией в датацентре
(для России более чем актуально).&lt;/p&gt;
&lt;p&gt;В списке дополнительных возможностей, предоставляемых файловой системой,
можно назвать возможность выделения квот на дисковое пространство для
каждого пользователя системы, аутентификацию пользователей с помощью
механизма Kerberos, повышение физической пропускной способности сетевого
соединения путем аггрегирования физических сетевых соединений в одно
логическое виртуально сетевое соединение (достаточно интересная
возможность, способная при выполнении определенных условий существенно
повлиять на быстродействие системы). Помимо этого предоставляется целый
ряд возможностей по созданию резервных копий данных на уровне файловой
системы в целом, отдельных устройств или же файлов.&lt;/p&gt;
&lt;h3 id="zakliuchenie"&gt;Заключение&lt;/h3&gt;
&lt;p&gt;Эта файловая система нашла свое применение во множестве крупнейших
кластеров и суперкомпьютеров по всему миру, но это не мешает ей с тем же
успехом демонстрировать и на кластерах существенно меньшего масштаба.
Около половины из самых производительных суперкомпьютеров во всем мире
используют Lustre в качестве файловой системы. Помимо этого многие
компании предоставляют ее в качестве основы для Linux кластеров
(например HP StorageWorks SFS, Cray XT3, Cray XD1). Чем не показатель ее
конкурентоспособности?&lt;/p&gt;
&lt;p&gt;В качестве источников информации были использованы &lt;a href="https://www.insight-it.ru/goto/4df3f908/" rel="nofollow" target="_blank" title="http://www.lustre.org"&gt;официальный сайт проекта&lt;/a&gt; и иногда &lt;a href="https://www.insight-it.ru/goto/9328120f/" rel="nofollow" target="_blank" title="http://en.wikipedia.org/wiki/Lustre_%28file_system%29"&gt;страница английской wikipedia.org&lt;/a&gt;.
На все том же официальном сайте всегда можно найти всю необходимую
документацию, а само программное обеспечение проекта доступно на
&lt;a href="https://www.insight-it.ru/goto/7a51c11f/" rel="nofollow" target="_blank" title="http://www.sun.com/software/products/lustre/get.jsp"&gt;соответствующей странице&lt;/a&gt; сайта Sun Mictosystems.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 21 Mar 2008 21:53:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-03-21:storage/2008/lustre/</guid><category>Cluster File Systems</category><category>Linux</category><category>Lustre</category><category>Sun</category><category>кластер</category><category>Масштабируемость</category><category>файловая система</category></item><item><title>Hadoop для разработчика</title><link>https://www.insight-it.ru//storage/2008/hadoop-dlya-razrabotchika/</link><description>&lt;p&gt;Для разработки приложений, работающих с использованием Hadoop, или же
алгоритмов для MapReduce framework'а совсем не нужен полномасштабный
кластер. На самом же деле для запуска всей системы, описанной мной в
&lt;a href="https://www.insight-it.ru/storage/2008/hadoop/"&gt;одном из предыдущих постов&lt;/a&gt;, вполне
достаточно одного компьютера и буквально минут 15 свободного времени,
как потратить их для решения этой задачи я Вам и поведаю.
&lt;!--more--&gt;
Рассказывать я буду на примере своего &lt;a href="/tag/gentoo-linux/"&gt;Gentoo
Linux&lt;/a&gt;, но большая часть этого повествования будет
справедлива и для других unix-like операционных систем.&lt;/p&gt;
&lt;h3 id="podgotovka"&gt;Подготовка&lt;/h3&gt;
&lt;p&gt;Перед тем, как приступить собственно говоря к установке
&lt;a href="https://www.insight-it.ru/goto/30a7481/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/"&gt;Hadoop&lt;/a&gt;, необходимо выполнить два
элементарных действия, необходимых для правильного функционирования
системы:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;открыть доступ одному из пользователей по &lt;code&gt;ssh&lt;/code&gt; к этому же
    компьютеру без пароля, можно например создать отдельного
    пользователя для этого &lt;code&gt;hadoop&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; useradd -m -n hadoop
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Далее действия выполняем от его имени:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; su hadoop
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Генерируем RSA-ключ для обеспечения аутентификации в условиях
отсутствия возможности использовать пароль:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; hadoop@localhost ~ &lt;span class="nv"&gt;$ &lt;/span&gt;ssh-keygen -t rsa -P &lt;span class="s2"&gt;""&lt;/span&gt;
Generating public/private rsa key pair.
Enter file in which to save the key &lt;span class="o"&gt;(&lt;/span&gt;/home/hadoop/.ssh/id_rsa&lt;span class="o"&gt;)&lt;/span&gt;:
Your identification has been saved in /home/hadoop/.ssh/id_rsa.
Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.
The key fingerprint is:
7b:5c:cf:79:6b:93:d6:d6:8d:41:e3:a6:9d:04:f9:85 hadoop@localhost
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;И добавляем его в список авторизованных ключей:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; cat ~/.ssh/id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Этого должно быть более чем достаточно, проверить работоспособность
соединения можно просто написав:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; ssh localhost
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Не забываем предварительно инициализировать &lt;strong&gt;sshd&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$$&lt;/span&gt; /etc/init.d/sshd start
&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Помимо этого необходимо убедиться в наличии установленной JVM версии
    1.5.0 или выше, а также узнать директорию, где она располагается,
    вариантов сделать это множество, я нашел ее просто заглянув в самое
    логичное место - &lt;code&gt;/usr/lib&lt;/code&gt;, но при желании никто не может Вам
    помешать воспользоваться услугами, например, &lt;code&gt;slocate&lt;/code&gt;. Найденную
    директорию с JVM лучше запомнить или записать куда-нибудь, для меня
    она оказалась: &lt;code&gt;/usr/lib/jvm/sun-jdk-1.6&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="ustanovka"&gt;Установка&lt;/h3&gt;
&lt;p&gt;Установка начинается с получения копии исходного кода системы, способов
для этого существует несколько. Я перепробовал практически все, самую
адекватную версию мне удалось получить из SVN. Для ее получения
необходимо выполнить следующую команду:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;svn checkout http://svn.apache.org/repos/asf/hadoop/core/branches/branch-0.16 ~
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;branch-0.16&lt;/strong&gt; - последняя доступная версия на данный момент, для
определения ее номера достаточно заглянуть &lt;a href="https://www.insight-it.ru/goto/99e3d37e/" rel="nofollow" target="_blank" title="http://svn.apache.org/repos/asf/hadoop/core/branches/"&gt;по тому же адресу&lt;/a&gt;
браузером. Предполагается, что Hadoop будет располагаться прямо в
&lt;code&gt;/home/hadoop&lt;/code&gt;, но запросто можно использовать и другую директорию.&lt;/p&gt;
&lt;p&gt;Сразу же стоит скомпилировать различные дополнительные компоненты
системы, особенно это актуально из-за &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt;, но и помимо
него соберется много чего интересного, например plug-in для отличной IDE
под названием &lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/b7976bc5/" rel="nofollow" target="_blank" title="http://www.eclipse.org"&gt;Eclipse&lt;/a&gt;&lt;/strong&gt; или &lt;a href="https://www.insight-it.ru/goto/9253da15/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/docs/r0.16.0/hod.html"&gt;Hadoop On
Demand&lt;/a&gt;. Задача
также элементарна:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; ~ &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; ant clean jar compile-contrib
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="nastroika"&gt;Настройка&lt;/h3&gt;
&lt;p&gt;Конфигурационные файлы можно редактировать в произвольном порядке, самое
главное ничего не забыть :)&lt;/p&gt;
&lt;h4&gt;conf/hadoop-env.sh&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;JAVA_HOME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/usr/lib/jvm/sun-jdk-1.6
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Единственная обязательная переменная окружения - &lt;code&gt;JAVA_HOME&lt;/code&gt;,
здесь как раз пригодится заранее найденный путь до JVM, все
остальное - по желанию.&lt;/p&gt;
&lt;h4&gt;conf/hadoop-site.xml&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;hadoop.tmp.dir&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;/home/hadoop/data/${user.name}&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;A base for other temporary directories.&lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;fs.default.name&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;hdfs://localhost:54310&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;The name of the default file system.  A URI whose
  scheme and authority determine the FileSystem implementation.  The
  uri's scheme determines the config property (fs.SCHEME.impl) naming
  the FileSystem implementation class.  The uri's authority is used to
  determine the host, port, etc. for a filesystem.&lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;mapred.job.tracker&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;localhost:54311&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;The host and port that the MapReduce job tracker runs
  at.  If "local", then jobs are run in-process as a single map
  and reduce task.
  &lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;dfs.replication&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;1&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  &lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Этот конфигурации файл является одним из ключевых, таким образом он
выглядит для конфигурации, состоящей из одного компьютера
(позаимствован из &lt;a href="https://www.insight-it.ru/goto/f1c9004a/" rel="nofollow" target="_blank" title="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/"&gt;англоязычного мануала&lt;/a&gt;
на ту же тему).&lt;/p&gt;
&lt;h4&gt;src/contrib/hbase/conf/hbase-site.xml&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;hbase.master&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;localhost:60000&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;The host and port that the HBase master runs at&lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;property&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;name&amp;gt;&lt;/span&gt;hbase.rootdir&lt;span class="nt"&gt;&amp;lt;/name&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;value&amp;gt;&lt;/span&gt;/hbase&lt;span class="nt"&gt;&amp;lt;/value&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;description&amp;gt;&lt;/span&gt;location of HBase instance in dfs&lt;span class="nt"&gt;&amp;lt;/description&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/property&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Как не сложно заметить, этот файл необходим для функционирования
&lt;strong&gt;HBase&lt;/strong&gt;, по-моему все просто и очевидно, &lt;code&gt;&amp;lt;description&amp;gt;&lt;/code&gt; говорят
сами за себя.&lt;/p&gt;
&lt;h3 id="zapusk"&gt;Запуск&lt;/h3&gt;
&lt;p&gt;Начать стоит с ознакомления с кратким описанием доступных команд Hadoop,
сделать это можно просто набрав &lt;code&gt;~/bin/hadoop&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Usage: hadoop &lt;span class="o"&gt;[&lt;/span&gt;--config confdir&lt;span class="o"&gt;]&lt;/span&gt; COMMAND
where COMMAND is one of:
  namenode -format     format the DFS filesystem
  secondarynamenode    run the DFS secondary namenode
  namenode             run the DFS namenode
  datanode             run a DFS datanode
  dfsadmin             run a DFS admin client
  fsck                 run a DFS filesystem checking utility
  fs                   run a generic filesystem user client
  balancer             run a cluster balancing utility
  jobtracker           run the MapReduce job Tracker node
  pipes                run a Pipes job
  tasktracker          run a MapReduce task Tracker node
  job                  manipulate MapReduce &lt;span class="nb"&gt;jobs&lt;/span&gt;
&lt;span class="nb"&gt;  &lt;/span&gt;version              print the version
  jar             run a jar file
  distcp   copy file or directories recursively
  daemonlog            get/set the log level &lt;span class="k"&gt;for&lt;/span&gt; each daemon
 or
  CLASSNAME            run the class named CLASSNAME
Most commands print &lt;span class="nb"&gt;help &lt;/span&gt;when invoked w/o parameters.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Первым делом необходимо отформатировать &lt;em&gt;Namenode&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;~/bin/hadoop namenode -format
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;И дело останется лишь за малым, запустить на выполнение пару
bash-скриптов, которые без вашего дальнейшего участия &lt;em&gt;инициализируют&lt;/em&gt;
всю систему, включая HBase:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;~/bin/hadoop/start-all.sh &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; ~/src/contrib/hbase/bin/start-hbase.sh
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Как только они закончат все необходимые действия, у Вас появится
возможность удостовериться, что все в порядке. Самым простым способом
является запуск клиента &lt;em&gt;Hbase Shell&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;~/bin/src/contrib/hbase/bin/hbase shell
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Если в ответ Вы получили соответствующее приглашение клиента, значит все
было сделано верно!&lt;/p&gt;
&lt;p&gt;Вот собственно говоря и все, псевдо-кластер функционирует, доступ к
HBase имеется, можно приступать к разработке :)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;P.S.:&lt;/strong&gt; Остановка системы производится по тому же принципу скриптами
&lt;code&gt;stop-all.sh&lt;/code&gt; и &lt;code&gt;stop-hbase.sh&lt;/code&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 26 Feb 2008 00:15:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-02-26:storage/2008/hadoop-dlya-razrabotchika/</guid><category>gentoo linux</category><category>Hadoop</category><category>HBase</category><category>HDFS</category><category>MapReduce</category><category>ПО</category><category>развертывание</category><category>разработка</category><category>установка</category></item><item><title>Hadoop</title><link>https://www.insight-it.ru//storage/2008/hadoop/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/30a7481/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/"&gt;Hadoop&lt;/a&gt; представляет собой платформу
для построения приложений, способных обрабатывать огромные объемы
данных. Система основывается на распределенном подходе к вычислениям и
хранению информации, основными ее особенностями являются:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Масштабируемость:&lt;/strong&gt; с помощью &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; возможно
    надежное хранение и обработка огромных объемов данных, которые могут
    измеряться петабайтами;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Экономичность:&lt;/strong&gt; информация и вычисления распределяются по
    &lt;a href="/tag/klaster/"&gt;кластеру&lt;/a&gt;, построенному на самом обыкновенном
    оборудовании. Такой кластер может состоять из тысяч узлов;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Эффективность:&lt;/strong&gt; распределение данных позволяет выполнять их
    обработку параллельно на множестве компьютеров, что существенно
    ускоряет этот процесс;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Надежность:&lt;/strong&gt; при хранении данных возможно предоставление
    избыточности, благодаря хранению нескольких копий. Такой подход
    позволяет гарантировать отсутствие потерь информации в случае сбоев
    в работе системы;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Кроссплатформенность:&lt;/strong&gt; так как основным языком программирования,
    используемым в этой системе является &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, развернуть
    ее можно на базе любой операционной системы, имеющей &lt;abbr title="Java Virtual Machine"&gt;JVM&lt;/abbr&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;h3 id="hdfs"&gt;HDFS&lt;/h3&gt;
&lt;p&gt;В основе всей системы лежит распределенная файловая система под
незамысловатым названием &lt;strong&gt;Hadoop Distributed File System&lt;/strong&gt;.
Представляет она собой вполне стандартную распределенную файловую
систему, но все же она обладает рядом особенностей:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Устойчивость к сбоям, разработчики рассматривали сбои в оборудовании
    скорее как норму, чем как исключение;&lt;/li&gt;
&lt;li&gt;Приспособленность к развертке на самом обыкновенном ненадежном
    оборудовании;&lt;/li&gt;
&lt;li&gt;Предоставление высокоскоростного потокового доступа ко всем данным;&lt;/li&gt;
&lt;li&gt;Настроена для работы с большими файлами и наборами файлов;&lt;/li&gt;
&lt;li&gt;Простая модель работы с данными: &lt;em&gt;один раз записали - много раз
    прочли&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;Следование принципу: &lt;em&gt;переместить вычисления проще, чем переместить
    данные&lt;/em&gt;;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Архитектура HDFS&lt;/h4&gt;
&lt;p&gt;Проще всего ее демонстрирует схема,
&lt;a href="https://www.insight-it.ru/goto/9c57006b/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/core/docs/current/images/hdfsarchitecture.gif"&gt;позаимствованная&lt;/a&gt; с официального сайта проекта и переведенная мной на руский:
&lt;img alt="Архитектура HDFS" class="responsive-img" src="https://www.insight-it.ru/images/hdfsarchitecture.jpg" title="Архитектура HDFS"/&gt;&lt;/p&gt;
&lt;p&gt;Действующие лица:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Namenode&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Этот компонент системы осуществляет всю работу с метаданными. Он
должен быть запущен только на одном компьютере в кластере. Именно он
управляет размещением информации и доступом ко всем данным,
расположенным на ресурсах кластера. Сами данные проходят с остальных
машин кластера к клиенту мимо него.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Datanode&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;На всех остальных компьютерах системы работает именно этот
компонент. Он располагает сами блоки данных в локальной файловой
системе для последующей передачи или обработки их по запросу
клиента. Группы узлов данных принято называть Rack, они
используются, например, в схемах репликации данных.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Клиент&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Просто приложение или пользователь, работающий с файловой системой.
В его роли может выступать практически что угодно.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Пространство имен &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt; имеет классическую иерархическую
структуру: пользователи и приложения имеют возможность создавать
директории и файлы. Файлы хранятся в виде блоков данных произвольной (но
одинаковой, за исключением последнего; по-умолчанию 64 mb) длины,
размещенных на &lt;strong&gt;Datanode&lt;/strong&gt;'ах. Для обеспечения отказоустойчивости блоки
хранятся в нескольких экземплярах на разных узлах, имеется возможность
настройки количества копий и алгоритма их распределения по системе.
Удаление файлов происходит не сразу, а через какое-то время после
соответствующего запроса, так как после получения запроса файл
перемещается в директорию &lt;strong&gt;/trash&lt;/strong&gt; и хранится там определенный период
времени на случай если пользователь или приложение передумают о своем
решении. В этом случае информацию можно будет восстановить, в противном
случае - физически удалить.&lt;/p&gt;
&lt;p&gt;Для обнаружения возникновения каких-либо неисправностей, &lt;strong&gt;Datanode&lt;/strong&gt;
периодически отправляют &lt;strong&gt;Namenode&lt;/strong&gt;'у сигналы о своей
работоспособности. При прекращении получения таких сигналов от одного из
узлов &lt;strong&gt;Namenode&lt;/strong&gt; помечает его как &lt;em&gt;"мертвый"&lt;/em&gt;, и прекращает какой-либо
с ним взаимодействие до возвращения его работоспособности. Данные,
хранившиеся на &lt;em&gt;"умершем"&lt;/em&gt; узле реплицируются дополнительный раз из
оставшихся &lt;em&gt;"в живых"&lt;/em&gt; копий и система продолжает свое функционирование
как ни в чем не бывало.&lt;/p&gt;
&lt;p&gt;Все коммуникации между компонентами файловой системы проходят по
специальным протоколам, основывающимся на стандартном &lt;strong&gt;TCP/IP&lt;/strong&gt;.
Клиенты работают с &lt;strong&gt;Namenode&lt;/strong&gt; с помощью так называемого
&lt;strong&gt;ClientProtocol&lt;/strong&gt;, а передача данных происходит по
&lt;strong&gt;DatanodeProtocol&lt;/strong&gt;, оба они &lt;em&gt;обернуты&lt;/em&gt; в &lt;strong&gt;Remote Procedure Call
(RPC)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Система предоставляет несколько интерфейсов, среди которых командная
оболочка &lt;strong&gt;DFSShell&lt;/strong&gt;, набор ПО для администрирования &lt;strong&gt;DFSAdmin&lt;/strong&gt;, а
также простой, но эффективный веб-интерфейс. Помимо этого существуют
несколько API для языков программирования: Java API, C pipeline, WebDAV
и так далее.&lt;/p&gt;
&lt;h3 id="mapreduce"&gt;MapReduce&lt;/h3&gt;
&lt;p&gt;Помимо файловой системы, &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; включает в себя framework
для проведения масштабных вычислений, обрабатывающих огромные объемы
данных. Каждое такое вычисление называется Job (задание) и состоит оно,
как видно из названия, из двух этапов:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Map&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Целью этого этапа является представление произвольных данных (на
практике чаще всего просто пары ключ-значение) в виде промежуточных
пар ключ-значение. Результаты сортируются и групируются по ключу и
передаются на следующий этап.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Reduce&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Полученные после &lt;strong&gt;map&lt;/strong&gt; значения используются для финального
вычисления требуемых данных. Практические любые данные могут быть
получены таким образом, все зависит от требований и функционала
приложения.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Задания выполняются, подобно файловой системе, на всех машинах в
кластере (чаще всего одних и тех же). Одна из них выполняет роль
управления работой остальных - &lt;strong&gt;JobTracker&lt;/strong&gt;, остальные же ее
бесприкословно слушаются - &lt;strong&gt;TaskTracker&lt;/strong&gt;. В задачи &lt;strong&gt;JobTracker&lt;/strong&gt;'а
входит составление расписания выполняемых работ, наблюдение за ходом
выполнения, и перераспределение в случае возникновения сбоев.&lt;/p&gt;
&lt;p&gt;В общем случае каждое приложение, работающее с этим framework'ом,
предоставляет методы для осуществления этапов &lt;strong&gt;map&lt;/strong&gt; и &lt;strong&gt;reduce&lt;/strong&gt;, а
также указывает расположения входных и выходных данных. После получения
этих данных &lt;strong&gt;JobTracker&lt;/strong&gt; распределяет задание между остальными
машинами и предоставляет клиенту полную информацию о ходе работ.&lt;/p&gt;
&lt;p&gt;Помимо основных вычислений могут выполняться вспомогательные процессы,
такие как составление отчетов о ходе работы, кэширование, сортировка и
так далее.&lt;/p&gt;
&lt;h3 id="hbase"&gt;HBase&lt;/h3&gt;
&lt;p&gt;&lt;img alt="HBase Logo" class="right" src="https://www.insight-it.ru/images/hbase-logo.png" title="HBase"/&gt;
В рамках &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; доступна еще и система хранения данных,
которую правда сложно назвать &lt;a href="/tag/subd/"&gt;СУБД&lt;/a&gt; в традиционном смысле
этого слова. Чаще проводят аналогии с проприетарной системой этого же
плана от &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; - &lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/12419d3d/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/hbase"&gt;HBase&lt;/a&gt; представляет собой
распределенную систему хранения больших объемов данных. Подобно
реляционным СУБД данные хранятся в виде таблиц, состоящих из строк и
столбцов. И даже для доступа к ним предоставляется язык запросов &lt;strong&gt;HQL&lt;/strong&gt;
(как ни странно - &lt;strong&gt;Hadoop Query Language&lt;/strong&gt;), отдаленно напоминающий
более распространенный &lt;a href="/tag/sql/"&gt;SQL&lt;/a&gt;. Помимо этого предоставляется
итерирующмй интерфейс для сканирования наборов строк.&lt;/p&gt;
&lt;p&gt;Одной из основных особенностей хранения данных в &lt;strong&gt;HBase&lt;/strong&gt; является
возможность наличия нескольких значений, соответствующих одной
комбинации таблица-строка-столбец, для их различения используется
информация о времени добавления записи. На концептуальном уровне таблицы
обычно представляют как набор строк, но физически же они хранятся по
столбцам, достаточно важный факт, который стоит учитывать при разработки
схемы хранения данных. Пустые ячейки не отображаются каким-либо образом
физически в хранимых данных, они просто отсутствуют. Существуют конечно
и другие нюансы, но я постарался упомянуть лишь основные.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;HQL&lt;/strong&gt; очень прост по своей сути, если Вы уже знаете &lt;a href="/tag/sql/"&gt;SQL&lt;/a&gt;,
то для изучения его Вам понадобится лишь просмотреть по диагонали
коротенький вывод команды &lt;strong&gt;help;&lt;/strong&gt;, занимающий всего пару экранов в
консоли. Все те же &lt;strong&gt;SELECT&lt;/strong&gt;, &lt;strong&gt;INSERT&lt;/strong&gt;, &lt;strong&gt;UPDATE&lt;/strong&gt;, &lt;strong&gt;DROP&lt;/strong&gt; и так
далее, лишь со слегка измененным синтаксисом.&lt;/p&gt;
&lt;p&gt;Помимо обычно командной оболочки &lt;strong&gt;HBase Shell&lt;/strong&gt;, для работы с &lt;strong&gt;HBase&lt;/strong&gt;
также предоставлено несколько API для различных языков программирования:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/f059ad5e/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/hbase/docs/current/api/index.html"&gt;Java&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/e44fcd5/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/Jython"&gt;Jython&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8282e2e2/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/HbaseRest"&gt;REST&lt;/a&gt; и&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/185bb3f7/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/Hbase/ThriftApi"&gt;Thrift&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="zakliuchenie"&gt;Заключение&lt;/h3&gt;
&lt;p&gt;&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; является отличным решением для построения
высоконагруженных приложений, которое уже активно используется
&lt;a href="https://www.insight-it.ru/goto/ab057c2a/" rel="nofollow" target="_blank" title="http://wiki.apache.org/hadoop/PoweredBy"&gt;множеством интернет-проектов&lt;/a&gt;.
В последующих постах на эту тему я постараюсь описать процесс
развертывания этой системы и написания приложений, работающих по
принципу &lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;. Не пропустить момент их публикации
Вам может помочь подписка на &lt;a href="/feed/"&gt;RSS-ленту&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 22 Feb 2008 22:41:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-02-22:storage/2008/hadoop/</guid><category>Hadoop</category><category>HBase</category><category>HDFS</category><category>Java</category><category>MapReduce</category><category>архитектура</category><category>информационные технологии</category><category>кластер</category><category>Масштабируемость</category><category>распределенные вычисления</category><category>технология</category></item><item><title>Обзор memcached</title><link>https://www.insight-it.ru//storage/2008/obzor-memcached/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/62123c99/" rel="nofollow" target="_blank" title="http://www.danga.com/memcached/"&gt;&lt;strong&gt;memcached&lt;/strong&gt;&lt;/a&gt; представляет собой
высокопроизводительную распределенную систему кэширования объектов в
оперативной памяти.&lt;/p&gt;
&lt;p&gt;Оформлена она в виде классического &lt;a href="/tag/daemon/"&gt;daemon&lt;/a&gt;'а, слушающего
подключения на одном из TCP-портов (по-умолчанию: 11211). Работа же с
ним осуществляется с помощью клиентских библиотек, доступных практически
для всех популярных языков программирования.
&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; не использует конфигурационные файлы, но
все же может быть в какой-то степени настроен под свои нужды с помощью
параметров, указываемых при запуске &lt;a href="/tag/daemon/"&gt;daemon&lt;/a&gt;'а, и
переменных окружения. Например, часто используется параметр &lt;strong&gt;-m&lt;/strong&gt;,
позволяющий указать объем используемой для хранения объектов оперативной
памяти.
По сути кэширование с помощью &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; представляет
собой некое подобие глобального ассоциативного массива, то есть набора
соответствий &lt;em&gt;ключ &amp;rarr; объект&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id="kak-zhe-ono-rabotaet"&gt;Как же оно работает?&lt;/h3&gt;
&lt;p&gt;Принцип очень прост: после установления соединения между клиентом
(произвольное приложение, воспользовавшееся услугами одной из клиентских
библиотек) и сервером (распределенной системой, состоящей из
&lt;a href="/tag/daemon/"&gt;daemon&lt;/a&gt;'ов), клиенту предоставляется возможность выполнять
четыре примитивных действия для организации кэширования:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;set&lt;/strong&gt; - установить соответствие между ключом и указанным объектом;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;add&lt;/strong&gt; - аналогично &lt;em&gt;set&lt;/em&gt;, но только при условии, что объекта с
    таким ключом в кэше нет;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;replace&lt;/strong&gt; - абсолютная противоположность &lt;em&gt;add&lt;/em&gt;, выполняется только
    если такой объект в кэше есть;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;get&lt;/strong&gt; - получить объект из кэша по указанному ключу.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Вывод напрашивается лишь один: проще не придумаешь.&lt;/p&gt;
&lt;h3 id="v-sravnenii"&gt;В сравнении&lt;/h3&gt;
&lt;p&gt;Многие &lt;a href="/tag/subd/"&gt;СУБД&lt;/a&gt; предоставляют встроенные средства кэширования,
но на практике они умеют кэшировать только результаты запросов, что не
всегда является именно тем, что необходимо веб-приложению. СУБД обычно
полностью очищают кэш таблицы при каждом изменении данных, что приводит
к полной его бесполезности при активном обновлении таблиц.&lt;/p&gt;
&lt;p&gt;Еще один альтернативный вариант кэширования может предоставить
http-сервер, в большинстве случаев кэш дублируется несколько раз для
каждого процесса &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;, &lt;a href="/tag/perl/"&gt;Perl&lt;/a&gt; или любого другого
используемого языка программирования. Помимо излишних затрат оперативной
памяти, такой вариант развития событий еще и снижает эффективность
самого кэша.&lt;/p&gt;
&lt;h3 id="na-praktike"&gt;На практике&lt;/h3&gt;
&lt;p&gt;Использование &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; на практике в написании
приложений ничуть не сложнее, чем в теории. Например, если говорить о
&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;, то для доступа к &lt;a href="/tag/daemon/"&gt;daemon&lt;/a&gt;'y достаточно
установить соответствующий &lt;a href="https://www.insight-it.ru/goto/a0e58a5c/" rel="nofollow" target="_blank" title="http://pecl.php.net/package/memcache"&gt;PECL extension&lt;/a&gt;, который предоставит класс &lt;strong&gt;Memcached&lt;/strong&gt;. С помощью его методов осуществляется доступ ко всем возможностям &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;, о которых я уже упоминал: &lt;strong&gt;connect&lt;/strong&gt;, &lt;strong&gt;set&lt;/strong&gt;, &lt;strong&gt;add&lt;/strong&gt;, &lt;strong&gt;get&lt;/strong&gt; и так далее.&lt;/p&gt;
&lt;p&gt;Для многих других языков программирования также существуют API, список
которых можно &lt;a href="https://www.insight-it.ru/goto/94c7c37e/" rel="nofollow" target="_blank" title="http://www.danga.com/memcached/apis.bml"&gt;найти на официальном сайте&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="o-chem-ne-stoit-zabyvat"&gt;О чем не стоит забывать&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Кэш не является базой данных!&lt;/em&gt; Не стоит забывать, что кэш является
&lt;em&gt;очень&lt;/em&gt; ненадежным местом хранения данных, не предоставляет избыточности
и каких-либо гарантий, что сохраненная в нем информация будет доступна
через какое-то время. За производительность приходится платить.&lt;/p&gt;
&lt;h3 id="v-zakliuchenii"&gt;В заключении&lt;/h3&gt;
&lt;p&gt;...хотелось бы сказать, что эта &lt;a href="/tag/tekhnologiya/"&gt;технология&lt;/a&gt; является
очень производительным и эффективным решением вопроса кэширования для
масштабных интернет-проектов. Возможности по ее применению не
ограничиваются Сетью, ведь она реализована в виде обычного daemon'а, что
открывает ее для всего спектра программного обеспечения, так или иначе
следующего &lt;a href="/tag/unix-way/"&gt;"Unix&amp;nbsp;way"&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 21 Feb 2008 18:08:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-02-21:storage/2008/obzor-memcached/</guid><category>cash</category><category>cashing</category><category>daemon</category><category>Memcached</category><category>информационные технологии</category><category>кэш</category><category>кэширование</category><category>производительность</category><category>реализация</category><category>технология</category><category>unix way</category></item></channel></rss>