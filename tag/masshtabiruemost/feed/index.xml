<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Insight IT</title><link>https://www.insight-it.ru/</link><description></description><atom:link href="https://www.insight-it.ru/tag/masshtabiruemost/feed/index.xml" rel="self"></atom:link><lastBuildDate>Wed, 15 Aug 2012 22:26:00 +0400</lastBuildDate><item><title>Архитектура Pinterest</title><link>https://www.insight-it.ru//highload/2012/arkhitektura-pinterest/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/c1302f36/" rel="nofollow" target="_blank" title="http://pinterest.com"&gt;Pinterest&lt;/a&gt; - по непонятным для меня причинам
популярная в определенных кругах социальная сеть, построенная вокруг
произвольных картинок чаще всего не собственного производства. Как и
&lt;a href="https://www.insight-it.ru/highload/2012/arkhitektura-instagram/"&gt;Instagram&lt;/a&gt;
проект довольно молодой, с очень похожей историей и стеком технологий.
Тем не менее, Pinterest определенно заслуживает внимания как один из
самых быстрорастущих по посещаемости вебсайтов за всю историю.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; &lt;a href="/tag/aws/"&gt;AWS&lt;/a&gt;&amp;nbsp;- хостинг и вспомогательные
    сервисы&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/nginx/"&gt;nginx&lt;/a&gt;&amp;nbsp;- вторичная балансировка нагрузки, отдача
    статики&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt;&amp;nbsp;- язык программирования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/django/"&gt;Django&lt;/a&gt;&amp;nbsp;- фреймворк&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&amp;nbsp;- основная СУБД&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;&amp;nbsp;- кэширование объектов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/redis/"&gt;Redis&lt;/a&gt;&amp;nbsp;- кэширование коллекций объектов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/solr/"&gt;Solr&lt;/a&gt;&amp;nbsp;- поиск&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt;&amp;nbsp;- анализ данных&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;3 миллиона уникальных посетителей в день&lt;/li&gt;
&lt;li&gt;18 миллионов уникальных посетителей в месяц&lt;/li&gt;
&lt;li&gt;4-я по популярности социальная сеть в США после
    &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt;,&amp;nbsp;&lt;a href="/tag/twitter/"&gt;Twitter&lt;/a&gt;&amp;nbsp;и
    &lt;a href="/tag/linkedin/"&gt;LinkedIn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Порядка 500 виртуальных машин в EC2&lt;/li&gt;
&lt;li&gt;80 миллионов объектов в S3&lt;/li&gt;
&lt;li&gt;410Тб пользовательских данных&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="razvitie"&gt;Развитие&lt;/h2&gt;
&lt;h4&gt;Март 2010&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;1 маленький виртуальный веб-сервер&lt;/li&gt;
&lt;li&gt;1 маленький виртуальный сервер MySQL&lt;/li&gt;
&lt;li&gt;Все это в Rackspace, 1 разработчик&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Январь 2011&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;1 сервер nginx для балансировки нагрузки, 4 веб-сервера&lt;/li&gt;
&lt;li&gt;2 сервера MySQL с master/slave репликацией&lt;/li&gt;
&lt;li&gt;3 сервера для отложенного выполнения задач&lt;/li&gt;
&lt;li&gt;1 сервер MongoDB&lt;/li&gt;
&lt;li&gt;Переехали на Amazon &lt;a href="/tag/ec2/"&gt;EC2&lt;/a&gt; + &lt;a href="/tag/s3/"&gt;S3&lt;/a&gt; +
    &lt;a href="/tag/cloudfront/"&gt;CloudFront&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Осень 2011&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;2 сервера nginx, 16 веб-серверов, 2 сервера для API&lt;/li&gt;
&lt;li&gt;5 функционально разделенных серверов MySQL с 9 read slave&lt;/li&gt;
&lt;li&gt;Кластер из 4 узлов Cassandra&lt;/li&gt;
&lt;li&gt;15 серверов Membase в 3 отдельных кластерах&lt;/li&gt;
&lt;li&gt;8 серверов memcached&lt;/li&gt;
&lt;li&gt;10 серверов Redis&lt;/li&gt;
&lt;li&gt;7 серверов для отложенной обработки задач&lt;/li&gt;
&lt;li&gt;4 сервера Elastic Search&lt;/li&gt;
&lt;li&gt;3 кластера MongoDB&lt;/li&gt;
&lt;li&gt;3 разработчика&lt;/li&gt;
&lt;li&gt;Если кто-то может объяснить зачем им сдался такой зоопарк, кроме как
    потестировать разные варианты, можете взять с полки пирожок.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Зима 2011-2012&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Заменили CloudFront на &lt;a href="/tag/akamai/"&gt;Akamai&lt;/a&gt; - вполне объяснимо,
    так как у Akamai намного лучше покрытие по миру, а
    качественный&amp;nbsp;&lt;a href="/tag/cdn/"&gt;CDN&lt;/a&gt; для сайта с большим количеством
    изображений - чуть ли не залог успеха.&lt;/li&gt;
&lt;li&gt;90 веб серверов и 50 серверов для API&lt;/li&gt;
&lt;li&gt;66 + 66 MySQL серверов на m1.xlarge инстансах EC2&lt;/li&gt;
&lt;li&gt;59 серверов Redis&lt;/li&gt;
&lt;li&gt;51 серверов memcached&lt;/li&gt;
&lt;li&gt;25+1 сервер для отложенной обработки задач на основе Redis&lt;/li&gt;
&lt;li&gt;Кластеризованный Solr&lt;/li&gt;
&lt;li&gt;6 разработчиков&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Весна-лето 2012&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Снова сменили CDN, на этот раз в пользу ранее неизвестного мне &lt;a href="/tag/edge-cast/"&gt;Edge
    Cast&lt;/a&gt;. Покрытие по всему миру довольно скромное,
    так что единственное логичное объяснение, которое мне приходит в
    голову - не потянули Akamai по деньгам.&lt;/li&gt;
&lt;li&gt;135 веб серверов и 75 серверов для API&lt;/li&gt;
&lt;li&gt;80 + 80 серверов MySQL&lt;/li&gt;
&lt;li&gt;110 серверов Redis&lt;/li&gt;
&lt;li&gt;60 серверов memcached&lt;/li&gt;
&lt;li&gt;60 + 2&amp;nbsp;сервера&amp;nbsp;для отложенной обработки задач на основе Redis&lt;/li&gt;
&lt;li&gt;25 разработчиков&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="vybor"&gt;Выбор&lt;/h2&gt;
&lt;h4&gt;Почему Amazon Ec2/S3?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Очень хорошая надежность, отчетность и поддержка&lt;/li&gt;
&lt;li&gt;Хорошие дополнительные сервисы: кэш, базы данных, балансировка
    нагрузки, &lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt; и т.п.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Новые виртуальные машины готовы за считанные секунды&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Почему MySQL?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Очень "зрелая", хорошо известная и любимая многими&lt;/li&gt;
&lt;li&gt;Редки катастрофичные потери данных&lt;/li&gt;
&lt;li&gt;Линейная зависимость времени отклика от частоты запросов&lt;/li&gt;
&lt;li&gt;Хорошая поддержка сторонним ПО (XtraBackup, Innotop, Maatkit)&lt;/li&gt;
&lt;li&gt;Надежное активное сообщество&lt;/li&gt;
&lt;li&gt;Отличная поддержка от Percona&lt;/li&gt;
&lt;li&gt;Бесплатна&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Почему memcached?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Очень "зрелый", отличная производительность, хорошо известный и
    любимый многими&lt;/li&gt;
&lt;li&gt;Никогда не ломается&lt;/li&gt;
&lt;li&gt;Бесплатен&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Почему Redis?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Много удобных &lt;strong&gt;структур данных&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Поддержка персистентности и репликации&lt;/li&gt;
&lt;li&gt;Также многим известен и нравится&lt;/li&gt;
&lt;li&gt;Стабильно хорошая производительность и надежность&lt;/li&gt;
&lt;li&gt;Также бесплатен&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;h4&gt;Сlustering vs Sharding&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Большую часть презентации, на основе которой написана данная статья
    (&lt;a href="https://www.insight-it.ru/goto/10005efe/" rel="nofollow" target="_blank" title="http://www.slideshare.net/eonarts/mysql-meetup-july2012scalingpinterest"&gt;ссылка&lt;/a&gt;,
    если не охота листать до секции источников информации), занимает
    раздел под названием "Clustering vs Sharding". В связи с путаницей в
    терминологии пришлось несколько раз перечитывать, чтобы понять к
    чему они клонят, сейчас попробую объяснить.&lt;/li&gt;
&lt;li&gt;Вообще есть два фундаментальных способа распределить данные между
    несколькими серверами:&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Вертикально:&lt;/strong&gt;&amp;nbsp;разные таблицы (или просто логически разные
    типы данных) разносятся на разные сервера.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Горизонтально:&lt;/strong&gt; каждая таблица разбивается на некоторое
    количество частей и эти части разносятся на разные сервера по
    определенному алгоритму.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;С первого взгляда казалось, что они пытаются вертикальное разбиение
    назвать &lt;em&gt;sharding&lt;/em&gt;, а горизонтальное - &lt;em&gt;clustering&lt;/em&gt;. Хотя вообще они
    почти синонимы и на русский я их обычно примерно одинаково перевожу.&lt;/li&gt;
&lt;li&gt;По факту же оказалось, что под словом clustering они понимают все
    программные продукты для хранения данных, которые имеют встроенную
    поддержку работы в кластере. В частности они имеют ввиду
    &lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt;, &lt;a href="/tag/membase/"&gt;Membase&lt;/a&gt;,
    &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt; и &lt;a href="/tag/riak/"&gt;Riak&lt;/a&gt;, которые прозрачно для
    пользователя горизонтально распределяют данные по кластеру.&lt;/li&gt;
&lt;li&gt;За словом &lt;em&gt;sharding&lt;/em&gt; в их терминологии стоит аналогичная схема
    собственной разработки, использующая огромное количество логических
    БД в MySQL, распределенных между меньшим количеством &lt;del&gt;физических серверов&lt;/del&gt; виртуальных машин. Именно по этому пути и пошли в
    Pinterest, плюс очень похожий подход используется в
    &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-facebook/"&gt;Facebook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;От себя добавлю, что хоть при наличии должных ресурсов разработка
    собственной системы распределения данных и может быть
    целесообразной, в большинстве случаев на начальном этапе проще
    основываться на готовых решениях вроде перечисленных выше. К слову в
    opensource доступны и основанные на MySQL подобные решения:&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/vitess/"&gt;Vitess&lt;/a&gt; от &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; /
    &lt;a href="/tag/youtube/"&gt;YouTube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/flockdb/"&gt;FlockDB&lt;/a&gt; от &lt;a href="/tag/twitter/"&gt;Twitter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;В их проекте данная подсистема развивалась следующим образом:&lt;ul&gt;
&lt;li&gt;1 БД + внешние ключи + join'ы&amp;nbsp;&amp;rarr;&lt;/li&gt;
&lt;li&gt;1 БД + денормализация + кэш&amp;nbsp;&amp;rarr;&lt;/li&gt;
&lt;li&gt;1 БД + master/slave + кэш&amp;nbsp;&amp;rarr;&lt;/li&gt;
&lt;li&gt;несколько функциональных разделенных БД + master/slave + кэш&amp;nbsp;&amp;rarr;&lt;/li&gt;
&lt;li&gt;вертикально и горизонтально разделенные БД (по
    идентификаторам) + по резервные БД (пассивный slave) + кэш&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;При использовании аналогичного решения остерегайтесь:&lt;ul&gt;
&lt;li&gt;Невозможности выполнять большинство запросов с join&lt;/li&gt;
&lt;li&gt;Отсутствия транзакций&lt;/li&gt;
&lt;li&gt;Дополнительных манипуляций для поддержания ограничений
    уникальности&lt;/li&gt;
&lt;li&gt;Необходимости тщательного планирования для изменений схемы&lt;/li&gt;
&lt;li&gt;Необходимости выполнения одного и того же запроса с последующей
    агрегацией для построения отчетов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Остальные моменты&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Кэширование многоуровневое:&lt;ul&gt;
&lt;li&gt;Коллекции объектов хранятся в списках Redis&lt;/li&gt;
&lt;li&gt;Сами объекты - в memcached&lt;/li&gt;
&lt;li&gt;На уровне SQL запросы в основном примитивны и написаны вручную,
    так что часты попадания в кэш MySQL&lt;/li&gt;
&lt;li&gt;Кэш файловой системы - само собой&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Еще пара фактов про кэширование в Pinterest:&lt;ul&gt;
&lt;li&gt;Кэш разбит также на несколько частей (шардов), для упрощения
    обслуживания и масштабирования&lt;/li&gt;
&lt;li&gt;В коде для кэширования используются Python'овские декораторы, на
    вид собственной разработки, хотя точно не уверен&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Балансировка нагрузки осуществляется в первую очередь за счет Amazon
    ELB, что позволяет легко подключать/отключать новые сервера
    посредством API.&lt;/li&gt;
&lt;li&gt;Так как большинство пользователей живут в США по ночам нагрузка
    сильно падает, что позволяет им по ночам отключать до 40%
    виртуальных машин. В пиковые часы EC2 обходится порядка 52$ в час,
    а по ночам - всего 15$.&lt;/li&gt;
&lt;li&gt;Elastic Map Reduce, основанный на Hadoop, используется для анализа
    данных и стоит всего несколько сотен долларов в месяц&lt;/li&gt;
&lt;li&gt;Текущие проблемы:&lt;ul&gt;
&lt;li&gt;Масштабирование команды&lt;/li&gt;
&lt;li&gt;Основанная на сервисах архитектура:&lt;ul&gt;
&lt;li&gt;Ограничения соединений&lt;/li&gt;
&lt;li&gt;Изоляция функционала&lt;/li&gt;
&lt;li&gt;Изоляция доступа (безопасность)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="uroki-ot-komandy-pinterest"&gt;Уроки от команды Pinterest&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;"Оно сломается. Все должно быть просто."&lt;/em&gt; - столько раз уже слышу
    это наставление, но ни разу не видел разработчиков, которые реально
    к нему прислушивались.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;"Кластеризация - страшная штука."&lt;/em&gt; - конечно страшная, большая и
    сложная. Но кому сейчас легко?&lt;/li&gt;
&lt;li&gt;&lt;em&gt;"Продолжайте получать удовольствие."&lt;/em&gt; - с этим не могу не
    согласиться, без удовольствия работать совершенно невозможно в любой
    сфере.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/10005efe/" rel="nofollow" target="_blank" title="http://www.slideshare.net/eonarts/mysql-meetup-july2012scalingpinterest"&gt;Scaling Pinterest @ MySQL Meetup&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;В презентации можно посмотреть примеры кода и SQL-запросов&lt;/li&gt;
&lt;li&gt;Если кто-то знает где можно посмотреть/послушать запись этого
    мероприятия - поделитесь ссылкой, пожалуйста&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/ac4a03d6/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2012/5/21/pinterest-architecture-update-18-million-visitors-10x-growth.html"&gt;Pinterest Architecture Update&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/db0647b0/" rel="nofollow" target="_blank" title="http://pinterest.com/about/careers/"&gt;Вакансии в Pinterest&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 15 Aug 2012 22:26:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-08-15:highload/2012/arkhitektura-pinterest/</guid><category>Akamai</category><category>Amazon</category><category>Apache Hadoop</category><category>AWS</category><category>CDN</category><category>CloudFront</category><category>django</category><category>EC2</category><category>Edge Cast</category><category>Hadoop</category><category>Memcached</category><category>MySQL</category><category>nginx</category><category>Pinterest</category><category>Python</category><category>Redis</category><category>S3</category><category>Solr</category><category>Архитектура Pinterest</category><category>архиткутера</category><category>Масштабируемость</category></item><item><title>Архитектура интерактивных сайтов</title><link>https://www.insight-it.ru//interactive/2012/arkhitektura-interaktivnykh-sajjtov/</link><description>&lt;p&gt;В &lt;a href="https://www.insight-it.ru/interactive/2012/anons-serii-statejj-interaktivnye-sajjty/"&gt;анонсе&lt;/a&gt;&amp;nbsp;серии статей &lt;a href="https://www.insight-it.ru/interactive/"&gt;"Интерактивные сайты"&lt;/a&gt; я постарался максимально доходчиво изложить свою мотивацию к ей созданию, да и актуальность самой темы, так что сразу к делу!
&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Итак, мы хотим сделать так, чтобы с точки зрения &lt;em&gt;пользователя&lt;/em&gt; наш сайт
выглядел &lt;strong&gt;интерактивным&lt;/strong&gt;. То есть воспринимался не как набор отдельно
загружающихся страниц, а скорее как обычное приложение для компьютера.
Ему, в целом, не важно как именно мы этого добьемся, главное чтобы при
этом браузер вел себя как обычно и визуально все реагировало почти
мгновенно.&lt;/p&gt;
&lt;p&gt;Сразу хочу предупредить, что далеко не всем сайтам такая
глобальная&amp;nbsp;&lt;em&gt;интерактивность&lt;/em&gt; пойдет на пользу. Если пользователи сайта
редко что-либо делают и большую часть времени читают длинные тексты, то,
вероятно, этот проект как раз из этой категории: быстрота реакции на
клик не так радует, когда большую часть времени приходится работать
скроллом. Если Ваш интернет-проект является сайтом с длинным контентом в
формате блога, новостей или вики - подумайте лишний раз, прежде чем
переделывать весь сайт, вероятно будет достаточно интерактивных
комментариев и голосований на &lt;a href="/tag/ajax/"&gt;AJAX&lt;/a&gt;. Для социальных сетей,
сайтов знакомств, интернет-магазинов, поисковых систем, корпоративных
сайтов и многих других визуально заметная&amp;nbsp;&lt;em&gt;интерактивность&lt;/em&gt;&amp;nbsp;определенно
станет одним из ключевых &lt;strong&gt;конкурентных преимуществ&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;В этом посте я постараюсь нарисовать крупными мазками картину того, как
этого можно достичь. Визуально её я представил следующим образом:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Архитектура интерактивного сайта" class="responsive-img" src="https://www.insight-it.ru/images/interactive-site-architecture.jpeg" title="Архитектура интерактивного сайта"/&gt;&lt;/p&gt;
&lt;h2 id="obshchie-zamechaniia"&gt;Общие замечания&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Здесь и далее я буду описывать "среднестатистический" интернет-проект, в
зависимости от специфики могут появляться дополнительные компоненты или
убираться за ненадобностью упомянутые. Например, если известно, что
пользоваться сайтом будут только сотрудники какой-то компании, то помимо
замены Интернета на Интранет, можно запросто избавиться и от
&lt;strong&gt;HTML-интерфейса&lt;/strong&gt; и всего, что с ним связано - никаким роботам доступ
к нему не нужен.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Наверное стоит прямым текстом сказать, что:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Голубые элементы - компоненты проекта, а серые - внешние.&lt;/li&gt;
&lt;li&gt;Связи в виде прямых линий означают&amp;nbsp;двусторонний&amp;nbsp;обмен данными,
транспорт и формат пока особо не важны.&lt;/li&gt;
&lt;li&gt;Схема логическая, так что вопросы балансировки нагрузки,
отказоустойчивости, репликации и т.п. остались в стороне; за каждым
блоком может стоять произвольное количество серверов.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Я решил не загромождать схему мониторингом, резервным копированием,
почтой, сервисом доменов и прочими хоть и важными, но не связанными
напрямую с темой компонентами системы.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;В этом посте не будет никаких конкретных примеров реализации и
технологий, оставим это на &lt;a href="https://www.insight-it.ru/interactive/"&gt;следующие статьи&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="klientskaia-chast"&gt;Клиентская часть&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Начнем наше путешествие. Пользователь вводит в адресной строке браузера
адрес нашего сайта и жмет Enter, инициируя тем самым сначала определение
IP-адреса через DNS, а затем и HTTP-запрос к нашему &lt;strong&gt;HTML-интерфейсу&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Получив в ответ &lt;strong&gt;страницу&lt;/strong&gt; в формате &lt;a href="/tag/html/"&gt;HTML&lt;/a&gt; браузер
начинает загружать указанные в нем внешние ресурсы, в том числе и
&lt;strong&gt;Javascript-клиент&lt;/strong&gt;, которому и передается слово. Сама страница
параллельно отрисовывается как и в статичном сайте.&lt;/li&gt;
&lt;li&gt;Как уже упоминалось выше, некоторые проекты решают не тратить силы на
поддержку двух интерфейсов к сайту, жертвуя тем самым доступом
большинства роботов и браузеров без поддержки
&lt;a href="/tag/javascript/"&gt;JavaScript&lt;/a&gt;. Стартовый HTML-документ в этом случае
превращается просто в практически пустой статичный файл, который служит
лишь для загрузки клиента.&lt;/li&gt;
&lt;li&gt;Так как наша цель стоит в интерактивном взаимодействии пользователем,
повторять эти действия при каждом переходе на другую страницу -
&lt;em&gt;непозволительная роскошь&lt;/em&gt;. Кстати, можно начинать думать и общаться не
в терминах страниц, а в терминах &lt;strong&gt;экранов&lt;/strong&gt;, которые видит
пользователь.&lt;/li&gt;
&lt;li&gt;Для обеспечения этого&amp;nbsp;&lt;strong&gt;JavaScript-клиент&lt;/strong&gt; должен переопределить
стандартные обработчики событий перехода по внутренним ссылкам сайта и
отправки форм. Ему нужно отменить стандартный механизм перехода на
другую страницу и вместо него отправить запрос через &lt;strong&gt;интерфейс
сериализованных данных&lt;/strong&gt;. На основе полученного в ответ сообщения он
меняет какую-то часть загруженного ранее HTML-документа, чтобы он
соответствовал тому &lt;em&gt;экрану&lt;/em&gt;, который ожидал увидеть пользователь. В
итоге пользователь видит в браузере ровно ту же картинку, как если бы он
ввел тот адрес, на который вела нажатая ссылка, или просто нажал на нее
с отключенным JavaScript.&lt;/li&gt;
&lt;li&gt;Важно не сломать при этом поведение браузера: кнопка "назад" должна
работать как обычно, а в адресной строке должны меняться ссылки (это
актуально, например, когда пользователь отправляет содержимое адресной
строки кому-то по почте или через мессенджер).&lt;/li&gt;
&lt;li&gt;При ожидании ответа от сервера стоит эмулировать курсор и иконку
загрузки страницы, чтобы пользователь &lt;em&gt;не паниковал&lt;/em&gt; в случае (пускай и
редком) визуально заметных задержек.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;На резонный вопрос &lt;em&gt;"Почему, собственно, этот трюк обеспечит
интерактивность?"&lt;/em&gt;, ответ хоть и не всегда однозначен, но он все же
есть:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Сериализованные &lt;strong&gt;изменения&lt;/strong&gt; страницы занимают меньший объем, чем
&lt;strong&gt;полный&lt;/strong&gt; HTML со всеми связанными ресурсами, заголовками, версткой
и прочим - &lt;em&gt;значительно меньше данных нужно передавать по сети&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Как правило, есть возможность держать &lt;strong&gt;постоянное&lt;/strong&gt; соединение
между браузером и интерфейсом сериализованных данных, что позволяет
&lt;em&gt;не делать лишние HTTP-запросы&lt;/em&gt;. Обратная сторона медали - это самое
соединение постоянно же использует часть серверных ресурсов, но есть
способы минимизировать эти издержки.&lt;/li&gt;
&lt;li&gt;Для некоторых действий изменения HTML не требуют ответа сервера и
могут быть сделаны параллельно с отправкой запроса (например
&amp;nbsp;различные вариации на тему +1 или написание комментария, текст
которого можно взять из формы).&lt;/li&gt;
&lt;li&gt;Как правило, можно предсказать наиболее вероятные переходы по
экранам и &lt;em&gt;загрузить необходимые изменения заранее&lt;/em&gt;. Хотя этот
вопрос скорее из области оптимизации.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Таким образом, в большинстве случаев есть техническая возможность
снизить время отклика &amp;nbsp;на действие пользователя с 500-2000 мс в
случае неплохо сделанного статического сайта до 20-200 мс., что
вполне сопоставимо с откликом десктопного приложения.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Как все это сделать на практике - тема следующей статьи из серии.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="servernaia-chast"&gt;Серверная часть&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;С серверной точки зрения основным отличием является четкое разделение
двух входных точек:&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;HTML-интерфейс&lt;/strong&gt;&amp;nbsp;отдает готовые документы в ответ на HTTP-запросы.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Интерфейс сериализованных данных&lt;/strong&gt;&amp;nbsp;использует какое-то постоянное
соединение, хотя в некоторых случаях целесообразно ограничиться
просто асинхронными HTTP-запросами.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Если для статичных сайтов полное &lt;em&gt;выделение бизнес-логики в отдельные
сервисы&lt;/em&gt; - просто хорошее архитектурное решение, то для интерактивных
сайтов - это практически &lt;strong&gt;необхохимо&lt;/strong&gt;. Иначе придется реализовывать и
поддерживать две копии кода для каждого интерфейса и надеяться, что они
постоянно будут оставаться совместимыми и выдавать одинаковый результат.&lt;/li&gt;
&lt;li&gt;Хорошей практикой является использование какого-то одного протокола
общения между компонентами системы, в частности пользовательских
интерфейсов с сервисами бизнес-логики и последних друг с другом.
Желательно использовать что-то бинарное и с поддержкой разных языков
программирования, хотя если весь проект на одной платформе и не
планирует это менять - можно использовать и стандартный для этой
платформы протокол.&lt;/li&gt;
&lt;li&gt;Чтобы не включать элементы верстки при передаче через интерфейс
сериализованных данных, рекомендую использовать кроссплатформенный
формат HTML-шаблонов. Об этом будет отдельная статья.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Интерфейс сериализованных данных&lt;/strong&gt; при необходимости легко может быть
адаптирован для использования в роли &lt;em&gt;API для сторонних сервисов или
собственных приложений&lt;/em&gt; для мобильных платформ или настольных
компьютеров.&lt;/li&gt;
&lt;li&gt;В целом внутренние сервисы общаются с остальными располагающимися на
серверной части компонентами системы вполне обычным образом.&lt;/li&gt;
&lt;li&gt;В статье про серверную часть подробно будет рассматриваться
использование брокера сообщений для уведомлений пользователей в реальном
времени.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Глобальная интерактивность&lt;/em&gt; сайта требует использования достаточно
    сложного и комплексного JavaScript-клиента и создания
    дополнительного более легковесного внешнего инетрфейса на серверной
    части.&lt;/li&gt;
&lt;li&gt;По-настоящему &lt;em&gt;мгновенной&lt;/em&gt; реакцией сайта смогут насладиться лишь
    пользователи с &lt;strong&gt;современным браузером&lt;/strong&gt; и относительно &lt;strong&gt;широким
    интернет-каналом&lt;/strong&gt;. Из-за возможных сетевых задержек или
    особенностей устаревших браузеров эффект мгновенного перехода все же
    может теряться, но при должном тестировании реально добиться
    нормального поведения сайта и в таких ситуациях. Хотя зачастую
    "проваливание" до обычного режима статичных страниц в подобных
    ситуациях - вполне резонное решение.&lt;/li&gt;
&lt;li&gt;Архитектура серверной части проекта в большинстве случаев не требует
    существенных изменений. Хотя если в ней все было хаотично и не
    продумано, то создание интерактивного клиента может послужить
    неплохим поводом пересмотреть и привести в порядок и её.&lt;/li&gt;
&lt;li&gt;Кроме очевидной потребности в использовании JavaScript для клиента,
    особых ограничений на используемые технологии и языки
    программирования, обсуждаемая схема не накладывает.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="card green"&gt;
&lt;p&gt;&lt;div class="card-content white-text"&gt;
Эта статья - первая в &lt;a class="green-text text-lighten-4" href="https://www.insight-it.ru/interactive/"&gt;серии про Интерактивные сайты&lt;/a&gt;, автор - &lt;a class="green-text text-lighten-4" href="https://www.insight-it.ru/goto/b03d9116/" rel="nofollow" target="_blank" title="http://blinkov.ru"&gt;Иван&amp;nbsp;Блинков&lt;/a&gt;, основано на личном опыте.
До встречи &lt;a class="green-text text-lighten-4" href="/feed/"&gt;на страницах Insight IT&lt;/a&gt;!
&lt;/div&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 04 Apr 2012 01:05:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-04-04:interactive/2012/arkhitektura-interaktivnykh-sajjtov/</guid><category>архитектура</category><category>Интерактивные сайты</category><category>клиентская часть</category><category>Масштабируемость</category><category>серверная часть</category><category>схема</category></item><item><title>Анонс серии статей: интерактивные сайты</title><link>https://www.insight-it.ru//interactive/2012/anons-serii-statejj-interaktivnye-sajjty/</link><description>&lt;p&gt;Интернет развивается огромными темпами. В борьбе за аудиторию крупные
интернет-компании поднимают стандарты качества веб-приложений на все
более и более высокий уровень. Одним из важнейших качеств современных
сайтов является &lt;strong&gt;интерактивность&lt;/strong&gt;, если раньше все они поголовно
представляли собой коллекцию статичных страниц, где можно что-то
почитать или посмотреть, то сегодня они - почти живой организм.&lt;/p&gt;
&lt;p&gt;Пользователи все больше привыкают узнавать о событиях и видеть реакцию
на свои действия мгновенно, не дожидаясь загрузок страниц и прочих
задержек. Раньше это было возможно только для обычных приложений, но с
сегодняшним уровнем технологий общаться с пользователем в реальном
времени можно и &lt;strong&gt;посредством браузера&lt;/strong&gt;, причем доступно это не только
интернет-гигантам, а практически любому интернет-проекту.&lt;/p&gt;
&lt;p&gt;За последний год &lt;em&gt;привнесение интерактивности в интернет-проекты&lt;/em&gt; -
пожалуй, одна из самых популярных тем, с которой ко мне &lt;a href="https://www.insight-it.ru/consulting/"&gt;обращаются за консультацией&lt;/a&gt;. В итоге я решил
не жадничать и поделиться с общественностью своими знаниями в этой
области, что в итоге должно вылиться в серию связанных статей
&lt;strong&gt;"Интерактивные сайты"&lt;/strong&gt;. В ней я хочу отразить практически пошаговую
инструкцию от А до Я для создания интерактивного интернет-приложения с
нуля или основываясь на существующем статичном проекте. Соответственно,
по ходу дела сделаю легко доступное оглавление по аналогии с
&lt;a href="https://www.insight-it.ru/highload/"&gt;архитектурой высоконагруженных интернет-проектов&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="orientirovochnye-temy-statei"&gt;Ориентировочные темы статей&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/interactive/2012/arkhitektura-interaktivnykh-sajjtov/"&gt;Общая архитектура&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/interactive/2012/klientskaya-chast-interaktivnogo-sajjta/"&gt;Организация клиентской части&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/interactive/2012/postoyannoe-soedinenie-mezhdu-brauzerom-i-serverom/"&gt;Постоянное соединение между браузером и сервером&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/interactive/2012/povtornoe-ispolzovanie-shablonov/"&gt;Повторное использование шаблонов&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/interactive/2012/servernaya-chast-interaktivnogo-sajjta-i-potoki-soobshhenijj/"&gt;Серверная часть интерактивного сайта и потоки сообщений&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/interactive/2012/optimizaciya-interaktivnykh-sajjtov/"&gt;Оптимизация&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Основной упор будет сделан именно на общую концепцию и сведение всех
компонентов воедино, как на серверной стороне, так и на клиентской.
Количество изобретаемых велосипедов постараюсь свести к минимуму: где-то
будут просто рекомендации по использованию публично доступных
технологий, где-то - сравнительные обзоры. Специфики каких-либо
определенных типов проектов постараюсь избегать.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Пожелания, предложения, советы и вопросы в комментариях к этому посту
очень приветствуются :)&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 01 Apr 2012 17:43:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-04-01:interactive/2012/anons-serii-statejj-interaktivnye-sajjty/</guid><category>JavaScript</category><category>архитектура</category><category>интерактив</category><category>интернет-приложения</category><category>клиентская оптимизация</category><category>Масштабируемость</category><category>сайты</category><category>технологии</category></item><item><title>Архитектура Google 2011</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-google-2011/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-google/"&gt;Архитектура Google&lt;/a&gt;
была одной из первых статьей на &lt;strong class="trebuchet"&gt;Insight IT&lt;/strong&gt;. Именно она дала толчок развитию проекта: после её публикации посещаемость блога увеличилась в десятки раз и появились первые сотни подписчиков. Прошли годы, информация устаревает стремительно, так что пришло время взглянуть на Google еще раз, теперь уже с позиции конца 2011 года. Что мы увидим нового в архитектуре интернет-гиганта?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Общее&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Ежедневная аудитория Google составляет около 1 миллиарда
    человек&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;По данным Alexa больше половины аудитории интернета каждый
    день пользуются Google&lt;/li&gt;
&lt;li&gt;По данным IWS аудитория интернета составляет 2.1 миллиарда
    человек&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Используется более 900 тысяч серверов&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;Планируется расширение до 10 миллионов серверов в обозримом
    будущем&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/4cb67b65/" rel="nofollow" target="_blank" title="http://www.google.com/about/datacenters/locations/index.html"&gt;12 основных датацентров в США&lt;/a&gt;,
    присутствие в большом количестве точек по всему миру (более 38)&lt;/li&gt;
&lt;li&gt;Около 32 тысяч сотрудников в 76 офисах по всему миру&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Поиск&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;За последние 14 лет среднее время обработки одного поискового
    запроса уменьшилось с 3 секунд до менее 100 миллисекунд, то есть
    в 30 раз&lt;/li&gt;
&lt;li&gt;Более 40 миллиардов страниц в индексе, если приравнять каждую к
    листу А4 они бы покрыли территорию США в 5 слоев&lt;/li&gt;
&lt;li&gt;Более 1 квинтиллиона уникальных URL (10 в 18 степени); если
    распечатать их в одну строку, её длина составит 51 миллион
    километров, треть расстояния от Земли до Солнца&lt;/li&gt;
&lt;li&gt;В интернете встречается примерно 100 квинтиллионов слов, чтобы
    набрать их на клавиатуре одному человеку потребовалось бы
    примерно 5 миллионов лет&lt;/li&gt;
&lt;li&gt;Проиндексировано более 1.5 миллиардов изображений, чтобы их
    сохранить потребовалось бы 112 миллионов дискет, которые можно
    сложить в стопку высотой 391 километр&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gmail&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Активных пользователей более 170 миллионов&lt;/li&gt;
&lt;li&gt;Второй по популярности почтовый сервис в США, третий в мире (по
    данным comScore)&lt;/li&gt;
&lt;li&gt;При текущем темпе роста аудитории GMail и конкурентов, он станет
    лидером рынка через 2-3 года&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Google+&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Более 40 миллионов пользователей на октябрь 2011, при запуске в
    июне 2011&lt;/li&gt;
&lt;li&gt;25 миллионов пользователей за первый месяц&lt;/li&gt;
&lt;li&gt;70:30 примерное соотношение мужчин и женщин&lt;/li&gt;
&lt;li&gt;Себестоимость разработки больше полумиллиарда долларов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;YouTube&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Загружается более 13 миллионов часов видео в год&lt;/li&gt;
&lt;li&gt;Каждую минуту загружается 48 часов видео, что соответствует
    почти 8 годам контента или 52 тысячам полнометражных фильмов в
    день&lt;/li&gt;
&lt;li&gt;Более 700 миллиардов просмотров видео в год&lt;/li&gt;
&lt;li&gt;Месячная аудитория составляет 800 миллионов уникальных
    посетителей&lt;/li&gt;
&lt;li&gt;Несколько тысяч полнометражных фильмов в &lt;a href="https://www.insight-it.ru/goto/18cd7d94/" rel="nofollow" target="_blank" title="http://www.youtube.com/movies"&gt;YouTube Movies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Более 10% всех видео в формате HD&lt;/li&gt;
&lt;li&gt;13% просмотров (400 миллионов в день) происходит с мобильных
    устройств&lt;/li&gt;
&lt;li&gt;До сих пор работает в убыток, лишь 14% просмотров видео приносят
    выручку с рекламы&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Финансы&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Выручка порядка 36 миллиардов долларов в год&lt;/li&gt;
&lt;li&gt;Прибыль после налогов порядка 10 миллиардов долларов в год&lt;/li&gt;
&lt;li&gt;Капитализация порядка 200 миллиардов долларов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Google&lt;/strong&gt; - огромная интернет-компания, неоспоримый лидер на рынке
поиска в Интернет и владелец большого количества продуктов, многие из
которых также добились определенного успеха в своей нише.&lt;/p&gt;
&lt;p&gt;В отличии от большинства интернет-компаний, которые занимаются лишь
одним продуктом (проектом), архитектура Google не может быть
представлена как единое конкретное техническое решение. Сегодня мы
скорее будем рассматривать общую стратегию технической реализации
интернет-проектов в Google, возможно слегка затрагивая и другие аспекты
ведения бизнеса в Интернет.&lt;/p&gt;
&lt;p&gt;Все продукты Google основываются на постоянно развивающейся программной
платформе, которая спроектирована с учетом работы на миллионах серверов,
находящихся в разных датацентрах по всему миру.&lt;/p&gt;
&lt;h3 id="oborudovanie"&gt;Оборудование&lt;/h3&gt;
&lt;p&gt;Обеспечение работы миллиона серверов и расширение их парка - одна из
ключевых статей расходов Google. Для минимизации этих издержек очень
большое внимание уделяется эффективности используемого серверного,
сетевого и инфраструктурного оборудования.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Использование энергии" class="right" src="https://www.insight-it.ru/images/dc-home-energy-use.png" title="Использование энергии"/&gt;&lt;/p&gt;
&lt;p&gt;В традиционных датацентрах потребление электричества серверами примерно
равно его потреблению остальной инфраструктурой, Google же удалось
снизить процент использования дополнительной электроэнергии до 14%.
Таким образом суммарное энергопотребление датацентром Google сравнимо с
потреблением только серверов в типичном датацентре и вдвое меньше его
общего энергопотребления. Основные концепции, которые используются для
достижения этого результата:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Точное измерение потребления электроэнергии всеми компонентами
    позволяет определить возможности по его уменьшению;&lt;/li&gt;
&lt;li&gt;В датацентрах Google тепло, что позволяет экономить на охлаждении;&lt;/li&gt;
&lt;li&gt;При проектировании датацентра уделяется внимание даже незначительным
    деталям, позволяющим сэкономить даже немного - при таком масштабе
    это окупается;&lt;/li&gt;
&lt;li&gt;Google умеет охлаждать датацентры практически без кондиционеров, с
    использованием воды и её испарения &lt;em&gt;(см. &lt;a href="https://www.insight-it.ru/goto/d1cf5d6b/" rel="nofollow" target="_blank" title="http://www.youtube.com/watch?v=VChOEvKicQQ"&gt;как это реализовано&lt;/a&gt; в Финляндии)&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В Google активно&amp;nbsp;пропагандируют максимальное использование
возобновляемой энергии. Для этого заключаются долгосрочные соглашения с
её поставщиками (на 20 и более лет), что позволяет отрасли активно
развиваться и наращивать мощности. Проекты по генерации возобновляемой
энергии, спонсируемые Google, имеют суммарную мощность более 1.7
гигаватт, что существенно больше, чем используется для работы Google.
Этой мощности хватило бы для обеспечения электричеством 350 тысяч домов.&lt;/p&gt;
&lt;p&gt;Если говорить о жизненном цикле оборудования, то используются следующие
принципы:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Уменьшение транспортировки:&lt;/strong&gt; там, где это возможно, тяжелые
    компоненты (вроде серверных стоек) закупаются у местных поставщиков,
    даже если в других местах аналогичный товар можно было бы купить
    дешевле.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Повторное использование:&lt;/strong&gt; прежде, чем покупать новое оборудование
    и материалы, рассматриваются возможности по использованию уже
    имеющихся. Этот принцип помог избежать покупки более 90 тысяч новых
    серверов.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Утилизация&lt;/strong&gt;: в тех случаях, когда повторное использование
    невозможно, оборудование полностью очищается от данных и продается
    на вторичном рынке. То, что не удается продать, разбирается на
    материалы (медь, сталь, алюминий, пластик и.т.п.) для последующей
    правильной утилизации специализированными компаниями.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Google известны за свои эксперименты и необычные решения в области
серверного оборудования и инфраструктуры. Некоторые запатентованы;
какие-то прижились, какие-то - нет. Подробно останавливаться на них не
буду, лишь вкратце о некоторых:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Резервное питание, интегрированное в блок питания
    &lt;a href="https://www.insight-it.ru/goto/ca5b8b43/" rel="nofollow" target="_blank" title="http://www.youtube.com/watch?v=xgRWURIxgbU"&gt;сервера&lt;/a&gt;, обеспеченное
    стандартными 12V батарейками;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/c7f3ff41/" rel="nofollow" target="_blank" title="http://www.datacenterknowledge.com/closer-look-googles-server-sandwich-design/"&gt;"Серверный сендвич"&lt;/a&gt;,
    где материнские платы с двух сторон окружают водяную систему
    теплоотвода в центре стойки;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8dc33e81/" rel="nofollow" target="_blank" title="http://www.youtube.com/watch?v=zRwPSFpLX8I"&gt;Датацентр из контейнеров&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В заключении этого раздела хотелось бы взглянуть правде в глаза:
&lt;strong&gt;идеального оборудования не бывает&lt;/strong&gt;. У любого современного устройства,
будь то сервер, коммутатор или маршрутизатор, есть шанс прийти в
негодность из-за производственного брака, случайного стечения
обстоятельств или других внешних факторов. Если умножить этот, казалось
бы, небольшой шанс на количество оборудования, которое используется в
Google, то окажется, что чуть ли не каждую минуту из строя выходит одно,
или несколько, устройств в системе. На оборудование полагаться нельзя,
по-этому вопрос отказоустойчивости переносится на плечи программной
платформы, которую мы сейчас и рассмотрим.&lt;/p&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;p&gt;В Google очень рано столкнулись с проблемами ненадежности оборудования и
работы с огромными массивами данных. Программная платформа,
спроектированная для работы на многих недорогих серверах, позволила им
абстрагироваться от сбоев и ограничений одного сервера.&lt;/p&gt;
&lt;p&gt;Основными задачами в ранние годы была минимизация точек отказа и
обработка больших объемов слабоструктурированных данных. Решением этих
задач стали три основных слоя платформы Google, работающие один поверх
другого:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/479dfa95/" rel="nofollow" target="_blank" title="http://research.google.com/archive/gfs.html"&gt;Google File System&lt;/a&gt;:&lt;/strong&gt;
    распределенная файловая система, состоящая из сервера с метаданными
    и теоретически неограниченного количества серверов, хранящих
    произвольные данные в блоках фиксированного размера.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/f56e059a/" rel="nofollow" target="_blank" title="http://research.google.com/archive/bigtable.html"&gt;BigTable&lt;/a&gt;:&lt;/strong&gt;
    распределенная база данных, использующая для доступа к данным две
    произвольных байтовых строки-ключа (обозначающие строку и столбец) и
    дату/время (обеспечивающие версионность).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/dbd8fea6/" rel="nofollow" target="_blank" title="http://research.google.com/archive/mapreduce.html"&gt;MapReduce&lt;/a&gt;:&lt;/strong&gt;
    механизм распределенной обработки больших объемов данных,
    оперирующий парами ключ-значение для получения требуемой информации.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Такая комбинация, дополненная другими технологиями, довольно долгое
время позволяла справляться с индексацией Интернета, пока... скорость
появления информации в Интернете не начала расти огромными темпами из-за
"бума социальных сетей". Информация, добавленная в индекс даже через
полчаса, уже зачастую становилась устаревшей.&amp;nbsp;В дополнение к этому в
рамках самого Google стало появляться все больше продуктов,
предназначенных для работы в реальном времени.&lt;/p&gt;
&lt;p&gt;Спроектированные с учетом совершенно других требований Интернета
пятилетней давности компоненты, составляющие ядро платформы Google,
потребовали фундаментальной смены архитектуры индексации и поиска,
который около года назад был представлен публике&amp;nbsp;под кодовым названием
&lt;strong&gt;Google Caffeine&lt;/strong&gt;. Новые, переработанные, версии старых "слоев" также
окрестили броскими именами, но резонанса у технической публики они
вызвали намного меньше, чем новый поисковый алгоритм в SEO-индустрии.&lt;/p&gt;
&lt;h4&gt;Google Colossus&lt;/h4&gt;
&lt;p&gt;Новая архитектура GFS была спроектирована для минимизации задержек при
доступе к данным (что критично для приложений вроде GMail и YouTube), не
в ущерб основным свойствам старой версии: отказоустойчивости и
прозрачной масштабируемости.&lt;/p&gt;
&lt;p&gt;В оригинальной же реализации упор был сделан на повышение общей
пропускной способности: операции объединялись в очереди и выполнялись
разом, при таком подходе можно было прождать пару секунд еще до того,
как первая операция в очереди начнет выполняться. Помимо этого в старой
версии было большое слабое место в виде единственно мастер-сервера с
метаданными, сбой в котором грозил недоступностью всей файловой системы
в течении небольшого промежутка времени (пока другой сервер не подхватит
его функции, изначально это занимало около 5 минут, в последних версиях
порядка 10 секунд) - это также было вполне допустимо при отсутствии
требования работы в реальном времени, но для приложений, напрямую
взаимодействующих с пользователями, это было неприемлемо с точки зрения
возможных задержек.&lt;/p&gt;
&lt;p&gt;Основным нововведением в Colossus стали распределенные мастер-сервера,
что позволило избавиться не только от единственной точки отказа, но и
существенно уменьшить размер одного блока с данными (с 64 до 1
мегабайта), что в целом очень положительно сказалось на работе с
небольшими объемами данных. В качестве бонуса исчез теоретический предел
количества файлов в одной системе.&lt;/p&gt;
&lt;p&gt;Детали распределения ответственности между мастер-серверами, сценариев
реакции на сбои, а также сравнение по задержкам и пропускной
способности обоих версий, к сожалению, по-прежнему конфиденциальны. Могу
предположить, что используется вариация на тему хэш-кольца с репликацией
метаданных на ~3 мастер-серверах, с созданием дополнительной копии на
следующем по кругу сервере в случае в случае сбоев, но это лишь догадки.
Если у кого-то есть относительно официальная информация на этот счет -
буду рад увидеть в комментариях.&lt;/p&gt;
&lt;p&gt;По прогнозам Google текущий вариант реализации распределенной файловой
системы "уйдет на пенсию" в 2014 году из-за популяризации твердотельных
накопителей и существенного скачка в области вычислительных технологий
(процессоров).&lt;/p&gt;
&lt;h4&gt;Google Percolator&lt;/h4&gt;
&lt;p&gt;MapReduce отлично справлялся с задачей полной перестройки поискового
индекса, но не предусматривал небольшие изменения, затрагивающие лишь
часть страниц. Из-за потоковой, последовательной природы MapReduce для
внесения изменений в небольшую часть документов все равно пришлось бы
обновлять весь индекс, так как новые страницы непременно будут каким-то
образом связаны со старыми. Таким образом задержка между появлением
страницы в Интернете и в поисковом индексе при использовании MapReduce
была пропорциональна общему размеру индекса (а значит и Интернета,
который постоянно растет), а не размеру набора измененных документов.&lt;/p&gt;
&lt;p&gt;Ключевые архитектурные решения, лежащие в основе MapReduce, не позволяли
повлиять на эту особенность и в итоге система индексации была построена
заново с нуля, а MapReduce продолжает использоваться в других проектах
Google для аналитики и прочих задач, по прежнему не связанных с реальным
временем.&lt;/p&gt;
&lt;p&gt;Новая система получила довольно своеобразное название &lt;strong&gt;Percolator&lt;/strong&gt;,
попытки узнать что оно значит приводит к различным устройствам по
фильтрации дыма, кофеваркам и непойми чему еще. Но наиболее адекватное
объяснение мне пришло в голову, когда я прочитал его по слогам: per
col - &lt;em&gt;по колонкам&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Percolator представляет собой надстройку над BigTable, позволяющую
выполнять комплексные вычисления на основе имеющихся данных,
затрагивающие много строк и даже таблиц одновременно (в стандартном API
BigTable это не предусмотрено).&lt;/p&gt;
&lt;p&gt;Веб-документы или любые другие данные изменяются/добавляются в систему
посредством модифицированного API BigTable, а дальнейшие изменения в
остальной базе осуществляются посредством механизма&amp;nbsp;"обозревателей".
Если говорить в терминах реляционных СУБД, то обозреватели - что-то
среднее между триггерами и хранимыми процедурами. Обозреватели
представляют собой подключаемый к базе данных код (на &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;),
который исполняется в случае возникновении изменений в определенных
&lt;em&gt;колонках&lt;/em&gt; BigTable (откуда, видимо, и пошло название). Все используемые
системой метаданные также хранятся в специальных колонках BigTable. При
использовании Percolator все изменения происходят в транзакциях,
удовлетворяющих принципу ACID, каждая из которых затрагивает именно те
сервера в кластере, на которых необходимо внести изменения. Механизм
транзакций на основе BigTable разрабатывался в рамках отдельного проекта
под названием &lt;a href="https://www.insight-it.ru/storage/2011/google-megastore/"&gt;Google Megastore&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Таким образом, при добавлении нового документа (или его версии) в
поисковый индекс, вызывается цепная реакция изменений в старых
документах, скорее всего ограниченная по своей рекурсивности. Эта
система при осуществлении случайного доступа поддерживает индекс в
актуальном состоянии.&lt;/p&gt;
&lt;p&gt;В качестве бонуса в этой схеме удалось избежать еще двух недостатков
MapReduce:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Проблемы "отстающих":&lt;/strong&gt; когда один из серверов (или одна из
    конкретных подзадач) оказывался существенно медленнее остальных, что
    также значительно задерживало общее время завершения работы
    кластера.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Пиковая нагрузка:&lt;/strong&gt; MapReduce не является непрерывным процессом, а
    разделяется на работы с ограниченной целью и временем исполнения.
    Таким образом помимо необходимости ручной настройки работ и их
    типов, кластер имеет очевидные периоды простоя и пиковой нагрузки,
    что ведет к неэффективному использованию вычислительных ресурсов.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Но все это оказалось не бесплатно: при переходе на новую систему удалось
достичь той же скорости индексации, но при этом использовалось &lt;em&gt;вдвое&lt;/em&gt;
больше вычислительных ресурсов. Производительность Percolator находится
где-то между производительностью MapReduce и производительностью
традиционных СУБД. Так как Percolator является распределенной системой,
для обработки фиксированного небольшого количества данных ей приходится
использовать существенно больше ресурсов, чем традиционной СУБД; такова
цена масштабируемости. По сравнению с MapReduce также пришлось платить
дополнительными потребляемыми вычислительными ресурсами за возможность
случайного доступа с низкой задержкой.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Google Percolator Benchmark" class="left" src="https://www.insight-it.ru/images/google-percolator-benchmark.png" title="Google Percolator Benchmark"/&gt;&lt;/p&gt;
&lt;p&gt;Тем не менее, при выбранной архитектуре Google удалось достичь
практически линейного масштабирования при увеличении вычислительных
мощностей на много порядков &lt;em&gt;(см. график, основан на тесте TPC-E)&lt;/em&gt;.
Дополнительные накладные расходы, связанные с распределенной природой
решения, в некоторых случаях до 30 раз превосходят аналогичный
показатель традиционных СУБД, но у данной системы есть солидный простор
для оптимизации в этом направлении, чем Google активно и занимается.&lt;/p&gt;
&lt;h4&gt;Google Spanner&lt;/h4&gt;
&lt;p&gt;Spanner представляет собой единую систему автоматического управления
ресурсами &lt;strong&gt;всего парка серверов&lt;/strong&gt; Google.&lt;/p&gt;
&lt;p&gt;Основные особенности:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Единое пространство имен:&lt;ul&gt;
&lt;li&gt;Иерархия каталогов&lt;/li&gt;
&lt;li&gt;Независимость от физического расположения данных&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Поддержка слабой и сильной целостности данных между датацентрами&lt;/li&gt;
&lt;li&gt;Автоматизация:&lt;ul&gt;
&lt;li&gt;Перемещение и добавление реплик данных&lt;/li&gt;
&lt;li&gt;Выполнение вычислений с учетом ограничений и способов
    использования&lt;/li&gt;
&lt;li&gt;Выделение ресурсов на всех доступных серверах&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Зоны полу-автономного управления&lt;/li&gt;
&lt;li&gt;Восстановление целостности после потерь соединения между
    датацентрами&lt;/li&gt;
&lt;li&gt;Возможность указания пользователями высокоуровневых требований,
    например:&lt;ul&gt;
&lt;li&gt;99% задержек при доступе к этим данным должны быть до 50 мс&lt;/li&gt;
&lt;li&gt;Расположи эти данные на как минимум 2 жестких дисках в Европе, 2
    в США и 1 в Азии&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Интеграция не только с серверами, но и с сетевым оборудованием, а
    также системами охлаждения в датацентрах&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Проектировалась из расчета на:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1-10 миллионов серверов&lt;/li&gt;
&lt;li&gt;~10 триллионов директорий&lt;/li&gt;
&lt;li&gt;~1000 петабайт данных&lt;/li&gt;
&lt;li&gt;100-1000 датацентров по всему миру&lt;/li&gt;
&lt;li&gt;~1 миллиард клиентских машин&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Об этом проекте Google известно очень мало, официально он был
представлен публике лишь однажды в 2009 году, с тех пор лишь местами
упоминался сотрудниками без особой конкретики. Точно не известно
развернута ли эта система на сегодняшний день и если да, то в какой
части датацентров, а также каков статус реализации заявленного
функционала.&lt;/p&gt;
&lt;h4&gt;Прочие компоненты платформы&lt;/h4&gt;
&lt;p&gt;Платформа Google в конечном итоге сводится к набору сетевых сервисов и
библиотек для доступа к ним из различных языков программирования (в
основном используются&amp;nbsp;&lt;a href="/tag/c/"&gt;C/C++&lt;/a&gt;,&amp;nbsp;&lt;a href="/tag/java/"&gt;Java&lt;/a&gt;,&amp;nbsp;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt; и&amp;nbsp;&lt;a href="/tag/perl/"&gt;Perl&lt;/a&gt;). Каждый продукт, разрабатываемый Google, в большинстве случаев использует эти библиотеки для осуществления доступа к данным, выполнения комплексных вычислений и других задач, вместо стандартных механизмов, предоставляемых операционной системой, языком программирования или opensource библиотеками.&lt;/p&gt;
&lt;p&gt;Вышеизложенные проекты составляют лишь основу платформы Google, хотя она
включает в себя куда больше готовых решений и библиотек, несколько
примеров из публично доступных проектов:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/f5686a9/" rel="nofollow" target="_blank" title="http://code.google.com/webtoolkit/"&gt;GWT&lt;/a&gt; для реализации
    пользовательских интерфейсов на &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7f9cb797/" rel="nofollow" target="_blank" title="http://code.google.com/closure/"&gt;Closure&lt;/a&gt; - набор инструментов для
    работы с &lt;a href="/tag/javascript/"&gt;JavaScript&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/41604156/" rel="nofollow" target="_blank" title="http://code.google.com/apis/protocolbuffers/"&gt;Protocol Buffers&lt;/a&gt; -
    не зависящий от языка программирования и платформы формат бинарной
    сериализации структурированных данных, используется при
    взаимодействии большинства компонентов системы внутри Google;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/bb496d06/" rel="nofollow" target="_blank" title="http://code.google.com/p/leveldb/"&gt;LevelDB&lt;/a&gt; -
    высокопроизводительная встраиваемая &lt;a href="/tag/subd/"&gt;СУБД&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/44c46d8/" rel="nofollow" target="_blank" title="http://code.google.com/p/snappy/"&gt;Snappy&lt;/a&gt; - быстрая компрессия
    данных, используется при хранении данных в GFS.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi_1"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Стабильные, проработанные и повторно используемые базовые
    компоненты проекта&lt;/strong&gt; &lt;em&gt;- залог её стремительного развития, а также
    создания новых проектов на той же кодовой базе&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Если задачи и обстоятельства, с учетом которых проектировалась
    система, существенно изменились&amp;nbsp;&lt;em&gt;- не бойтесь вернуться на стадию проектирования и реализовать новое решение&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Используйте инструменты, подходящие для решения каждой конкретной
    задачи&lt;/em&gt;, а не те, которые навязывает мода или привычки участников
    команды.&lt;/li&gt;
&lt;li&gt;Даже, казалось бы, незначительные недоработки и допущения на большом
    масштабе могут вылиться в огромные потери &lt;em&gt;- уделяйте максимум
    внимания деталям при реализации проекта.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Нельзя полагаться даже на очень дорогое оборудование &lt;em&gt;- все ключевые
    сервисы должны работать минимум на двух серверах, в том числе и базы
    данных.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Распределенная платформа, общая для всех проектов, позволит новым
    разработчикам легко вливаться в работу над конкретными продуктами, с
    минимумом представления о внутренней архитектуре компонентов
    платформы.&lt;/li&gt;
&lt;li&gt;Прозрачная работа приложений в нескольких датацентрах - одна из
    самых тяжелых задач, с которыми сталкиваются интернет-компании.
    Сегодня каждая из них решает её по-своему и держит подробности в
    секрете, что сильно замедляет развитие opensource решений.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;p&gt;Не гарантирую достоверность всех нижеизложенных источников информации,
ставших основой для данной статьи, но ввиду конфиденциальности подобной
информации на большее рассчитывать не приходится.&lt;/p&gt;
&lt;p&gt;Поправки и уточнения приветствуются :)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/4431029a/" rel="nofollow" target="_blank" title="http://www.google.com/about/datacenters/index.html"&gt;Official Google Data Centers
    Site&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7437fe8a/" rel="nofollow" target="_blank" title="http://research.google.com/people/jeff/WSDM09-keynote.pdf"&gt;Challenges in Building Large-Scale Information Retrieval
    Systems&lt;/a&gt;
    (Jeff Dean, WCDMA '09)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7f1ddbff/" rel="nofollow" target="_blank" title="http://www.odbms.org/download/dean-keynote-ladis2009.pdf"&gt;Designs, Lessons and Advice from Building Large Distributed
    Systems&lt;/a&gt;
    (Jeff Dean, Ladis '09)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/6ec4bc06/" rel="nofollow" target="_blank" title="http://research.google.com/pubs/pub36726.html"&gt;Google Percolator official
    paper&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/3473b129/" rel="nofollow" target="_blank" title="http://research.google.com/pubs/pub36971.html"&gt;&lt;em&gt;Google Megastore official
    paper&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/dd8bec64/" rel="nofollow" target="_blank" title="http://www.theregister.co.uk/2010/09/24/google_percolator/"&gt;Google
    Percolator&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/82efffd8/" rel="nofollow" target="_blank" title="http://www.theregister.co.uk/2010/09/09/google_caffeine_explained/"&gt;Google Caffeine
    Explained&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/63b6ec67/" rel="nofollow" target="_blank" title="http://www.theregister.co.uk/2009/10/23/google_spanner/"&gt;Google
    Spanner&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/46520ede/" rel="nofollow" target="_blank" title="http://www.theregister.co.uk/2011/06/08/google_software_infrastructure_dubbed_obsolete_by_ex_employee/"&gt;Google Software Infrastructure Dubbed Obsolete by
    ex-Employee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/49879386/" rel="nofollow" target="_blank" title="http://www.theregister.co.uk/2011/06/23/google_moves_off_the_google_file_system/"&gt;Google Moves Off the Google File
    System&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/745a779b/" rel="nofollow" target="_blank" title="http://www.google.co.uk/intl/en/landing/internetstats/"&gt;Google Internet
    Stats&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/d11024ba/" rel="nofollow" target="_blank" title="http://www.businessblogshub.com/2010/10/google-statistics-yes-they-are-very-big/"&gt;Google
    Statistics&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/bb33b2dd/" rel="nofollow" target="_blank" title="http://www.splashnology.com/article/google-plus-killer-facts-and-statistics-inforgaphics/2689/"&gt;Google Plus - Killer Facts and
    Statistics&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/d20404a7/" rel="nofollow" target="_blank" title="http://www.youtube.com/t/press_statistics"&gt;YouTube statistics&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/9909f6c8/" rel="nofollow" target="_blank" title="http://www.alexa.com/siteinfo/google.com"&gt;&lt;em&gt;Alexa on Google&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/b088e740/" rel="nofollow" target="_blank" title="http://www.internetworldstats.com/stats.htm"&gt;&lt;em&gt;Internet World
    Stats&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/86a009d0/" rel="nofollow" target="_blank" title="http://www.google.com/finance?fstype=bi&amp;amp;cid=694653"&gt;Google Inc.
    financials&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/2515e26d/" rel="nofollow" target="_blank" title="http://www.geekwire.com/2011/stats-hotmail-top-worldwide-gmail-posts-big-gains"&gt;Hotmail still on top worldwide; Gmail gets
    bigger&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/2428f635/" rel="nofollow" target="_blank" title="http://www.datacenterknowledge.com/archives/2011/08/01/report-google-uses-about-900000-servers/"&gt;&lt;em&gt;Google Server
    Count&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/b6d09313/" rel="nofollow" target="_blank" title="http://www.datacenterknowledge.com/archives/2009/10/20/google-envisions-10-million-servers/"&gt;Google Envisions 10 Million
    Servers&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/bb3d77a4/" rel="nofollow" target="_blank" title="http://www.datacenterknowledge.com/archives/2008/03/27/google-data-center-faq/"&gt;&lt;em&gt;Google Data Center
    FAQ&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="bonus-tipichnyi-pervyi-god-klastera-v-google"&gt;Бонус: типичный первый год кластера в Google&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;~1/2 перегрева (большинство серверов выключаются в течении 5 минут,
    1-2 дня на восстановление)&lt;/li&gt;
&lt;li&gt;~1 отказ распределителя питания (~500-1000 резко пропадают, ~6
    часов на восстановление)&lt;/li&gt;
&lt;li&gt;~1 передвижение стойки (много передвижений, 500-100 машин, 6 часов)&lt;/li&gt;
&lt;li&gt;~1 перепрокладка сети (последовательной отключение ~5% серверов на
    протяжении 2 дней)&lt;/li&gt;
&lt;li&gt;~20 отказов стоек (40-80 машин мгновенно исчезают, 1-6 часов на
    восстановление)&lt;/li&gt;
&lt;li&gt;~5 стоек становится нестабильными (40-80 машин сталкиваются с 50%
    потерей пакетов)&lt;/li&gt;
&lt;li&gt;~8 запланированных технических работ с сетью (при четырех могут
    случаться случайные получасовые потери соединения)&lt;/li&gt;
&lt;li&gt;~12 перезагрузок маршрутизаторов (потеря DNS и внешних виртуальных
    IP на несколько минут)&lt;/li&gt;
&lt;li&gt;~3 сбоя маршрутизаторов (восстановление в течении часа)&lt;/li&gt;
&lt;li&gt;Десятки небольших 30-секундных пропаданий DNS&lt;/li&gt;
&lt;li&gt;~1000 сбоев конкретных серверов (~3 в день)&lt;/li&gt;
&lt;li&gt;Много тысяч сбоев жестких дисков, проблем с памятью, ошибок
    конфигурации и т.п.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 28 Nov 2011 01:32:00 +0400</pubDate><guid>tag:www.insight-it.ru,2011-11-28:highload/2011/arkhitektura-google-2011/</guid><category>BigTable</category><category>Caffeine</category><category>Closure</category><category>GFS</category><category>Google</category><category>GWT</category><category>highload</category><category>Percolator</category><category>Protocol Buffers</category><category>Snappy</category><category>Spanner</category><category>архитектура Google</category><category>датацентры</category><category>Масштабируемость</category><category>энергоэффективность</category></item><item><title>Есть вопросы?</title><link>https://www.insight-it.ru//misc/2011/est-voprosy/</link><description>&lt;p&gt;Недавно несколько человек довольно независимо друг от друга подтолкнули
меня к новой странице-рубрике на Insight IT. Как не трудно догадаться по
заголовку, это &lt;strong&gt;&lt;a href="https://www.insight-it.ru/highload/faq/"&gt;F.A.Q. по высоконагруженным проектам&lt;/a&gt;&lt;/strong&gt; и связанным темам.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Я не считаю себя истиной в последней инстанции, так что публикую этот
анонс, чтобы попросить Вас, лояльных читателей, помочь мне в составлении
данного несомненно полезного для общества материала. Очень хотелось бы
увидеть дополнения, поправки и комментарии к моим ответам, а также
предложения по поводу новых вопросов и под-тем, которые стоило бы
осветить.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Текущая версия состоит из тех вопросов, которые мне задавали в последнее
время. В дальнейшем я постараюсь дополнить её более старыми вопросами от
читателей и клиентов, но в большей степени я все же надеюсь на вашу
поддержку :)&lt;/p&gt;
&lt;div class="card blue lighten-4"&gt;
&lt;p&gt;&lt;div class="card-content"&gt;
Еще раз ссылка на основной материал: &lt;strong&gt;&lt;a href="https://www.insight-it.ru/highload/faq/"&gt;Вопросы и ответы&lt;/a&gt;&lt;/strong&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Комментарии здесь закрываю, всё обсуждение этой темы по ссылке.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 26 Nov 2011 01:54:00 +0400</pubDate><guid>tag:www.insight-it.ru,2011-11-26:misc/2011/est-voprosy/</guid><category>FAQ</category><category>highload</category><category>Вопросы и ответы</category><category>Масштабируемость</category></item><item><title>6 способов порадовать инвестора</title><link>https://www.insight-it.ru//theory/2011/6-sposobov-poradovat-investora/</link><description>&lt;blockquote&gt;
&lt;p&gt;...или как не надо масштабировать интернет-проекты&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Недавно наткнулся на полу-поучительный и полу-юмористический материал по
масштабируемости интернет-проектов. Кому-то он может показаться, как и
мне, забавным, а кому-то - в таком формате может оказаться легче
воспринимать информацию. Надеюсь тебе тоже понравится, так что спешу
поделиться как переводом, так и оригинальным видео =)&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;На конференции&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/d4ae0945/" rel="nofollow" target="_blank" title="http://en.oreilly.com/mysql2011/"&gt;O'Reilly MySQL CE 2011&lt;/a&gt; выступил &lt;a href="https://www.insight-it.ru/goto/147f0d48/" rel="nofollow" target="_blank" title="http://it.toolbox.com/people/josh_berkus/"&gt;Josh Berkus&lt;/a&gt; c пламенной речью о том, как быть уважаемым и известным, при этом не уделяя ни капли
внимания масштабируемости. Его очень сильно удивляет, почему самыми
популярными, известными и инвестиционно-привлекательными
интернет-компаниями становятся именно те, чей интерфейс неработающего
состояния (в частности "киты" и "роботы" у &lt;a href="/tag/twitter/"&gt;Twitter&lt;/a&gt;),
известен больше, чем когда они работают. Так что Josh предлагает всем
придерживаться следующей стратегии:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Всегда следуй трендам:&lt;/strong&gt; используй только те технологии, которые
    навели больше всего шумихи в Интернете: NoSQL, "облака",
    MapReduce, Rails, RabbitMQ. Основным инструментом для выбора
    технологий должен быть Reddit (или Хабр, если адаптировать к
    российским реалиям). За что больше голосуют - то и используйте.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Не следите за текущей ситуацией:&lt;/strong&gt; математика и статистика -
    абсолютно бесполезны. Мониторинг использования ресурсов, нагрузочное
    тестирование, отслеживание трафика, тестирование производительности
    и тонкая настройка - да кому оно надо? Лучше доверять интуиции - с
    какими проблемами мы сталкивались на предыдущей работе, с такими же
    столкнемся и в этот раз&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ни о чем не беспокойтесь:&lt;/strong&gt; параллельное программирование не в
    моде, даже не смотря на то, что &lt;a href="/tag/erlang/"&gt;Erlang&lt;/a&gt; позволяет
    приложениям работать на кластере из тысяч серверов. Крутые ребята не
    заботятся об оперативной памяти и управлении потоками. Нужно не
    париться и использовать однопоточные приложения с кучей блокировок,
    игнорируя области видимости и контексты памяти. Часто обновляющиеся
    таблицы из одной строки и мастер-очередь, в которую попадают все
    задания - лучшие паттерны из всех существующих.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Каждый запрос должен попадать напрямик в базу данных:&lt;/strong&gt;
    кэширование - твой смертный враг. Каждый запрос должен идти напрямую
    к СУБД, ни шага в сторону!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Масштабировать нужно невозможные вещи:&lt;/strong&gt; масштабирование простых и
    очевидных вещей - для слабаков! Это совершенно не круто заниматься
    масштабированием веб-серверов, кэшей (хотя ими пользоваться и так
    категорически запрещено) и серверов предложений.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Создавайте точки отказа:&lt;/strong&gt; вне зависимости от того, насколько
    большой ваш проект, в нем обязательно должно быть место, при отказе
    которого перестанет работать вся система. Лучшие кандидаты на эту
    роль: балансировщик нагрузки, очередь задач и мастер база данных.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Если следовать этим правилам, то ты станешь самым крутым и уважаемым
техническим специалистом в команде: ведь у тебя всегда будут истории,
где ты как настоящий герой, несмотря на все преграды, проработав все
выходные напролет решил-таки поставленную задачу! В противном же случае,
если твой код всегда работает и позволяет легко масштабироваться, то,
парадоксально, ты будешь просто старым-добрым Васей или Петей, который
просто работает и на которого никто не обращает внимания до тех пор,
пока он в один прекрасный день не уволится.&lt;/p&gt;
&lt;p&gt;Если все нормально с устным восприятием английского, очень рекомендую
посмотреть видео - с эмоциями и более лаконичным техническим английским
выглядит намного более впечатляюще:&lt;/p&gt;
&lt;div class="video-container"&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="480" src="//www.youtube.com/embed/nPG4sK_glls?rel=0" width="853"&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Если вы еще не читаете Insight IT регулярно, настоятельно &lt;a href="/feed/"&gt;рекомендую подписаться на RSS&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 30 Apr 2011 01:27:00 +0400</pubDate><guid>tag:www.insight-it.ru,2011-04-30:theory/2011/6-sposobov-poradovat-investora/</guid><category>Масштабируемость</category><category>вредные советы</category></item><item><title>Архитектура Одноклассников</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-odnoklassnikov/</link><description>&lt;p&gt;Сегодня представители &lt;a href="https://www.insight-it.ru/goto/2c99aef2/" rel="nofollow" target="_blank" title="http://www.odnoklassniki.ru"&gt;Одноклассников&lt;/a&gt;
рассказали о накопленном за 5 лет опыте по поддержанию высоконагруженного
проекта. Была опубликована довольно детальная информация о том, как
устроена эта социальная сеть для аудитории "постарше". Далее можно
прочитать мою версию материала, либо перейти на оригинал &lt;a href="https://www.insight-it.ru/goto/b762a864/" rel="nofollow" target="_blank" title="http://habrahabr.ru/company/odnoklassniki/blog/115881/"&gt;по сссылке&lt;/a&gt;.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/windows/"&gt;Windows&lt;/a&gt; и &lt;a href="/tag/opensuse/"&gt;openSUSE&lt;/a&gt; - основные
    операционные системы&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/java/"&gt;Java&lt;/a&gt; - основной язык программирования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/c/"&gt;С/С++&lt;/a&gt; - для некоторых модулей&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/gwt/"&gt;GWT&lt;/a&gt; - реализация динамического веб-интерфейса&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/tomcat/"&gt;Apache Tomcat&lt;/a&gt; - сервера приложений&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/jboss/"&gt;JBoss 4&lt;/a&gt; - сервера бизнес-логики&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/lvs/"&gt;LVS&lt;/a&gt; и &lt;a href="/tag/ipvs/"&gt;IPVS&lt;/a&gt; - балансировка нагрузки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mssql/"&gt;MS SQL 2005 и 2008&lt;/a&gt; - основная СУБД&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/berkleydb/"&gt;BerkleyDB&lt;/a&gt; - дополнительная СУБД&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/lucene/"&gt;Apache Lucene&lt;/a&gt; - индексация и поиск текстовой
    информации&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;До 2.8 млн. пользователей онлайн в часы пик&lt;/li&gt;
&lt;li&gt;7,5 миллиардов запросов в день (150 000 запросов в секунду в часы
    пик)&lt;/li&gt;
&lt;li&gt;2 400 серверов и систем хранения данных, из которых 150 являются
    веб-серверами&lt;/li&gt;
&lt;li&gt;Сетевой трафик в час пик: 32 Gb/s&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="oborudovanie"&gt;Оборудование&lt;/h2&gt;
&lt;p&gt;Сервера используются двухпроцессорные с 4 ядрами, объемом памяти от 4 до
48 Гб. В зависимости от роли сервера данные хранятся либо в памяти, либо
на дисках, либо на внешних системах хранения данных.&lt;/p&gt;
&lt;p&gt;Все оборудование размещено в 3 датацентрах, объединенных в оптическое
кольцо. На данный момент на каждом из маршрутов пропускная способность
составляет 30Гбит/с. Каждый из маршрутов состоит из физически
независимых друг от друга оптоволоконных пар, которые агрегируются в
общую &amp;ldquo;трубу&amp;rdquo; на корневых маршрутизаторах.&lt;/p&gt;
&lt;p&gt;Сеть физически разделена на внутреннюю и внешнюю, разные интерфейсы
серверов подключены в разные коммутаторы и работают в разных сетях. По
внешней сети HTTP сервера, общаются с Интернетом, по внутренней сети все
сервера общаются между собой.&amp;nbsp;Топология внутренней сети &amp;ndash; звезда.
Сервера подключены в L2 коммутаторы (access switches), которые, в свою
очередь, подключены как минимум двумя гигабитными линками к aggregation
стеку маршрутизаторов. Каждый линк идет к отдельному коммутатору в
стеке. Для того, чтобы эта схема работала, используется
протокол&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/a03e0548/" rel="nofollow" target="_blank" title="http://ru.wikipedia.org/wiki/RSTP"&gt;RSTP&lt;/a&gt;. При необходимости,
подключения access коммутаторов к agregation стеку осуществляются более
чем двумя линками с использованием link aggregation портов.&amp;nbsp;Aggregation
коммутаторы подключены 10Гб линками в корневые маршрутизаторы, которые
обеспечивают как связь между датацентрами, так и связь с внешним
миром.&amp;nbsp;Используются коммутаторы и маршрутизаторы от компании Cisco.&lt;/p&gt;
&lt;p&gt;Для связи с внешним миром используются прямые подключения с несколькими
крупнейшими операторами связи, общий сетевой&amp;nbsp;трафик в часы пик доходит
до 32Гбит/с.&lt;/p&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;Архитектура проекта имеет традиционную многоуровневую структуру:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;презентационный уровень;&lt;/li&gt;
&lt;li&gt;уровень бизнес-логики;&lt;/li&gt;
&lt;li&gt;уровень кэширования;&lt;/li&gt;
&lt;li&gt;уровень баз данных;&lt;/li&gt;
&lt;li&gt;уровень инфраструктуры (логирование, конфигурация и мониторинг).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Код проекта в целом написан на Java, но есть исключения в виде модулей
для кэширования на C и C++.
Java был выбран так как он является удобным языком для разработки,
доступно множество наработок в различных сферах, библиотек и opensource
проектов.&lt;/p&gt;
&lt;h3 id="prezentatsionnyi-uroven"&gt;Презентационный уровень&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Используем собственный фреймворк, позволяющий строить композицию
    страниц на языке Jаvа, с использованием собственные GUI фабрик (для
    оформления текста, списков, таблиц и портлетов).&lt;/li&gt;
&lt;li&gt;Страницы состоят из независимых блоков (обычно портлетов), что
    позволяет обновлять информацию на них частями с помощью AJAX
    запросов.&lt;/li&gt;
&lt;li&gt;При данном подходе одновременно обеспечивается минимум перезагрузок
    страниц для пользователей с включенным JavaScript, так и полная
    работоспособность сайта для пользователей, у которых он отключен.&lt;/li&gt;
&lt;li&gt;Google Web Toolkit используется для реализации функциональные
    компонент, таких как Сообщения, Обсуждения и Оповещения, а также все
    динамических элементов (меню шорткатов, метки на фотографиях,
    сортировка фотографий,&amp;nbsp;ротация подарков и.т.д.).&amp;nbsp;В GWT используются
    UIBinder и HTMLPanel для создания интерфейсов.&lt;/li&gt;
&lt;li&gt;Кешируются все внешние ресурсы (Expires и Cache-Control заголовки).
    CSS и JavaScript файлы минимизируются и сжимаются (gzip).&lt;/li&gt;
&lt;li&gt;Для уменьшения количества HTTP запросов с браузера, все JavaScript и
    CSS файлы объединяются в один. Маленькие графические изображения
    объединяются в спрайты.&lt;/li&gt;
&lt;li&gt;При загрузке страницы скачиваются только те ресурсы, которые на
    самом деле необходимы для начала работы.&lt;/li&gt;
&lt;li&gt;Никаких универсальных CSS селекторов. Стараются не использовать
    типовые селекторы (по имени тэга), что повышает скорость отрисовки
    страниц внутри браузера.&lt;/li&gt;
&lt;li&gt;Если необходимы CSS expressions, то пишутся &amp;laquo;одноразовые&amp;raquo;. По
    возможности избегаются фильтры.&lt;/li&gt;
&lt;li&gt;Кешируется обращения к DOM дереву, а так же свойства элементов,
    приводящие к reflow. Обновляется DOM дерево в &amp;laquo;оффлайне&amp;raquo;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="uroven-biznes-logiki_1"&gt;Уровень бизнес-логики&lt;/h2&gt;
&lt;p&gt;На уровне бизнес логики располагаются около 25 типов серверов и
компонентов, общающихся между собой через удаленные интерфейсы. Каждую
секунду происходит около 3 миллионов удаленных запросов между этими
модулями.
Сервера на уровне бизнес логики разбиты на группы. Каждая группа
обрабатывает различные события. Есть механизм маршрутизации событий, то
есть любое событие или группу событий можно выделить и направить на
обработку на определенную группу серверов.&amp;nbsp;При общении серверов между
собой используется свое решение, основанное на&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/5e85a755/" rel="nofollow" target="_blank" title="http://www.jboss.org/jbossremoting"&gt;JBoss
Remoting&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="uroven-keshirovaniia"&gt;Уровень кэширования&lt;/h2&gt;
&lt;p&gt;Для кэширования данных используется самописный модуль
odnoklassniki-cache. Он предоставляет возможность хранения данных в
памяти средствами Java Unsafe. Кэшируются все данные, к которым
происходит частое обращение, например: профили пользователей, списки
участников сообществ, информация о самих сообществах, граф связей
пользователей и групп, праздники, мета информация о фотографиях и многое
другое.Для хранения больших объемов данных в памяти используется память
Java off heap memory для снятия ненужной нагрузки с сборщика
мусора.&amp;nbsp;Кеши могут использовать локальный диск для хранения данных, что
превращает их в высокопроизводительный сервер БД.&amp;nbsp;Кеш сервера, кроме
обычных операций ключ-значение, могут выполнять запросы по данным,
хранящимся в памяти, минимизируют таким образом передачу данных по сети.
Используется map-reduce для выполнения запросов и операций на кластере.
В особо сложных случаях, например для реализации запросов по социальному
графу, используется язык C. Это помогает повысить производительность.&lt;/p&gt;
&lt;p&gt;Данные распределяются между кластерами кеш серверов, а также
используется репликация партиций для обеспечения надежности.&amp;nbsp;Иногда
требования к быстродействию настолько велики, что используются локальные
короткоживущие кеши данных полученных с кеш серверов, расположенные
непосредственно в памяти серверов бизнес логики.&lt;/p&gt;
&lt;p&gt;Для примера, один сервер, кэширующий граф связей пользователей, в час
пик может обработать около 16 600 запросов в секунду. Процессоры при
этом заняты до 7%, максимальный load average за 5 минут &amp;mdash; 1.2.
Количество вершин графа - более 85 миллионов, связей 2.5 миллиарда. В
памяти граф занимает 30 GB.&lt;/p&gt;
&lt;h2 id="uroven-baz-dannykh"&gt;Уровень баз данных&lt;/h2&gt;
&lt;p&gt;Суммарный объем данных без резервирования составляет 160Тб. Используются
два решения для хранения данных: MS SQL и BerkeleyDB. Данные хранятся в
нескольких копиях, в зависимости от их типа от двух до четырех. Полное
резервное копирование всех данных осуществляется раз в сутки, плюс
каждые 15 минут делаются резервные копии новых данных. В результате
максимально возможная потеря данных составляет 15 минут.&lt;/p&gt;
&lt;p&gt;Сервера с MS SQL объединены в failover кластера, при выходе из строя
одного из серверов, находящийся в режиме ожидания сервер берет на себя
его функции. Общение с MS SQL происходит посредством JDBC драйверов.&lt;/p&gt;
&lt;p&gt;Используются как вертикальное, так и горизонтальное разбиение данных,
т.е. разные группы таблиц располагаются на разных серверах (вертикальное
партиционирование), а данные больших таблицы дополнительно
распределяются между серверами (горизонтальное партиционирование).
Встроенный в СУБД аппарат партиционирования не используется &amp;mdash; весь
процесс реализован на уровне бизнес-логики.&amp;nbsp;Распределенные транзакции не
используются &amp;mdash; всё только в пределах одного сервера. Для обеспечения
целостности, связанные данные помещаются на один сервер или, если это
невозможно, дополнительно разрабатывается логика обеспечения целостности
данных.&amp;nbsp;В запросах к БД не используются JOIN даже среди локальных таблиц
для минимизации нагрузки на CPU. Вместо этого используется
денормализация данных или JOIN происходят на уровне бизнес сервисов, что
позволяет осуществлять JOIN как с данными из баз данных, так и с данными
из кэша.&amp;nbsp;При проектировании структуры данных не используются внешние
ключи, хранимые процедуры и триггеры. Опять же для снижения потребления
вычислительных ресурсов на серверах баз данных.
SQL операторы DELETE также используются с осторожностью &amp;mdash; это самая
тяжелая операция. Данные удаляются чаще всего через маркер: запись
сначала отмечается как удаленная, а потом удаляется окончательно с
помощью фонового процесса.&amp;nbsp;Широко используются индексы, как обычные, так
и кластерные. Последние для оптимизации наиболее высокочастотных
запросов в таблицу.&lt;/p&gt;
&lt;p&gt;Используется C реализация BerkleyDB версии 4.5. Для работы с BerkleydDB
используется своя библиотека, позволяющая организовывать двухнодовые
master-slave кластера с использованием родной BDB репликация. Запись
происходит только в master, чтение происходит с обеих нод. Данные
хранятся в tmpfs, transaction логи сохраняются на дисках. Резервная
копия логов делается каждые 15 минут. Сервера одного кластера размещены
на разных лучах питания дабы не потерять обе копии одновременно. Помимо
прочего, BerkleyDB используется и в роли очереди заданий.&lt;/p&gt;
&lt;p&gt;Внутри системы используется взвешенный round robin, а также вертикальное
и горизонтальное разбиение данных как на уровне СУБД, так и на уровне
кэширования.&lt;/p&gt;
&lt;p&gt;В разработке новое решение для хранения данных, так как необходим еще
более быстрый и надежный доступ к данным.&lt;/p&gt;
&lt;h2 id="uroven-infrastruktury"&gt;Уровень инфраструктуры&lt;/h2&gt;
&lt;p&gt;Для агрегации статистики используется собственная библиотека, основанная
на log4j. Сохраняется такая информация, как количество вызовов, среднее,
максимальное и минимальное время выполнения, количество ошибок. Данные
сохраняются во временные базы, но раз в минуту данные переносятся из них
в общий склад данных (data warehouse), а временные базы очищаются. Сам
склад реализован на базе решений от Microsoft: MS SQL 2008 и сиситема
генерации отчетов Reporting Services. Он расположен на 13 серверах,
находящихся в отдельной от production среде. Некоторые из них отвечают
за статистику в реальном времени, а некоторые за ведение и
предоставление доступа к архиву. Общий объем статистических данных
составляет 13Тб.&amp;nbsp;Планируется внедрение многомерного анализа статистики
на основе OLAP.&lt;/p&gt;
&lt;p&gt;Управление сервисами происходит через самописную централизованную
систему конфигурации. Через веб-интерфейс доступно изменение
расположения портлетов, конфигурации кластеров, изменение логики
сервисов и прочее. Вся конфигурация сохраняется в базе данных. Каждый из
серверов периодически проверяет, есть ли обновления для приложений,
которые на нем запущены, и, если есть, применяет их.&lt;/p&gt;
&lt;p&gt;Мониторинг логически разделен на две части:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Мониторинг сервисов и компонентов&lt;/li&gt;
&lt;li&gt;Мониторинг ресурсов, оборудования и сети&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Система мониторинга сервисов также самописная и основывается на
оперативных данных с упомянутого выше склада. Мониторинг ресурсов и
здоровья оборудования же онован на Zabbix, а статистика по
использованию ресурсов серверов и сети накапливаетя в Cacti.&amp;nbsp;Для
предпринятия мер по устранению чрезвычайных ситуаций работают дежурные,
которые следят за всеми основными параметрами.&amp;nbsp;Оповещения о наиболее
критичных аномалиях приходят по смс, остальные оповещения отсылаются по
емейлу.&lt;/p&gt;
&lt;h2 id="komanda"&gt;Команда&lt;/h2&gt;
&lt;p&gt;Над проектом работают около 70 технических специалистов:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;40 разработчиков;&lt;/li&gt;
&lt;li&gt;20 системных администраторов и инженеров;&lt;/li&gt;
&lt;li&gt;8 тестеров.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Все разработчики разделены на небольшие команды до 3х человек. Каждая из
команд работает автономно и разрабатывает либо какой-то новый сервис,
либо работает над улучшением существующих. В каждой команде есть
технический лидер или архитектор, который ответственен за архитектуру
сервиса, выбор технологий и подходов. На разных этапах к команде могут
примыкать дизайнеры, тестеры и системные администраторы.&lt;/p&gt;
&lt;p&gt;Разработка ведется итерациями в несколько недель. Как пример жизненного
цикла разработки можно привести 3х недельный цикл:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;определение архитектуры;&lt;/li&gt;
&lt;li&gt;разработка, тестирование на компьютерах разработчиков;&lt;/li&gt;
&lt;li&gt;тестирование на pre-production среде, релиз на production среду.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Практически весь новый функционал делается &amp;laquo;отключаемым&amp;raquo;, типичный
процесс запуска новой функциональной возможности:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Функционал разрабатывается и попадает в production релиз;&lt;/li&gt;
&lt;li&gt;Через централизованную систему конфигурации функционал включается
    для небольшой части пользователей;&lt;/li&gt;
&lt;li&gt;Анализируется статистика активности пользователей, нагрузка на
    инфраструктуру;&lt;/li&gt;
&lt;li&gt;Если предыдущий этап прошел успешно, функционал включается
    постепенно для все большей аудитории;&lt;/li&gt;
&lt;li&gt;Если в процессе запуска собранная статистика выглядет
    неудовлетворительно, либо непозволительно вырастает нагрузка на
    инфраструктуру, то функционал отключается, анализируются причины,
    исправляются ошибки, происходит оптимизация и все повторяется с
    начала.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;В отличии от остальных популярных социальных сетей в Одноклассниках
    используются технологии, рассчитанные в первую очередь на
    корпоративный рынок, начиная от обоих СУБД и заканчивая
    операционными системами.&lt;/li&gt;
&lt;li&gt;Во многом этот факт обуславливает комплексный подход к генерации
    пользовательского интерфейса, не слишком высокую производительность
    и многие другие особенности этой социальной сети.&lt;/li&gt;
&lt;li&gt;Использование "тяжелых" технологий с самого начала оставило
    Одноклассники с большим количеством доставшегося по наследству от
    ранних версий устаревшего кода и купленных давно лицензий на
    проприетарный софт, которые выступают в роли оков, от которых
    довольно сложно избавиться.&lt;/li&gt;
&lt;li&gt;Возможно эти факторы и являются одними из основных препятствий на
    пути к завоеванию большей доли рынка и быстрому развитию платформы
    как в функциональном, так и техническом плане.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 22 Mar 2011 00:17:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-03-22:highload/2011/arkhitektura-odnoklassnikov/</guid><category>BerkleyDB</category><category>C. GWT</category><category>IPVS</category><category>Java</category><category>Jboss</category><category>Lucene</category><category>LVS</category><category>MSSQL</category><category>openSUSE</category><category>Tomcat</category><category>Windows</category><category>Архитектура Одноклассников</category><category>Масштабируемость</category><category>Одноклассники</category></item><item><title>Архитектура Dropbox</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-dropbox/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/4f16cf21/" rel="nofollow" target="_blank" title="http://db.tt/4TDAr1L"&gt;Dropbox&lt;/a&gt; - это самый простой способ...
&lt;img alt="Dropbox" class="right" src="https://www.insight-it.ru/images/dropbox_logo.png" title="Dropbox Logo"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Хранить файлы в безопасном месте&lt;/li&gt;
&lt;li&gt;Делиться файлами с другими людьми&lt;/li&gt;
&lt;li&gt;Постоянно иметь к ним доступ вне зависимости от своего месторасположения&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;h2 id="vzryvnoi-rost"&gt;Взрывной рост&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;1 миллион файлов сохраняются в Dropbox каждые 15 минут (по
    презентации это больше, чем твитов в Twitter за тот же период
    времени, но это &lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-twitter-dva-goda-spustya/"&gt;несколько преувеличено&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Одно из самых скачиваемых приложений, уступает лишь Skype&lt;/li&gt;
&lt;li&gt;Важная часть жизни многих пользователей: "не могу жить без этого"&lt;/li&gt;
&lt;li&gt;Рост обеспечен "сарафанным радио", практически без рекламы&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Dropbox" class="responsive-img" src="https://www.insight-it.ru/images/dropbox.jpeg" title="Dropbox"/&gt;&lt;/p&gt;
&lt;h2 id="komanda-v-nachale-proekta"&gt;Команда в начале проекта&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Все хорошие друзья&lt;/li&gt;
&lt;li&gt;Самые умные, голодные и страстные, которых они знали :)&lt;/li&gt;
&lt;li&gt;Для каждого это была первая реальная работа, очень ограниченный опыт
    в данной индустрии&lt;/li&gt;
&lt;li&gt;Эти качества хорошо сочетаются с итеративной методологией разработки
    на &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="rannie-dostizheniia"&gt;Ранние достижения&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Reverse engineering приложения Finder для Mac OSX 10.4/5 для
    отображения иконок статуса синхронизации ("в процессе", "все
    готово")&lt;/li&gt;
&lt;li&gt;Сложная инфраструктура HTTP нотификаций для избежания регулярного
    опроса серверов на каждом клиенте, основанная на
    &lt;a href="/tag/twisted/"&gt;Twisted&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Общая кодовая база, работающая на всех основных операционных
    системах: Windows, Mac OS, Linux (на основе PyObjC, wxPython,
    ctypes, py2exe, py2app, PyWin32)&lt;/li&gt;
&lt;li&gt;Горизонтально масштабируемое хранилище для информации о файлах на
    основе &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Собственная инфраструктура для аналитики в реальном времени&lt;/li&gt;
&lt;li&gt;Собственный механизм выделения памяти для Python, позволивший
    сократить её потребление на 90%&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="sfery-primeneniia-python"&gt;Сферы применения Python&lt;/h2&gt;
&lt;p&gt;По сути он используется во всех частях проекта:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;логика backend синхронизации;&lt;/li&gt;
&lt;li&gt;клиенты для основных ОС (Windows, Mac, Linux);&lt;/li&gt;
&lt;li&gt;контроллер основного сайта;&lt;/li&gt;
&lt;li&gt;обработка API запросов;&lt;/li&gt;
&lt;li&gt;аналитика.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Исключения из-за ограничений по доступной оперативной памяти:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Android (мог бы быть Jython)&lt;/li&gt;
&lt;li&gt;iPhone (мог бы быть Cpython)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="pochemu-imenno-python"&gt;Почему именно Python?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Легок в изучении и понимании:&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;новые люди легко втягиваются в процесс;&lt;/li&gt;
&lt;li&gt;позволяет людям переключаться с проекта на проект.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Легок в написании:&lt;/strong&gt; важно для быстрой реализации функционала и
    выпуска новых версий.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Легок в изменении:&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;нет необходимости в перекомпиляции;&lt;/li&gt;
&lt;li&gt;высокая скорость итерации;&lt;/li&gt;
&lt;li&gt;динамическая типизация:&lt;ul&gt;
&lt;li&gt;рефакторинг очень прост;&lt;/li&gt;
&lt;li&gt;уменьшение прямых зависимостей модулей;&lt;/li&gt;
&lt;li&gt;динамические инструменты.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Позволяет обеспечивать качество&lt;/strong&gt; путем создания более-менее
    работающей версии продукта и доведения её до качественного уровня
    путем быстрых итераций.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="ne-bez-trudnostei"&gt;Не без трудностей&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Аргх!!! Python потребляет слишком много оперативной памяти и о-о-очень
медленный!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;На серверной стороне можно вообще не заморачиваться и купить больше
оборудования - всегда помогает. Но для клиентской части такой подход не
прокатит, если только Вы не собираетесь купить новые компьютеры и
телефоны всем своим клиентам.&lt;/p&gt;
&lt;p&gt;Как с этим бороться?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Убедитесь, что все длинные внутренние циклы выполняются в C (может
    сэкономить до 44% процессорного времени)&lt;/li&gt;
&lt;li&gt;С оптимизацией потребления вычислительных ресурсов все просто:&lt;ul&gt;
&lt;li&gt;есть много готовых решений для профайлинга;&lt;/li&gt;
&lt;li&gt;проблемный код чаще всего не раскидан по всей кодовой базе.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;С оптимизацией потребляемой памяти все сложнее:&lt;ul&gt;
&lt;li&gt;нет готовых решений для профайлинга память (одновременно в
    Python и C);&lt;/li&gt;
&lt;li&gt;много потенциальных причин для повышенного потребления памяти:&lt;ul&gt;
&lt;li&gt;утечки в Python и C;&lt;/li&gt;
&lt;li&gt;фрагментация памяти;&lt;/li&gt;
&lt;li&gt;неэффективное её использование.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Как исправить? Однозначного решения нет - чаще всего приходится
    перерывать всю кодовую базу в поисках неоптимальных мест и
    источников утечек.&lt;/li&gt;
&lt;li&gt;Почему память фрагментируется?&lt;ul&gt;
&lt;li&gt;Большинство объектов расположены в heap памяти (большой
    последовательный кусок, выделенный приложению);&lt;/li&gt;
&lt;li&gt;Много маленьких объектов вперемешку с большими;&lt;/li&gt;
&lt;li&gt;Много временных объектов вперемешку с постоянными;&lt;/li&gt;
&lt;li&gt;В CPython нет сборщика мусора, позволяющего собрать объекты
    компактно в одной части heap'а.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Решением проблемы с фрагментацией стал собственный аллокатор
    памяти (механизм её выделения под определенные типы объектов):&lt;ul&gt;
&lt;li&gt;В Dropbox обнаружили, что большая часть heap-памяти
    захламляется контейнерами с метаданными файлов, которые
    синхронизируется;&lt;/li&gt;
&lt;li&gt;Было решено вынести их куда-нибудь еще;&lt;/li&gt;
&lt;li&gt;CPython позволяет управлять процессом выделения памяти для
    типов данных расширений;&lt;/li&gt;
&lt;li&gt;Делается это с помощью простых структур на C;&lt;/li&gt;
&lt;li&gt;"Not Rocket Science, just C code";&lt;/li&gt;
&lt;li&gt;Выделяются области памяти по 4Мб, состоящие из заголовка и
    буферов фиксированной длины.&lt;/li&gt;
&lt;li&gt;Что насчет внутренней фрагментации?&lt;ul&gt;
&lt;li&gt;по идее же даже в одном буфере останутся данные, весь
    блок нельзя будет освободить...&lt;/li&gt;
&lt;li&gt;потенциально этот факт может привести к еще большему
    расходу оперативной памяти;&lt;/li&gt;
&lt;li&gt;но это оказывается не так, если учесть, что все объекты
    являются временными, то есть живут не долго!&lt;/li&gt;
&lt;li&gt;в результате Dropbox редко использует более 100Мб памяти
    для больших синхронизаций (раньше эта цифра могла
    достигать 1.5Гб)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Но писать C расширение для каждой структуры данных, которую
    может понадобиться вынести в отдельный аллокатор - дело
    неблагодарное:&amp;nbsp;в результате эта история закончилась
    изменением &lt;code&gt;type_new()&lt;/code&gt; в &lt;code&gt;Objects/typeobject.c&lt;/code&gt; для того,
    чтобы можно было указывать &lt;code&gt;__use_region_allocator__&lt;/code&gt;
    = True** для тех классов, которым требуется такой механизм
    выделения памяти.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Python стал ключом к технической реализации обоих сторон Dropbox:
    серверной и клиентской.&lt;/li&gt;
&lt;li&gt;Залогом успеха является максимальная простота с пользовательской
    точки зрения: "положил файл в папку и он становится доступен
    отовсюду"&lt;/li&gt;
&lt;li&gt;Dropbox не брали на себя реализацию стороннего функционала, вроде
    собственно создания хранилища файлов - используется &lt;a href="https://www.insight-it.ru/goto/d0869408/" rel="nofollow" target="_blank" title="http://aws.amazon.com/s3/"&gt;Amazon S3&lt;/a&gt;, что позволило им запуститься очень
    быстро и стремительно.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Лучше решать одну задачу, но качественно, чем много, но так себе!&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/afbbb2b7/" rel="nofollow" target="_blank" title="http://pycon.blip.tv/file/4878722/"&gt;Видео&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/f70a4522/" rel="nofollow" target="_blank" title="https://www.dropbox.com/s/mwxsyxtu9qieboo/pycon%20talk%20minus%20video.pptx"&gt;Презентация&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 18 Mar 2011 18:57:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-03-18:highload/2011/arkhitektura-dropbox/</guid><category>ctypes</category><category>Dropbox</category><category>MySQL</category><category>py2app</category><category>py2exe</category><category>PyObjC</category><category>PyWin32</category><category>Twisted</category><category>wxPython</category><category>Архитектура Dropbox</category><category>Масштабируемость</category></item><item><title>Архитектура Twitter. Два года спустя.</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-twitter-dva-goda-spustya/</link><description>&lt;p&gt;В далеком 2008м я уже публиковал статью про &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-twitter/"&gt;архитектуру Twitter&lt;/a&gt;, но время летит
стремительно и она уже абсолютно устарела. За это время аудитория
Twitter росла просто фантастическими темпами и многое поменялось и с
технической точки зрения. Интересно что новенького у одного из самых
популярных социальных интернет-проектов?&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;3 год, 2 месяца и 1 день потребовалось Twitter, чтобы набрать 1
    миллиард твитов&lt;/li&gt;
&lt;li&gt;На сегодняшний день, чтобы отправить миллиард твитов пользователям
    нужна всего одна неделя&lt;/li&gt;
&lt;li&gt;752% рост аудитории за 2008 год&lt;/li&gt;
&lt;li&gt;1358% рост аудитории за 2009 год&amp;nbsp;(без учета API, по данным comScore)&lt;/li&gt;
&lt;li&gt;175 миллионов зарегистрированных пользователей на сентябрь 2010 года&lt;/li&gt;
&lt;li&gt;460 тысяч регистраций пользователей в день&lt;/li&gt;
&lt;li&gt;9й сайт в мире по популярности (по данным Alexa, год назад был на 12
    месте)&lt;/li&gt;
&lt;li&gt;50 миллионов твитов в день год назад, 140 миллионов твитов в день
    месяц назад, 177 миллионов твитов в день на 11 марта 2011г.&lt;/li&gt;
&lt;li&gt;Рекорд по количеству твитов за секунду 6939, установлен через минуту
    после того, как Новый Год 2011 наступил в Японии&lt;/li&gt;
&lt;li&gt;600 миллионов поисков в день&lt;/li&gt;
&lt;li&gt;Лишь 25% трафика приходится на веб сайт, остальное идет через API&lt;/li&gt;
&lt;li&gt;Росто числа мобильных пользователей за последний год 182%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;6 миллиардов&lt;/strong&gt; запросов к API в день, около 70 тысяч в секунду&lt;/li&gt;
&lt;li&gt;8, 29, 130, 350, 400 - это количество сотрудников Twitter на январь
    2008, январь 2009, январь 2010, январь и март 2011, соответственно&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Самая свежая &lt;a href="https://www.insight-it.ru/goto/682783c0/" rel="nofollow" target="_blank" title="http://blog.twitter.com/2011/03/numbers.html"&gt;статистика про Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; + &lt;code&gt;mod_proxy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/unicorn/"&gt;Unicorn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt; +&amp;nbsp;&lt;a href="/tag/ror/"&gt;Ruby on Rails&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/flock/"&gt;Flock&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/kestrel/"&gt;Kestrel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/scribe/"&gt;Scribe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt;, &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt; и &lt;a href="/tag/pig/"&gt;Pig&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Сравните с аналогичным разделом предыдущей статьи о Twitter - увидите
много новых лиц, подробнее ниже.&lt;/p&gt;
&lt;h2 id="oborudovanie"&gt;Оборудование&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Сервера расположены в NTT America&lt;/li&gt;
&lt;li&gt;Никаких облаков и виртуализации, существующие решения страдают
    слишком высокими задержками&lt;/li&gt;
&lt;li&gt;Более тысячи серверов&lt;/li&gt;
&lt;li&gt;Планируется переезд в собственный датацентр&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="chto-takoe-tvit"&gt;Что такое твит?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Сообщение длиной до 140 символов + метаданные&lt;/li&gt;
&lt;li&gt;Типичные запросы:&lt;ul&gt;
&lt;li&gt;по идентификатору&lt;/li&gt;
&lt;li&gt;по автору&lt;/li&gt;
&lt;li&gt;по @упоминаниям пользователей&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Процесс обработки запроса в Twitter" class="responsive-img" src="https://www.insight-it.ru/images/twitter-request-flow.jpeg" title="Процесс обработки запроса в Twitter"/&gt;&lt;/p&gt;
&lt;h3 id="unicorn"&gt;Unicorn&lt;/h3&gt;
&lt;p&gt;Сервер приложений для Rails:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Развертывание новых версий кода &lt;strong&gt;без простоя&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;На 30% меньше расход вычислительных ресурсов и оперативной памяти,
    по сравнению с другими решениями&lt;/li&gt;
&lt;li&gt;Перешли с &lt;code&gt;mod_proxy_balancer&lt;/code&gt; на &lt;code&gt;mod_proxy_pass&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="rails"&gt;Rails&lt;/h3&gt;
&lt;p&gt;Используется в основном для генерации страниц, работа за сценой
реализована на чистом Ruby или Scala.&lt;/p&gt;
&lt;p&gt;Столкнулись со следующими проблемами:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Проблемы с кэшированием, особенно по части инвалидации&lt;/li&gt;
&lt;li&gt;ActiveRecord генерирует не самые удачные SQL-запросы, что замедляло
    время отклика&lt;/li&gt;
&lt;li&gt;Высокие задержки в очереди и при репликации&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="memcached"&gt;memcached&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;memcached не идеален. Twitter начал сталкиваться с Segmentation
    Fault в нем очень рано.&lt;/li&gt;
&lt;li&gt;Большинство стратегий кэширования основываются на длинных TTL
    (более минуты).&lt;/li&gt;
&lt;li&gt;Вытеснение данных делает его непригодным для важных конфигурационных
    данных (например флагов "темного режима", о котором пойдет речь
    ниже).&lt;/li&gt;
&lt;li&gt;Разбивается на несколько пулов для улучшения производительности и
    снижения риска вытеснения.&lt;/li&gt;
&lt;li&gt;Оптимизированная библиотека для доступа к memcached из Ruby на
    основе libmemcached + FNV hash, вместо чистого Ruby и md5.&lt;/li&gt;
&lt;li&gt;Twitter является одним их наиболее активных проектов, участвующих в
    разработке libmemcached.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="mysql"&gt;MySQL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Разбиение больших объемов данных является тяжелой задачей.&lt;/li&gt;
&lt;li&gt;Задержки в репликации и вытеснение данных из кэша является причиной
    нарушения целостности данных с точки зрения конечного пользователя.&lt;/li&gt;
&lt;li&gt;Блокировки создают борьбу за ресурсы для популярных данных.&lt;/li&gt;
&lt;li&gt;Репликация однопоточна и происходит недостаточно быстро.&lt;/li&gt;
&lt;li&gt;Данные социальных сетей плохо подходят для реляционных СУБД:&lt;ul&gt;
&lt;li&gt;NxN отношения, социальный граф и обход деревьев - не самые
    подходящие задачи для таких баз данных&lt;/li&gt;
&lt;li&gt;Проблемы с дисковой подсистемой (выбор файловой системы,
    noatime, алгоритм планирования)&lt;/li&gt;
&lt;li&gt;ACID практически не требуется&lt;/li&gt;
&lt;li&gt;Для очередей также практически непригодны&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Twitter сталкивался с большими проблемами касательно таблиц
    пользователей и их статусов&lt;/li&gt;
&lt;li&gt;Читать данные с мастера при Master/Slave репликации = медленная
    смерть&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="flockdb"&gt;FlockDB&lt;/h3&gt;
&lt;p&gt;Масштабируемое хранилище для данных социального графа:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Разбиение данных через Gizzard&lt;/li&gt;
&lt;li&gt;Множество серверов MySQL в качестве низлежащей системы хранения&lt;/li&gt;
&lt;li&gt;В Twitter содержит 13 миллиардов ребер графа и обеспечивает 20 тысяч
    операций записи и 100 тысяч операций чтения в секунду&lt;/li&gt;
&lt;li&gt;Грани хранятся и индексируются в обоих направлениях&lt;/li&gt;
&lt;li&gt;Поддерживает распределенный подсчет количества строк&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/4fe0530b/" rel="nofollow" target="_blank" title="https://github.com/twitter/flockdb"&gt;Open source!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Среднее время на выполнение операций:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Подсчет количества строк: 1мс&lt;/li&gt;
&lt;li&gt;Временные запросы: 2мс&lt;/li&gt;
&lt;li&gt;Запись: 1мс для журнала, 16мс для надежной записи&lt;/li&gt;
&lt;li&gt;Обход дерева: 100 граней/мс&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Подробнее про эволюцию систем хранения данных в Twitter &lt;a href="https://www.insight-it.ru/goto/32077a90/" rel="nofollow" target="_blank" title="http://www.slideshare.net/nkallen/q-con-3770885"&gt;в презентации
Nick Kallen&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="cassandra"&gt;Cassandra&lt;/h3&gt;
&lt;p&gt;Распределенная система хранения данных, ориентированная на работу в
реальном времени:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Изначально разработана в &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Очень высокая производительность на запись&lt;/li&gt;
&lt;li&gt;Из слабых сторон: высокая задержка при случайном доступе&lt;/li&gt;
&lt;li&gt;Децентрализованная, способна переносить сбои оборудования&lt;/li&gt;
&lt;li&gt;Гибкая схема данных&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Планируется полный переход на нее по
    следующему алгоритму:&lt;/del&gt;&lt;ul&gt;
&lt;li&gt;&lt;del&gt;Все твиты пишутся и в Cassandra
    и в MySQL&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Динамически часть операций
    чтения переводится на Cassandra&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Анализируется реакция системы,
    что сломалось&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Полностью отключаем чтение из
    Cassandra, чиним неисправности&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Начинаем сначала&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/e83e4e8e/" rel="nofollow" target="_blank" title="http://engineering.twitter.com/2010/07/cassandra-at-twitter-today.html"&gt;Обновление:&lt;/a&gt;&lt;/strong&gt; стратегия по поводу использования Cassandra изменилась, попытки
    использовать её в роли основного хранилища для твитов прекратились,
    но она продолжает использоваться для аналитики и географической
    информации.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Подробнее почему Twitter пришел к решению использовать Cassandra можно
прочитать &lt;a href="https://www.insight-it.ru/goto/ffc31d1/" rel="nofollow" target="_blank" title="http://www.slideshare.net/ryansking/scaling-twitter-with-cassandra"&gt;в отдельной презентации&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Помимо всего прочего Cassandra&amp;nbsp;&lt;del&gt;планируется использовать&lt;/del&gt; используется для аналитики в реальном времени.&lt;/p&gt;
&lt;h3 id="scribe"&gt;Scribe&lt;/h3&gt;
&lt;p&gt;Пользователи Twitter генерируют огромное количество данных, около 15-25
Гб в минуту, более 12 Тб в день, и эта цифра удваивается несколько раз
в год.&lt;/p&gt;
&lt;p&gt;Изначально для сбора логов использовали &lt;code&gt;syslog-ng&lt;/code&gt;, но он очень быстро
перестал справляться с нагрузкой.&lt;/p&gt;
&lt;p&gt;Решение нашлось очень просто: &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt; столкнулся с
аналогичной проблемой и разработал проект Scribe, который был
опубликован в opensource.&lt;/p&gt;
&lt;p&gt;По сути это фреймворк для сбора и агрегации логов, основанный на
&lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt;. Вы пишете текст для логов и указываете
категорию, остальное он берет на себя.&lt;/p&gt;
&lt;p&gt;Работает локально, надежен даже в случае потери сетевого соединения,
каждый узел знает только на какой сервер передавать логи, что позволяет
создавать масштабируемую иерархию для сбора логов.&lt;/p&gt;
&lt;p&gt;Поддерживаются различные системы для записи в данным, &amp;nbsp;в том числе
обычные файлы и HDFS (о ней ниже).&lt;/p&gt;
&lt;p&gt;Этот продукт полностью решил проблему Twitter со сбором логов,
используется около 30 различных категорий. В процессе использования была
создана и опубликована масса доработок. Активно сотрудничают с командой
Facebook в развитии проекта.&lt;/p&gt;
&lt;h3 id="hadoop"&gt;Hadoop&lt;/h3&gt;
&lt;p&gt;Как Вы обычно сохраняете 12Тб новых данных, поступающих каждый день?&lt;/p&gt;
&lt;p&gt;Если считать, что средняя скорость записи современного жесткого диска
составляет 80Мбайт в секунду, запись 12Тб данных заняла бы почти 48
часов.&lt;/p&gt;
&lt;p&gt;На одном даже очень большом сервере данную задачу не решить, логичным
решением задачи стало использование кластера для хранения и анализа
таких объемов данных.&lt;/p&gt;
&lt;p&gt;Использование кластерной файловой системы добавляет сложности, но
позволяет меньше заботиться о деталях.&lt;/p&gt;
&lt;p&gt;Hadoop Distributed File System (HDFS) предоставляет возможность
автоматической репликации и помогает справляться со сбоями оборудования.&lt;/p&gt;
&lt;p&gt;MapReduce framework позволяет обрабатывать огромные объемы данных,
анализируя пары ключ-значение.&lt;/p&gt;
&lt;p&gt;Типичные вычислительные задачи, которые решаются с помощью Hadoop в
Twitter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Вычисление связей дружбы в социальном графе (&lt;code&gt;grep&lt;/code&gt; и &lt;code&gt;awk&lt;/code&gt; не
    справились бы, self join в MySQL на таблицах с миллиардами строк -
    тоже)&lt;/li&gt;
&lt;li&gt;Подсчет статистики (количество пользователей и твитов, например
    подсчет количества твитов занимает 5 минут при 12 миллиардах
    записей)&lt;/li&gt;
&lt;li&gt;Подсчет PageRank между пользователями для вычисления репутации.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В твиттер используется бесплатный дистрибутив от Cloudera, версия Hadoop
0.20.1, данные храняться &lt;a href="https://www.insight-it.ru/goto/1ac5bba3/" rel="nofollow" target="_blank" title="https://github.com/kevinweil/hadoop-lzo"&gt;в сжатом по алгоритму LZO виде&lt;/a&gt;, библиотеки для работы с
данными опубликованы под названием
&lt;a href="https://www.insight-it.ru/goto/a1b5430e/" rel="nofollow" target="_blank" title="https://github.com/kevinweil/elephant-bird"&gt;elephant-bird&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="pig"&gt;Pig&lt;/h3&gt;
&lt;p&gt;Для того чтобы анализировать данные с помощью MapReduce обычно
необходимо разрабатывать код на Java, что далеко не все умеют делать, да
и трудоемко это.&lt;/p&gt;
&lt;p&gt;Pig представляет собой высокоуровневый язык, позволяющий
трансформировать огромные наборы данных шаг за шагом.&lt;/p&gt;
&lt;p&gt;Немного напоминает SQL, но намного проще. Это позволяет писать в 20 раз
меньше кода, чем при анализе данных с помощью обычных MapReduce работ.
Большая часть работы по анализу данных в Twitter осуществляется с
помощью Pig.&lt;/p&gt;
&lt;h3 id="dannye"&gt;Данные&lt;/h3&gt;
&lt;p&gt;Полу-структурированные данные:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;логи Apache, RoR, MySQL, A/B тестирования, процесса регистрации&lt;/li&gt;
&lt;li&gt;поисковые запросы&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Структурированные данные:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Твиты&lt;/li&gt;
&lt;li&gt;Пользователи&lt;/li&gt;
&lt;li&gt;Блок-листы&lt;/li&gt;
&lt;li&gt;Номера телефонов&lt;/li&gt;
&lt;li&gt;Любимые твиты&lt;/li&gt;
&lt;li&gt;Сохраненные поиски&lt;/li&gt;
&lt;li&gt;Ретвиты&lt;/li&gt;
&lt;li&gt;Авторизации&lt;/li&gt;
&lt;li&gt;Подписки&lt;/li&gt;
&lt;li&gt;Сторонние клиенты&lt;/li&gt;
&lt;li&gt;География&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Запутанные данные:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Социальный граф&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Что же они делают с этим всем?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Подсчет математического ожидания, минимума, максимума и дисперсии
    следующих показателей:&lt;ul&gt;
&lt;li&gt;Количество запросов за сутки&lt;/li&gt;
&lt;li&gt;Средняя задержка, 95% задержка&lt;/li&gt;
&lt;li&gt;Распределение кодов HTTP-ответов (по часам)&lt;/li&gt;
&lt;li&gt;Количество поисков осуществляется каждый день&lt;/li&gt;
&lt;li&gt;Количество уникальных запросов и пользователей&lt;/li&gt;
&lt;li&gt;Географическое распределение запросов и пользователей&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Подсчет вероятности, ковариации, влияния:&lt;ul&gt;
&lt;li&gt;Как отличается использование через мобильные устройства?&lt;/li&gt;
&lt;li&gt;Как влияет использование клиентов сторонних разработчиков?&lt;/li&gt;
&lt;li&gt;Когортный анализ&lt;/li&gt;
&lt;li&gt;Проблемы с сайтом (киты и роботы, подробнее ниже)&lt;/li&gt;
&lt;li&gt;Какие функциональные возможности цепляют пользователей?&lt;/li&gt;
&lt;li&gt;Какие функциональные возможности чаще используются популярными
    пользователями?&lt;/li&gt;
&lt;li&gt;Корректировка и предложение поисковых запросов&lt;/li&gt;
&lt;li&gt;A/B тестирование&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Предсказания, анализ графов, естественные языки:&lt;ul&gt;
&lt;li&gt;Анализ пользователей по их твитам, твитов, на которые они
    подписаны, твитам их фоловеров&lt;/li&gt;
&lt;li&gt;Какая структура графа ведет к успешным популярным сетям&lt;/li&gt;
&lt;li&gt;Пользовательская репутация&lt;/li&gt;
&lt;li&gt;Анализ эмоциональной окраски&lt;/li&gt;
&lt;li&gt;Какие особенности заставляют людей ретвитнуть твит?&lt;/li&gt;
&lt;li&gt;Что влияет на глубину дерева ретвитов ?&lt;/li&gt;
&lt;li&gt;Долгосрочное обнаружение дубликатов&lt;/li&gt;
&lt;li&gt;Машинное обучение&lt;/li&gt;
&lt;li&gt;Обнаружения языка&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Подробнее про обработку данных &lt;a href="https://www.insight-it.ru/goto/3d4649ef/" rel="nofollow" target="_blank" title="http://www.slideshare.net/kevinweil/nosql-at-twitter-nosql-eu-2010"&gt;в презентации Kevin Weil&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="hbase"&gt;HBase&lt;/h3&gt;
&lt;p&gt;Twitter начинают строить настоящие сервисы на основе Hadoop, например
поиск людей:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HBase используется как изменяемая прослойка над HDFS&lt;/li&gt;
&lt;li&gt;Данные экспортируются из HBase c помощью периодической MapReduce
    работы:&lt;ul&gt;
&lt;li&gt;На этапе Map используются также данные из FlockDB и нескольких
    внутренних сервисов&lt;/li&gt;
&lt;li&gt;Собственная схема разбиения данных&lt;/li&gt;
&lt;li&gt;Данные подтягиваются через высокопроизводительный, горизонтально
    масштабируемый сервис на Scala (&lt;a href="https://www.insight-it.ru/goto/917f8c95/" rel="nofollow" target="_blank" title="http://www.slideshare.net/al3x/building-distributed-systems-in-scala"&gt;подробнее о построении распределенных сервисов на Scala&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;На основе HBase разрабатываются и другие продукты внутри Twitter.&lt;/p&gt;
&lt;p&gt;Основными её достоинствами являются гибкость и легкая интеграция с
Hadoop и Pig.&lt;/p&gt;
&lt;p&gt;По сравнению с Cassandra:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"Их происхождение объясняет их сильные и слабые стороны"&lt;/li&gt;
&lt;li&gt;HBase построен на основе системы по пакетной обработке данных,
    высокие задержки, работает далеко не в реальном времени&lt;/li&gt;
&lt;li&gt;Cassandra построена с нуля для работы с низкими задержками&lt;/li&gt;
&lt;li&gt;HBase легко использовать при анализе данных как источник или место
    сохранения результатов, Cassandra для этого подходит меньше, но они
    работают над этим&lt;/li&gt;
&lt;li&gt;HBase на данный момент единственную точку отказа в виде мастер-узла&lt;/li&gt;
&lt;li&gt;В твиттере HBase используется для аналитики, анализа и создания
    наборов данных, а Cassandra - для онлайн систем&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="loony"&gt;Loony&lt;/h3&gt;
&lt;p&gt;Централизованная система управления оборудованием.&lt;/p&gt;
&lt;p&gt;Реализована с использованием:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/django/"&gt;Django&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8927633f/" rel="nofollow" target="_blank" title="http://www.lag.net/paramiko"&gt;Paraminko&lt;/a&gt; (реализация протокола SSH
    на Python, разработана и опубликована в opensource в Twitter)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Интегрирована с LDAP, анализирует входящую почту от датацентра и
автоматически вносит изменения в базу.&lt;/p&gt;
&lt;h3 id="murder"&gt;Murder&lt;/h3&gt;
&lt;p&gt;Система развертывания кода и ПО, основанная на протоколе BitTorrent.&lt;/p&gt;
&lt;p&gt;Благодаря своей P2P природе позволяет обновить более тысячи серверов за
30-60 секунд.&lt;/p&gt;
&lt;h3 id="kestrel"&gt;Kestrel&lt;/h3&gt;
&lt;p&gt;Распределенная очередь, работающая по протоколу memcache:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;set&lt;/code&gt; - поставить в очередь&lt;/li&gt;
&lt;li&gt;&lt;code&gt;get&lt;/code&gt; - взять из очереди&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Особенности:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Отсутствие строгого порядка выполнения заданий&lt;/li&gt;
&lt;li&gt;Отсутствие общего состояния между серверами&lt;/li&gt;
&lt;li&gt;Разработана на &lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="daemony"&gt;Daemon'ы&lt;/h3&gt;
&lt;p&gt;Каждый твит обрабатывается с помощью daemon'ов.&lt;/p&gt;
&lt;p&gt;В unicorn обрабатываются только HTTP запросы, вся работа за сценой
реализована в виде отдельных daemon'ов.&lt;/p&gt;
&lt;p&gt;Раньше использовалось много разных демонов, по одному на каждую задачу
(Rails), но перешли к меньшему их количеству, способному решать
несколько задач одновременно.&lt;/p&gt;
&lt;h3 id="kak-oni-spravliaiutsia-s-takimi-tempami-rosta"&gt;Как они справляются с такими темпами роста?&lt;/h3&gt;
&lt;p&gt;Рецепт прост, но эффективен, подходит практически для любого
интернет-проекта:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;обнаружить самое слабое место в системе;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;принять меры по его устранению;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;перейти к следующему самому слабому месту.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;На словах звучит и правда примитивно, но на практике нужно предпринять
ряд мер, чтобы такой подход был бы реализуем:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Автоматический сбор метрик (причем в агрегированном виде)&lt;/li&gt;
&lt;li&gt;Построение графиков (RRD, Ganglia)&lt;/li&gt;
&lt;li&gt;Сбор и анализ&amp;nbsp;логов&lt;/li&gt;
&lt;li&gt;Все данные должны получаться с минимальной задержкой, как можно
    более близко к реальному времени&lt;/li&gt;
&lt;li&gt;Анализ:&lt;ul&gt;
&lt;li&gt;Из данных необходимо получать &lt;em&gt;информацию&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Следить за динамикой показателей: стало лучше или хуже?&lt;/li&gt;
&lt;li&gt;Особенно при развертывании новых версий кода&lt;/li&gt;
&lt;li&gt;Планирование использования ресурсов намного проще, чем решение
    экстренных ситуаций, когда они на исходу&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Примерами агрегированных метрик в Twitter являются "киты" и "роботы",
вернее их количество в единицу времени.&lt;/p&gt;
&lt;h5&gt;Что такое "робот"?&lt;/h5&gt;
&lt;p&gt;&lt;img alt="Twitter Робот" class="responsive-img" src="https://www.insight-it.ru/images/twitter-bot.jpg" title="Twitter Робот"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ошибка внутри Rails (HTTP 500)&lt;/li&gt;
&lt;li&gt;Непойманное исключение&lt;/li&gt;
&lt;li&gt;Проблема в коде или нулевой результат&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Что такое "кит"?&lt;/h5&gt;
&lt;p&gt;&lt;img alt="Twitter Кит" class="responsive-img" src="https://www.insight-it.ru/images/twitter-whale.jpg" title="Twitter Кит"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HTTP ошибка 502 или 503&lt;/li&gt;
&lt;li&gt;В твиттер используется фиксированный таймаут в 5 секунд (лучше
    кому-то показать ошибку, чем захлебнуться в запросах)&lt;/li&gt;
&lt;li&gt;Убитый слишком длинный запрос к базе данных (mkill)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Значительное превышение нормального количества китов или роботов в
минуту является поводом для беспокойством.&lt;/p&gt;
&lt;p&gt;Реализован этот механизм простым bash-скриптом, который просматривает
агрегированные логи за последние 60 секунд, подсчитывает количество
китов/роботов и рассылает уведомления, если значение оказалось выше
порогового значения. Подробнее про работу команды оперативного
реагирования &lt;a href="https://www.insight-it.ru/goto/30562be/" rel="nofollow" target="_blank" title="http://www.slideshare.net/netik/billions-of-hits-scaling-twitter"&gt;в презентации John Adams&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="temnyi-rezhim"&gt;"Темный режим"&lt;/h3&gt;
&lt;p&gt;Для экстренных ситуаций в Twitter предусмотрен так называемый "темный
режим", который представляет собой набор механизмов для отключения
тяжелых по вычислительным ресурсам или вводу-выводу функциональных
частей сайта. Что-то вроде стоп-крана для сайта.&lt;/p&gt;
&lt;p&gt;Имеется около 60 выключателей, в том числе и полный режим "только для
чтения".&lt;/p&gt;
&lt;p&gt;Все изменения в настройках этого режима фиксируются в логах и сообщаются
руководству, чтобы никто не баловался.&lt;/p&gt;
&lt;h2 id="podvodim-itogi_1"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Не бросайте систему на самотек, начинайте собирать метрики и их
    визуализировать как можно раньше&lt;/li&gt;
&lt;li&gt;Заранее планируйте рост требуемых ресурсов и свои действия в случае
    экстренных ситуаций&lt;/li&gt;
&lt;li&gt;Кэшируйте по максимуму все, что возможно&lt;/li&gt;
&lt;li&gt;Все инженерные решения не вечны, ни одно из решений не идеально, но
    многие будут нормально работать в течение какого-то периода времени&lt;/li&gt;
&lt;li&gt;Заранее начинайте задумываться о плане масштабирования&lt;/li&gt;
&lt;li&gt;Не полагайтесь полностью на memcached и базу данных - они могут Вас
    подвести в самый неподходящий момент&lt;/li&gt;
&lt;li&gt;Все данные для запросов в реальном времени должны находиться в
    памяти, диски в основном для записи&lt;/li&gt;
&lt;li&gt;Убивайте медленные запросы (mkill) прежде, чем они убьют всю систему&lt;/li&gt;
&lt;li&gt;Некоторые задачи могут решаться путем предварительного подсчета и
    анализа, но далеко не все&lt;/li&gt;
&lt;li&gt;Приближайте вычисления к данным по возможности&lt;/li&gt;
&lt;li&gt;Используйте не mongrel, а unicorn для RoR&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Спасибо за внимание, &lt;a href="/feed/"&gt;жду Вас снова&lt;/a&gt;! Буду рад, если Вы
&lt;a href="https://www.insight-it.ru/goto/26b8fa1/" rel="nofollow" target="_blank" title="http://twitter.com/blinkov"&gt;подпишитесь на меня в Twitter&lt;/a&gt;, с
удовольствием пообщаюсь со всеми читателями :)&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 05 Mar 2011 20:47:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-03-05:highload/2011/arkhitektura-twitter-dva-goda-spustya/</guid><category>Apache</category><category>Cassandra</category><category>featured</category><category>Flock</category><category>FlockDB</category><category>Hadoop</category><category>HBase</category><category>Kestrel</category><category>Memcached</category><category>MySQL</category><category>Pig</category><category>Ruby</category><category>Ruby on Rails</category><category>Scala</category><category>Scribe</category><category>Twitter</category><category>Unicorn</category><category>архитектура Twitter</category><category>интернет-проекты</category><category>Масштабируемость</category><category>социальная сеть</category></item><item><title>Архитектура DISQUS</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-disqus/</link><description>&lt;p&gt;&lt;img alt="DISQUS" class="left" src="https://www.insight-it.ru/images/disqus.jpg" title="DISQUS"/&gt;
&lt;a href="https://www.insight-it.ru/goto/a754581e/" rel="nofollow" target="_blank" title="https://disqus.com"&gt;DISQUS&lt;/a&gt; - самая популярная система
комментирования и одновременно самое большое в мире Django-приложение.
Она установлена более чем на полумиллионе сайтов и блогов, в том числе и
очень крупных, таких как Engadget, CNN, MTV, IGN. Основной особенностью
в её реализации является тот факт, что DISQUS не является тем сайтом,
который хотят увидеть пользователи, он лишь предоставляет механизмы
комментирования, авторизации и интеграции с социальными сетями. Пики
нагрузки возникают одновременно c появлением какой-то шумихи в
Интернете, что достаточно непредсказуемо. Как же им удается справляться
с этой ситуацией?&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; - операционная система&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt; - язык программирования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/django/"&gt;Django&lt;/a&gt; - основной framework&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache 2.2&lt;/a&gt; +&amp;nbsp;&lt;a href="/tag/wsgi/"&gt;mod_wsgi&lt;/a&gt; - веб-сервер&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/postgresql/"&gt;PostgreSQL&lt;/a&gt; - СУБД&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; - кэширование&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/haproxy/"&gt;HAProxy&lt;/a&gt; - балансировка нагрузки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/slony/"&gt;Slony&lt;/a&gt; - репликация данных&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/heartbeat/"&gt;heartbeat&lt;/a&gt; - обеспечение
    доступности&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;До 17 тысяч запросов в секунду&lt;/li&gt;
&lt;li&gt;500 000 сайтов&lt;/li&gt;
&lt;li&gt;15 миллионов зарегистрированных пользователей&lt;/li&gt;
&lt;li&gt;75 миллионов комментариев&lt;/li&gt;
&lt;li&gt;250 миллионов посетителей (на август 2010г.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="osnovnye-trudnosti"&gt;Основные трудности&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Непредсказуемость нагрузки (основными причинами шумихи в Интернете
    являются катастрофы и выходки знаменитостей)&lt;/li&gt;
&lt;li&gt;Обсуждения никогда не теряют актуальность (нельзя держать в кэше все
    дискуссии с 2008 года)&lt;/li&gt;
&lt;li&gt;Нельзя угадать на каком сайте из тысяч возникнет пик трафика&lt;/li&gt;
&lt;li&gt;Персональные настройки, динамическое разбиение на страницы и
    сортировки снижают эффективность кэширования&lt;/li&gt;
&lt;li&gt;Высокая доступность (из-за разнообразия сайтов и их аудитории сложно
    запланировать технические работы)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Архитектура DISQUS" class="responsive-img" src="https://www.insight-it.ru/images/disqus_architecture.jpeg" title="Архитектура DISQUS"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Оборудование&lt;/strong&gt;, в сумме около 100 серверов:&lt;ul&gt;
&lt;li&gt;30% веб-серверов (Apache + &lt;code&gt;mod_wsgi&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;10% серверов баз данных (PostgreSQL)&lt;/li&gt;
&lt;li&gt;25% кэш-серверов (memcached)&lt;/li&gt;
&lt;li&gt;20% балансировка нагрузки и обеспечение доступности (HAProxy +
    heartbeat)&lt;/li&gt;
&lt;li&gt;15% прочие сервера (Python скрипты)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Балансировка нагрузки&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;HAProxy:&lt;ul&gt;
&lt;li&gt;Высокая производительность&lt;/li&gt;
&lt;li&gt;Интеллектуальная проверка доступности&lt;/li&gt;
&lt;li&gt;Неплохая статистика&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Репликация&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Используется Slony-I&lt;/li&gt;
&lt;li&gt;Основана на триггерах&lt;/li&gt;
&lt;li&gt;Master/Slave для обеспечения большего объема операций чтения&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Высокая доступность&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;heartbeat&lt;/li&gt;
&lt;li&gt;Пассивная копия мастер баз данных на случай сбоя основной&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Партиционирование&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Реализовано на уровне кода&lt;/li&gt;
&lt;li&gt;Простая реализация, быстрые положительные результаты&lt;/li&gt;
&lt;li&gt;Два метода разделения данных:&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Вертикальное:&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;Создание нескольких таблиц с меньшим количеством колонок
    вместо одной (она же нормализация)&lt;/li&gt;
&lt;li&gt;Позволяет разделять базы данных&lt;/li&gt;
&lt;li&gt;Данные объединяются в коде (медленнее, чем на уровне
    СУБД, но не намного)&lt;/li&gt;
&lt;li&gt;Бартер производительности на масштабируемость&lt;/li&gt;
&lt;li&gt;Более эффективное кэшировние&lt;/li&gt;
&lt;li&gt;Механизм роутеров в Django позволяет достаточно легко
    реализовать данный функционал&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Горизонтальное:&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;Некоторые сайты имеют очень большие массивы данных&lt;/li&gt;
&lt;li&gt;Партнеры требуют повышенного уровня доступности&lt;/li&gt;
&lt;li&gt;Помогает снижать загрузку по записи на мастер базе
    данных&lt;/li&gt;
&lt;li&gt;В основном используется все же вертикальное
    партиционирование&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Производительность базы данных&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Особое внимание уделяется тому, чтобы индексы помещались в
    оперативную память&lt;/li&gt;
&lt;li&gt;Логирование медленных запросов (автоматизировано с помощью
    syslog-ng + pgFouine + cron)&lt;/li&gt;
&lt;li&gt;Использование пулов соединений (Django не умеет этого,
    используется pgbouncer, позволяет экономить на ресурсоемких
    операциях установления и прекращения соединений)&lt;/li&gt;
&lt;li&gt;Оптимизация QuerySet'ов:&lt;ul&gt;
&lt;li&gt;Не используется чистый SQL&lt;/li&gt;
&lt;li&gt;Встроенный кэш позволяет выделять части выборки&lt;/li&gt;
&lt;li&gt;Но это не всегда нужно, они убрали этот кэш&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Атомарные операции:&lt;ul&gt;
&lt;li&gt;Поддерживают консистентность данных&lt;/li&gt;
&lt;li&gt;Использование update(), так как save() не является
    thread-safe&lt;/li&gt;
&lt;li&gt;Отлично работают для таких вещей, как счетчики&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Транзакции:&lt;ul&gt;
&lt;li&gt;TransactionMiddleware поначалу использовалось, но со
    временем стало обузой&lt;/li&gt;
&lt;li&gt;В &lt;code&gt;postgrrsql_psycopg2&lt;/code&gt; есть опция autocommit:&lt;ul&gt;
&lt;li&gt;Это означает что каждый запрос выполняется в отдельной
    транзакции&lt;/li&gt;
&lt;li&gt;Обработка каждого пользовательского HTTP-запроса не
    начинает новую транзакцию&lt;/li&gt;
&lt;li&gt;Но все же транзакции из нескольких операций записи в
    СУБД нужны (сохранение нескольких объектов одновременно
    и полный откат в случае ошибки)&lt;/li&gt;
&lt;li&gt;В итоге все HTTP-запросы по-умолчанию начинаются в
    режиме autocommit, но в случае необходимости
    переключаются в транзакционный режим&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Отложенные сигналы&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Постановка в очередь низкоприоритетных задач (даже если они не
    длинные по времени)&lt;/li&gt;
&lt;li&gt;Асинхронные сигналы очень удобны для разработчика (но не так,
    как настоящие сигналы)&lt;/li&gt;
&lt;li&gt;Модели отправляются в очередь в сериализованном виде&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Кэширование&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Используется memcached&lt;/li&gt;
&lt;li&gt;Новый pylibmcна основе libmemcached в качестве клиента (проекты
    django-pylibmc и django-newcache)&lt;/li&gt;
&lt;li&gt;Настраиваемые алгоритмы поведения клиента&lt;/li&gt;
&lt;li&gt;Используется &lt;code&gt;_auto_reject_hosts&lt;/code&gt; и &lt;code&gt;_retry_timeout&lt;/code&gt; для
    предотвращения повторных подключений к вышедшим из строя
    кэш-серверам&lt;/li&gt;
&lt;li&gt;Алгоритм размещения ключей: консистентное хэширование на основе
    libketama&lt;/li&gt;
&lt;li&gt;Существует проблема, когда одно очень часто используемое
    значение в кэше инвалидируется:&lt;ul&gt;
&lt;li&gt;Множество клиентов одновременно пытаются получить новое
    значение из СУБД одновременно&lt;/li&gt;
&lt;li&gt;В большинстве случаев правильным решением было бы вернуть
    большинству устаревшие данные и позволить одному клиенту
    обновить кэш&lt;/li&gt;
&lt;li&gt;django-newcache и MintCache умеют это делать&lt;/li&gt;
&lt;li&gt;Заполнение кэша новым значением вместо удаления при
    инвалидации также помогает избежать этой проблемы&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Мониторинг&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Информация о производительности запросов к БД, внешних вызовов и
    рендеринге шаблонов записывается через собственный middleware&lt;/li&gt;
&lt;li&gt;Сбор и отображение с помощью Ganglia&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Отключение функционала&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Необходим способ быстро отключить новый функционал, если
    оказывается, что он работает не так, как планировалось&lt;/li&gt;
&lt;li&gt;Система должна срабатывать мгновенно, по всем серверам, без
    записи на диск&lt;/li&gt;
&lt;li&gt;Позволяет запускать новые возможности постепенно, лишь для части
    аудитории&lt;/li&gt;
&lt;li&gt;Позволяет постоянно использовать основную ветку кода&lt;/li&gt;
&lt;li&gt;Аналогичная система используется и в &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Масштабирование команды разработчиков&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Небольшая команда&lt;/li&gt;
&lt;li&gt;Месячная аудитория / количество разработчиков = 40 миллионов&lt;/li&gt;
&lt;li&gt;Это означает:&lt;ul&gt;
&lt;li&gt;Автоматическое тестирование&lt;/li&gt;
&lt;li&gt;И максимально простой процесс разработки&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Новый сотрудник может начать работать уже через несколько минут,
    нужно лишь:&lt;ul&gt;
&lt;li&gt;Установить и настроить PostgreSQL&lt;/li&gt;
&lt;li&gt;Скачать исходный код из &lt;a href="/tag/git/"&gt;git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;С помощью pip и virtualenv установить зависимости&lt;/li&gt;
&lt;li&gt;Изменить настройки в settings.py&lt;/li&gt;
&lt;li&gt;Выполнить автоматическое создание структуры данных
    средствами Django&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Непрерывное тестирование&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Ежедневное развертывание с помощью &lt;a href="/tag/fabric/"&gt;Fabric&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/hudson/"&gt;Hudson&lt;/a&gt; обеспечивает регулярно осуществляет и
    тестирует сборки&lt;/li&gt;
&lt;li&gt;Интегрирован &lt;a href="/tag/selenium/"&gt;Selenium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Быстрое тестирование с помощью &lt;a href="/tag/pyflakes/"&gt;Pyflakes&lt;/a&gt; и
    post-commit hooks&lt;/li&gt;
&lt;li&gt;70 тысяч строк Python кода, 73% покрытие тестами, прогон всех
    тестов занимает 20 минут&lt;/li&gt;
&lt;li&gt;Собственная система исполнения тестов с поддержкой XML,
    Selenium, подсчета количества запросов, тестирования
    Master/Slave базы данных и интеграцией с очередью&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Отслеживание проблем и задач&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Переключились с Trac на Redmine (из-за поддержки под-задач)&lt;/li&gt;
&lt;li&gt;Отправка исключений на e-mail - плохая идея&lt;/li&gt;
&lt;li&gt;Раньше использовали django-db-log, но теперь опубликовали свою
    систему сбора ошибок и логов под названием
    &lt;a href="https://www.insight-it.ru/goto/2e33ac0/" rel="nofollow" target="_blank" title="https://github.com/dcramer/django-sentry"&gt;Sentry&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="delaem-vyvody"&gt;Делаем выводы&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Язык программирования, каким бы он ни был, не является проблемой&lt;/li&gt;
&lt;li&gt;Django в целом очень хорош (но приходится все же использовать набор
    собственных патчей)&lt;/li&gt;
&lt;li&gt;Даже при использовании низкопроизводительного framework можно
    построить масштабируемую систему&lt;/li&gt;
&lt;li&gt;Вертикальное партиционирование позволяет пожертвовать
    производительностью в пользу масштабируемости&lt;/li&gt;
&lt;li&gt;Даже небольшой командой разработчиков можно добиться высоких
    результатов, если не пренебрегать автоматизацией тестирования&lt;/li&gt;
&lt;li&gt;Большое значение имеет возможность вовремя отслеживать и оперативно
    реагировать на сбои&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="istochnik-informatsii"&gt;Источник информации&lt;/h2&gt;
&lt;p&gt;Данная статья написана на основе выступления Jason Yan и David Cramer на
DjangoConf 2010. В презентации можно найти примеры кода, ссылки на
упоминаемые проекты и дополнительные материалы:&lt;/p&gt;
&lt;div class="video-container no-controls"&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="355" marginheight="0" marginwidth="0" scrolling="no" src="//www.slideshare.net/slideshow/embed_code/key/21F2PzBmYATx2Y" width="425"&gt; &lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Другие статьи по масштабируемости высоконагруженных систем можно
почитать &lt;a href="https://www.insight-it.ru/highload/"&gt;в соответствующем разделе&lt;/a&gt;, а вовремя узнавать о
новых - &lt;a href="/feed/"&gt;подписавшись на RSS&lt;/a&gt;. Вчера, кстати, прикрутил DISQUS к
Insight IT, приглашаю постоянных читателей и всех остальных
потестировать :)&lt;/em&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 02 Mar 2011 03:37:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-03-02:highload/2011/arkhitektura-disqus/</guid><category>Apache</category><category>DISQUS</category><category>django</category><category>Fabric</category><category>Ganglia</category><category>Git</category><category>HAProxy</category><category>heartbeat</category><category>Hudson</category><category>Linux</category><category>Memcached</category><category>pgbouncer</category><category>pgFouine</category><category>PostgreSQL</category><category>Pyflakes</category><category>Python</category><category>Selenium</category><category>Slony</category><category>syslog-ng</category><category>WSGI</category><category>Архитектура DISQUS</category><category>Масштабируемость</category></item><item><title>Новое поколение MapReduce в Apache Hadoop</title><link>https://www.insight-it.ru//storage/2011/novoe-pokolenie-mapreduce-v-apache-hadoop/</link><description>&lt;p&gt;В большом бизнесе использование нескольких больших кластеров с
финансовой точки зрения более&amp;nbsp;эффективно, чем много маленьких. Чем
больше машин в кластере, тем большими наборами данных он может
оперировать, больше задач могут выполняться одновременно. Реализация
&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt; в &lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt;
&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; столкнулась с потолком масштабируемости на уровне
около 4000 машин в кластере. Разрабатывается следующее поколение Apaсhe
Hadoop MapReduce, &amp;nbsp;в котором появится общий планировщик ресурсов и
отдельный мастер для каждой отдельной задач, управляющий выполнением
программного кода. Так как простой оборудования по техническим причинам
обходится дорого на таком масштабе, высокий уровень доступности
проектируется с самого начала, ровно как и безопасность и
многозадачность, необходимые для поддержки одновременного использования
большого кластера многими пользователями. Новая архитектура также будет
более инновационной, гибкой и эффективной с точки зрения использования
вычислительных ресурсов.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="predistoriia"&gt;Предистория&lt;/h2&gt;
&lt;p&gt;Текущая реализация Hadoop MapReduce устаревает на глазах. Основываясь на
текущих тенденциях в размерах кластеров и нагрузок на них, JobTracker
требует кардинальных доработок, чтобы исправить его дефекты в области
масштабируемости, потребления памяти, многопоточности, надежности и
производительности. С точки зрения работы с Hadoop при каждом обновлении
кластера (даже если это просто багфикс), абсолютно все компоненты
кластера, так и приложений, которые на нем работают, должны быть
обновлены одновременно. Это так же очень неудобно, так как каждый раз
необходимо тестировать все приложения на совместимость с новой версией.&lt;/p&gt;
&lt;h2 id="trebovaniia"&gt;Требования&lt;/h2&gt;
&lt;p&gt;Прежде чем кардинально что-то менять в Hadoop mapreduce, необходимо
понять какие же основные требования предъявляются к вычислительным
кластерам на практике. Наиболее значительными требованиями к Hadoop
следующего поколения являются:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Надежность&lt;/li&gt;
&lt;li&gt;Доступность&lt;/li&gt;
&lt;li&gt;Масштабируемость - кластеры из как минимум 10 тысяч машин, 200 тысяч
    вычислительных ядер и даже больше&lt;/li&gt;
&lt;li&gt;Обратная и прямая совместимость - возможность быть уверенным, что
    приложение будет работать на новой версии так же, как оно работало
    на старой&lt;/li&gt;
&lt;li&gt;Контроль над обновлениями&lt;/li&gt;
&lt;li&gt;Предсказуемые задержки&lt;/li&gt;
&lt;li&gt;Эффективное использование ресурсов&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Среди менее значительных требований:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Поддержка альтернативных парадигм разработки (помимо MapReduce)&lt;/li&gt;
&lt;li&gt;Поддержка сервисов с коротким жизненным циклом&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Если учесть перечисленные выше требования, то становится очевидно, что
инфраструктура обработки данных в Hadoop должна быть кардинальным
образом изменена. В сообществе Hadoop люди в целом приходят к общему
мнению, что текущая архитектура MapReduce не способна решить текущие
задачи, которые перед ней ставится, и что требуется кардинальный
рефакторинг кодовой базы.&lt;/p&gt;
&lt;h2 id="mapreduce-sleduiushchego-pokoleniia"&gt;MapReduce следующего поколения&lt;/h2&gt;
&lt;p&gt;Фундаментальной идеей смены архитектуры является разделение двух
основных функций JobTracker'а на два отдельных части:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;управление ресурсами;&lt;/li&gt;
&lt;li&gt;планирования и мониторинга задач.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В итоге появляется несколько новых ролей:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ResourceManager&lt;/strong&gt; управляет глобальным распределением
    вычислительных ресурсов между приложениями;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ApplicationMaster&lt;/strong&gt; управляет планированием и координацией внутри
    приложения;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NodeManager&lt;/strong&gt; управляет процессами в рамках одной машины.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ApplicationMaster представляет собой библиотеку, с помощью которой можно
получить у ResourceManager квоту на вычислительные ресурсы и работать с
NodeManager(ами) для выполнения и мониторинга задач.&lt;/p&gt;
&lt;p&gt;ResourceManager поддерживает иерархическим очереди приложений, которым
может гарантированно выделяться некоторый процент ресурсов кластера. Его
функционал ограничивается планированием, никакого мониторинга и
отслеживания задач не происходит, а также нет никаких гарантий
перезапуска задач, провалившихся из-за проблем с оборудованием или
кодом. Планирование основывается на требованиях, которые выставляет
приложение с помощью ряда запросов ресурсов (среди них: запросы на
вычислительные ресурсы, память, дисковое пространство, сетевой доступ и
т.п.). Обратите внимание, что это значительное изменение по сравнению с
текущей моделью слотов фиксированного размера, которая является одной из
основных причин неэффективного использования ресурсов кластера на данный
момент.&lt;/p&gt;
&lt;p&gt;NodeManager - это агент, который работает на каждой машине и несет
ответственность за запуск контейнеров приложений, мониторинг
используемых ими ресурсов (плюс отчет планировщику).&lt;/p&gt;
&lt;p&gt;По одному ApplicationMaster запускается для каждого приложения, они
ответственны за запрос необходимых ресурсов у планировщика, запуск
задач, отслеживание статусов, мониторинг прогресса и обработку сбоев.&lt;/p&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Следующее поколение MapReduce" class="responsive-img" src="https://www.insight-it.ru/images/mapreduce-nextgen.jpg" title="Следующее поколение MapReduce"/&gt;&lt;/p&gt;
&lt;h2 id="uluchsheniia-po-sravneniiu-s-tekushchei-realizatsiei-mapreduce"&gt;Улучшения по сравнению с текущей реализацией MapReduce&lt;/h2&gt;
&lt;h3 id="masshtabiruemost"&gt;Масштабируемость&lt;/h3&gt;
&lt;p&gt;Разделение управления ресурсами и прикладными задачами позволяет
горизонтально расширять кластер более просто и эффективно. JobTracker
проводит значительную часть времени пытаясь управлять жизненным циклом
каждого приложения, что часто может приводить к различным
происшествиям - переход к отдельному менеджеру для каждого приложения
является значительным шагом вперед.&lt;/p&gt;
&lt;p&gt;Масштабируемость особенно важна в свете текущих трендов в оборудовании -
на данный момент Hadoop может быть развернут на кластере из 4000 машин.
Но 4000 средних машин 2009го года (т.е. по 8 ядер, 16Гб памяти, 4Тб
дискового пространства) только вдвое менее &amp;nbsp;ресурсоемки, чем 4000 машин
2011го года (16 ядер, 48гб памяти, 24Тб дискового пространства). Помимо
этого с точки зрения операционных издержек было выгоднее работать в еще
больших кластере от 6000 машин и выше.&lt;/p&gt;
&lt;h3 id="dostupnost"&gt;Доступность&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ResourceManager использует&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/e7095d3/" rel="nofollow" target="_blank" title="http://hadoop.apache.org/zookeeper/"&gt;Apache ZooKeeper&lt;/a&gt; для обработки сбоев.
    Когда ResourceManager перестает работать, аналогичный процесс может
    быстро запуститься на другой машине благодаря тому, что состояние
    кластера было сохранено в ZooKeeper. При таком сценарии все
    запланированные и выполняющиеся приложения максимум лишь
    перезапустятся.&lt;/li&gt;
&lt;li&gt;ApplicationMaster - поддерживается создание точек восстановления на
    уровне приложений. ApplicationMaster может восстановить работу из
    состояния, сохраненного в HDFS, в случае сбоя.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="sovmestimost-protokola"&gt;Совместимость протокола&lt;/h3&gt;
&lt;p&gt;Это позволит различным версиям клиентов и серверов Hadoop общаться между
собой. Помимо решения многих существующих проблем с обновлением, в
будующих релизах появится возможность последовательного обновления кода
без простоя системы в целом - очень большое достижения с точки зрения
системного администрирования.&lt;/p&gt;
&lt;h3 id="innovatsionnost-i-gibkost"&gt;Инновационность и гибкость&lt;/h3&gt;
&lt;p&gt;Основным плюсом предложенной архитектуры является тот факт, что
MapReduce по сути становится просто пользовательской библиотекой.
Вычислительная же система (ResourceManager и NodeManager) становятся
полностью независимыми от специфики MapReduce.&lt;/p&gt;
&lt;p&gt;Клиенты получат возможность одновременного использования разных версий
MapReduce в одном и том же кластере. Это становится тривиальным, так как
отдельная копия ApplicationMaster'а запускается для каждого приложения.
Это дает гибкость в исправлении багов, улучшений и новых возможностей,
так как полное обновление кластер перестает быть обязательной
процедурой. Это позволяет клиентам обновлять их приложения до новых
версий MapReduce вне зависимости от обновлений кластера.&lt;/p&gt;
&lt;h3 id="effektivnost-ispolzovaniia-vychislitelnykh-resursov"&gt;Эффективность использования вычислительных ресурсов&lt;/h3&gt;
&lt;p&gt;ResourceManager использует общую концепцию для управления ресурсами и
планирования по отношению к каждому конкретному приложению. Каждая
машина в кластере на концептуальном уровне рассматривается просто как
набор ресурсов: память, процессор, ввод-вывод и др. Все машины
взаимозаменяемы и приложение может быть назначено на любую из них,
основываясь на доступных и запрашиваемых ресурсах. При этом приложения
работают в контейнерах, изолированно от других приложений, что дает
сильную поддержку многозадачности.&lt;/p&gt;
&lt;p&gt;Таким образом эта схема избавляет от текущего механизма map и reduce
слотов в Hadoop, который негативно влияет на эффективную утилизацию
вычислительных ресурсов.&lt;/p&gt;
&lt;h3 id="podderzhka-drugikh-paradigm-programmirovaniia-pomimo-mapreduce"&gt;Поддержка других парадигм программирования помимо MapReduce&lt;/h3&gt;
&lt;p&gt;В предложенной архитектуре используется общий механизм вычислений, не
привязанный конкретно к MapReduce, что позволит использовать и другие
парадигмы. Имеется возможность реализовать собственный
ApplicationMaster, способный запрашивать ресурсы у ResourceManager и
использовать их в соответствии с задачей, при этом сохраняются общие
принципы изоляции и гарантированного наличия полученных ресурсов. Среди
потенциально поддерживаемых парадигм можно назвать MapReduce, MPI,
Мaster-Worker, итеративные модели. Все они могут одновременно работать
на одном и том же кластере. Это особенно актуально для приложений
(например К-средний или Page Rank), где &lt;a href="https://www.insight-it.ru/python/2011/piccolo-postroenie-raspredelennykh-sistem-v-11-raz-bystree-hadoop/"&gt;другие подходы более чем на порядок эффективнее&lt;/a&gt; MapReduce.&lt;/p&gt;
&lt;h2 id="vyvody_1"&gt;Выводы&lt;/h2&gt;
&lt;p&gt;Apache Hadoop, и в частности Hadoop MapReduce - очень успешный
opensource проект по обработке больших объемов данных. Предложенный
Yahoo путь его переработки направлен на исправление недостатков
архитектуры текущей реализации, при этом повышая доступность,
эффективность использования ресурсов и предоставляя поддержку других
парадигм распределенных вычислений.&lt;/p&gt;
&lt;p&gt;Осталось дело за малым - собственно реализовать задуманное! :)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/336fc03c/" rel="nofollow" target="_blank" title="http://developer.yahoo.com/blogs/hadoop/posts/2011/02/mapreduce-nextgen/"&gt;Источник информации&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="/feed/"&gt;Подписаться на RSS можно здесь.&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 19 Feb 2011 21:23:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-02-19:storage/2011/novoe-pokolenie-mapreduce-v-apache-hadoop/</guid><category>Apache</category><category>Apache Hadoop</category><category>Hadoop</category><category>архитектура</category><category>кластер</category><category>кластеризация</category><category>кластерные вычисления</category><category>Масштабируемость</category><category>разработка</category><category>технологии</category></item><item><title>Piccolo - построение распределенных систем в 11 раз быстрее Hadoop</title><link>https://www.insight-it.ru//python/2011/piccolo-postroenie-raspredelennykh-sistem-v-11-raz-bystree-hadoop/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/20aba73a/" rel="nofollow" target="_blank" title="http://piccolo.news.cs.nyu.edu/"&gt;Piccolo&lt;/a&gt; - это система для
распределенных вычислений, использующая новую ориентированную на данные
модель программирования для разработки приложений по параллельным
вычислениям в памяти в масштабах дата-центров. В отличии от существующих
моделей, основывающихся на &lt;em&gt;потоках&lt;/em&gt; данных, Piccolo позволяет
вычислениям выполняться на различных машинах, при этом имея общее
изменяющееся состояния через интерфейс таблиц пар "ключ-значение".
Традиционные ориентированные на данные модели (такие как используются в
&lt;a href="/tag/hadoop/"&gt;Apache Hadoop&lt;/a&gt;) предоставляют пользователю для работы
лишь единственный объект в определенный момент времени, когда в Piccolo
используется глобальная таблица состояний, одновременно доступная для
всех частей вычисления. Это позволяет пользователям указывать алгоритм
вычисления в интуитивно-понятной манере, очень похожей на разработку
программ для одного компьютера.&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Использование хранилища, позволяющего хранить в памяти пары
"ключ-значение", сильно отличается от канонического подхода
&lt;a href="/tag/mapreduce/"&gt;map-reduce&lt;/a&gt;, который основан на распределенных
файловых системах. Результаты впечатляют:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Эксперименты показали, что Piccolo очень быстр и отличные возможности
по масштабируемости для многих прикладных задач. Производительность
вычисления PageRank и k-средних выросла в 11 и 4 раза, соответственно,
по сравнению с Hadoop. Вычисление PageRank для связанного графа из 1
миллиарда страниц заняло лишь 70 секунд на 100 машинах в &lt;a href="/tag/ec2/"&gt;Amazon
EC2&lt;/a&gt;. Распределенная система по скачиванию веб-страниц
легко может полностью загрузить 100Мбит интернет-канал при работе на
12 машинах.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;При разработке на Piccolo программисты создают наборы прикладных
функций, которые принято называть ядром. Функции ядра запускаются
параллельно на нескольких вычислительных узлах, при этом у них есть
доступ к общему изменяемому состоянию, которое реализовано в виде набора
таблиц, располагающихся в оперативной памяти различных узлов системы.
Для доступа к этому состоянию используется примитивный интерфейс,
позволяющий узнать &lt;em&gt;(get)&lt;/em&gt; и изменить &lt;em&gt;(put)&lt;/em&gt; то или иное состояние.
Процесс отправки сообщений удаленным узлам, непосредственно имеющим в
памяти требуемые данные, полностью берет на себя сам код Piccolo.&lt;/p&gt;
&lt;p&gt;Предоставляя разработчикам доступ к глобальному общему состоянию,
Piccolo предлагает несколько привлекательных возможностей:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Алгоритмы, основанные на общем промежуточном состоянии, могут быть
    реализованы естественным, логичным и эффективным образом&lt;/li&gt;
&lt;li&gt;Асинхронные online приложения получают возможность иметь
    &lt;em&gt;оперативный&lt;/em&gt; доступ к новым и изменившимся данным, расположенным на
    других узлах системы&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В Piccolo используется ряд оптимизаций, обеспечивающий не только удобное
использование интерфейса к таблице состояний, но и его быстроту:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Локальность&lt;/strong&gt; - для обеспечения выполнения локальности исполнения,
    таблицы явным образом разбиваются на части, располагающиеся на
    разных машинах. В пользовательском коде при взаимодействии с
    таблицами доступна настройка локальности, обеспечивающая выполнение
    кода на том же узле, где располагаются даннын.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Балансировка нагрузки&lt;/strong&gt; - далеко не вся нагрузка равномерна, часто
    какая-то часть вычислений требует намного больше ресурсов, чем все
    остальные. Ожидание без дела пока такая задача будет выполнена
    впустую тратит ценное время и ресурсы. Для решения данной проблемы
    Piccolo может мигрировать часть задач с загруженных машин на
    простаивающие, при этом сохраняя настройки локальности и
    корректность выполнения программы.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Обработка сбоев&lt;/strong&gt; - сбои оборудования неизбежны и обычно они
    случаются в самые критические моменты. Piccolo делает создание
    контрольных точек и восстановление простым и быстрым, обеспечивая
    быстрое восстановление в случае сбоев.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Синхронизация&lt;/strong&gt; - управление корректной синхронизацией и
    обновлениями в условиях распределенной системы может быть сложным и
    медленным. Piccolo позволяет пользователям поручить реализацию
    логики синхронизации системе. Вместо явной блокировки таблиц при
    выполнении обновлении данных, пользователи могут присоединять
    аккумулирующие функции к таблицам: они используются автоматически
    системой для корректного комбинирования параллельных обновлений
    ячеек таблиц.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Проект реализован в виде библиотеки для &lt;a href="/tag/python/"&gt;Python&lt;/a&gt; и
&lt;a href="/tag/c/"&gt;C++&lt;/a&gt;. Более детально примеры использования и принципы работы
системы разбираются в источниках информации (правда на английском), не
поленитесь - загляните. Вместо заключения хотелось бы по традиции
порекомендовать подписаться на &lt;a href="/feed/"&gt;RSS блога&lt;/a&gt;, если Вы еще этого не
сделали.&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/1ccc4a5b/" rel="nofollow" target="_blank" title="http://www.cs.nyu.edu/~power/"&gt;Russell Power&lt;/a&gt; - автор проекта Piccolo&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/9d1ef591/" rel="nofollow" target="_blank" title="http://www.usenix.org/event/osdi10/tech/full_papers/Power.pdf"&gt;Piccolo: Building Fast, Distributed Programs with Partitioned Tables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Проект был презентован на&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/5b92a3e0/" rel="nofollow" target="_blank" title="http://www.usenix.org/event/osdi10/tech/"&gt;OSDI10&lt;/a&gt;:&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/389782f7/" rel="nofollow" target="_blank" title="https://docs.google.com/viewer?url=http%3A%2F%2Fwww.usenix.org%2Fevent%2Fosdi10%2Ftech%2Fslides%2Fpower.pdf"&gt;презентация&lt;/a&gt; и&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/90a31eff/" rel="nofollow" target="_blank" title="http://piccolo.news.cs.nyu.edu/osditalk.mp4"&gt;видео&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 12 Feb 2011 23:49:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-02-12:python/2011/piccolo-postroenie-raspredelennykh-sistem-v-11-raz-bystree-hadoop/</guid><category>C++</category><category>piccolo</category><category>Python</category><category>вычисления</category><category>Масштабируемость</category><category>разработка</category><category>распределенные вычисления</category></item><item><title>HighLoad++ 2010</title><link>https://www.insight-it.ru//event/2010/highload-2010/</link><description>&lt;p&gt;&lt;img alt="HighLoad++ Logo" class="left" src="https://www.insight-it.ru/images/highload-conference-logo.png" title="HighLoad++ Logo"/&gt;
25-26 октября прошла конференция &lt;a href="https://www.insight-it.ru/goto/727c9436/" rel="nofollow" target="_blank" title="http://www.highload.ru"&gt;HighLoad++ 2010&lt;/a&gt;,
посвященная разработке высоконагруженных систем. После конференции у
меня сразу родились планы на два поста: типичный отчет и описание
архитектуры Вконтакте. С порядком написания я, видимо, не прогадал -
получился один из самых успешных постов на &lt;strong&gt;Insight IT&lt;/strong&gt;. Остальные
доклады на мероприятии были, пожалуй, существенно менее животрепещущими
для общественности, но все же не менее интересными.
Приступим.&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="organizatsionnye-momenty"&gt;Организационные моменты&lt;/h2&gt;
&lt;p&gt;Прежде чем переходить собственно к рассказу о докладах, хочется сразу
высказаться по организационным вопросам, чтобы далее не отвлекаться.
Возможно организаторы учтут при проведении последующих мероприятий.&lt;/p&gt;
&lt;p&gt;Во-первых, участие в конференции: цены конечно не самые высокие для
двухдневных конференций, но все равно слегка зашкаливают - лично я бы не
пошел на данное мероприятие за такие деньги, даже не смотря на то что
тематика полностью совпадает со сферой моих профессиональных интересов.
За кого-то заплатил работодатель, а мне вот пришлось доставать
бесплатное участие через знакомых знакомых... &lt;em&gt;(спасибо добрым людям,
если вдруг читают :) )&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Во-вторых, удивила ситуация со связью со внешним миром: интернет был на
очень хорошем для конференций уровне - тупил местами, но в целом
стабильно работал, а вот мобильная связь не работала практически
совсем - уезжал домой с почти севшим телефоном.&lt;/p&gt;
&lt;p&gt;Политика организовывать не ставить два потенциально интересных доклада
параллельно меня очень порадовал - послушал в живую все, что хотел. А
небольшая давка в первом зале в начале первого дня мне кажется была
очень даже справедливой платой за отсутствие необходимости разрываться
на части.&lt;/p&gt;
&lt;p&gt;С едой все было в порядке, очереди конечно великоваты не смотря на два
обеда в разное время, но всегда можно было обойти данное неудобство
(перейти в другой "раздаточный пункт" или залезть на сцену, хоть и не
разрешали).&lt;/p&gt;
&lt;p&gt;Еще очень порадовало, что презентации первого дня конференции были уже
доступны участникам еще за пару часов до окончания первого дня. Но вот с
оставшимися презентациями и видео с мероприятия видимо произошла
какая-то заминка и я так и не получил ссылку на них до сих пор, судя по
всему они так и не доступны.&lt;/p&gt;
&lt;h2 id="den-pervyi"&gt;День первый&lt;/h2&gt;
&lt;p&gt;Основной особенностью первого дня было выделение целого зала под
англоязычные доклады зарубежных коллег. Как я уже писал, желающих
послушать иностранцев, было очень много - и в первой половине дня люди
толпились чуть ли не в коридоре, но ближе к вечеру ситуация
стабилизировалась.&lt;/p&gt;
&lt;p&gt;После приветственного слова Олега Бунина (одного из основных
организаторов конференции) слово взял &lt;a href="https://www.insight-it.ru/goto/c9ed5ba0/" rel="nofollow" target="_blank" title="http://www.timetobleed.com"&gt;Joe
Damato&lt;/a&gt;, которого позиционировали как
известного хакера, активно работающего над развитием &lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt;.
Темой выступления был обзор различных инструментов и приемов для анализа
ситуации в серверном &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;-окружении. Некоторые моменты
были мне известны и ранее, но в целом больше половины доклада было для
меня очень интересно и ново. Перечислять упомянутые им приемы я, честно
говоря, не вижу смысла - будет просто дублирование презентации. Если
ранжировать доклады первого дня по интересу лично для меня, то это
выступление заняло бы, пожалуй, второе место.&lt;/p&gt;
&lt;p&gt;Вторым докладчиком был также приверженец секты Рубистов, &lt;a href="https://www.insight-it.ru/goto/6a62c9ef/" rel="nofollow" target="_blank" title="http://jamesgolick.com/"&gt;James
Golick&lt;/a&gt;, один из основателей социальной сети
для фетишистов (простите за отсутствие ссылки). Основной фишкой доклада
было "разоблачение мифов", в частности об облачных вычислениях и NoSQL.
Количество пользователей этой социальной сети, но они очень активны и
генерируют достаточно много контента (особенно по Российским меркам).
Проект изначально располагался в компании, которая предоставляла услуги
managed hosting (хостинг на арендуемых серверах + за тебя
администрируют), но они посчитали, что слишком много переплачивают за
этот самый "managed", и решили поддаться тренду и переехать в облако
(&lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; &lt;a href="/tag/ec2/"&gt;EC2&lt;/a&gt;). По деньгам получилось не сильно
дешевле, но больше всего из расстроила производительность виртуальных
машин (кажется, был слайд со скоростью доступа к дисковой подсистеме,
выставляющий облако не в лучшем свете). Второй эпопеей в их проекте были
попытки оптимизировать подсистему хранения данных путем перенесения ее
части в NoSQL хранилище: пробовали &lt;a href="/tag/mongodb/"&gt;MongoDB&lt;/a&gt; (выкинули
из-за блокировок на операциях удаления) и &lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt;
(выкинули из-за медленного случайного чтения). Финальным решением стал
&lt;a href="/tag/redis/"&gt;Redis&lt;/a&gt; + &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;, просто и со вкусом - их
всецело устраивает на данный момент, как я понял.&lt;/p&gt;
&lt;p&gt;Третьим выступал &lt;a href="https://www.insight-it.ru/goto/aab04203/" rel="nofollow" target="_blank" title="http://www.facebook.com/robert"&gt;Robert Johnson&lt;/a&gt; из
&lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt;, доклад был практически таким же, как и в
ГУ-ВШЭ за несколько дней до этого - о нем &lt;a href="https://www.insight-it.ru/event/2010/facebook-how-we-scaled-to-500-000-000-users-by-robert-johnson/"&gt;я уже
писал&lt;/a&gt;,
так что подробно останавливаться не буду. Основным отличием были
дополнительные технические детали, но подавляющее большинство из них и
так уже были описаны в статье &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-facebook/"&gt;"Архитектура
Facebook"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;После обеда выступал Patrice Pelland из &lt;a href="/tag/microsoft/"&gt;Microsoft&lt;/a&gt;,
доклад&amp;nbsp; был о том, как работают их облачные сервисы (видимо live,
skydrive и прочие). Естественно все на их же продуктах, большинство
названий я даже не слышал. Единственное, что запомнил из выступления - у
мелкомягких есть даже клон &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;, но с какими-то
дополнительными плюшками. Это был единственный доклад, после которого
никто не захотел задать даже одного вопроса, что в целом наглядно
продемонстрировало незаинтересованность аудитории в платных решениях. В
твиттере после этого выступления проскользнула обиженная фраза
докладчика, что-то в духе: "До них просто не дошло, о чем я говорил".&lt;/p&gt;
&lt;p&gt;После этого недоразумения от MS началась длинная серия докладов от
людей, причастных к созданию &lt;a href="/tag/postgresql/"&gt;PostgreSQL&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simon Riggs из &lt;a href="https://www.insight-it.ru/goto/8078c405/" rel="nofollow" target="_blank" title="http://www.2ndquadrant.com/"&gt;2nd Quadrant&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Robert Treat из &lt;a href="https://www.insight-it.ru/goto/556109e4/" rel="nofollow" target="_blank" title="http://www.omniti.com"&gt;Omni TI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bruce Momjian из &lt;a href="https://www.insight-it.ru/goto/37e2eb7f/" rel="nofollow" target="_blank" title="http://www.enterprisedb.com/"&gt;EnterpriseDB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Было 5 выступлений о PostgreSQL подряд:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Повышение производительности&lt;/li&gt;
&lt;li&gt;Управление репликацией&lt;/li&gt;
&lt;li&gt;Масштабирование&lt;/li&gt;
&lt;li&gt;Быстрая смена версии средствами pg_update&lt;/li&gt;
&lt;li&gt;Потоковая репликация&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В целом очень актуальные доклады, если Вы плотно работаете с PostgreSQL
в своем проекте или на своей работе. Я вообще тоже когда стоит выбор
между доступными реляционными СУБД чаще всего склоняюсь к PostgreSQL, но
доклады были детализированными не там, где нужно, и было скучновато. В
этой секции порадовали три вещи:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;очень качественный английский у докладчиков&lt;/li&gt;
&lt;li&gt;забавная манера выступления Роберта, особенно про красные кроки
    (что-то типа галош)&lt;/li&gt;
&lt;li&gt;активная реклама новых вкусностей PostgreSQL 9.0, релиз которой я по
    каким-то причинам проворонил - надо будет обязательно попробовать ее
    в деле&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;После кофе-брейка я пошел на больше всего понравившийся мне доклад (за
первый день) - выступал &lt;a href="https://www.insight-it.ru/goto/217e612/" rel="nofollow" target="_blank" title="http://www.phpied.com/"&gt;Stoyan Stefanov&lt;/a&gt; из
Yahoo! Темой доклада была заявлена неочевидная формулировка "Progressive
Downloads and Rendering", хотя на самом деле все свелось к грамотно
построенному докладу о клиентской оптимизации: несколько вводных
картинок, один слайд с базовыми приемами и много-много примеров
очевидных и не очень случаев, когда с точки зрения пользователя сайт
начинает тупить, даже если серверная часть проекта написано грамотно и
работает достаточно быстро. По некоторым аспектам, в частности про
кроссбраузерному использованию data:URL+MHTML, он ссылался на русские
источники, а также очень позитивно отзывался о &lt;a href="https://www.insight-it.ru/goto/209c3c57/" rel="nofollow" target="_blank" title="http://sunnybear.moikrug.ru/"&gt;Николае
Мациевском&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Последним, что я посетил в первый день, была "открытая встреча" c James
Golick и Joe Damato про сам &lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt;. Ожидал большего: в итоге
Joe вообще не выступил, а большая часть времени ушла на разжёвывание
базовых возможностей языка и несколько мелких холиваров.&lt;/p&gt;
&lt;h2 id="den-vtoroi"&gt;День второй&lt;/h2&gt;
&lt;p&gt;На второй день я немного проспал и приехал ближе к концу первого
доклада: оказалось, что я не один такой - людей было раза в три меньше,
чем за день до этого. Выбор потока куда пойти был легок: в первом зале
все утро было посвящено &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;, с которым я последнее
время довольно плотненько работал.&lt;/p&gt;
&lt;p&gt;После докладов на английском "отечественные" выступления смотрелись
совсем блекло. Конец выступления Андрея Смирнова про
&lt;a href="/tag/twisted/"&gt;Twisted&lt;/a&gt; не принес мне хоть какой-либо полезной
информации, тем более мне все равно больше по душе
&lt;a href="/tag/tornado/"&gt;Tornado&lt;/a&gt;. Вопрос про их сравнение от одного из слушателей
вызвал у докладчика рассказать историю о том, как будущий автор Tornado
тусовался в сообществе Twisted, а потом взял и сделал свой продукт.&lt;/p&gt;
&lt;p&gt;Следующий доклад был про профилирование памяти в &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;
от Антона Грицая - начал он историю очень издалека, с того что такое
утечки памяти, какие бывают "сборщики мусора", какие есть варианты
искать утечки в &lt;a href="/tag/python/"&gt;Python&lt;/a&gt; и чем они плохи, собственно до
"дела" он дошел лишь к концу доклада. Было предложено пользоваться
продуктом под названием &lt;a href="https://www.insight-it.ru/goto/f077b18a/" rel="nofollow" target="_blank" title="http://guppy-pe.sourceforge.net/"&gt;heapy&lt;/a&gt;,
который обладает широким спектром возможности, но при этом документация
сильно хромает.&lt;/p&gt;
&lt;p&gt;Последним докладом в секции про &lt;a href="/tag/python/"&gt;Python&lt;/a&gt; было выступление
трех бравых ребят из HeadHunter, которые рассказывали про их внутренний
продукт под названием Frontik, представляющий собой надстройку над
&lt;a href="/tag/tornado/"&gt;Tornado&lt;/a&gt;, аггрегирующую данные с нескольких
HTTP-сервисов. В целом идея мне понравилась, но ввиду исторических
причин реализация у них накручена очень муторно:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;основной формат передачи данных - &lt;a href="/tag/html/"&gt;XML&lt;/a&gt; по HTTP&lt;/li&gt;
&lt;li&gt;генерация HTML посредством &lt;a href="/tag/xslt/"&gt;XSLT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;регулярные выражения где надо и где не надо (для повышения
    производительности)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Основным событием оставшейся части второго дня, как Вы уже наверное
поняли, был аншлаг с участием Вконтакте и лично Павлом Дуровым в главной
роли. Результаты подробно расписаны в статье &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-vkontakte/"&gt;"Архитектура
Вконтакте"&lt;/a&gt;,
повторяться не буду, с Вашего позволения.&lt;/p&gt;
&lt;p&gt;Остальные доклады я застал лишь частями, так как блуждал по залам без
особого энтузиазма, да и в толпе вокруг Павла чуток потусовался.
Расскажу вкратце запомнившиеся моменты:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Юрий Востриков из Mail.ru рассказывал про
    &lt;a href="https://www.insight-it.ru/goto/4cece2af/" rel="nofollow" target="_blank" title="http://opensource.mail.ru"&gt;Tarantool/Silverbox&lt;/a&gt; - еще после их
    технологического форума подумываю попробовать этот продукт в деле,
    но после этого выступления понял, что пока рановато: не известно ни
    об одном успешном применении вне компании-разработчика, да и
    библиотеки с реализацией полноценного протокола есть далеко не под
    все языки программирования.&lt;/li&gt;
&lt;li&gt;На доклад про реализацию одного из топовых приложений Вконтакте на
    &lt;a href="/tag/rails/"&gt;Rails&lt;/a&gt; я попал почти к самой сессии вопросов-ответов,
    запомнился только тот факт, что после того как компания, в которой
    работал докладчик, передала приложение заказчику - они почти сразу
    же переписали его на &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;. Заставляет задуматься.&lt;/li&gt;
&lt;li&gt;В третьем, дополнительном, зале во второй половине дня расположился
    тренинг Start in Garage для людей, планирующих сделать свой стартап;
    ребята рассказывали весело и непринужденно, но по сути все было
    очень примитивно - ушел минут через 20 после начала на аншлаг
    вконтакте.&lt;/li&gt;
&lt;li&gt;Про &lt;a href="https://www.insight-it.ru/goto/e72a7ec5/" rel="nofollow" target="_blank" title="http://www.scalaxy.ru"&gt;Scalaxy&lt;/a&gt; было бы интересно написать
    отдельную статью, больно часто они всплывают на конференциях и в
    онлайн-сообществах. На этот раз рассказывали о том, как они выделяют
    избыточные дисковые массивы для виртуальных машин (которые они
    собственно в аренду сдают). Технология называется Vast Sky, родом
    откуда-то из Азии, позволяет легко выделять заданное количество
    блоковых устройств на разных дисковых системах и подключать их к
    виртуальной машине в виде софтверного RAID. В сочетании с их QDR
    Infiniband от Voltaire работает очень даже шустро (по крайней мере
    если верить их бенчмаркам по сравнению с альтернативными
    технологиями).&lt;/li&gt;
&lt;li&gt;Scalaxy же запускает сервис ddosme, предназначенный для нагрузочного
    тестирования интернет-проектов. Попал опять только на
    вопросы-ответы, из них понял, что они предлагают через прокси
    походить по своему ресурсу, затем на основе логов составляются
    маршруты движения ботов по сайту и тестирование запускается на
    нужных мощностях. Сколько стоит не понял.&lt;/li&gt;
&lt;li&gt;Последним докладом, который я застал краем глаза, было обсуждение
    основных косяков, мешающих 1С-Битрикс обслуживать пристойное
    количество пользователей - для меня совершенно не актуальный вопрос,
    так что после этого я начал собираться в сторону выхода и отправился
    смотреть &lt;a href="https://www.insight-it.ru/event/2010/socialnaya-set/"&gt;"Социальная сеть"&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="zakliuchenie"&gt;Заключение&lt;/h2&gt;
&lt;p&gt;Впечатления от конференции очень положительные: большинство докладов
хотя бы немного полезны, рекламы вообще минимум, организация на уровне.
По-прежнему не знаю стоит ли она своих денег, но потраченного времени
точно стоит.&amp;nbsp; Надеюсь в следующем году будет по-проще попасть.&lt;/p&gt;
&lt;p&gt;Хотелось бы видеть больше докладов не о конкретных инструментах и
технологиях, а о их применении в рамках построения общей архитектуры
проекта или решения конкретных нетривиальных задач. Доклады в духе "у
нас вот такая классная штука есть, но стоит денег" и "приходите к нам
работать, чтобы попробовать в деле эту технологию" как обычно скучны, но
вроде их было довольно мало (надеюсь докладчики хотябы платят
организатором за возможность порекламироваться?). Приглашенные
иностранные гости - ход очень классный, мне кажется основной ключ успеха
прошедшего мероприятия, в этом направлении определенно стоит двигаться -
хотелось бы увидеть представителей известных проектов (Google, Ebay,
Amazon, Flickr, Twitter, Baidu, QQ и.т.д.) и людей, решающих реально
нетривиальные задачи, вроде Joe Damato.&lt;/p&gt;
&lt;p&gt;В любом случае спасибо организаторам за два с толком проведенных дня :)&lt;/p&gt;
&lt;p&gt;Да, думаю Вы уже заметили, что блог Insight IT снова потихоньку
возвращается к жизни, так что &lt;a href="/feed/"&gt;подписываться на RSS никогда не поздно&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 31 Oct 2010 23:24:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-10-31:event/2010/highload-2010/</guid><category>highload</category><category>высоконагруженные системы</category><category>конференции</category><category>Масштабируемость</category><category>мероприятия</category><category>разработка</category></item><item><title>Архитектура Plenty of Fish</title><link>https://www.insight-it.ru//highload/2010/arkhitektura-plenty-of-fish/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/fa2360fd/" rel="nofollow" target="_blank" title="http://www.plentyoffish.com/"&gt;Plenty of Fish&lt;/a&gt; представляет собой очень
популярный сервис онлайн знакомств, насчитывающий более 45 миллионов
посетителей в месяц и 30+ миллионов просмотров страниц в сутки (что
составляет около 500-600 страниц в секунду). Но это не самая интересная
часть истории... Все это управляется единственным человеком при
использовании нескольких серверов, при этом он тратит на работу всего
пару часов в день и зарабатывает 6 миллионов долларов на рекламе от
Google. Завидуете? Я тоже :) Как же ему удалось соединить столько
влюбленных пар, используя так мало ресурсов?&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Данный пост является переводом &lt;a href="https://www.insight-it.ru/goto/e06f86e0/" rel="nofollow" target="_blank" title="http://highscalability.com/plentyoffish-architecture"&gt;англоязычной
статьи&lt;/a&gt;, автор
оригинала: Todd Hoff.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/35763c73/" rel="nofollow" target="_blank" title="http://channel9.msdn.com/ShowPost.aspx?PostID=331501#331501"&gt;Channel9 интервью с Markus Frind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/fa591b8c/" rel="nofollow" target="_blank" title="http://plentyoffish.wordpress.com/%20target="&gt;Блог Markus Frind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/ed31cb93/" rel="nofollow" target="_blank" title="http://www.readwriteweb.com/archives/plentyoffish_one_billion.php"&gt;Plentyoffish: компания одного человека может стоить 1 миллиард
долларов&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Microsoft Windows&lt;/li&gt;
&lt;li&gt;ASP.NET&lt;/li&gt;
&lt;li&gt;IIS&lt;/li&gt;
&lt;li&gt;Akamai CDN&lt;/li&gt;
&lt;li&gt;Foundry ServerIron Load Balancer&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;PlentyOfFish (POF) имеет 1.2 миллиарда просмотров страниц в месяц, в
среднем 500 тысяч уникальных авторизованных пользователей в день.
Пиковый сезон приходится на январь каждого года, когда эти цифры
возрастают на 30%.&lt;/li&gt;
&lt;li&gt;POF имеет единственного сотрудника: создатель и генеральный директор
Markus Frind.&lt;/li&gt;
&lt;li&gt;Зарабатывает до 10 миллионов долларов в год на рекламе от Google,
работает при этом только около двух часов в день.&lt;/li&gt;
&lt;li&gt;30+ миллионов просмотров страниц в день (500 - 600 страниц в секунду).&lt;/li&gt;
&lt;li&gt;1.2 миллиарда просмотров страниц и 45 миллионов посетителей в месяц.&lt;/li&gt;
&lt;li&gt;Имеет &lt;a href="https://www.insight-it.ru/goto/ca40875e/" rel="nofollow" target="_blank" title="http://ru.wikipedia.org/wiki/CTR_(%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%BD%D0%B5%D1%82)"&gt;CTR&lt;/a&gt; в 5-10 раз выше, чем Facebook.&lt;/li&gt;
&lt;li&gt;Находится в top 30 сайтов США по данным Competes Attention, top 10 в Канаде и top 30 в Великобритании.&lt;/li&gt;
&lt;li&gt;Нагрузка балансируется между двумя веб-серверами с 2 Quad Core Intel
Xeon X5355 @ 2.66Ghz, 8GB RAM (используется около 800 MB), 2 жесткими
дисками, работают под управлением Windows x64 Server 2003.&lt;/li&gt;
&lt;li&gt;3 сервера баз данных. Информация об их конфигурации не предоставляется.&lt;/li&gt;
&lt;li&gt;Приближается к 64000 одновременных соединений и 2 миллионам просмотрам
страниц в час.&lt;/li&gt;
&lt;li&gt;Интернет-канал в 1Gbps, из которых используется только 200Mbps.&lt;/li&gt;
&lt;li&gt;1 TB трафика от отдачи 171 миллионов изображений через Akamai.&lt;/li&gt;
&lt;li&gt;6TB система хранения данных для обработки миллионов полноразмерных
изображений, которые загружаются на сайт каждый месяц.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="chto-vnutri"&gt;Что внутри?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Модель монетизации заключалась в использовании рекламы от Google.
Match.com, для сравнения, получает 300 миллионов долларов в год, в
основном с платных подписок. Источник дохода POF должен измениться,
чтобы позволить ему получать больше выручки от имеющихся пользователей.
Планируется нанять больше сотрудников, в частности людей, которые будут
заниматься продажей рекламы напрямую вместо того, чтобы полностью
полагаться на AdSense.&lt;/li&gt;
&lt;li&gt;При 30 миллионах просмотрах страниц в день можно зарабатывать неплохие
деньги на рекламе, даже
если&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/b1c7d720/" rel="nofollow" target="_blank" title="http://en.wikipedia.org/wiki/Cost_per_mille"&gt;CPM&lt;/a&gt; будет всего 5-10
центов.&lt;/li&gt;
&lt;li&gt;Akamai используется для отдачи более 100 миллионов изображений в день.
Если на странице 8 изображений и каждое загружается за 100 миллисекунд -
их загрузка займет почти секунду, так что распределение изображений
целесообразно.&lt;/li&gt;
&lt;li&gt;Десятки миллионов изображений отдаются с серверов POF, но большинство из
них размером меньше 2KB и практически полностью закешированы в
оперативной памяти.&lt;/li&gt;
&lt;li&gt;Все динамично. Практически никакой статики.&lt;/li&gt;
&lt;li&gt;Все исходящие данные сжимаются с использованием Gzip, что обходится
всего 30% использованием процессорного времени. Используется много
вычислительных ресурсов, но зато существенно сокращается использование
пропускной способности интернет-канала.&lt;/li&gt;
&lt;li&gt;Кэширование ASP .NET не используется, так как данные теряют свою
актуальность практически сразу же.&lt;/li&gt;
&lt;li&gt;Встроенные компоненты ASP также не используется. Почти все написано с
чистого листа. Ничего не может быть более сложным, чем кучка простых
if-then-else и циклов. Все максимально элементарно.&lt;/li&gt;
&lt;li&gt;Балансировка нагрузки:&lt;/li&gt;
&lt;li&gt;IIS произвольно ограничивает общее количество соединений до 64000,
таким образом балансировщик нагрузки был добавлен для обработки большего
количества одновременных соединений. Вариант с добавлением второго IP
адреса и использованием round robin DNS также рассматривался, но вариант
с балансировщиком нагрузки выглядел более избыточным и позволял более
легко расширять количество серверов. Помимо этого ServerIron позволял
использовать более продвинутую функциональность, вроде блокировки ботов
и балансировку запросов по cookies, сессиям или IP-адресам
пользователей.&lt;/li&gt;
&lt;li&gt;Windows Network Load Balancing (NLB) функция не использовалась, так
как не поддерживает привязку сессий к серверам. Обходным путем было бы
хранение сессионных данных в базе данных или общей файловой системе.&lt;/li&gt;
&lt;li&gt;8-12 NLB серверов могут объединяться в кластер и может использоваться
неограниченное количество таких кластеров. Схема DNS round robin может
использоваться для распределения запросов между кластерами. Теоретически
такая архитектура могла бы позволить 70 веб-серверам обрабатывать более&amp;nbsp;300 тысяч одновременных соединений.&lt;/li&gt;
&lt;li&gt;NLB имеет опцию для отправки каждого пользователя на конкретный
сервер, таким образом не используется внешнее хранилище для сессионных
данных и если сервер выходит из строя - пользователи просто
разлогиниваются из системы. Если это состояние включает в себя например
корзину интернет-магазина или какую-то другую важную информацию, то
такой подход мог бы показаться&amp;nbsp;неприемлемым, но для сайта знакомств это
было бы не так критично.&lt;/li&gt;
&lt;li&gt;Было решено, что хранение и получение сессионных данных программными
средствами слишком дорого. Аппаратная балансировка нагрузка проще:
пользователи просто назначаются конкретным серверам и в случае сбоя
сервера назначенным ему пользователям предлагается пройти процесс
авторизации еще раз.&lt;/li&gt;
&lt;li&gt;Покупка ServerIron была дешевле и проще, чем использование NLB.
Многие крупные сайты используют их для создания пулов TCP соединений,
автоматическому определению ботов и так далее. ServerIron может делать
намного больше, чем просто балансировать нагрузку и такие функции
достаточно привлекательные за эту цену.&lt;/li&gt;
&lt;li&gt;Была большая проблема с выбором системы размещения рекламы. Многие из
них хотели несколько сотен тысяч в год и многолетний контракт.&lt;/li&gt;
&lt;li&gt;В процессе избавления от ASP.NET повторителей и использование взамен
конкатенации строк или response.write. Если у вас миллионы просмотров
страниц в день - просто напишите весь код для отображения на экране
пользователя.&lt;/li&gt;
&lt;li&gt;Большинство изначальных вложений ушло на построение SAN. Избыточность
любой ценой.&lt;/li&gt;
&lt;li&gt;Рост был за счет вирусного эффекта. Портал начал набирать популярность в
Канаде, затем о нем узнали в Великобритании и Австралии, и только потом
в США.&lt;/li&gt;
&lt;li&gt;База данных:&lt;/li&gt;
&lt;li&gt;Одна база данных является основной.&lt;/li&gt;
&lt;li&gt;Две базы данных для поиска. Поисковые запросы распределяются по их типу.&lt;/li&gt;
&lt;li&gt;Производительность наблюдается через диспетчер задач. Когда
появляются пики - ситуация рассматривается более детально. Проблемы
обычно заключались в блокировках на уровне СУБД. Собственно говоря почти
всегда это были проблемы с базами данных, очень редко они возникают на
уровне .NET. Так как POF не использует библиотеки .NET, отследить
проблемы с производительностью оказывается достаточно просто. Если бы
использовалось много уровней framework'ов, поиск мест, где скрываются
проблемы, был бы трудным и утомляющим.&lt;/li&gt;
&lt;li&gt;Если Вы делаете запрос к базе данных 20 раз при отображении одной
страницы, &amp;nbsp;Вы проиграли в любом случае, вне зависимости от того, что Вы
будете делать.&lt;/li&gt;
&lt;li&gt;Разделяйте запросы чтения и записи к базе данных. Если у вас нет
избыточного количества оперативной памяти не следование этому правилу
может заставить систему зависнуть на несколько секунд.&lt;/li&gt;
&lt;li&gt;Постарайтесь делать базы данных только для чтения.&lt;/li&gt;
&lt;li&gt;Денормализуйте данные. Если Вам приходится доставать данные из 20
разных таблиц, попробуйте сделать просто одну таблицу, где будут лежать
все нужные для чтения данные.&lt;/li&gt;
&lt;li&gt;Один день может проработать почти что угодно, но когда Ваша база
данных удвоится - использованные подход может внезапно перестать
работать.&lt;/li&gt;
&lt;li&gt;Если система делает только что-то одно, она будет делать это реально
хорошо. Только записывайте данные и все будет нормально. Только читайте
данные и все будет нормально. Делайте и то и другое - и все испортится.
База данных погрязнет в проблемах с блокировками.&lt;/li&gt;
&lt;li&gt;Если Вы полностью используете вычислительные мощности, Вы либо
делаете что-то не так, либо Ваша система на самом деле очень
оптимизирована. Если вы можете разместить всю базу в оперативной
памяти - обязательно делайте это.&lt;/li&gt;
&lt;li&gt;Процесс разработки выглядит примерно следующим образом: появляется идея,
быстро реализуется и выдается пользователям в пределах 24 часов. Отклик
от пользователей получается по слежению за тем, что они делают на сайте:
выросло количество сообщений на пользователя? среднее время сессий
выросло? Если пользователям новая фишка не пришлась по вкусу - просто
уберите её.&lt;/li&gt;
&lt;li&gt;При небольшом количестве серверов системные сбои достаточно редки и
краткосрочны. Наибольшими сложностями были проблемы с DNS, когда
некоторые интернет-провайдеры говорили, что POF больше не существует. Но
так как сайт бесплатен, пользователи нормально относятся к небольшим
периодам его недоступности. Люди часто не замечают простой сайта, так
как думают, что это какая-то проблема у них, с интернет-соединением или
еще чем-то.&lt;/li&gt;
&lt;li&gt;Переход от миллиона пользователей к 12 миллионам пользователей был
большим прыжком. Система может обслуживать и 60 миллионов пользователей
с двумя веб-серверами.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Часто смотрите на конкурентов для идей новых функциональных
возможностей.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Рассмотрите использование чего-то вроде S3, когда система начнет
требовать географической балансировки.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Вам не нужны миллионы в финансировании, размашистая инфраструктура и
целое здание сотрудников для того, чтобы создать вебсайт мирового
уровня, который обслуживает кучу пользователей и приносит неплохие
деньги. Все что нужно - всего лишь привлекательная идея, которая
понравится большому количеству идей, сайт, который становится популярным
благодаря слухам, а также опыт и видение для построения сайта, не
наступая на типичные "грабли". Вот и все, что Вам нужно :-)&lt;/li&gt;
&lt;li&gt;Необходимость - мать всех изменений.&lt;/li&gt;
&lt;li&gt;Когда вы растете быстро, но не слишком быстро, у Вас появляется шанс
расти, модифицировать и адаптироваться.&lt;/li&gt;
&lt;li&gt;Максимальное использование оперативной памяти решает массу проблем.
После этого рост возможен просто за счет использование более мощных
серверов.&lt;/li&gt;
&lt;li&gt;В начале старайтесь держать все максимально простым. Практически все
дают этот же самый совет, а Markus говорит, что все что он делает -
всего лишь очевидный здравый смысл. Но то что просто, не всегда означает
всего лишь осмысленную вещь. Создание простых вещей является результатом
многих лет практического опыта.&lt;/li&gt;
&lt;li&gt;Поддерживайте время доступа к базе данных быстрым и у Вас не будет
проблем.&lt;/li&gt;
&lt;li&gt;Одной из основных причин, по которой POF может работать с таким
небольшим количеством сотрудников и оборудования, является использование
CDN для отдачи активно используемого контента. Использование CDN может
оказаться секретным соусом для многих крупных сайтов. Markus считает,
что в top 100 не существует ни одного сайта, не использующего CDN. Без
CDN время загрузки страницы в Австралии возросло бы до 3-4 секунд только
за счет изображений.&lt;/li&gt;
&lt;li&gt;Реклама на Facebook принесла плохие результаты. Из 2000 кликов только 1
человек регистрировался. С CTR равным 0.04% Facebook выдавал 0.4 клика
на 1000 показов рекламы (CPM). При 5 центах CPM = 12.5 центов за клик,
50 центах CPM = 1.25\$ за клик. 1 доллар CPM = 2.50\$ за клик. 15\$ CPM
= 37.50\$ за клик.&lt;/li&gt;
&lt;li&gt;Это просто продавать несколько миллионов просмотров страниц с высоким
CPM, но НАМНОГО сложнее продавать миллиарды просмотров с высоким CPM,
как это делают Myspace и Facebook.&lt;/li&gt;
&lt;li&gt;Модель монетизации, основанная на рекламе, ограничивает Ваши доходы. Вам
придется переходить к платной модели чтобы повышать прибыль.
Генерировать 100 миллионов долларов в год за счет бесплатного сайта
практически невозможно - Вам потребуется слишком большой рынок.&lt;/li&gt;
&lt;li&gt;Повышение количества просмотров за счет Facebook не работает для сайтов
знакомств. Иметь посетителя на собственном сайте намного более
прибыльно. Большинство просмотров страниц на Facebook находятся за
пределами США и Вам придется делить 5 центов CPM с Facebook.&lt;/li&gt;
&lt;li&gt;Предложение пользователям при регистрации получить информацию об ипотеке
или каком-то другом продукте, может стать неплохим источником
дополнительной выручки.&lt;/li&gt;
&lt;li&gt;Вы не можете постоянно прислушиваться к отзывам пользователей. Кому-то
всегда будут нравиться новые функции, а кто-то всегда будет их
ненавидеть, но только часть из них сообщит Вам об этом. Вместо этого
лучше смотреть как новые функции влияют на то, чем люди на самом деле
занимаются, просто смотря на Ваш сайт и статистику его использования.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 18 Jan 2010 16:43:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-01-18:highload/2010/arkhitektura-plenty-of-fish/</guid><category>Akamai CDN</category><category>ASP</category><category>ASP .NET</category><category>dating</category><category>Foundry</category><category>IIS</category><category>Microsoft</category><category>online</category><category>Plenty of Fish</category><category>POF</category><category>ServerIron</category><category>Windows</category><category>Windows Server</category><category>архитектура</category><category>Архитектура Plenty of Fish</category><category>Масштабируемость</category><category>сайт знакомств</category></item><item><title>Aladdin от Baidu</title><link>https://www.insight-it.ru//highload/2010/aladdin-ot-baidu/</link><description>&lt;p&gt;Наверняка все прекрасно знают о лидерах интернет-поиска в российской
части интернета: про Google, Яндекс или Рамблер сказано уже не мало
слов, все много раз о них читали, пользовались, обсуждали - ведь уже
прошло больше 10 лет с момента создания каждой из этих поисковых систем
и, как следствие, их конкуренции на просторах рунета. Намного меньше же
внимания на российских информационных сайтах уделяется национальным
проектам других стран, а ведь среди них тоже есть заслуживающие внимания
экземпляры, об одном из них я бы и хотел сегодня поведать.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="istochniki-dannykh"&gt;Источники данных&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7e247449/" rel="nofollow" target="_blank" title="http://tech.sina.com.cn/i/2009-12-16/14423683386.shtml"&gt;Baidu Aladdin Technology Guashudila&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/9d98f3c7/" rel="nofollow" target="_blank" title="http://tech.sina.com.cn/i/2009-08-18/16063362415.shtml"&gt;Rachel Liao, лекция директора по архитектуре Baidu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/d1f8deb1/" rel="nofollow" target="_blank" title="http://news.xinhuanet.com/it/2006-04/06/content_4390847.htm"&gt;Baidu Chief Architect: алгоритмы на службе разработчиков Baidu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/cc13f208/" rel="nofollow" target="_blank" title="http://baike.baidu.com/view/2086291.htm"&gt;Aladdin Plans&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Если кто-то достаточно любопытен, чтобы нажать на приведенные ссылки -
они все на китайском, так что статья написана на основе перевода Google
Translate со всеми вытекающими последствиями. Даже за название "Aladdin"
не ручаюсь, его тоже он придумал :)&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="o-kompanii-baidu"&gt;О компании Baidu&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/50a1d1a7/" rel="nofollow" target="_blank" title="http://www.baidu.com"&gt;Baidu.com&lt;/a&gt; является лидером китайского рынка
интернет-поиска, объем которого достаточно значителен. На данный момент
Китай насчитывает около 340-360 миллионов интернет-пользователей, что
превышает общую численность населения США. Не трудно представить с каким
трафиком приходится сталкиваться крупнейшей китайской поисковой системе.&lt;/p&gt;
&lt;p&gt;Чтобы не быть голословным, еще немного цифр о Baidu:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;100 миллионов поисковых запросов в день&lt;/li&gt;
&lt;li&gt;Более миллиарда проиндексированных страниц&lt;/li&gt;
&lt;li&gt;300-400 миллионов проиндексированных сайтов&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Уже на сегодняшний день размеры китайской части интернета производят
впечатление и с каждым днем она расширяется все больше. Как следствие,
на рынке образуются все новые и новые возможности для создания сервисов,
удовлетворяющих потребности китайских пользователей Интернет.
Компания&amp;nbsp;&lt;strong&gt;Baidu Inc.&lt;/strong&gt; пристально наблюдает за развитием ситуации и
обнаружила огромную потребность среди сервис-провайдеров в удобной
платформе для создания и предоставления пользователям новых сервисов.
Baidu считает создание платформы для использования их технологии
сторонними разработчиками и сервис-провайдерами очень важным
направлением развития на пути к повышению качества пользовательского
опыта в целом. Эти наблюдения стали толчком к рождению в рамках Baidu
новой технологии под названием&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/e0a5512c/" rel="nofollow" target="_blank" title="http://open.baidu.com/"&gt;Aladdin&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Как крупнейшей китайской поисковой системе, Baidu приходится быть чем-то
большим, чем просто инструментом для поиска, это позволяет удовлетворять
потребности потенциальных клиентов наиболее гармоничным и целесообразным
образом. Помимо неустанной погони за технологическими инновациями, Baidu
предпочитает придерживаться политики "потребности клиентов важнее
всего".&lt;/p&gt;
&lt;h2 id="aladdin"&gt;Aladdin&lt;/h2&gt;
&lt;p&gt;Согласно официальному сайту Baidu, эта технология представляет собой
открытую поисковую платформу, позволяющую сторонним разработчикам
использовать технологию Baidu в своих приложениях и сервисах. Владельцы
интернет-проектов и разработчики могут предоставить Baidu данные в уже
структурированном виде для того, чтобы создать еще более мощные и
функционально-насыщенные приложения, позволяя интернет-сайтам получать
еще более значимый трафик, а пользователям - еще больше облегчить
использование сайтов и поиск в сети Интернет.&lt;/p&gt;
&lt;p&gt;В декабре 2008 года Baidu объявили о высокоприоритетной программе под
кодовым названием&amp;nbsp;&lt;em&gt;"Aladdin"&lt;/em&gt;, основной идеей была попытка расширить
текущие рамки веб-поиска, по большей части за счет включения так
называемого "глубинного интернета" в поисковую базу, проведения более
глубокого анализа контента. Помимо этого упоминались возможность
интеграции и управляемой обработки информации, направленных на
минимизацию издержек поиска и времени обработки запроса при повышение
общего качества поисковых результатов. В том же заявлении Baidu также
описали их общую позицию по данному направлению: платформа Aladdin
является надстройкой над текущей поисковой системой Baidu, позволяющей
дополнение и расширение функциональных возможностей.&lt;/p&gt;
&lt;p&gt;Согласно исследованиям Baidu, только 75% пользователей поисковых систем
в конечном итоге удовлетворяют свои информационные потребности. В
процессе анализа причин данного факта было выявлено, что в большом
количестве случаев искомая информация находится на ресурсах по каким-то
причинам находящимся вне доступа поисковых систем (начиная от
технических ограничений, отсутствия внешних ссылок на ресурс и
заканчивая искусственными&amp;nbsp;барьерами вроде REP или принудительной
авторизации).&lt;/p&gt;
&lt;p&gt;Перед разработчиками Aladdin встают две основные проблемы с точки зрения
технической реализации: "как определить пользовательские потребности" и
"как сортировать". Конечно же они очень тесно связаны между собой, это
хорошо демонстрирует пример с поисковым запросом "полное солнечное
затмение": до затмения пользователи хотят когда оно будет и откуда лучше
смотреть, а во время и после него намного актуальнее будет увидеть
видео-запись или прямую трансляцию, а также прочитать и поделиться
комментариями. Самым простым методом решения данного класса задач
является статистический анализ - Aladdin выделяет два основных фактора,
используемых для сортировки результатом в соответствии с потребностями
пользователей: "удовлетворенность потребностей" и "уровень отклика на
спрос". Конечно же оценочные характеристики спроса и потребностей не
означают сам спрос, то есть возможны и более сложные ситуации, когда за
пользовательским запросом стоит целый комплекс более простых
потребностей.&lt;/p&gt;
&lt;p&gt;Алгоритмы, используемые в Aladdin для решения упомянутых проблем,
основаны на машинном обучении, анализе поведения пользователей, а также
обратной связи от использования технологии на практике. Конечная цель
данной платформы заключается в построении целой интеллектуальной
экосистемы, &amp;nbsp;которая станет новым шагом в развитии компании Baidu и
китайской части интернета в целом.&lt;/p&gt;
&lt;h3 id="vozmozhnosti-platformy"&gt;Возможности платформы&lt;/h3&gt;
&lt;p&gt;С технической точки зрения Aladdin от Baidu представляет собой открытый
API к поисковой технологии Baidu, позволяющий добавлять свои данные в
структурированном виде в поисковый индекс, отмечать релевантные ключевые
слова, методы отображения информации и пометки данных гео-метками.&lt;/p&gt;
&lt;p&gt;Одним из важнейших направлений развития поисковых систем является
повышение "интеллектуальности" поиска, Baidu уделяет внимание не только
обнаружению более ценной информации в глубинах Интернета, но и
предоставлению более удобных, точных и сообразительных поисковых
сервисов.&lt;/p&gt;
&lt;p&gt;На сегодняшний день, технология Aladdin была интегрирована в ряд
приложений, позволив тем самым реализовать на страницах с результатами
поиска&amp;nbsp;множество интересных возможностей: прямой звонок клиенту для
обсуждения каких-то товаров или услуг, интеграция с почтовым сервисом,
прослушивание музыки с использованием встроенного flash-плеера и многие
другие.&lt;/p&gt;
&lt;p&gt;После обязательной процедуры подачи и рассмотрения заявки пользователям
платформы Aladdin предоставляются следующие возможности:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Добавление данных в индекс в структурированном виде&lt;/li&gt;
&lt;li&gt;Указание ключевых слов для более точного прямого воздействия на
    целевую аудиторию&lt;/li&gt;
&lt;li&gt;Управление сортировкой и отображением информационного контента&lt;/li&gt;
&lt;li&gt;Управление стилем и внешним видом имеющихся ресурсов, причем не
    только текстовых&lt;/li&gt;
&lt;li&gt;Выбор частоты обновления информации для синхронизации данных&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;На первый взгляд все эти рассуждения и заявления о функциональных
возможностях кажутся абсурдными, даже отчасти ироничными. Ну кому может
понадобиться вручную управлять результатами поиска, добавлять и
структурировать данные, возиться с сортировкой и внешним видом?&lt;/p&gt;
&lt;h3 id="vzgliad-s-drugoi-storony"&gt;Взгляд с другой стороны&lt;/h3&gt;
&lt;p&gt;Да, вся платформа Aladdin по своей задумке очень искуственна:
практически все делается вручную, но по сути это лишь процесс
интеграции, а не работа с самим контентом. Для большинства других
поисковых систем такой подход неприемлем: где найти столько людей, чтобы
управлять огромными массивами данных вручную? Наоборот все поисковые
системы стремятся по максимуму все автоматизировать и борятся с
искуственным вмешательством в поисковый индекс (т.н. SEO), но... если
вспомнить, что Baidu работает в Китае - вся затея начинает обретать
здравый смысл. Как сама компания Baidu, так и большинство их
потенциальных партнеров, клиентов и пользователей находится в примерно
одинаковой ситуации: большое количество дешевой рабочей силы,
относительно низкий уровень образования и профессиональной подготовки, а
также прочие национальные особенности. В их ситуации не выгодно идти по
пути Google и делать&amp;nbsp;&lt;em&gt;основной&lt;/em&gt; акцент на построении полностью
автоматизированных систем анализа контента, добавления дополнительного
материала к поисковым результатам и самим делать различные
дополнительные приложения и сервисы. Намного выгоднее пойти по
собственному пути, более адаптированному к ситуации в Китае, большое
количество трудолюбивых людей позволяет строить сервисы коллективно, с
привлечением партнеров, клиентов и заинтересованных лиц. Да, во многом
вручную, за счет интеграции совершенно различных систем и сервисов, но
зато более качественно и продуманно. В этом-то и заключается вся магия
Китая.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 14 Jan 2010 00:01:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-01-14:highload/2010/aladdin-ot-baidu/</guid><category>Aladdin</category><category>Baidu.com</category><category>online</category><category>Масштабируемость</category><category>поиск</category><category>поисковые системы</category></item><item><title>Архитектура Stack Overflow</title><link>https://www.insight-it.ru//highload/2010/arkhitektura-stack-overflow/</link><description>&lt;p&gt;&lt;img alt="Stack Overflow" class="right" src="https://www.insight-it.ru/images/stack-overflow-logo.png" title="Stack Overflow"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/33fc61d9/" rel="nofollow" target="_blank" title="https://stackoverflow.com/"&gt;Stack Overflow&lt;/a&gt; является любимым многими
программистами сайтом, где можно задать профессиональный вопрос и
получить ответы от коллег. Этот проект был написан двумя никому не
известными парнями, о которых никто никогда раньше не слышал. Хорошо, не
совсем так. Stack Overflow был создан топовыми программистами и звездами
блогосферы:&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/374a081/" rel="nofollow" target="_blank" title="http://www.codinghorror.com/blog/"&gt;Jeff Atwood&lt;/a&gt; и&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/31657700/" rel="nofollow" target="_blank" title="http://www.joelonsoftware.com/"&gt;Joel Spolsky&lt;/a&gt;. В этом отношении Stack
Overflow похож на ресторан, владельцами которого являются знаменитости.
По оценкам Joel'а около 1/3 программистов всего мира использовали этот
интернет-ресурс, так что должно быть он представляет собой что-то
достаточно полезное и интересное.&lt;/p&gt;
&lt;p&gt;Одним из ключевых моментов в истории Stack Overflow является
использование вертикального масштабирования, как достаточно
работоспособного решения достаточного большого класса проблем. Не смотря
на то, что публика на сегодняшний день больше склоняется к подходу с
использованием горизонтальным масштабирования и&amp;nbsp;не-SQL баз данных.&lt;/p&gt;
&lt;p&gt;Если Вы стремитесь к масштабу Google, у Вас нет другого выхода, как
двигаться в направлении не-SQL. Но Stack Overflow - это не Google, ровно
как и подавляющее большинство других сайтов. Когда Вы задумываетесь о
возможных вариантов дизайна Вашего проекта, попробуйте учесть и историю
Stack Overflow, она тоже имеет право на жизнь. В этот век многоядерных
машин с большим объемом оперативной памяти и невероятными темпами
развития методов параллельного программирования, вертикальное
масштабирование все еще является жизнеспособной стратегией и не должна
сразу же отбрасываться в сторону просто так как это теперь больше не
модно. Возможно в один прекрасный день мы получим лучшее из обоих миров,
но на сегодняшний момент перед нами лежит большой болезненный выбор
стратегии масштабирования, от которого определенно зависит судьба Вашего
проекта.&lt;/p&gt;
&lt;p&gt;Joel любит похвастаться тем, что они достигли производительности,
сравнимой с другими сайтами аналогичных размеров, используя в 10 раз
меньше оборудования. Он удивляется, работали над этими сайтами
по-настоящему хорошие программисты. Давайте взглянем на то, как им это
удалось, и дадим Вам возможность побыть судьей.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;&lt;em&gt;Перевод &lt;a href="https://www.insight-it.ru/goto/98b904e0/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2009/8/5/stack-overflow-architecture.html"&gt;статьи&lt;/a&gt;, автор оригинала - Todd Hoff. Возможно будет еще один пост с менее формальной информацией на ту же тему.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;16 миллионов просмотров страниц в месяц&lt;/li&gt;
&lt;li&gt;3 миллионов уникальных пользователей в месяц (для сравнения: Facebook
насчитывает около 77 миллионов уникальных пользователей в месяц)&lt;/li&gt;
&lt;li&gt;6 миллионов посещений в месяц&lt;/li&gt;
&lt;li&gt;86% трафика приходит с Google&lt;/li&gt;
&lt;li&gt;9 миллионов активных программистов во всем мире и 30% пользуются Stack
Overflow&lt;/li&gt;
&lt;li&gt;Более дешевые лицензии были получены через программу
Microsoft&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/205088ae/" rel="nofollow" target="_blank" title="http://www.microsoft.com/BizSpark"&gt;BizSpark&lt;/a&gt;. Скорее всего
они заплатили около 11000\$ за лицензии на ОС и MSSQL.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Стратегия монетизации: ненавязчивая реклама, вакансии, конференции
DevDays, достижения других смежных ниш (Server Fault, Super User),
разработка&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/631e88c2/" rel="nofollow" target="_blank" title="https://stackexchange.com/"&gt;StackExchange&lt;/a&gt; и возможно
каких-то других систем рейтингов для программистов.&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Microsoft ASP.NET MVC&lt;/li&gt;
&lt;li&gt;SQL Server 2008&lt;/li&gt;
&lt;li&gt;C#&lt;/li&gt;
&lt;li&gt;Visual Studio 2008 Team Suite&lt;/li&gt;
&lt;li&gt;jQuery&lt;/li&gt;
&lt;li&gt;LINQ to SQL&lt;/li&gt;
&lt;li&gt;Subversion&lt;/li&gt;
&lt;li&gt;Beyond Compare 3&lt;/li&gt;
&lt;li&gt;VisualSVN 1.5&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Веб уровень:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 x Lenovo ThinkServer RS110 1U&lt;/li&gt;
&lt;li&gt;4 ядра, 2.83 Ghz, 12 MB L2 cache&lt;/li&gt;
&lt;li&gt;500 GB жесткие диски, зеркалирование RAID1&lt;/li&gt;
&lt;li&gt;8 GB RAM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Уровень базы данных:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 x Lenovo ThinkServer RD120 2U&lt;/li&gt;
&lt;li&gt;8 ядер, 2.5 Ghz, 24 MB L2 cache&lt;/li&gt;
&lt;li&gt;48 GB RAM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Четвертый сервер был добавлен для запуска
&lt;a href="https://www.insight-it.ru/goto/d3829019/" rel="nofollow" target="_blank" title="https://superuser.com/"&gt;superuser.com&lt;/a&gt;. Все сервера вместе обеспечивают
работу &lt;a href="https://www.insight-it.ru/goto/33fc61d9/" rel="nofollow" target="_blank" title="https://stackoverflow.com/"&gt;Stack Overflow&lt;/a&gt;,&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/9b0a2d16/" rel="nofollow" target="_blank" title="https://serverfault.com/"&gt;Server
Fault&lt;/a&gt;, и&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/d3829019/" rel="nofollow" target="_blank" title="https://superuser.com/"&gt;Super User&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;QNAP TS-409U NAS для резервного копирования данных. Было принято решение
не использовать "облачные" решения, так как вызванные ими дополнительные
5GB трафика ежедневно были бы накладными.&lt;/li&gt;
&lt;li&gt;Сервера располагаются у&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/4f3f2e08/" rel="nofollow" target="_blank" title="http://www.peakinternet.com/"&gt;Peak Internet&lt;/a&gt;. В
основном из-за впечатляющей детализации технических ответов и разумных
расценок.&lt;/li&gt;
&lt;li&gt;Полнотекстный поиск в SQL Server активно используется для реализации
поиска по сайту и выявления повторных вопросов. Lucene .NET
рассматривается как достаточно заманчивая альтернатива.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;p&gt;Данный список является сборником уроков от Jeff и Joel, а также из
комментариев к их записям:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Если Вы комфортно себя чувствуете в деле управления серверами - не
бойтесь покупать их. Две основных проблемы с издержками аренды
оборудования:&lt;ol&gt;
&lt;li&gt;невероятные цены на дополнительную оперативную память и
жесткие диски;&lt;/li&gt;
&lt;li&gt;хостинг-провайдеры на самом деле не могут управлять
чем-либо за Вас.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Делайте одноразовые более крупные инвестиции в оборудование, чтобы
избежать быстро растущих ежемесячных издержек по аренде, которые
окажутся более высокими в долгосрочном периоде.&lt;/li&gt;
&lt;li&gt;Обновляйте сетевые драйвера. Производительность запросто может
удвоиться.&lt;/li&gt;
&lt;li&gt;Использование 48GB RAM требует обновления до MS Enterprise edition.&lt;/li&gt;
&lt;li&gt;Оперативная память невероятно дешевая. Используйте возможности по её
расширению по максимуму для получения практически бесплатной
производительности. У Dell, например, переход от 4GB памяти до 128GB
стоит всего 4378\$.&lt;/li&gt;
&lt;li&gt;Stack Overflow скопировали ключевую часть структуры базы данных у
Wikipedia. Это обернулось огромной ошибкой, для исправления которой
потребуется большой и болезненный рефакторинг базы данных. Основным
направлением изменений будет избавление от излишних операций по
объединению данных в большом количестве ключевых запросов. Это ключевой
урок, который стоит усвоить у гигантских много-терабайтных схем (вроде
Google BigTable), которые полностью избавлены от операций объединения
данных. Этот вопрос был достаточно важен для Stack Overflow, так как их
база данных практически полностью располагается в оперативной памяти и
операции join по прежнему требуют относительно много вычислительных
ресурсов.&lt;/li&gt;
&lt;li&gt;Производительность CPU оказывается на удивление важным фактором для
серверов баз данных. Переход от 1.86 GHz, к 2.5 GHz, и к 3.5 GHz
процессорам дает практически линейный прирост к времени выполнения
типичных запросов. Исключение: запросы, которые затрагивают не только
оперативную память.&lt;/li&gt;
&lt;li&gt;Когда оборудование арендуется, обычно никто не платит за дополнительную
оперативную память, если только вы не на помесячном контракте.&lt;/li&gt;
&lt;li&gt;В 90% случаев наиболее узким местом является база данных.&lt;/li&gt;
&lt;li&gt;При небольшом количестве серверов, &amp;nbsp;ключевым компонентом издержек
становится не место в стойках, электроэнергия, интернет-канал, сервера
или программное обеспечение, а СЕТЕВОЕ ОБОРУДОВАНИЕ. Вам потребуется как
минимум гигабитное соединение между уровнями веб-серверов и баз данных.
Между интернетом и веб-серверами потребуется firewall, маршрутизатор и
VPN. К моменту добавления второго веб-сервера понадобится решение для
балансировки нагрузки. Суммарная стоимость такого оборудования может
запросто вдвое превосходить стоимость пяти серверов.&lt;/li&gt;
&lt;li&gt;EC2 предназначен для горизонтального масштабирования, для того чтобы
нагрузка могла быть распределена между большим количеством машин
(достаточно хорошая идея, если Вы планируете расширяться). Еще больше
смысла в таком подходе появляется, если вы планируете масштабироваться
по необходимости (то есть добавлять и убирать машины в зависимости от
уровня нагрузки).&lt;/li&gt;
&lt;li&gt;Горизонтальное масштабирование может проходить относительно
безболезненно только при использовании open source программного
обеспечения. В противном случае вертикальное масштабирование значит
сокращение издержек, связанных с лицензиями, в ущерб стоимости
оборудования, а горизонтальное масштабирование - наоборот: экономия на
оборудовании, но требуется существенно больше лицензий на программное
обеспечение.&lt;/li&gt;
&lt;li&gt;RAID-10 отлично работает для баз данных с высокой нагрузкой операций
чтения и записи.&lt;/li&gt;
&lt;li&gt;Разделяйте работу приложений и баз данных таким образом, чтобы они могли
масштабироваться независимо друг от друга. Например, базы данных могут
масштабироваться вертикально, а сервера приложений - горизонтально.&lt;/li&gt;
&lt;li&gt;Приложения должны хранить все информацию о своем состоянии в базе данных
для обеспечения возможности роста путем простого добавления серверов
приложений в кластер.&lt;/li&gt;
&lt;li&gt;Одна из основных проблем со стратегией вертикального масштабирования -
недостаток избыточности. Кластеризация добавляет надежности, но когда
стоимость каждого сервера высока - это не так просто реализовать.&lt;/li&gt;
&lt;li&gt;Некоторые приложения могут масштабироваться линейно относительно числа
процессоров. Но зачастую будут использоваться механизмы блокировки, что
приведет к сериализации вычислений и в итоге к существенному уменьшению
эффективности приложения.&lt;/li&gt;
&lt;li&gt;С более крупными серверами, занимающими от 7U в стойке, электроэнергия и охлаждение становятся критичными вопросами. Возможно использование
чего-то среднего между 1U и 7U может облегчить Ваши взаимоотношения с
датацентром.&lt;/li&gt;
&lt;li&gt;С добавлением все новых и новых серверов баз данных издержки на лицензии SQL Server могут стать очень существенными. Если Вы начнете с
вертикального масштабирования и постепенно начнете переходить к
горизонтальному с использованием не open source продуктов, возможно это
сильно ударит по Вашему финансовому состоянию. Это справедливо, что в
этой заметке речь идет не совсем об архитектуре проекта. Мы знаем об их
серверах, об используемом наборе инструментов, об их двухуровневой
схеме, где база данных используется напрямую из кода веб-серверов. Но мы
не знаем практически ничего о самой реализации, например таких мелочей
как теги. Если Вам интересен этот вопрос, возможно Вам удастся получить
интересующую Вас информацию из &lt;a href="https://www.insight-it.ru/goto/61237733/" rel="nofollow" target="_blank" title="http://sqlserverpedia.com/wiki/Understanding_the_StackOverflow_Database_Schema"&gt;описания их схемы базы данных&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 08 Jan 2010 00:31:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-01-08:highload/2010/arkhitektura-stack-overflow/</guid><category>ASP</category><category>ASP .NET</category><category>Beyond Compare 3</category><category>C++</category><category>highload</category><category>JQuery</category><category>Lenovo</category><category>Lenovo ThinkServer</category><category>LINQ</category><category>Microsoft</category><category>MSSQL</category><category>MVC</category><category>Server Fault</category><category>SQL Server 2008</category><category>Stack Overflow</category><category>Subversion</category><category>Super User</category><category>Visual Studio 2008 Team Suite</category><category>VisualSVN</category><category>архитектура</category><category>архитектура Stack Overflow</category><category>архитектура высоконагруженных сайтов</category><category>Масштабируемость</category></item><item><title>Terrastore</title><link>https://www.insight-it.ru//storage/2010/terrastore/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/76e0ccd6/" rel="nofollow" target="_blank" title="http://code.google.com/p/terrastore/"&gt;Terrastore&lt;/a&gt; является
свежеиспеченной системой хранения документов, с отличными возможностями
по масштабируемости и эластичной настройке, при этом без жертв со
стороны консистентности данных.&lt;/p&gt;
&lt;p&gt;Вместо подробного описания несколько ключевых характеристик продукта:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Легкодоступность:&lt;/strong&gt; данные доступны посредством повсеместно
    используемого протокола HTTP.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Распреденность:&lt;/strong&gt; узлы могут работать и существовать на любых
    доступных серверах.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Эластичность:&lt;/strong&gt; имеется возможность динамического добавления и
    удаления узлов кластера на лету, без малейшего простоя системы и
    каких-либо изменений в конфигурации.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Масштабируемость на уровне данных:&lt;/strong&gt; документы разбиваются на
    группы и распределяются между доступными узлами с автоматической
    прозрачной балансировкой, в том числе и при добавлении и исключении
    узлов в кластере.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Масштабируемость на вычислительном уровне:&lt;/strong&gt; запросы и обновление
    данных распределяются по узлам, которые физически хранят
    используемые данные, тем самым минимизируется трафик и
    распределяется вычислительная нагрузка.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Консистентность:&lt;/strong&gt; система обеспечивает по-документную
    консистентность данных, таким образом гарантируя тот факт, что
    пользователь всегда получает самую свежую версию документа,
    обеспечивая изоляцию для параллельных модификаций документов.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Отсутствие схемы:&lt;/strong&gt; предоставляет JSON интерфейс, основанный на
    коллекциях; пользователям предоставляется возможность просто создать
    свою коллекцию и положить туда что угодно.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Простота в работе:&lt;/strong&gt; установка полностью работоспособного кластера
    заключается в вводе всего нескольких команд и не требует какого-либо
    редактирование XML-конфигов.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Богатый функционал:&lt;/strong&gt; поддерживаются push-down предикаты, запросы
    по диапазонам и серверные функции обновления.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Если Вам показалось интересным, у Вас есть возможность &lt;a href="https://www.insight-it.ru/goto/76e0ccd6/" rel="nofollow" target="_blank" title="http://code.google.com/p/terrastore/"&gt;получить более подробную информацию&lt;/a&gt;,&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/ff8d489f/" rel="nofollow" target="_blank" title="http://groups.google.com/group/terrastore-discussions"&gt;принять участие в проекте&lt;/a&gt;,&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/b14ec8b0/" rel="nofollow" target="_blank" title="http://code.google.com/p/terrastore/downloads/list"&gt;скачать дистрибутив&lt;/a&gt;
или&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/caed0d40/" rel="nofollow" target="_blank" title="http://code.google.com/p/terrastore/source/createClone"&gt;получить копию исходного кода&lt;/a&gt;!
﻿&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;В очередной раз спасибо &lt;a href="https://www.insight-it.ru/goto/7b0eba82/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2009/12/30/terrastore-scalable-elastic-consistent-document-store.html"&gt;highscalability.com за источник
информации&lt;/a&gt;,
за одно хотелось бы услышать мнения о таком формате постов. Я тут уже
почти неделю копаюсь над постом-долгостроем про Baidu, а такой можно
сочинить за полчаса.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Кстати про &lt;a href="https://www.insight-it.ru/goto/65f34522/" rel="nofollow" target="_blank" title="http://www.terracotta.org"&gt;Terracotta&lt;/a&gt;, на основе
которой работает данный продукт, тоже давно пора было уже написать, в
ближайшее время займусь :)&lt;/em&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 07 Jan 2010 01:22:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-01-07:storage/2010/terrastore/</guid><category>HTTP</category><category>Terracotta</category><category>Terrastore</category><category>документы</category><category>Масштабируемость</category><category>хранение</category></item><item><title>Архитектура MySpace</title><link>https://www.insight-it.ru//highload/2009/arkhitektura-myspace/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/f989d588/" rel="nofollow" target="_blank" title="http://www.myspace.com"&gt;MySpace.com&lt;/a&gt; является одним из наиболее быстро
набирающих популярность сайтов в Интернете с 65 миллионами пользователей
и 260000 регистрациями в день. Этот сайт часто подвергается критике
из-за не достаточной производительности, хотя на самом деле MySpace
удалось избежать ряда проблем с масштабируемостью, с которыми
большинство других сайтов неизбежно сталкивались. Как же им это
удалось?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Данная статья является переводом статьи &lt;a href="https://www.insight-it.ru/goto/e9f0b809/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2009/2/12/myspace-architecture.html"&gt;MySpace
Architecture&lt;/a&gt;,
автором которой является Todd Hoff. Когда-то давно один из читателей
этого блога просил меня осветить и эту тему, тогда я так и не решился
из-за отсутствия моего личного интереса, но сейчас снова случайно
наткнулся на эту статью и подумал: а почему бы и нет?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/45549701/" rel="nofollow" target="_blank" title="http://www.infoq.com/news/2009/02/MySpace-Dan-Farino"&gt;Презентация: за сценой MySpace.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8d5c1d4d/" rel="nofollow" target="_blank" title="http://www.baselinemag.com/c/a/Projects-Networks-and-Storage/Inside-MySpacecom/"&gt;Внутри MySpace.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ASP .NET 2.0&lt;/li&gt;
&lt;li&gt;Windows&lt;/li&gt;
&lt;li&gt;IIS&lt;/li&gt;
&lt;li&gt;MSSQL Server&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="chto-vnutri"&gt;Что внутри?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;300 миллионов пользователей.&lt;/li&gt;
&lt;li&gt;Отдает 100Gbps в Интернет. 10Gbps из них является HTML контентом.&lt;/li&gt;
&lt;li&gt;4,500+ веб серверов со связкой: Windows 2003 / IIS 6.0 / ASP .NET.&lt;/li&gt;
&lt;li&gt;1,200+ кэширующих серверов, работающих на 64-bit Windows 2003. На
    каждом 16GB объектов находятся в кэше в оперативной памяти.&lt;/li&gt;
&lt;li&gt;500+ серверов баз данных, работающих на 64-bit Windows и SQL Server
    2005.&lt;/li&gt;
&lt;li&gt;MySpace обрабатывает 1.5 миллиарда просмотров страниц в день, а
    также 2.3 миллионов одновременно работающих пользователей в течении
    дня.&lt;/li&gt;
&lt;li&gt;Вехи по количеству пользователей:&lt;ul&gt;
&lt;li&gt;500 тысяч пользователей: простая архитектура перестает
справляться&lt;/li&gt;
&lt;li&gt;1 миллион пользователей: вертикальное партиционирование временно
спасает от основных болезненных вопросов с масштабированием&lt;/li&gt;
&lt;li&gt;3 миллиона пользователей: горизонтальное масштабирование
побеждает над вертикальным&lt;/li&gt;
&lt;li&gt;9 миллионов пользователей: сайт мигрирует на ASP.NET, создается
виртуализированная система хранения данных (SAN)&lt;/li&gt;
&lt;li&gt;26 миллионов пользователей: MySpace переходит на 64-битную
технологию.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;500 тысяч учетных записей было многовато для двух веб-серверов и
    одного сервера баз данных.&lt;/li&gt;
&lt;li&gt;На 1-2 миллионах учетных записей:&lt;ul&gt;
&lt;li&gt;Они использовали архитектуру базы данных, построенную на
концепции вертикального партиционирования, с отдельными базами
данных для разных частей сайта, которые использовались для
выполнения различных функций, таких как экран авторизации, профили
пользователей и блоги.&lt;/li&gt;
&lt;li&gt;Схема с вертикальным партиционированием помогала разделить
нагрузку как для операций чтения, так и для операций записи, а если
пользователям в друг оказывалась нужна новая функциональная
возможность - достаточно было просто добавить еще один сервер баз
данных для её обслуживания.&lt;/li&gt;
&lt;li&gt;MySpace переходит от использования систем хранения, подключенных
к серверам баз данных напрямую, к сетям хранения данных (SAN), при
таком подходе целый массив систем хранения объединяется вместе
специализированной сетью с высокой пропускной способностью, и
сервера баз данных также получают доступ к хранилищам через эту
сеть. Переход к SAN оказал положительное влияние как на
производительность, так и на доступность и надежность системы.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;На 3 миллионах учетных записей:&lt;ul&gt;
&lt;li&gt;Решение с вертикальным партиционированием не протянуло долго, так
как им приходилось реплицировать какую-то часть информации (например
информацию об учетных записях) по всем вертикальным частям базы
данных. С таким большим количеством операций репликации данных один
узел даже при незначительном сбое мог существенно замедлить
обновление информации во всей системе.&lt;/li&gt;
&lt;li&gt;Индивидуальные приложения вроде блогов на под-секциях сайта
достаточно быстро стали слишком большими для нормальной работы с
единственным сервером базы данных&lt;/li&gt;
&lt;li&gt;Произведена реорганизация всех ключевых данных для более логичной
организации в единственную базу данных&lt;/li&gt;
&lt;li&gt;Пользователи были разбиты на группы по миллиону в каждой и каждая
такая группа была перемещена на отдельный SQL Server&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;9&amp;ndash;17 миллионов учетных записей:&lt;ul&gt;
&lt;li&gt;Переход на ASP .NET, который требовал меньше ресурсов по
сравнению с их предыдущим вариантом архитектуры. 150 серверов,
использовавших новый код могли обработать нагрузку, для которой
раньше требовалось 246 серверов.&lt;/li&gt;
&lt;li&gt;Снова пришлось столкнуться с узким местом в системе хранения
данных. Реализация SAN решило какую-то часть старых проблем с
производительностью, но на тот момент потребности сайта начали
периодически превосходить возможности SAN по пропускной способности
операций ввода-вывода - той скорости, с которой она может читать и
писать данные на дисковые массивы.&lt;/li&gt;
&lt;li&gt;Столкнулись с лимитом производительности при размещении миллиона
учетных записей на одном сервере, ресурсы некоторых серверов начали
исчерпываться.&lt;/li&gt;
&lt;li&gt;Переход к виртуальному хранилищу, где весь SAN рассматривается
как одно большое общее место для хранения данных, без необходимости
назначать конкретные диски для хранения данных определенной части
приложения. MySpace на данный момент работает со стандартизированным
оборудованием от достаточно нового вендора SAN - 3PARdata&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Был добавлен кэширующий уровень &amp;mdash; прослойка из специализированных
    серверов, расположенных между веб-серверами и серверами данных, чья
    единственная задача была захватывать копии часто запрашиваемых
    объектов с данными в памяти и отдавать их веб-серверам для
    минимизации количества поиска данных в СУБД.&lt;/li&gt;
&lt;li&gt;26 миллионов учетных записей:&lt;ul&gt;
&lt;li&gt;Переход на 64-битные сервера с SQL Server на правах решения
проблемы с недостатком оперативной памяти. С тех пор их стандартный
сервер баз данных оснащен 64 GB RAM.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Горизонтальная федерация баз данных&lt;/strong&gt;. Базы данных
    партиционируются в зависимости от своего назначения. У них есть базы
    данных с профилями, электронными сообщениями и так далее. Каждая
    партиция основана на диапазоне пользователей. По миллиону в каждой
    базе данных. Таким образом, у них есть Profile1, Profile2 и все
    остальные базы данных вплоть до Profile300, если считать, что у них
    на данный момент зарегистрировано 300 миллионов учетных записей.&lt;/li&gt;
&lt;li&gt;Кэш ASP не используется, так как он не обеспечивает достаточного
    процента попаданий на веб серверах. Кэш, организованный как
    промежуточный слой, имеет существенно более высокое значение данного
    показателя.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Изоляция сбоев&lt;/strong&gt;. Внутри веб-сервера запросы сегментируются по
    базам данным. Разрешено использование только 7 потоков для работы с
    каждой базой данных. Таким образом, если база данных по каким-то
    причинам начинает работать медленно, только эти потоки замедлятся, в
    то время как остальные потоки будут успешно продолжать обрабатывать
    поток трафика.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="rabota-saita"&gt;Работа сайта&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Коллектор данных о производительности&lt;/strong&gt;. Централизованная система
    сбора информации о производительности через UDP. Такой подход более
    надежен, чем стандартный механизм Windows, а также позволяет любому
    клиенту подключиться и увидеть статистику.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Веб-система по просмотру дампов стеков процессов&lt;/strong&gt;. Можно просто
    сделать клик правой кнопкой мыши на проблемном сервере и увидеть
    дамп стека процессов, управляемых .NET. И это после привычки каждой
    раз удаленно подключаться к серверу, включать дебаггер и через
    полчаса получать свой ответ о том что же все таки происходит.
    Медленно, немасштабируемо и утомительно. Эта же система позволяет
    увидеть не просто стек процесса, но и предоставляет большое
    количество информации о контексте, в котором он работает.
    Обнаружение проблем намного проще при таком подходе, например можно
    легко увидеть, что база не отвечает, так как 90 ее потоков
    заблокировано.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Веб-система создания дампа heap-памяти&lt;/strong&gt;. Создает дамп всей
    выделенной памяти. Очень удобно и полезно для разработчиков.
    Сэкономьте часы на выполнение этой работы вручную.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Профайлер&lt;/strong&gt;. Прослеживает запрос от начала до конца и выводит
    подробный отчет. В нем можно увидеть URL, методы, статус, а также
    все, что поможет идентифицировать медленный запрос и его причины.
    Обнаруживает проблемы с блокировкой потоков, непредвиденными
    исключениями, другими словами все, что может оказаться интересным. В
    то же время остается очень легковесным решением. Работает на одной
    машине из каждой VIP (группа из 100 серверов) в production-среде.
    Опрашивает 1 поток каждые 10 секунд. Постоянно следит за системой в
    фоновом режиме.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Powershell&lt;/strong&gt;. Новая программная оболочка от Microsoft, которая
    работает в процессе и передаем объекты между командами вместо работы
    с текстовыми данными. MySpace разрабатывает множество так называемых
    commandlets'ов для поддержки различных операций.&lt;/li&gt;
&lt;li&gt;Разработана собственная технология асинхронной коммуникации для
    того, чтобы обойти проблемы с сетевыми проблемами Windows и работать
    с серверами как с группой. Например, она позволяет доставить файл
    .cs, скомпилировать его, запустить, и доставить результат обратно.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Развертывание&lt;/strong&gt;. Обновление кодовой базы происходит с помощью
    упомянутой выше собственной технологии. Ранее происходило до 5 таких
    обновлений в день, сейчас же они происходят лишь раз в неделю.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;С помощью стека Microsoft тоже можно делать большие веб-сайты.&lt;/li&gt;
&lt;li&gt;Стоит использовать кэширование с самого начала.&lt;/li&gt;
&lt;li&gt;Кэш является более подходящим местом для хранения временных данных,
    не требующих персистентности, например информации о пользовательских
    сессиях.&lt;/li&gt;
&lt;li&gt;Встроенные в операционные систему возможности, например по
    обнаружению DDoS-атака, могут приводить к необъяснимым сбоям.&lt;/li&gt;
&lt;li&gt;Храните свои данные в географически удаленных датацентрах для
    минимизации проблем, связанных со сбоями в электросети.&lt;/li&gt;
&lt;li&gt;Рассматривайте возможности использования виртуализированных систем
    хранения данных или кластерных файловых систем с самого начала. Это
    позволит существенно параллелизировать операции ввода-вывода, а
    также увеличивать дисковое пространство без необходимости какой-либо
    реорганизации.&lt;/li&gt;
&lt;li&gt;Разрабатывайте утилиты для работы с production окружением.
    Невозможно смоделировать все ситуации в тестовой среде.
    Масштабируемость и все различные варианты использования API не могут
    быть симулированы в процессе тестирования качества программного
    обеспечения. Обычные пользователи и хакеры обязательно найдут такие
    способы использования вашего продукта, о которых вы даже никогда и
    не подумаете в процессе тестирования, хотя конечно большая часть все
    же обнаружима в процессе QA тестирования.&lt;/li&gt;
&lt;li&gt;Когда это возможно - лучше просто использовать дополнительное
    оборудование для решения проблем. Это намного проще, чем изменять
    поведение программного обеспечения для того чтобы решать задачи
    как-то по-другому. Примером может служить добавление нового сервера
    на каждый миллион пользователей. Возможно было бы более эффективным
    изменить подход к самой работе с СУБД, но на практике все же проще и
    дешевле добавлять все новые и новые сервера. По крайней мере на
    данный момент.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 21 Dec 2009 16:15:00 +0300</pubDate><guid>tag:www.insight-it.ru,2009-12-21:highload/2009/arkhitektura-myspace/</guid><category>ASP</category><category>ASP .NET</category><category>highload</category><category>IIS</category><category>Microsoft</category><category>MSSQL</category><category>MySpace</category><category>myspace.com</category><category>online</category><category>Windows</category><category>Windows Server</category><category>архитектура</category><category>архитектура MySpace</category><category>Масштабируемость</category></item><item><title>Django в гостях у Google</title><link>https://www.insight-it.ru//python/2009/django-v-gostyakh-u-google/</link><description>&lt;p&gt;&lt;img alt="Google App Engine" class="left" src="https://www.insight-it.ru/images/appengine.jpg" title="Google App Engine"/&gt;
&lt;del&gt;Давным-давно, в далекой-предалекой галактике...&lt;/del&gt;&lt;/p&gt;
&lt;p&gt;Хотя да, достаточно давно уже Google выпустили в свет платформу &lt;a href="https://www.insight-it.ru/goto/99b033c0/" rel="nofollow" target="_blank" title="Google App Engine"&gt;Google App Engine&lt;/a&gt;. Описание
этого продукта меня заинтересовало еще до открытия публичного доступа к
системе и я даже записался на полу-закрытое тестирование. Вскоре пришло
подтверждение, что мол "мы рады сообщить, что Ваша учетная запись
активирована и теперь у Вас есть возможность попробовать наш новый
продукт, для этого нажмите ссылку такую-то". Но пришло оно как-то не
очень удачно, когда ни лишнего свободного времени не было, да и идеи
подходящей для создания чего-нибудь эдакого на новой платформе тоже на
горизонте не наблюдалось. В общем зашел на их сайт, посмотрел админку,
поставил демо-приложение, поигрался чуток и забросил. Но с тех пор руки
так и не прекращали чесаться от желания попробовать GAE на каком-нибудь
более приближенном к реальности приложении, что мне совсем недавно и
довелось сделать. Спешу поделиться впечатлениями.
&lt;!--more--&gt;
Если Вы даже краем уха не слышали о платформе &lt;code&gt;Google App Engine&lt;/code&gt; и после
прочтения вступления не удосужились скопировать это название в свою
любимую поисковую систему, чтобы почитать по-подробнее, то Вам повезло:
для порядка я все-таки расскажу чуть-чуть о тех вкусностях, которые так
долго поддерживали мой интерес к данному проекту.&lt;/p&gt;
&lt;p&gt;Если взглянуть издалека, то GAE представляет собой условно-бесплатный
хостинг для веб-приложений, для разработчиков предоставляется все
необходимое: начиная от минимально-необходимого &lt;a href="https://www.insight-it.ru/goto/d7620107/" rel="nofollow" target="_blank" title="SDK"&gt;SDK&lt;/a&gt;
со встроенным веб-сервером, локально эмулирующим саму платформу,
заканчивая неплохой документацией по самой системе и доступным из нее
API от Google. Почему условно-бесплатный? Бесплатно приложениям
выделяется лишь ограниченное количество вычислительных ресурсов, при
превышении которых по выбору владельца приложения либо взимается вполне
&lt;a href="https://www.insight-it.ru/goto/69548105/" rel="nofollow" target="_blank" title="скромная плата"&gt;скромная плата&lt;/a&gt;,
либо всем пользователям начинают показывать "извиняйте, заходите завтра"
(в прямом смысле, счетчики потребления ресурсов сбрасываются ежедневно).&lt;/p&gt;
&lt;p&gt;Но финансовый вопрос далеко не самый интересный, давайте взглянем на
техническую сторону медали. Написанное с использованием SDK приложение
загружается в production-окружение, которое физически размещается на тех
самых известных кластерах Google, о которых у меня даже &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-google/" title="есть пост"&gt;есть пост&lt;/a&gt;
(конечно же под GAE используется только очень небольшая часть их
вычислительных можностей). Причем все заботы о распределенной работе
приложения на большом количестве машин платформа берет на себя:
разработчику не нужно думать ни о балансировке нагрузки, ни о
партиционировании данных, ни о других аспектах. Сразу же после окончания
процессов загрузки и развертывания приложение готово становится готово к
работе и доступно по домену третьего уровня на &lt;code&gt;*.appspot.com&lt;/code&gt;, либо можно
подключить свой отдельный домен.&lt;/p&gt;
&lt;p&gt;Технические ограничения тоже имеют быть: для разработки под GAE можно
использовать лишь небольшой набор языков программирования, в частности
Python 2.5, а также Java и все остальные языки, компилируемые или
интерпретируемые под JVM (JRuby, Scala, Rhino, etc.). Все приложения
исполняются в песочнице, ограничивающей доступ к окружающему миру, то
есть определенные подмножества языков становятся недоступны, например:
доступ к файловым системам, встроенные средства обработки изображений,
доступ к сторонним ресурсам по HTTP, отправка почты. Про реляционные
базы данных, memcached и библиотеки, использующие нативный,
платформозависимый код, также стоит забыть. Но не все так плохо, как
кажется: для реализации всех "отобранных" у разработчиков функциональных
компонент Google предоставляет собственные сервисы-заменители, доступные
через хорошо документированный API или вовсе замаскированные под
стандартные методы языка. В качестве дополнительных бонусов
предоставляются и возможности по интеграции с другими продуктами Google,
скажем можно легко сделать авторизацию пользователей в приложении по
учетным записям от &lt;em&gt;GMail&lt;/em&gt; или нотификацию пользователей по Jabber через
&lt;em&gt;GTalk&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Отдельного внимания заслуживает используемая в данной платформе система
хранения данных, основанная на &lt;strong&gt;BigTable&lt;/strong&gt;, о которой более подробно
можно почитать в уже упомянутом &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-google/" title="посте об архитектуре Google"&gt;посте об архитектуре Google&lt;/a&gt;.
Если в двух словах, то она представляет собой распределенное
&lt;strong&gt;не&lt;/strong&gt;реляционное хранилище данных, автоматически обеспечивающее
репликацию и кеширование данных, а также практически гарантирующее
постоянную доступность данных вне зависимости от сбоев низлежащего
оборудования. Для доступа к нему разработчикам предоставляется
специальный API и язык доступа к данным &lt;em&gt;GQL&lt;/em&gt;, слегка напоминающий
упрощенный диалект &lt;em&gt;SQL&lt;/em&gt; (лишь отдаленно). Продукт в обращении
достаточно своеобразен, как оказалось самый простой способ привыкнуть к
работе с ним - выкинуть из головы все знания о традиционных СУБД и
взглянуть на процесс хранения данных с чистого листа. Разномастные
JOIN'ы и прочие изыски лишь мешают думать в терминах подобных систем.&lt;/p&gt;
&lt;p&gt;Закончив тему с рекламой GAE, позвольте перейти к моим личным
впечатлениям. Попробовал я данную платформу на вполне конкретном примере
(в конце поста дам ссылочку на частично-готовый результат, если кому
интересно), надо же в конце-концов на что-то с пользой убивать внезапно
появившееся свободное время. ОтJava и прочей компании языков, основанных
на JVM, я невероятно устал на теперь уже "прошлой" работе, так что взор
мой упал на Python и давно находящийся у меня на слуху (в основном
благодаря &lt;a href="https://www.insight-it.ru/goto/d12c91be/" rel="nofollow" target="_blank" title="Ивану Сагалаеву"&gt;Ивану Сагалаеву&lt;/a&gt;)
фреймворк &lt;a href="https://www.insight-it.ru/goto/8e1e1008/" rel="nofollow" target="_blank" title="Django"&gt;Django&lt;/a&gt;. Ни с
тем, ни с другим я ранее почти не был знаком на практике, разве что
когда-то пытался помогать своим очень хорошим подругам с прохождением
Python в университете (пользуясь случаем, передаю привет Полине, Кате и
Юле, очень по вам скучаю ;) ). Стоит упомянуть, что существует несколько
сборок Django, адаптированных под GAE, наиболее продуманным и готовым к
эксплуатации мне показался проект под названием &lt;a href="https://www.insight-it.ru/goto/2c5c0602/" rel="nofollow" target="_blank" title="app engine patch"&gt;app engine patch&lt;/a&gt;,
которым я и воспользовался для экспериментов.&lt;/p&gt;
&lt;p&gt;Django, как известно, является вполне традиционным веб-фрейморком,
пропагандирующим свою вариацию на тему MVC (именуемую &lt;strong&gt;MVT&lt;/strong&gt; - &lt;code&gt;Model-View-Template&lt;/code&gt;, но по сути абсолютно то же самое), а также целый ряд философских верований (вроде &lt;em&gt;DRY, Don't repeat yourself&lt;/em&gt;), которым даже отведена &lt;a href="https://www.insight-it.ru/goto/ecd0c9e6/" rel="nofollow" target="_blank" title="отдельная страница на сайте"&gt;отдельная страница на официальном сайте&lt;/a&gt;.
Адаптированная под GAE версия фреймворка отличается от стандартной по
большому счету лишь замененной частью &lt;code&gt;Model&lt;/code&gt;, в которую очень неплохо
вписался предоставляемый API к уже упоминавшемуся хранилищу данных. По
всем остальным компонентам системы официальная документация по Django
практически полностью актуальна и сильно помогла понять всю картину
разработки веб-приложений с использованием данных технологий.&lt;/p&gt;
&lt;p&gt;Пересказывать функциональные возможности Django как-то не входило в мои
планы, все кому интересно и так уже в курсе или знают где посмотреть.
Хочу лишь сказать, что со своей задачей упрощения и ускорения процесса
разработки веб-приложений он полностью справляется: все основные
функциональные компоненты реализуются просто, легко и быстро, при этом
особой необходимости (да и желания) вникать в то, как оно в итоге
работает не возникает. Если же взглянуть на Django в совокупности с
возможностями GAE - вопросы масштабируемости также по большей части с
плеч разработчика снимаются (если не забыть прочитать документацию по
хранилищу и не творить глупостей). В общем что-что, а количество
человекочасов, требуемых на создание качественного масштабируемого
веб-приложения, эта парочка способна сократить изрядно.&lt;/p&gt;
&lt;p&gt;Предложение Google по использованию платформы GAE выглядит очень
заманчиво, не смотря на все ограничения под нее можно как портировать
существующие приложения, так и легко создавать новые. Бесплатное
использование до превышения квот также не может не радовать (кстати
квоты там рассчитаны на мировой рынок, превысить большинство из них в
рамках рунета - надо постараться, мне кажется). Но закончить данное
повествование мне всетаки хотелось парой недокументированных или вкратце
официально упоминавшихся "ложек дегтя". Первая неприятная особенность:
процессы, обрабатывающие пользовательские запросы приложений, умирают
после очень небольшого времени простоя (таймаут судя по всему секунд
20-30). По истечении таймаута система освобождает использующиеся
приложением ресурсы и когда после перерыва приходит очередной
пользователь система вынуждена заново инициализироваться (чуть ли не
заново компилировать байткод, хотя не уверен), что занимает около 5
секунд, а то и больше, во время которых пользователю ничего не остается
кроме как терпеливо ждать. Сделали данный механизм видимо в связи с тем
фактом, что подавляющее большинство развернутых приложений были сделаны
просто чтобы побаловаться и были сразу же заброшены, что делает
неэффективным постоянное держание в готовом состоянии даже одного
процесса для каждого приложения. Таким образом использование GAE для
тяжелых веб-приложений с небольшой целевой аудиторией не очень
эффективно.&amp;nbsp; Минус второй: существуют некоторые жесткие ограничения,
которые не разрешают увеличивать даже за деньги (по крайней мере
расценок не видно). В их число входят максимальное время обработки
одного запроса (30 секунд, правда не ясно распространяется ли это на
выполнение задач в Task Queue и местном аналоге Cron'а), 30 активных
процессов, обрабатывающих запросы приложения (что влечет за собой
достаточно жесткое ограничение на количество запросов в секунду в районе
нескольких сотен), максимальный размер HTTP запроса/ответа в 10 мегабайт
и некоторые другие. В итоге "тяжелые" вычисления на GAE не погоняешь
(хотя есть варианты с применением AJAX и, соответственно, большого
количества запросов к GAE), от Digg-эффекта или DDOS'а есть шанс не
уберечься, хостинг файлов не соорудить, но... разве это ограничения?
Есть масса более интересных типов веб-приложений, способных прекрасно
существовать в такой среде. Да и в крайнем случае всегда можно связаться
с представителями Google с просьбой в виде исключение для Вашего
приложения, судя по их заявлениям все ограничения носят искусственный
характер и служат лишь для защиты от потребления неоправданно большого
количества вычислительных ресурсов плохо спроектированных приложениями.&lt;/p&gt;
&lt;p&gt;Кстати в американской части Интернета о GAE ходят в основном негативные
мнения, мол тормозит, большое время отклика, сплошные таймауты и ошибки.
На практике пока не удалось столкнуться с чем-то подобным, но реально
работающего приложения с активной пользовательской базой у меня пока нет
для того, чтобы делать какие-то относительно объективные выводы. Может
быть со временем что-нибудь изменится и более тонкие нюансы станут
выползать на поверхность - время покажет. Как раз будет повод написать
еще один пост на эту же тему :)&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 19 Oct 2009 23:53:00 +0400</pubDate><guid>tag:www.insight-it.ru,2009-10-19:python/2009/django-v-gostyakh-u-google/</guid><category>app engine patch</category><category>BigTable</category><category>django</category><category>gae</category><category>Google</category><category>google app engine</category><category>Python</category><category>Масштабируемость</category><category>платформа</category></item><item><title>РИТ: Высокие нагрузки</title><link>https://www.insight-it.ru//event/2008/rit-vysokie-nagruzki/</link><description>&lt;p&gt;Если кто не в курсе, 22 и 23 сентября в Москве проходила конференция для
разработчиков высоконагруженных систем. Не знаю могу ли я себя
полноценно отнести к этой категории людей, но тем не менее данной
мероприятие я сегодня посетил, пост будет опубликован скорее всего
несколько позже, но начинаю писать прямо с ходу в первый же день
конференции. Здесь будут лишь мои личные впечатления вперемешку с кратким пересказом происходившего. Что ж, приступим.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="predistoriia"&gt;Предистория&lt;/h3&gt;
&lt;p&gt;Попал я на данное мероприятие достаточно тривиально: благодаря акции для
студентов, предоставляющей бесплатное участие по результатам
online-тестирования (не знаю заплатил ли бы я недельную зарплату за
участие, если бы ее не было, хотя впринципе со скидками тоже не так уж и
много получилось бы), а также стратежно добавленной в RSS-агрегатор
нужной ленте новостей.&lt;/p&gt;
&lt;p&gt;Само же тестирование меня несколько позабавило. Не знаю кто его
составлял, но суть его была примерно в следующем: за полтора часа
предлагплось ответить на 10 вопросов, семь из которых формулировалось
примерно как "какой результат будет получен после выполнения программы
&amp;lt;код на перле&amp;gt;". Основной прикол был в том, что этот код написан на Perl
там сказано не было, если честно с первого вопроса я его даже не
признал, приняв за какой-то псевдо-код. На втором вопросе до меня
дошла-таки истина, благодаря не столь отдаленному во времени посещению
&lt;a href="https://www.insight-it.ru/event/2008/yapcrussia-2008-may-perl/"&gt;YAPC::Russia 2008 "May Perl"&lt;/a&gt; и, в
частности, проходившему там мастер-классу &lt;em&gt;Ивана Сережкина&lt;/em&gt;. Дальнейшее
тестированме проходило с открытым &lt;code&gt;vim&lt;/code&gt;'ом и &lt;code&gt;/bin/perl&lt;/code&gt; под рукой,
лишь один или два вопроса пришлось неторопливо погуглить...&lt;/p&gt;
&lt;p&gt;За пару дней до конференции на почту пришло-таки поздравление с успешным
прохождением тестирования и приглашением. В планы на эти два дня входило
как минимум посещение ВУЗа и работы, но особого труда их освободить не
составило. Так я и оказался сегодня утром в "Инфопространстве".&lt;/p&gt;
&lt;h3 id="den-pervyi"&gt;День первый&lt;/h3&gt;
&lt;p&gt;Добравшись-таки до точки назначения на 20 минут позже запланированного
открытия регистрация я как раз успел к ее началу. В нагрузку к бэйджику
девушки выдали пакетик с программкой и кучей макулатуры, а также адский
круглый девайс, на проверку оказавшийся шлепанцами.&lt;/p&gt;
&lt;p&gt;Оставшееся до открытия время пришлось коротать по большей части изучая
программку и здороваясь с тем очень скромным количеством лично знакомых
мне людей, которые тоже посетили данное мероприятие. На удивление с ходу
увидел много каким-то непонятным образом виртуально-знакомых лиц, с
которыми лично определенно никогда не общался. Конференция проходила в
два потока, так что в процессе ознакомления с программкой пришлось и
определяться с более полезными для себя докладами; на практике дело
оказалось непростым, так как даже прилагавшаяся книжечка с тезисами не
позволяла заранее оценить качество доклада. В итоге в этом нелегком деле
даже пришлось припомнить пару когда-то давно прочитанных статей по
эвристике - очень помогло.&lt;/p&gt;
&lt;p&gt;После непродолжительно-формального открытия начался первый доклад от
Анатолия Орлова из Яндекс. Доклад был, как ни странно, вводно-обзорным,
и посвящен он был по большей части взгляду на потенциальную нагрузку на
веб-сервера с точки зрения оборудования и операционной системы. Никаких
деталей, достаточно тривиально, но напомнить никогда не помешает, да и
со своей задачей - задать соответствующий настрой в аудитории он
определенно справился.&lt;/p&gt;
&lt;p&gt;После непродолжительного перерыва на кофе слово перешло еще одному
представителю компании Яндекс - Филиппу Дельгядо. На этот раз речь пошла
о веб-проектах средних размеров, которым необходимо справляться с
серьезной нагрузкой (наравне с прочими нефункциональными требованиями)
но с несколько другой точки зрения, программиста. Выступление также
носило обзорный характер и содержало в себе повествования о достаточно
большом количестве граблей, на которые могут натыкаться те или иные
проекты. На самом деле по затрагиваемым темам мне в свое время довелось
прочитать достаточно много материалов, пару раз споткнуться на практике
и даже написать пару постов, так что лично я рассматривал этот доклад
как закрепление знакомых тем. Разве что затронутый вопрос
горизонтального масштабирования хранения данных меня несколько смутил:
не знаю, может быть я несколько не в теме или еще что, но суть была
примерно такова, что под partitioning'ом рассматривались только
псевдо-универсальные решения, входящие в комплект некоторых СУБД, в то
время как sharding'ом было предложено понимать все остальные,
"самопальные", решения, специфичные для конкретного проекта. Я же
почему-то всегда рассматривал такой подход как нечто
концептуально-общее, так как конкретика реализации не важна, а важна
сама идея данного подхода, и считаю эти два слова достаточно близкими
синонимами (ровно как и многочисленные их русские аналоги), хотя может
быть я и не прав...&lt;/p&gt;
&lt;p&gt;Следующее выступление было, пожалуй, лучшим из увиденных мной в первый
день. В программке оно значилось как: "Как писать высокопроизводительные
веб-сервера", Антон Самохвалов (Яндекс). Были рассмотрен ряд возможных
подходов к написанию веб-серверов: начиная банальными неработоспособными
однопоточными вариантами и заканчивая противопоставлению вполне рабочих
вариантов: threads, &lt;abbr title="Finite State Machine"&gt;FSM&lt;/abbr&gt; и co-routine. Докладчик оказался убежденным сторонником подхода с использованием co-routine или на худой конец thread'ов, а также
использующего схожую идеологию веб-сервера от ASF. Презентация сыграла
лишь роль детонатора для последующей бурной дискуссии с участием многих
слушателей, а также ведущего этой секции - Игоря Сысоева, по
совместительству разработчика известного в "узких" кругах проекта nginx.
Ничего удивительного, что обсуждение плавно перетекло в Holy War:
Threads vs &lt;abbr title="Finite State Machine"&gt;FSM&lt;/abbr&gt;, но в отличии от знаменитых войн анонимусов на
linux.org.ru, шоу выдалось захватывающим и информационно-полезным.
Основными аргументами были даже не цифры о производительности,
полученные ребятами из Яндекса в полу-синтетических тестах (Антон
активно упоминал тот факт, что на примере базового поиска Яндекса
использованние thread'ы давало примерно 10% прирост производительности
по сравнению с &lt;abbr title="Finite State Machine"&gt;FSM&lt;/abbr&gt;), а скорее споры о maintenability кода и его
простоте, а также теоретических пределах использования этих подходов в
разных условиях (речь даже дошла до гипотетических ситуаций с
использованием более чем четырехядерных процессоров). Довелось выслушать
мнения профессионалов по обе стороны "баррикад" и самостоятельно сделать
все требуемые выводы, самый банальный и немаловажный из которых
заключается просто в том факте, что оба подхода имеют право на жизнь в
реальных проектах и какой из них использовать очень во многом зависит от
личных предпочтений разработчиков и специфики проекта.&lt;/p&gt;
&lt;p&gt;Далее по расписанию был запланировал обеденный перерыв, на котором я
решил заглянуть в аудиторию параллельного потока (где весь день
рассказывали о различных системах хранения данных). В это время там как
раз проходил доклад Павла Уварова из Рамблер о их собственной разработке
для хранения и передачи данных под названием HCS (читается почему-то как
"хикс"). Самое начало доклада я, к сожалению, пропустил, но почти с
первых же увиденных мной слайдов проект меня заинтересовал. Суть данной
технологии примерно характеризуется ее названием: Hierarchically
Compressed Stream. С одной стороны это формат хранения иерархически
структурированных данных, с другой же - данные в нем представляют
обычный файл, который легко обернуть в произвольный протокол передачи
потоковых данных (т.е. например просто раздавать через любой
HTTP-сервер). По представленным в презентации benchmark'ам запись и
линейный доступ к данным в этом формате происходит в разы, а то и
порядки быстрее по сравнению с РСУБД, а для ускорения случайного
доступа, на сколько я понял, имеется возможность построения индекса по
верхнему уровню иерархии. Помимо всего прочего, Павел обрадовал
аудиторию, что данная технология в ближайшем времени будет опубликована
в opensource, обязательно надо будет попробовать ее в деле как только
появится возможность.&lt;/p&gt;
&lt;p&gt;Вернувшись после обеда в аудиторию первого потока я сверился с
программкой и обнаружил, что сейчас будут рассказывать про архитектуру
сервиса SpyLog. Выступление Сергея Скворцова было очень общим и не
углублялось в какие-либо детали, если априори представлять себе как
работают сервисы веб-аналитики в целом, то становилось и вовсе скучно.
Единственной конкретикой было разве что упоминание о том, что для сбора
первичных данных используется nginx (модифицированный, если я правильно
понял)&lt;/p&gt;
&lt;p&gt;После этого доклада можно было стать свидетелем повествования работников
mail.ru сначала об их собственной разработке под названием Imagine
Framework, которая тоже вполне соответствует своему названию: ее можно
разве что представить - ни одного примера кода, закрыт для использования
только в рамках mail.ru, много слов ни о чем и общий стиль изложения в
духе "вот какую клевую штуку мы придумали, но никому не дадим и не
покажем". В общем я даже, если честно, не понял для какого он языка
программирования предназначен и чем он лучше (или хотябы просто
отличается) от opensource аналогов (упоминались собственные системы
кэширования, мониторинга и бинарный протокол). В итоге я до конца так и
не дослушал и ушел снова в соседний поток слушать про хранение
слабосвязанных данных.&lt;/p&gt;
&lt;p&gt;Илья Космодемьянцев из SUP Fabrik вел рассказ о том, как могут
развиваться события, если тот или иной проект сталкивается с хранением
данных, плохо вписывающихся в модель РСУБД. Разработчикам приходят мысли
о создании собственного специфического формата хранения данных, о том с
какими проблемами могут столкнуться в процессе. Возможные варианты
решения проблем также рассматривались, например в виде написания не
формата хранения данных, а собственного хранилища для, например, MySQL -
это может помочь избежать части проблем. Помимо этого выдвигалось очень
спорное предложение о создании некого "UseCase API", которое могло бы
позволить инкапсулировать для бэкэнда все особенности хранения данных в
случае их нетрадиционности. Хотя на практике подобные методики
применяются достаточно давно, особенно в объектно-ориентированных языках
программирования в виде data-access objects.&lt;/p&gt;
&lt;p&gt;После еще одного кофе-брейка оставалось лишь два заключительных доклада
первого дня, из которых я решил посетить нечто под названием "Масштабный
Jabber-кластер, IM и не только" от Андрея Федоровского из РБК. Начало
доклада было достаточно примитивным - что такое Jabber и с чем его едят,
всем это и без того было известно, так что было достаточно скучно. Даже
обзор доступных расширений не открыл ничего нового - все те же давно
знакомые вещи, разве что некий интерес вызвал процесс отправки бинарных
данных (т.е. произвольных файлов): было заявлено что отправка файла
временно обрывает поток XML-данных и отправляет вместо него бинарный
поток, но не заставил себя ждать очевидный вопрос: а как же тогда
происходит общение в этот момент. Вокруг этого образовалась небольшая
дискуссия, но к консенсусу прийти так и не удалось. Далее пошел рассказ
собственно о том как на сегодняшний день можно построить
Jabber-кластер - подробностей повествованию явно не хватало, но суть
была примерно такова: единственным более-менее сносным Jabber-сервером
является ejabberd, написанный на до сих пор экзотическом языке Erlang
(что впоследствии подтвердили сотрудники Яндекс: в своем проекте
Я.Онлайн они используют тоже его). По-умолчанию он использует нативное
для Erlang хранилище данных под названием Mnesia, но в том проекте РБК,
на основе которого и строился весь доклад (правда я так и не понял как
он называется), почему-то побоялись его использовать (видимо из-за
нежелания разбираться в том как оно функционирует) и использовали
имеющуюся возможность интеграции с более традиционной СУБД - PostgreSQL.
В дискуссии после доклада речь шла скорее о самом ejabberd, Erlang'е,
острой нехватке программистов на нем и тому подобных вещах, чем о самом
докладе.&lt;/p&gt;
&lt;p&gt;Подошедший к концу первый день конференции оставил массу положительных
впечатлений и тем для размышления. Практическая полезность конференции
не вызывает никаких сомнений, чего-то подобное невозможно увидеть и
услышать в интернете или где-либо еще. Были конечно же и негативные
моменты в виде абсолютно бестолковых докладов, но обычно это удавалось с
ходу обнаружить и вовремя ретироваться в соседний поток.&lt;/p&gt;
&lt;h3 id="den-vtoroi"&gt;День второй&lt;/h3&gt;
&lt;p&gt;Вот уже почти как неделю никак не могу найти времени дописать этот пост,
в связи с жесткой нехваткой свободного времени, благодаря работе и
учебе, так что позволю себе опубликовать пока в недописанном варианте.
Как дойдут руки - обязательно допишу, благо во второй день тоже было
несколько интересных выступлений, из которых особенно запомнилось
выступление Тимура Хайруллина из Яндекс о методиках нагрузочного
тестирования.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 02 Oct 2008 01:49:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-10-02:event/2008/rit-vysokie-nagruzki/</guid><category>highload</category><category>Life</category><category>ProfyClub</category><category>высокие нагрузки</category><category>конференция</category><category>Масштабируемость</category><category>РИТ</category></item><item><title>Архитектура LinkedIn</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-linkedin/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/801d7bb4/" rel="nofollow" target="_blank" title="https://www.linkedin.com"&gt;LinkedIn&lt;/a&gt; является крупнейшей в мире
социальной сетью для профессионалов. Популярность этого проекта может
быть далека, от более общетематических социальных сетей, таких как,
скажем Facebook, но, тем не менее, нагрузка на серверную часть проекта
создается пользователями серьезная. О том как этот проект с ней
справляется и пойдет речь далее.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="predislovie"&gt;Предисловие&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Сообщение о публикации двух презентаций c JavaOne 2008 о LinkedIn и их
&lt;a href="https://www.insight-it.ru/goto/36e64126/" rel="nofollow" target="_blank" title="http://hurvitz.org/blog/2008/06/linkedin-architecture"&gt;обобщении&lt;/a&gt; от
Overn Hurvitz пронеслось по русскоязычным новостным ресурсам уже
достаточно давно, но время черкнуть пару строк обо всем этом нашлось у
меня только сейчас.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/aef5ee94/" rel="nofollow" target="_blank" title="http://www.slideshare.net/linkedin/linkedins-communication-architecture"&gt;LinkedIn - A Professional Social Network Built with Java&amp;trade; Technologies and Agile Practices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/515b891c/" rel="nofollow" target="_blank" title="http://www.slideshare.net/linkedin/linked-in-javaone-2008-tech-session-comm"&gt;LinkedIn Communication Architecture&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="statistika"&gt;Статистика&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;22 миллиона пользователей;&lt;/li&gt;
&lt;li&gt;4+ миллиона уникальных посетителей в день;&lt;/li&gt;
&lt;li&gt;40 миллионов просмотров страниц в день;&lt;/li&gt;
&lt;li&gt;2 миллиона поисковых запросов в день;&lt;/li&gt;
&lt;li&gt;ежедневно отправляются 250 тысяч приглашений;&lt;/li&gt;
&lt;li&gt;1 миллион ответов в день;&lt;/li&gt;
&lt;li&gt;2 миллиона электронных сообщений ежедневно.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/solaris/"&gt;Solaris&lt;/a&gt; (как x86, так и SPARC)&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/tomcat/"&gt;Tomcat&lt;/a&gt; и &lt;a href="/tag/jetty/"&gt;Jetty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/oracle/"&gt;Oracle&lt;/a&gt; и &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Никакого ORM&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/activemq/"&gt;ActiveMQ&lt;/a&gt; для JMS&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/lucene/"&gt;Lucene&lt;/a&gt; в качестве основы для поиска&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/spring/"&gt;Spring&lt;/a&gt; в роли "клея"&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="servernaia-arkhitektura"&gt;Серверная архитектура&lt;/h3&gt;
&lt;h4&gt;2003-2005&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;одно монолитное веб-приложение;&lt;/li&gt;
&lt;li&gt;одна общая база данных;&lt;/li&gt;
&lt;li&gt;сетевой граф кэшируется в памяти в "Облаке";&lt;/li&gt;
&lt;li&gt;поиск пользователей реализован с помощью &lt;a href="/tag/lucene/"&gt;Lucene&lt;/a&gt;, он
    работал на той же машине, что и "Облако", так как поиск был
    отфильтрован в соответствии с сетью пользователя, таким образом было
    удобно совмещать эти две функции на одной машине;&lt;/li&gt;
&lt;li&gt;веб-приложение напрямую обновляет базу данных, а она, в свою
    очередь, обновляет "Облако".&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2006&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Добавлена репликация для уменьшения нагрузки на основную базу
    данных. Реплики предоставляют данные в режиме "только для чтения", а
    репликация ведется в асинхронном режиме с помощью дополнительного
    компонента под названием Databus, с его появлением обновление данных
    стало выглядеть следующим образом:&lt;ul&gt;
&lt;li&gt;сначала какие-либо изменения происходят в веб-приложении;&lt;/li&gt;
&lt;li&gt;веб-приложение обновляет основную базу данных;&lt;/li&gt;
&lt;li&gt;она, в свою очередь, отправляет обновления на Databus;&lt;/li&gt;
&lt;li&gt;далее уже Databus обновляет: реплики, Облако и поисковый индекс.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Поиск был вынесен на отдельный сервер.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2008&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;веб-приложение само по себе практически ничего не делает: бизнес
    логика распределена по отдельным сервисам;&lt;/li&gt;
&lt;li&gt;веб-приложение все так же предоставляет пользователям графический
    интерфейс, но для его генерации она теперь вызывает сервисы;&lt;/li&gt;
&lt;li&gt;каждый сервис имеет свою специфическую базу данных (т.е.
    вертикальное сегментирование);&lt;/li&gt;
&lt;li&gt;такой подход позволяет другим приложениям (помимо основного)
    получать доступ к LinkedIn, такие приложения были созданы для
    работодателей, рекламных служб, и так далее.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="oblako"&gt;Облако&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;"Облаком" в LinkedIn называют сервер, который кэширует весь граф
    социальной сети в памяти;&lt;/li&gt;
&lt;li&gt;его размеры: 22 миллиона вершин и 120 миллионов ребер;&lt;/li&gt;
&lt;li&gt;занимает 12GB оперативной памяти;&lt;/li&gt;
&lt;li&gt;одновременно держится в памяти в 40 экземплярах;&lt;/li&gt;
&lt;li&gt;построение Облака из данных, в дисковой системе, занимает 8 часов;&lt;/li&gt;
&lt;li&gt;обновления происходят в режиме реального времени с помощью Databus;&lt;/li&gt;
&lt;li&gt;во время остановки данные записываются на диск;&lt;/li&gt;
&lt;li&gt;кэш реализован с помощью C++, а доступ предоставляется по JNI;&lt;/li&gt;
&lt;li&gt;они выбрали именно C++ так как требовалось использовать минимум
    оперативной памяти, а также, задержки, связанные с Garbage
    Collection, были неприемлемыми.&lt;/li&gt;
&lt;li&gt;размещение всех данных в памяти является ограничением, но, как
    удалось выяснить в LinkedIn, разбиение графов на части - не самая
    тривиальная задача.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Облако кэширует целиком весь граф социальной сети LinkedIn, но на
практике же пользователям требуется видеть его со своей точки зрения.
Данная задача является вычислительно сложной, по-этому она выполняется
лишь один раз при создании новой сессии, а затем система поддерживает
результат в кэше. Такой подход требует 2 MB оперативной памяти на
каждого активного пользователя. В течении сессии такой кэш обновляется
только если сам пользователь сделал какие-либо изменения в нем, если же
изменение вызвано другими пользователями - владелец сессии не заметит
изменений.&lt;/p&gt;
&lt;p&gt;Помимо этого используется кэширование профилей пользователей средствами
&lt;a href="/tag/ehcache/"&gt;EHcache&lt;/a&gt;. Одновременно в памяти хранится до 2 миллионов
профилей (из 22 миллионов). Изначально планировалось использовать
алгоритм &lt;abbr title="Least Frequently Used"&gt;LFU&lt;/abbr&gt;, но оказалось,
что иногда &lt;a href="/tag/ehcache/"&gt;EHcache&lt;/a&gt; зависал секунд на 30 во время
перерасчета &lt;abbr title="Least Frequently Used"&gt;LFU&lt;/abbr&gt;, таким
образом было принято решение о использовании вместо него алгоритма
&lt;abbr title="Least Recently Used"&gt;LRU&lt;/abbr&gt;.&lt;/p&gt;
&lt;h3 id="arkhitektura-kommunikatsii"&gt;Архитектура коммуникации&lt;/h3&gt;
&lt;p&gt;Как известно, пользователи практически любой социальной сети генерируют
огромное количество сообщений в единицу времени, причем каждый тип
сообщений обычно требует индивидуального подхода, но в целом их можно
разделить на две категории: постоянные и временные. В LinkedIn
разработчики построили по отдельному сервису, для обработки каждой из
этих категорий. Каждый из них определенно заслуживает отдельного
внимания, так как общего в них мало.&lt;/p&gt;
&lt;h4&gt;Сервис постоянных сообщений&lt;/h4&gt;
&lt;p&gt;Этот коммуникационный сервис выполняет все операции, связанные с
постоянными сообщениями: приватными сообщениями и электронной почтой.
Перед ним ставится вполне тривиальный ряд задач: доставлять сообщения
получателям и сохранять их на постоянной основе, но на самом деле этим
все не ограничивается: должны также поддерживаться, скажем, доставка
сообщений с задержкой, массовые рассылки, отмена отправки сообщения,
возможность добавления в сообщения какого-либо интерактивного контента.
Реализован он был примерно следующим образом:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;вся система работает асинхронно и активно использует JMS;&lt;/li&gt;
&lt;li&gt;клиенты отправляют сообщения так же через JMS;&lt;/li&gt;
&lt;li&gt;далее сообщения перенаправляются с помощью сервиса маршрутизации в
    соответствующий почтовый ящик или напрямую в обработку электронной
    почты;&lt;/li&gt;
&lt;li&gt;доставка сообщений происходит как с помощью Pull (клиенты
    запрашивают свои сообщения), так и с использованием Push (т.е.
    отправки сообщений);&lt;/li&gt;
&lt;li&gt;помимо этого используется &lt;a href="/tag/spring/"&gt;Spring&lt;/a&gt; с их собственными
    закрытыми расширениями, использующими HTTP-RPC.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Приемы, способствующие масштабируемости&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Функциональное сегментирование:&lt;/strong&gt; отправленные, полученные,
    архивные сообщения. &lt;em&gt;(т.е. вертикальное сегментирование)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Классовое сегментирование:&lt;/strong&gt; пользовательские, гостевые,
    корпоративные почтовые ящики.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Сегментирование по диапазонам:&lt;/strong&gt; по идентификаторам пользователей
    или по лексикографическим диапазонам самих сообщений. &lt;em&gt;(т.е.
    горизонтальное сегментирование)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Асинхронное выполнение операций&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Сервис сетевых обновлений&lt;/h4&gt;
&lt;p&gt;Этот сервис обеспечивает работу любых временных уведомлений, например,
вызванных изменением статуса пользователей в контакт-листах. Такие
сообщения должны с течением времени удаляться из-за быстрой потери
актуальности, а также должна поддерживаться группировка и приоритезация
сообщений. Функционирование этого сервиса оказалось не настолько
очевидно, по сравнению с предыдущим, так что до итогового варианта было
перепробовано масса менее удачных решений, но обо всем по порядку.&lt;/p&gt;
&lt;h5&gt;Изначальная архитектура (до 2007 года)&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;используется много серверов, которые могут содержать обновления;&lt;/li&gt;
&lt;li&gt;клиенты отправляют запросы на каждый сервис отдельно: вопросы,
    обновления профилей и т.д.&lt;/li&gt;
&lt;li&gt;на сбор всех данных требовалось относительно много времени.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В 2008 году вся эта система поэтапно эволюционировала собственно в сам
сервис сетевых обновлений:&lt;/p&gt;
&lt;h5&gt;Первая итерация&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;клиент отправляет единственный запрос сервису сетевых обновлений;&lt;/li&gt;
&lt;li&gt;этот сервис в свою очередь параллельно отправляет всем остальным сервисам соответствующие запросы.&lt;/li&gt;
&lt;li&gt;результаты агрегируются и все вместе возвращаются клиенту;&lt;/li&gt;
&lt;li&gt;весь процесс основывается на Pull.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Вторая итерация&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;стал использоваться метод Push: каждый раз, когда происходит
    какое-либо событие, они помещаются в пользовательский "почтовый
    ящик", в момент запроса пользователя ему возвращается просто
    содержимое, уже ожидающее своего звездного часа в специально том
    самом "ящике";&lt;/li&gt;
&lt;li&gt;такой подход сильно ускоряет процесс чтения, так как на тот
    момент данные уже готовы;&lt;/li&gt;
&lt;li&gt;с другой стороны, какая-то часть данных может так никогда и не
    понадобиться, что приводит к бесполезным передвижениям данных и
    лишнему используемому дисковому пространству;&lt;/li&gt;
&lt;li&gt;небольшая часть обработки данных все же производится уже в
    момент запроса пользователя (например, объединение нескольких
    обновлений от определенного пользователя в одно);&lt;/li&gt;
&lt;li&gt;обновления хранятся в &lt;abbr title="Character Large OBject"&gt;CLOB&lt;/abbr&gt;'ах: по одному &lt;abbr title="Character Large OBject"&gt;CLOB&lt;/abbr&gt;'у на каждый тип
    обновления для каждого пользователя (то есть в сумму около 15 &lt;abbr title="Character Large OBject"&gt;CLOB&lt;/abbr&gt;'ов на каждого пользователя);&lt;/li&gt;
&lt;li&gt;сначала использовался размер &lt;abbr title="Character Large OBject"&gt;CLOB&lt;/abbr&gt;'ов равный 8 KB,
    что было явно больше требуемого и приводило к существенному
    количеству неиспользуемого дискового пространства.&lt;/li&gt;
&lt;li&gt;вместо &lt;abbr title="Character Large OBject"&gt;CLOB&lt;/abbr&gt;'ов можно
    было бы использовать дополнительные таблици по одной на каждый
    тип обновлений, но в этом случае пришлось бы постоянно удалять
    из них устаревшие записи, что было бы чрезвычайно неэффективно.&lt;/li&gt;
&lt;li&gt;в дополнение к этому использовался &lt;abbr title="Java Management eXtensions"&gt;JMX&lt;/abbr&gt; для
    мониторинга и изменения конфигурации в реальном времени, что
    оказалось очень удобным и полезным.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Третья итерация&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Цель: повысить производительность путем сокращения количества
    обновлений &lt;abbr title="Character Large OBject"&gt;CLOB&lt;/abbr&gt;'ов,
    так как они требуют много вычислительных ресурсов.&lt;/li&gt;
&lt;li&gt;Был добавлен буфер: колонки в таблицах типа &lt;code&gt;varchar(4000)&lt;/code&gt;, в
    которых данные помещались изначально. При полном заполнении
    ячейки данные перемещаются в &lt;abbr title="Character Large OBject"&gt;CLOB&lt;/abbr&gt;; это позволило
    на порядок сократить количество их обновлений.&lt;/li&gt;
&lt;li&gt;Уменьшен размер самих сообщений об обновлениях.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="i-naposledok-paru-sovetov-ot-linkedin"&gt;И напоследок пару советов от LinkedIn&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;нельзя бесконечно долго ограничиваться одной базой данных:
    используйте много баз данных как с вертикальным, так и с
    горизонтальным сегментированием данных;&lt;/li&gt;
&lt;li&gt;забудьте о ссылочной целостности и кросс-серверных JOIN'ах;&lt;/li&gt;
&lt;li&gt;забудьте о 100% целостности данных;&lt;/li&gt;
&lt;li&gt;при большом масштабе издержки могут стать проблемой: оборудование,
    базы данных, лицензии, системы хранения данных, электроэнергия и так
    далее;&lt;/li&gt;
&lt;li&gt;как только вы станете достаточно крупны и популярны, спаммеры и
    прочие злые люди не заставят себя долго ждать;&lt;/li&gt;
&lt;li&gt;не забывайте про кэширование!!!&lt;/li&gt;
&lt;li&gt;используйте асинхронные потоки данных;&lt;/li&gt;
&lt;li&gt;аналитика и построение отчетов может стать непростой задачей,
    постарайтесь задуматься о них заранее в процессе планирования
    системы;&lt;/li&gt;
&lt;li&gt;имейте всегда ввиду, что Ваша система может упасть в любой момент;&lt;/li&gt;
&lt;li&gt;не стоит недооценивать траекторию своего роста.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="p-s"&gt;P.S.&lt;/h3&gt;
&lt;p&gt;Когда уже закончил переводить в голову пришла мысль, что если читателям
будет интересно взглянуть на оригинальные презентации (хотябы ради
иллюстрационного материала, который там вполне нагляден), то было бы
проще сделать это прямо здесь, так что вот, для Вашего же удобства:&lt;/p&gt;
&lt;div class="video-container"&gt;&lt;iframe allowfullscreen="" frameborder="0" height="355" marginheight="0" marginwidth="0" scrolling="no" src="//www.slideshare.net/slideshow/embed_code/key/uHbsRNnQFZwThD" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" width="425"&gt; &lt;/iframe&gt;&lt;/div&gt;
&lt;div class="video-container"&gt;&lt;iframe allowfullscreen="" frameborder="0" height="355" marginheight="0" marginwidth="0" scrolling="no" src="//www.slideshare.net/slideshow/embed_code/key/16CML2N96CDeWv" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" width="425"&gt; &lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Кстати если Вы еще не успели подписаться на &lt;a href="/feed/"&gt;RSS&lt;/a&gt; - сейчас
самое время!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 11 Sep 2008 04:00:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-09-11:highload/2008/arkhitektura-linkedin/</guid><category>ActiveMQ</category><category>C++</category><category>EHcache</category><category>Java</category><category>Jetty</category><category>LinkedIn</category><category>Lucene</category><category>MySQL</category><category>Oracle</category><category>Solaris</category><category>Spring</category><category>Tomcat</category><category>архитектура</category><category>архитектура LinkedIn</category><category>Масштабируемость</category></item><item><title>Архитектура 37signals</title><link>https://www.insight-it.ru//highload/2008/arkhitektura-37signals/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/7457badf/" rel="nofollow" target="_blank" title="http://www.37signals.com"&gt;37signals&lt;/a&gt; больше всего известны благодаря
выпуску в свет &lt;a href="/tag/ruby-on-rails/"&gt;Ruby on Rails&lt;/a&gt; грамотному его
использованию для запуска их очень популярных продуктов: Basecamp,
Highrise, Backpack и Campfire. RoR как обычно пытаются винить во всех
проблемах с производительностью, но 37signals казалось бы справляется с
большой нагрузкой, используя вполне разумное количество вычислительных
ресурсов.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="istochniki-informatsii"&gt;Источники информации&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Этот текст является переводом
&lt;a href="https://www.insight-it.ru/goto/23721415/" rel="nofollow" target="_blank" title="http://highscalability.com/37signals-architecture"&gt;статьи&lt;/a&gt;, автор -
&lt;a href="https://www.insight-it.ru/goto/f3f1b405/" rel="nofollow" target="_blank" title="http://highscalability.com/user/todd-hoff"&gt;Todd Hoff&lt;/a&gt;. Извиняюсь за не
сильно оригинальный контент (да и, как выяснилось, практически не
технической направленности), но на написание своих полноценных текстов у
меня последнее время все никак не хватает то креатива, то времени :( .&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/d744e02/" rel="nofollow" target="_blank" title="http://www.37signals.com/svn/posts/749-ask-37signals-numbers"&gt;Спросите 37signals:
    цифры?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/aed84967/" rel="nofollow" target="_blank" title="http://www.37signals.com/svn/posts/753-ask-37signals-how-do-you-process-credit-cards"&gt;Спросите 37signals: как вы работаете с кредитными
    картами?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/df60cd7c/" rel="nofollow" target="_blank" title="http://www.37signals.com/svn/posts/759-behind-the-scenes-at-37signals-support"&gt;За сценой 37signals:
    поддержка.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/61db4355/" rel="nofollow" target="_blank" title="http://www.37signals.com/svn/posts/772-ask-37signals-why-did-you-restart-highrise"&gt;Спросите 37signals: почему вы перезапустили
    Highrise?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/ruby-on-rails/"&gt;Ruby on Rails&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/xen/"&gt;Xen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/amazon-s3/"&gt;Amazon S3&lt;/a&gt; для хранения изображений&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="statistika"&gt;Статистика&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;30 серверов: от простых однопроцессорных файловых серверов до
    восьмипроцессорных серверов приложений, в сумме около 100
    процессоров и 200 GB оперативной памяти.&lt;/li&gt;
&lt;li&gt;В планах диагональное масштабирование: уменьшение количества
    серверов до 16, но более производительных - в сумме 92 процессоров и
    230 GB RAM.&lt;/li&gt;
&lt;li&gt;Виртуализация средствами &lt;a href="/tag/xen/"&gt;Xen&lt;/a&gt; для более гибкого
    управления системой.&lt;/li&gt;
&lt;li&gt;Basecamp (управление проектами):&lt;ul&gt;
&lt;li&gt;2000000 пользователей с учетными записями&lt;/li&gt;
&lt;li&gt;13200000 задач&lt;/li&gt;
&lt;li&gt;9200000 сообщений&lt;/li&gt;
&lt;li&gt;12200000 комментариев&lt;/li&gt;
&lt;li&gt;5500000 записей о распределении времени&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Backpack (управление информацией для личного использования и малого
    бизнеса):&lt;ul&gt;
&lt;li&gt;Около миллиона страниц&lt;/li&gt;
&lt;li&gt;6800000 задач&lt;/li&gt;
&lt;li&gt;1500000 записей&lt;/li&gt;
&lt;li&gt;829000 фотографий&lt;/li&gt;
&lt;li&gt;370000 файлов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Общая статистика проектов (на ноябрь 2007 г.):&lt;ul&gt;
&lt;li&gt;5.9 TB загруженных пользователями данных&lt;/li&gt;
&lt;li&gt;888 GB загружено (upload)&lt;/li&gt;
&lt;li&gt;2 TB скачано (download)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="arkhitektura"&gt;Архитектура&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Кэширование средствами &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; уже используется,
    но они ищут способы более активно его применять. Позволяет достичь
    впечатляющих результатов в плане производительности.&lt;/li&gt;
&lt;li&gt;Методы для составления URL используются вместо ручного их
    составления.&lt;/li&gt;
&lt;li&gt;Используются стандартные ActiveRecord запросы, но при необходимости
    они используют и ручную настройку.&lt;/li&gt;
&lt;li&gt;Иногда они дорабатывают Rails, если сталкиваются с проблемами с
    производительностью.&lt;/li&gt;
&lt;li&gt;Amazon S3 используется для хранения данных, загруженных
    пользователями. Разработчики очень довольны результатами.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="rabota-s-kreditnymi-kartami"&gt;Работа с кредитными картами&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Ежемесячное выписывание счетов.&lt;/em&gt; Это позволяет компаниям,
    занимающимся кредитными картами, чувствовать себя более комфортно,
    так как им не придется столкнуться с огромным количеством работы,
    если ваша фирма вдруг выйдет из бизнеса. Пользователям такой подход
    также по душе, так как издержки в итоге оказываются относительно
    невелики, а также не требуется подписание контракта, имеется
    возможность пользоваться сервисами необходимый промежуток времени и
    оплачивать именно ту сумму, которую потратили.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Получение учетной записи продавца услуг.&lt;/em&gt; Кто-то определенно должен
    обрабатывать операции с кредитными картами. Они используют Chase
    Bank. Воспользуйтесь услугами кого-нибудь, кому вы доверяете, а
    когда масштаб ваших сделок станет существенным - будет возможность
    получить более выгодные условия контракта.&lt;/li&gt;
&lt;li&gt;Они используют authorize.net в роли шлюза для процессинга операций с
    кредитными картами.&lt;/li&gt;
&lt;li&gt;Ежемесячным выписыванием счетов занимается специально написанная для
    этого проекта система. Она запускается каждую ночь, отправляет счета
    нужным людям и записывает результаты выполненных операций.&lt;/li&gt;
&lt;li&gt;В случае успеха счет-фактура высылается по электронной почте.&lt;/li&gt;
&lt;li&gt;В случае каких-либо сбоев - пользователю отправляется объяснение
    причин.&lt;/li&gt;
&lt;li&gt;Если три раза с помощью кредитной карты не удается оплатить счет -
    учетная запись замораживается до тех пор, пока не будет предоставлен
    платежеспособный номер кредитной карты.&lt;/li&gt;
&lt;li&gt;Обработка ошибок является критичным моментом, так как проблемы с
    оплатой возникают достаточно часто.&lt;/li&gt;
&lt;li&gt;Все продукты конвертируются для использования централизованным
    биллинговым сервисом.&lt;/li&gt;
&lt;li&gt;Необходимо быть совместимыми с &lt;a href="https://www.insight-it.ru/goto/963f6a2d/" rel="nofollow" target="_blank" title="http://en.wikipedia.org/wiki/PCI_DSS"&gt;PCI DSS (Payment Card Industry Data
    Security Standard)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Пользуйтесь услугами шлюзов, это позволит не хранить номера
    кредитных карт на своем сайте, что сильно упрощает жизнь в плане
    безопасности. Некоторые из них предоставляют и биллинговые услуги,
    что позволяет также не заниматься этим самостоятельно.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="podderzhka-polzovatelei"&gt;Поддержка пользователей&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Campfire используется для работы с клиентами. По сути эта система
    представляет собой групповой веб-чат с расширенными возможностями в
    виде опциональной защиты паролем, личных сообщений, обмена файлами,
    предпросмотра изображений, а также коллективного принятия решений.&lt;/li&gt;
&lt;li&gt;Поднимаемые вопросы используются для поиска ошибок в коде, а
    обновление данных в SVN отображается прямо в процессе общения.
    Bugtracking система обходится стороной, что казалось бы должно лишь
    усложнять исправление неполадок, например из-за отсутствия
    возможности проследить связь между конкретным изменением в коде и
    вызвавшей его неполадкой.&lt;/li&gt;
&lt;li&gt;Зато служба поддержки может решать проблемы клиентов с помощью
    общения в чате в реальном времени, с возможностью обмена
    изображениями и снимками экранов, демонстрирующими неисправность.&lt;/li&gt;
&lt;li&gt;Разработчики также всегда доступны с помощью Campfire для помощи в
    решении проблем клиентов.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="podvodim-itogi"&gt;Подводим итоги&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Возьмите пример с &lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; и постройте все внутренние
    функции в виде сервисов с самого начала. Это позволит более легко
    использовать их в разных продуктах и прозрачно обновлять
    предоставляемые возможности.&lt;/li&gt;
&lt;li&gt;Не храните номера кредитных карт внутри своей базы данных - это
    существенно снижает риски, связанные с безопасностью.&lt;/li&gt;
&lt;li&gt;Разработчики и пользователи должны иметь возможность легко общаться
    друг с другом публично. Пользователи получают сервис намного более
    высокого качества, если общаются напрямую с разработчиками, которые
    решают все проблемы прямо в рамках нормального хода процесса
    разработки. Это позволяет избежать нескольких излишних промежуточных
    этапов при обработке заявок о неисправности. Помимо этого
    разработчикам предоставляется возможность узнать пользовательское
    мнение об их продукте, что делает дальнейшее развитие проекта более
    эффективным. Потенциальные клиенты могут убедиться в отзывчивости
    компании, просто посмотрев на такое общение, что подталкивает их к
    более охотной регистрации.&lt;/li&gt;
&lt;li&gt;Развивайте свой продукт добавлением новых функций, которые нужны
    конкретным клиентам, а не тех, которые когда-нибудь может быть
    кому-нибудь понадобятся.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 05 Jun 2008 18:49:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-06-05:highload/2008/arkhitektura-37signals/</guid><category>Amazon S3</category><category>Memcached</category><category>MySQL</category><category>RoR</category><category>Ruby on Rails</category><category>Xen</category><category>архитектура</category><category>архитектура 37signals</category><category>Масштабируемость</category></item><item><title>GlusterFS</title><link>https://www.insight-it.ru//storage/2008/glusterfs/</link><description>&lt;p&gt;&lt;img alt="GlusterFS Logo" class="right" src="https://www.insight-it.ru/images/glusterfs-logo.png" title="GlusterFS"/&gt;
&lt;a href="https://www.insight-it.ru/goto/12ccc1c7/" rel="nofollow" target="_blank" title="http://www.gluster.org/glusterfs.php"&gt;GlusterFS&lt;/a&gt; представляет собой
кластерную файловую систему, способную масштабироваться для хранения
далеко не одного петабайта данных. Как и многие другие кластерные
файловые системы, GlusterFS агрегирует дисковое пространство большого
количества машин в одну общую параллельную сетевую файловую систему
через Infiniband RDMA или TCP/IP соединение. Обычно в качестве
аппаратной основы для этой файловой системы используется ничем не
выдающееся недорогое серверное оборудование, в полной мере реализуя
принцип программного построения стабильности при использовании на
ненадежном оборудовании.
&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Кластерные файловые системы еще не достаточно приспособлены для
использования на крупных предприятиях: обычно процесс их развертывания и
поддержания в работающем состоянии не так уж прост. Но зато они отлично
масштабируются и достаточно дешевы, ведь для них достаточно самого
простого серверного оборудования и opensource операционных систем и
програмного обеспечения. Основной целью разработчиков
&lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; как раз и является построение кластерной
файловой системы, адаптированной для использования в рамках серьезных
компаний.&lt;/p&gt;
&lt;p&gt;Список основных ее особенностей по большей части мало чем отличается от
других кластерных файловых систем:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Состоит из клиентской и серверной частей. Клиентская часть позволяет
    монтировать файловую систему, а серверная - &lt;strong&gt;glusterfsd&lt;/strong&gt; -
    экспортировать в нее локальное дисковое пространство.&lt;/li&gt;
&lt;li&gt;Масштабируемость близка к &lt;em&gt;O(1)&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Широкий спектр возможностей за счет использования модульной
    архитектуры.&lt;/li&gt;
&lt;li&gt;Имеется возможность восстановления файлов и директорий из файловой
    системы даже без ее инициализации.&lt;/li&gt;
&lt;li&gt;Отсутствие централизованного сервера метаданных, что делает ее более
    устойчивой к потенциальным сбоям.&lt;/li&gt;
&lt;li&gt;Расширяемый интерфейс выполнения задач, с поддержкой загрузки
    модулей в зависимости от особенностей выполнения пользователями
    операций по работе с данными.&lt;/li&gt;
&lt;li&gt;Расширяющий функциональность механизм трансляторов.&lt;/li&gt;
&lt;li&gt;Поддержка &lt;em&gt;Infiniband RDMA&lt;/em&gt; и &lt;em&gt;TCP/IP&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Возможность автоматического восстановления в случае сбоев.&lt;/li&gt;
&lt;li&gt;Полностью реализована на уровне приложений, что упрощает ее
    поддержание в рабочем состоянии, портирование и дальнейшую
    разработку.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Но некоторые моменты все же заслуживают отдельного внимания:&lt;/p&gt;
&lt;h4&gt;Совместимость&lt;/h4&gt;
&lt;p&gt;Как уже упоминалось, файловая система реализована полностью на
уровне пользовательских приложений, что делает возможным ее
монтирование без каких-либо дополнительных патчей в ядре
операционной системы, единственное требование к нему: поддержка
FUSE. Серверная часть &lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; может
функционировать на любой POSIX-совместимой операционной системе и
протестирована на &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;, &lt;a href="/tag/freebsd/"&gt;FreeBSD&lt;/a&gt;,
&lt;a href="/tag/solaris/"&gt;OpenSolaris&lt;/a&gt;, в отличии от клиентской части, которая
может работать только в &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Модули&lt;/h4&gt;
&lt;p&gt;В виде модулей реализованы различные варианты выполнения
основополагающих операций: передачи данных и балансировки нагрузки в
рамках кластера. Транспортные модули обеспечивают передачу данных по
различным типам соединений:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TCP/IP&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Infiniband-verbs&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Infiniband-&lt;abbr title="Socket Direct Protocol"&gt;SDP&lt;/abbr&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Балансировка нагрузки может выполняться по следующим алгоритмам:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="Adaptive Least Usage"&gt;ALU&lt;/abbr&gt;&lt;/strong&gt; - использует
    целый ряд факторов, включающий объем свободного локального
    дискового пространства, активность операций чтения и записи,
    количество одновременно открытых файлов, скорость физического
    вращения дисков. Значимость, придаваемая каждому из показателей,
    может достаточно гибко настраиваться.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="Round Robin"&gt;RR&lt;/abbr&gt;&lt;/strong&gt; - по очереди размещает
    файлы последовательно на каждом узле, после чего начинает
    процесс заново, образуя своеобразный цикл. Этот метод эффективен
    если файлы имеют примерно одинаковый размер, а узлы кластера -
    одинаковый размер экспортированного локального дискового
    пространства.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Random&lt;/strong&gt; - распределяют файлы случайным образом.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="Non-Uniform File Access"&gt;NUFA&lt;/abbr&gt;&lt;/strong&gt; -
    приоритет отдается созданию файлов локально, а не на других
    узлах кластера.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Switch&lt;/strong&gt; - располагает файлы по определенным указанным
    особенностям имен файлов, по аналогии со &lt;strong&gt;switch(filename)&lt;/strong&gt; в
    программировании, обычно в качестве критерия распределения
    файлов имеет смысл использовать их расширение.&lt;/li&gt;
&lt;/ul&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Трансляторы&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Они представляют собой очень мощный механизм для расширения
возможностей GlusterFS, сама идея трансляторов бала позаимствована у
&lt;a href="https://www.insight-it.ru/goto/a8e9005c/" rel="nofollow" target="_blank" title="http://hurd.gnu.org"&gt;GNU/Hurd&lt;/a&gt; и заключается она в загрузке
бинарных библиотек (.so) в процессе работы системы в зависимости от
использованных настроек и использовании их в виде своеобразной
цепочки обработчиков при работе с файлами как на серверной, так и на
клиентской стороне. В &lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; практически все
дополнительные возможности реализованы именно виде трансляторов,
начиная от дополнений, увеличивающих производительность, заканчивая
средствами отладки. Вкратце перечислю основные из них:
+   &lt;strong&gt;&lt;abbr title="Automatic File Replication"&gt;AFR&lt;/abbr&gt;&lt;/strong&gt; - автоматическая репликация файлов.
+   &lt;strong&gt;Stripe&lt;/strong&gt; - разбивает файлы на блоки фиксированного размера.
+   &lt;strong&gt;Unify&lt;/strong&gt; - объединяет несколько узлов кластера в один большой
    виртуальный узел, один узел выделяется для обеспечения
    внутреннего namespace. Директории создаются на всех узлах,
    составляющих unify, а каждый файл - лишь на одном (если не
    используется &lt;abbr title="Automatic File Replication"&gt;AFR&lt;/abbr&gt;).
+   &lt;strong&gt;Trace&lt;/strong&gt; - предоставляют информацию для отладки в виде
    дополнительных записей в лог.
+   &lt;strong&gt;Filter&lt;/strong&gt; - фильтрация файлов на основании их имен и/или
    атрибутов.
+   &lt;strong&gt;Posix-locks&lt;/strong&gt; - обеспечивает POSIX блокировку записей
    независимую от используемой системы хранения.
+   &lt;strong&gt;Trash&lt;/strong&gt; - предоставляет функциональность сопоставимую с
    libtrash (или "корзиной" - если так понятнее).
+   &lt;strong&gt;Fixed-id&lt;/strong&gt; - обеспечивает доступ только для пользователей с
    определенными UID и GUID.
+   &lt;strong&gt;Posix&lt;/strong&gt; - соединяет GlusterFS с низлежащей локальной файловой
    системой.
+   &lt;strong&gt;rot-13&lt;/strong&gt; - транслятор обеспечивает возможность шифрования и
    дешифрования данных по примитивному одноименному алгоритму.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Список возможностей, обеспечиваемых широким набором модулей и
транслятором, впечатляет, большинство других opensource кластерных
файловых систем не могут похвастаться подобной функциональностью
(&lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; выпускается под GPL). Благодаря возможности
работы через Infiniband производительность передачи данных также
достаточно высока - она может достигать десятков гигабит в секунду.
Обработка сбоев в отдельных узлах также осуществляется достаточно
эффективно, так как может быть автоматизирована. Из потенциальных
недостатков можно назвать некоторое количество редко проявляющих себя
багов в коде, а также достаточно большой размер заголовков в
используемом протоколе (несколько сотен байт). В целом эта система
вполне работоспособна и полноценно выдерживает конкуренцию со стороны
своих opensource "коллег".&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 18 May 2008 21:16:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-05-18:storage/2008/glusterfs/</guid><category>GlusterFS</category><category>GPL</category><category>Infiniband</category><category>кластер</category><category>Масштабируемость</category><category>файловая система</category></item><item><title>Масштабируемые веб-архитектуры</title><link>https://www.insight-it.ru//theory/2008/masshtabiruemye-veb-arkhitektury/</link><description>&lt;p&gt;&lt;img alt="Масштабируемость" class="right" src="https://www.insight-it.ru/images/display.png"/&gt;
Уже немало слов было сказано по этой теме как в моем блоге, так и за
его пределами. Мне кажется настал подходящий момент для того, чтобы
перейти от частного к общему и попытаться взглянуть на данную тему
отдельно от какой-либо успешной ее реализации.&lt;/p&gt;
&lt;p&gt;Приступим?
&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Для начала имеет смысл определиться с тем, о чем мы вообще будем
говорить. В данном контексте перед веб-приложением ставятся три основные
цели:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;масштабируемость&lt;/strong&gt; - способность своевременно реагировать на
    непрерывный рост нагрузки и непредвиденные наплывы пользователей;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;доступность&lt;/strong&gt; - предоставление доступа к приложению даже в случае
    чрезвычайных обстоятельств;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;производительность&lt;/strong&gt; - даже малейшая задержка в загрузке страницы
    может оставить негативное впечатление у пользователя.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Основной темой разговора будет, как не трудно догадаться,
масштабируемость, но и остальные цели не думаю, что останутся в стороне.
Сразу хочется сказать пару слов про доступность, чтобы не возвращаться к
этому позднее, подразумевая как "само собой разумеется": любой сайт так
или иначе стремится к тому, чтобы функционировать максимально стабильно,
то есть быть доступным абсолютно всем своим потенциальным посетителям в
абсолютно каждый момент времени, но порой случаются всякие
непредвиденные ситуации, которые могут стать причиной временной
недоступности. Для минимизации потенциального ущерба доступности
приложения необходимо избегать наличия компонентов в системе,
потенциальный сбой в которых привел бы к недоступности какой-либо
функциональности или данных (или хотябы сайта в целом). Таким образом
каждый сервер или любой другой компонент системы должен иметь хотябы
одного дублера (не важно в каком режиме они будут работать: параллельно
или один "подстраховывает" другой, находясь при этом в пассивном
режиме), а данные должны быть реплицированы как минимум в двух
экземплярах (причем желательно не на уровне RAID, а на разных физических
машинах). Хранение нескольких резервных копий данных где-то отдельно от
основной системы (например на специальных сервисах или на отдельном
кластере) также поможет избежать многих проблем, если что-то пойдет не
так. Не стоит забывать и о финансовой стороне вопроса: подстраховка на
случай сбоев требует дополнительных существенных вложений в
оборудование, которые имеет смысл стараться минимизировать.&lt;/p&gt;
&lt;p&gt;Масштабируемость принято разделять на два направления:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Вертикальная масштабируемость&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Увеличение производительности каждого компонента системы c целью
повышения общей производительности.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Горизонтальная масштабируемость&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Разбиение системы на более мелкие структурные компоненты и
разнесение их по отдельным физическим машинам (или их группам) и/или
увеличение количества серверов параллельно выполняющих одну и ту же
функцию.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Так или иначе, при разработке стратегии роста системы приходится искать
компромис между ценой, временем разработки, итоговой производительность,
стабильностью и еще массой других критериев. С финансовой точки зрения
вертикальная масштабируемость является далеко не самым привлекательным
решением, ведь цены на сервера с большим количеством процессоров всегда
растут практически экспоненциально относительно количества процессоров.
Именно по-этому наиболее интересен горизонтальный подход, так как именно
он используется в большинстве случаев. Но и вертикальная
масштабируемость порой имеет право на существование, особенно в
ситуациях, когда основную роль играет время и скорость решения задачи, а
не финансовый вопрос: ведь купить БОЛЬШОЙ сервер существенно быстрее,
чем практически заново разрабатывать приложения, адаптируя его к работе
на большом количестве параллельно работающих серверов.&lt;/p&gt;
&lt;p&gt;Закончив с общими словами давайте перейдем к обзору потенциальных
проблем и вариантов их решений при горизонтальном масштабировании.
Просьба особо не критиковать - на абсолютную правильность и
достоверность не претендую, просто "мысли вслух", да и даже упомянуть
все моменты данной темы у меня определенно не получится.&lt;/p&gt;
&lt;h3 id="servery-prilozhenii"&gt;Серверы приложений&lt;/h3&gt;
&lt;p&gt;В процессе масштабирования самих приложений редко возникают проблемы,
если при разработке всегда иметь ввиду, что каждый экземпляр приложения
должен быть непосредственно никак не связан со своими "коллегами" и
должен иметь возможность обработать абсолютно любой запрос пользователя
вне зависимости от того где обрабатывались предыдущие запросы данного
пользователя и что конкретно он хочет от приложения в целом в текущий
момень.&lt;/p&gt;
&lt;p&gt;Далее, обеспечив независимость каждого отдельного запущенного
приложения, можно обрабатывать все большее и большее количество запросов
в единицу времени просто увеличивая количество параллельно
функционирующих серверов приложений, участвующих в системе. Все
достаточно просто (относительно).&lt;/p&gt;
&lt;h3 id="balansirovka-nagruzki"&gt;Балансировка нагрузки&lt;/h3&gt;
&lt;p&gt;Следущая задача - равномерно распределить запросы между доступными
серверами приложений. Существует масса подходов к решению этой задачи и
еще больше продуктов, предлагающих их конкретную реализацию.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Оборудование&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Сетевое оборудование, позволяющее распределять нагрузку между
несколькими серверами, обычно стоит достаточно внушительные суммы,
но среди прочих вариантов обычно именно этот подход предлагает
наивысшую производительность и стабильность (в основном благодаря
качеству, плюс такое оборудование иногда поставляется парами,
работающими по принципу
&lt;a href="https://www.insight-it.ru/goto/a40f2b94/" rel="nofollow" target="_blank" title="http://en.wikipedia.org/wiki/Heartbeat_%28program%29"&gt;HeartBeat&lt;/a&gt;).
В этой индустрии достаточно много серьезных брендов, предлагающих
свои решения - есть из чего выбрать: &lt;em&gt;Cisco&lt;/em&gt;, &lt;em&gt;Foundry&lt;/em&gt;, &lt;em&gt;NetScalar&lt;/em&gt;
и многие другие.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Программное обеспечение&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;В этой области еще большее разнообразие возможных вариантов.
Получить программно производительность сопоставимую с аппаратными
решениями не так-то просто, да и HeartBeat придется обеспечивать
программно, но зато оборудование для функционирования такого решения
представляет собой обычный сервер (возможно не один). Таких
программных продуктов достаточно много, обычно они представляют
собой просто HTTP-серверы, перенаправляющие запросы своим коллегам
на других серверах вместо отправки напрямую на обработку
интерпретатору языка программирования. Для примера можно упомянуть,
скажем, &lt;a href="/tag/nginx/"&gt;nginx&lt;/a&gt; с &lt;code&gt;mod_proxy&lt;/code&gt;. Помимо этого имеют
место более экзотические варианты, основанные на DNS, то есть в
процессе определения клиентом IP-адреса сервера с необходимым ему
интернет-ресурсов адрес выдается с учетом нагрузки на доступные
сервера, а также некоторых географических соображений.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Каждый вариант имеет свой ассортимент положительных и отрицательных
сторон, именно по-этому однозначного решения этой задачи не существует -
каждый вариант хорош в своей конкретной ситуации. Не стоит забывать, что
никто не ограничивает Вас в использовании лишь одного из них, при
необходимости может запросто быть реализована и практически произвольная
комбинация из них.&lt;/p&gt;
&lt;h3 id="resursoemkie-vychisleniia"&gt;Ресурсоемкие вычисления&lt;/h3&gt;
&lt;p&gt;Во многих приложениях используются какие-либо сложные механизмы, это
может быть конвертирование видео, изображений, звука, или просто
выполнение каких-либо ресурсоемких вычислений. Такие задачи требует
отдельного внимания если мы говорим о Сети, так как пользователь
интернет-ресурса врядли будет счастлив наблюдать за загружающейся
несколько минут страницей в ожидании лишь для того, чтобы увидеть
сообщение вроде: "Операция завершена успешно!".&lt;/p&gt;
&lt;p&gt;Для избежания подобных ситуаций стоит постараться минимизировать
выполнение ресурсоемких операций синхронно с генерацией интернет
страниц. Если какая-то конкретная операция не влияет на новую страницу,
отправляемую пользователю, то можно просто организовать &lt;em&gt;очередь&lt;/em&gt;
заданий, которые необходимо выполнить. В таком случае в момент когда
пользователь совершил все действия, необходимые для начала операции,
сервер приложений просто добавляет новое задание в очередь и сразу
начинает генерировать следущую страницу, не дожидаясь результатов. Если
задача на самом деле очень трудоемкая, то такая очередь и обработчики
заданий могут располагаться на отдельном сервере или кластере.&lt;/p&gt;
&lt;p&gt;Если результат выполнения операции задействован в следующей странице,
отправляемой пользователю, то при асинхронном ее выполнении придется
несколько схитрить и как-либо отвлечь пользователя на время ее
выполнения. Например, если речь идет о конвертировании видео в &lt;strong&gt;flv&lt;/strong&gt;,
то например можно быстро сгенерировать скриншот с первым кадром в
процессе составления страницы и подставить его на место видео, а
возможность просмотра динамически добавить на страницу уже после, когда
конвертирование будет завершено.&lt;/p&gt;
&lt;p&gt;Еще один неплохой метод обработки таких ситуаций заключается просто в
том, чтобы попросить пользователя "зайти попозже". Например, если сервис
генерирует скриншоты веб-сайтов из различных браузеров с целью
продемонстрировать правильность их отображения владельцам или просто
интересующимся, то генерация страницы с ними может занимать даже не
секунды, а минуты. Наиболее удобным для пользователя в такой ситуации
будет предложение посетить страницу по указанному адресу через
столько-то минут, а не ждать у моря погоды неопределенный срок.&lt;/p&gt;
&lt;h3 id="sessii"&gt;Сессии&lt;/h3&gt;
&lt;p&gt;Практически все веб-приложения каким-либо образом взаимодействуют со
своими посетителями и в подавляющем большинстве случаев в них
присутствует необходимость отслеживать перемещения пользователей по
страницам сайта. Для решения этой задачи обычно используется механизм
&lt;em&gt;сессий&lt;/em&gt;, который заключается в присвоении каждому посетителю
уникального идентификационного номера, который ему передается для
хранения в cookies или, в случае их отсутствия, для постоянного
"таскания" за собой через GET. Получив от пользователя некий ID вместе с
очередным HTTP-запросом сервер может посмотреть в список уже выданных
номеров и однозначно определить кто его отправил. С каждым ID может
ассоциироваться некий набор данных, который веб-приложение может
использовать по своему усмотрению, эти данные обычно по-умолчанию
хранятся в файле во временной директории на сервере.&lt;/p&gt;
&lt;p&gt;Казалось бы все просто, но... но запросы посетителей одного и того же
сайта могут обрабатывать сразу несколько серверов, как же тогда
определить не был ли выдан полученный ID на другом сервере и где вообще
хранятся его данные?&lt;/p&gt;
&lt;p&gt;Наиболее распространенными решениями является централизация или
децентрализация сессионных данных. Несколько абсурдная фраза, но,
надеюсь, пара примеров сможет прояснить ситуацию:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Централизованное хранение сессий&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Идея проста: создать для всех серверов общую "копилку", куда они
смогут складывать выданные ими сессии и узнавать о сессиях
посетителей других серверов. В роли такой "копилки" теоретически
может выступать и просто примонтированная по сети файловая система,
но по некоторым причинам более перспективным выглядит использование
какой-либо СУБД, так как это избавляет от массы проблем, связанных с
хранением сессионных данных в файлах. Но в варианте с общей базой
данных не стоит забывать, что нагрузка на него будет неуклонно расти
с ростом количества посетителей, а также стоит заранее предусмотреть
варианты выхода из проблематичных ситуаций, связанных с
потенциальными сбоями в работе сервера с этой СУБД.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Децентрализованное хранение сессий&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Наглядный пример - хранение сессий в &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;,
изначально расчитанная на распределенное хранение данных в
оперативной памяти система позволит получать всем серверам быстрый
доступ к любым сессионным данным, но при этом (в отличии от
предыдущего способа) какой-либо единый центр их хранения будет
отсутствовать. Это позволит избежать узких мест с точек зрения
производительности и стабильности в периоды повышенных нагрузок.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;В качестве альтернативы сессиям иногда используют похожие по
предназначению механизмы, построенные на cookies, то есть все
необходимые приложению данные о пользователе хранятся на клиентской
стороне (вероятно в зашифрованном виде) и запрашиваются по мере
необходимости. Но помимо очевидных преимуществ, связанных с отсутствием
необходимости хранить лишние данные на сервере, возникает ряд проблем с
безопасностью. Данные, хранимые на стороне клиента даже в зашифрованном
виде, представляют собой потенциальную угрозу для функционирования
многих приложений, так как любой желающий может попытаться
модифицировать их в своих интересах или с целью навредить приложению.
Такой подход хорош только если есть уверенность, что абсолютно любые
манипуляции с хранимые у пользователей данными безопасны. Но можно ли
быть уверенными на 100%?&lt;/p&gt;
&lt;h3 id="staticheskii-kontent"&gt;Статический контент&lt;/h3&gt;
&lt;p&gt;Пока объемы статических данных невелики - никто не мешает хранить их в
локальной файловой системе и предоставлять доступ к ним просто через
отдельный легковесный веб-сервер вроде &lt;a href="/tag/lighttpd/"&gt;lighttpd&lt;/a&gt; (я
подразумеваю в основном разные формы медиа-данных), но рано или поздно
лимит сервера по дисковому пространству или файловой системы по
количеству файлов в одной директории будет достигнут, и придется думать
о перераспределении контента. Временным решением может стать
распределение данных по их типу на разные сервера, или, возможно,
использование иерархической структуры каталогов.&lt;/p&gt;
&lt;p&gt;Если статический контент играет одну из основных ролей в работе
приложения, то стоит задуматься о применении распределенной файловой
системы для его хранения. Это, пожалуй, один из немногих способов
горизонтально масштабировать объем дискового пространства путем
добавления дополнительных серверов без каких-либо кардинальных изменений
в работе самого приложения. На какой именно кластерной файловой системе
остановить свой выбор ничего сейчас советовать не хочу, я уже
опубликовал далеко не один обзор конкретных реализаций - попробуйте
прочитать их все и сравнить, если этого мало - вся остальная Сеть в
Вашем распоряжении.&lt;/p&gt;
&lt;p&gt;Возможно такой вариант по каким-либо причинам будет нереализуем, тогда
придется "изобретать велосипед" для реализации на уровне приложения
принципов схожих с сегментированием данных в отношении СУБД, о которых я
еще упомяну далее. Этот вариант также вполне эффективен, но требует
модификации логики приложения, а значит и выполнение дополнительной
работы разработчиками.&lt;/p&gt;
&lt;p&gt;Альтернативой этим подходам выступает использование так называемых
&lt;strong&gt;Content Delievery Network&lt;/strong&gt; - внешних сервисов, обеспечивающих
доступность Вашего контента пользователям за определенное материальное
вознаграждение сервису. Преимущество очевидно - нет необходимости
организовывать собственную инфраструктуру для решения этой задачи, но
зато появляется другая дополнительная статья расходов. Список таких
сервисов приводить не буду, если кому-нибудь понадобится - найти будет
не трудно.&lt;/p&gt;
&lt;h3 id="keshirovanie"&gt;Кэширование&lt;/h3&gt;
&lt;p&gt;Кэширование имеет смысл проводить на всех этапах обработки данных, но в
разных типах приложений наиболее эффективными являются лишь некоторые
методы кэширования.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;СУБД&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Практически все современные СУБД предоставляют встроенные механизмы
для кэширования результатов определенных запросов. Этот метод
достаточно эффективен, если Ваша система регулярно делает одни и те
же выборки данных, но также имеет ряд недостатков, основными из
которых является инвалидация кэша всей таблицы при малейшем ее
изменении, а также локальное расположение кэша, что неэффективно при
наличии нескольких серверов в системе хранения данных.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Приложение&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;На уровне приложений обычно производится кэширование объектов любого
языка программирования. Этот метод позволяет вовсе избежать
существенной части запросов к СУБД, сильно снижая нагрузку на нее.
Как и сами приложения такой кэш должен быть независим от конкретного
запроса и сервера, на котором он выполняется, то есть быть доступным
всем серверам приложений одновременно, а еще лучше - быть
распределенным по нескольким машинам для более эффективной
утилизации оперативной памяти. Лидером в этом аспекте кэширования по
праву можно назвать &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;, о котором я в свое
время уже успел &lt;a href="https://www.insight-it.ru/storage/2008/obzor-memcached/"&gt;подробно рассказать&lt;/a&gt;.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;HTTP-сервер&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Многие веб-серверы имеют модули для кэширования как статического
контента, так и результатов работы скриптов. Если страница редко
обновляется, то использование этого метода позволяет без каких-либо
видимых для пользователя изменений избегать генерации страницы в
ответ на достаточно большую часть запросов.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Reverse proxy&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Поставив между пользователем и веб-сервером прозрачный
прокси-сервер, можно выдавать пользователю данные из кэша прокси
(который может быть как в оперативной памяти, так и дисковым), не
доводя запросы даже до HTTP-серверов. В большинстве случаев этот
подход актуален только для статического контента, в основном разных
форм медиа-данных: изображений, видео и тому подобного. Это
позволяет веб-серверам сосредоточиться только на работе с самими
страницами.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Кэширование по своей сути практически не требует дополнительных затрат
на оборудование, особенно если внимательно наблюдать за использованием
оперативной памяти остальными компонентами серверами и утилизировать все
доступные "излишки" под наиболее подходящие конкретному приложению формы
кэша.&lt;/p&gt;
&lt;p&gt;Инвалидация кэша в некоторых случаях может стать нетривиальной задачей,
но так или иначе универсального решения всех возможных проблем с ней
связанных написать не представляется возможным (по крайней мере лично
мне), так что оставим этот вопрос до лучших времен. В общем случае
решение этой задачи ложится на само веб-приложение, которое обычно
реализует некий механизм инвалидации средствами удаления объекта кэша
через определенный &lt;em&gt;период времени&lt;/em&gt; после его создания или последнего
использования, либо "вручную" при возникновении определенных &lt;em&gt;событий&lt;/em&gt;
со стороны пользователя или других компонентов системы.&lt;/p&gt;
&lt;h3 id="bazy-dannykh"&gt;Базы данных&lt;/h3&gt;
&lt;p&gt;На закуску я оставил самое интересное, ведь этот неотъемлемый компонент
любого веб-приложения вызывает больше проблем при росте нагрузок, чем
все остальные вместе взятые. Порой даже может показаться, что стоит
вообще отказаться от горизонтального масштабирования системы хранения
данных в пользу вертикального - просто купить тот самый БОЛЬШОЙ сервер
за шести- или семизначную сумму не-рублей и не забивать себе голову
лишними проблемами.&lt;/p&gt;
&lt;p&gt;Но для многих проектов такое кардинальное решение (и то, по большому
счету, временное) не подходит, а значит перед ними осталась лишь одна
дорога - горизонтальное масштабирование. О ней и поговорим.&lt;/p&gt;
&lt;p&gt;Путь практически любого веб проекта с точки зрения баз данных начинался
с одного простого сервера, на котором работал весь проект целиком. Затем
в один прекрасный момент наступает необходимость вынести СУБД на
отдельный сервер, но и он со временем начинает не справляться с
нагрузкой. Подробно останавливаться на этих двух этапах смысла особого
нет - все относительно тривиально.&lt;/p&gt;
&lt;p&gt;Следующим шагом обычно бывает &lt;strong&gt;master-slave&lt;/strong&gt; с асинхронной репликацией
данных, как работает эта схема уже неоднократно упоминалось в блоге, но,
пожалуй, повторюсь: при таком подходе все операции записи выполняются
лишь на одном сервере (master), а остальные сервера (slave) получают
данные напрямую от "мастера", обрабатывая при этом лишь запросы на
чтение данных. Как известно, операции чтения и записи любого веб-проекта
всегда растут пропорционально росту нагрузки, при этом сохраняется почти
фиксированным соотношение между обоими типами запросов: на каждый запрос
на обновление данных обычно приходится в среднем около десятка запросов
на чтение. Со временем нагрузка растет, а значит растет и количество
операций записи в единицу времени, а сервер-то обрабатывает их всего
один, а затем он же еще и обеспечивает создание некоторого количества
копий на других серверах. Рано или поздно издержки операций репликации
данных станут быть настолько высоки, что этот процесс станет занимать
очень большую часть процессорного времени каждого сервера, а каждый
slave сможет обрабатывать лишь сравнительно небольшое количество
операций чтения, и, как следствие, каждый дополнительный slave-сервер
начнет увеличивать суммарную производительность лишь незначительно, тоже
занимаясь по большей части лишь поддержанием своих данных в соответствии
с "мастером".&lt;/p&gt;
&lt;p&gt;Временным решением этой проблемы, возможно, может стать замена
master-сервера на более производительный, но так или иначе не выйдет
бесконечно откладывать переход на следующий "уровень" развития системы
хранения данных: &lt;strong&gt;"sharding"&lt;/strong&gt;, которому я совсем недавно посвятил
&lt;a href="https://www.insight-it.ru/theory/2008/segmentirovanie-bazy-dannykh/"&gt;отдельный пост "Сегментирование баз данных"&lt;/a&gt;.
Так что позволю себе остановиться на нем лишь вкратце: идея заключается
в том, чтобы разделить все данные на части по какому-либо признаку и
хранить каждую часть на отдельном сервере или кластере, такую часть
данных в совокупности с системой хранения данных, в которой она
находится, и называют сегментом или &lt;em&gt;shard&lt;/em&gt;&amp;rsquo;ом. Такой подход позволяет
избежать издержек, связанных с реплицированием данных (или сократить их
во много раз), а значит и &lt;em&gt;существенно&lt;/em&gt; увеличить общую
производительность системы хранения данных. Но, к сожалению, переход к
этой схеме организации данных требует массу издержек другого рода. Так
как готового решения для ее реализации не существует, приходится
модифицировать логику приложения или добавлять дополнительную
"прослойку" между приложением и СУБД, причем все это чаще всего
реализуется силами разработчиков проекта. Готовые продукты способны лишь
облегчить их работу, предоставив некий каркас для построения основной
архитектуры системы хранения данных и ее взаимодействия с остальными
компонентами приложения.&lt;/p&gt;
&lt;p&gt;На этом этапе цепочка обычно заканчивается, так как сегментированные
базы данных могут горизонтально масштабироваться для того, чтобы в
полной мере удовлетворить потребности даже самых высоконагруженных
интернет-ресурсов. К месту было бы сказать пару слов и о собственно
самой структуре данных в рамках баз данных и организации доступа к ним,
но какие-либо решения сильно зависят от конкретного приложения и
реализации, так что позволю себе лишь дать пару общих рекомендаций:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Денормализация&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Запросы, комбинирующие данные из нескольких таблиц, обычно при
прочих равных требуют большего процессорного времени для выполнения,
чем запрос, затрагивающий лишь одну таблицу. А производительность,
как уже упоминалось в начале повествования, чрезвычайно важна на
просторах Сети.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Логическое разбиение данных&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Если какая-то часть данных всегда используется отдельно от основной
массы, то иногда имеет смысл выделить ее в отдельную независимую
систему хранения данных.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Низкоуровневая оптимизация запросов&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Ведя и анализируя логи запросов, можно определить наиболее медленные
из них. Замена найденных запросов на более эффективные с той же
функциональностью может помочь более рационально использовать
вычислительные мощности.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;В этом разделе стоит упомянуть еще один, более специфический, тип
интернет-проектов. Такие проекты оперируют данными, не имеющими четко
формализованную структуру, в таких ситуациях использование реляционных
СУБД в качестве хранилища данных, мягко говоря, нецелесообразно. В этих
случаях обычно используют менее строгие базы данных, с более примитивной
функциональностью в плане обработки данных, но зато они способны
обрабатывать огромные объемы информации не придираясь к его качеству и
соответствию формату. В качестве основы для такого хранилища данных
может служить кластерная файловая система, а для анализа же данных в
таком случае используется механизм под названием
&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt;, принцип его работы я расскажу лишь вкратце,
так как в полном своем масштабе он несколько выходит за рамки данного
повествования.&lt;/p&gt;
&lt;p&gt;Итак, мы имеем на входе некие произвольные данные в не факт что
правильно соблюденном формате. В результате нужно получить некое
итоговое значение или информацию. Согласно данному механизму практически
любой анализ данных можно провести в следующие два этапа:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Map&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Основной целью данного этапа является представление произвольных
входных данных в виде промежуточных пар ключ-значение, имеющих
определенный смысл и формально оформленных. Результаты подвергаются
сортировке и группированию по ключу, а после чего передаются на
следующий этап.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Reduce&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Полученные после &lt;strong&gt;map&lt;/strong&gt; значения используются для финального
вычисления требуемых итоговых данных.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Каждый этап каждого конкретного вычисления реализуется в виде
независимого мини-приложения. Такой подход позволяет практически
неограниченно распараллеливать вычисления на огромном количестве машин,
что позволяет в мгновения обрабатывать объемы практически произвольных
данных. Для этого достаточно лишь запустить эти приложения на каждом
доступном сервере одновременно, а затем собрать воедино все результаты.&lt;/p&gt;
&lt;p&gt;Примером готового каркаса для реализации работы с данными по такому
принципу служит opensource проект Apache Foundation под названием
&lt;a href="https://www.insight-it.ru/storage/2008/hadoop/"&gt;&lt;em&gt;Hadoop&lt;/em&gt;&lt;/a&gt;, о котором я уже неоднократно
рассказывал ранее, да и &lt;a href="https://www.insight-it.ru/goto/1a5b89d0/" rel="nofollow" target="_blank" title="http://ru.wikipedia.org/wiki/Hadoop"&gt;статейку в Википедию&lt;/a&gt; написал в свое время.&lt;/p&gt;
&lt;h3 id="vmesto-zakliucheniia"&gt;Вместо заключения&lt;/h3&gt;
&lt;p&gt;Если честно, мне с трудом верится, что я смог написать настолько
всеобъемлющий пост и сил на подведение итогов уже практически не
осталось. Хочется лишь сказать, что в разработке крупных проектов важна
каждая деталь, а неучтенная мелочь может стать причиной провала. Именно
по-этому в этом деле учиться стоит не на своих ошибках, а на чужих.&lt;/p&gt;
&lt;p&gt;Хоть может быть этот текст и выглядит как некое обобщение всех постов из
серии &lt;a href="https://www.insight-it.ru/highload/"&gt;"Архитектуры высоконагруженных систем"&lt;/a&gt;, но врядли он
станет финальной точкой, надеюсь мне &lt;a href="/feed/"&gt;найдется что сказать&lt;/a&gt; по
этой теме и в будущем, может быть однажды это будет основано и на личном
опыте, а не просто будет результатом переработки массы полученной мной
информации. Кто знает?...&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 12 May 2008 09:00:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-05-12:theory/2008/masshtabiruemye-veb-arkhitektury/</guid><category>online</category><category>архитектура</category><category>веб-приложение</category><category>веб-проект</category><category>веб-сервер</category><category>геораспределение</category><category>информационные технологии</category><category>кэширование</category><category>Масштабируемость</category><category>распределенные вычисления</category><category>сегментирование баз данных</category></item><item><title>Сегментирование базы данных</title><link>https://www.insight-it.ru//theory/2008/segmentirovanie-bazy-dannykh/</link><description>&lt;p&gt;&lt;img alt="Сегментирование базы данных" class="left" src="https://www.insight-it.ru/images/partitions.png"/&gt;
В процессе чтения моего блога у вас наверняка возникал вопрос: а что же
имеется в виду под фразой &lt;em&gt;сегментирование базы данных&lt;/em&gt;?
На самом деле это просто приглянувшийся мне вариант перевода термина
&lt;em&gt;sharding&lt;/em&gt; (или он же - &lt;em&gt;partitioning&lt;/em&gt;), в качестве альтернатив можно
было бы использовать &lt;em&gt;партиционирование&lt;/em&gt;, &lt;em&gt;секционирование&lt;/em&gt; или
что-нибудь еще менее звучное, суть от этого не меняется.&lt;/p&gt;
&lt;p&gt;Сильно сомневаюсь, что предыдущий абзац предоставил Вам полную
информацию по данному вопросу, так что позволю себе перейти к более
детальным ответам...
&lt;!--more--&gt;&lt;/p&gt;
&lt;h3 id="chto-eto-takoe"&gt;Что это такое?&lt;/h3&gt;
&lt;p&gt;Особенно в условиях Сети данные накапливаются и запрашиваются с
невероятной скоростью, рано или поздно даже самый мощный сервер
перестанет справляться с задачей хранения и предоставления данных.
Количество запросов в секунду, которое способен обеспечивать один сервер
ограничено ничуть не меньше, чем его дисковое пространство. Когда
запросы данных начинают поступать слишком интенсивно, чаще всего
прибегают к наиболее тривиальному решению: реплицирование данных на
несколько серверов и обработка запросов на чтение данных параллельно, а
записи - лишь на одном, классическая схема master-slave. Но и у такого
подхода есть предел, с ростом системы затраты вычислительных мощностей
на реплицирование данных рано или поздно начнут потреблять большую часть
процессорного времени, что сделает прирост производительности от
простого добавления в систему дополнительных серверов минимальным.
Единственным выходом из такой ситуации становится пересмотр архитектуры
всей системы хранения данным, одним из возможных исходов которого и
сожет стать сегментирование базы данных.&lt;/p&gt;
&lt;p&gt;Сама идея сегментирования проста: разбить все данные на части по
какому-либо признаку и хранить каждую часть на отдельном сервере или
кластере, такую часть данных в совокупности с системой хранения данных,
в которой она находится, и называют сегментом или shard'ом.&lt;/p&gt;
&lt;p&gt;Признак по которому разделяются данные должен быть максимально прост и
очевиден, ведь в случае необходимости получения конкретных данных именно
он будет служить путеводителем для определения в каком именно сегменте
эти данные необходимо искать. В большинстве ситуаций такая необходимость
возникает очень часто, а значит и проверка признака должна выполняться
очень быстро. Сами эти признаки можно распределить на несколько групп по
принципу их проверки:&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Диапазон&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Самый простой и тривиальный вариант, выбирается один из параметров
какой-либо записи, например идентификационный номер, и проверяется
его принадлежность определенному диапазону, например все записи с ID
от 0 до 999 хранятся на одном сервере, от 1000 до 1999 - на другом,
и так далее.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Список&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Принцип остается такой же как и при использовании диапазонов, с той
лишь разницей что проверяется принадлежность параметра какому-либо
списку значений (который может состоять и из одного значения), а не
диапазону, например: людей можно разбить по районам проживания или
их имени.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Хэш-функция&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;О принципах работы хэширующих функций &lt;a href="https://www.insight-it.ru/security/2008/obratnogo-puti-net/"&gt;я уже рассказывал&lt;/a&gt;, так что
лишь вкратце опишу принцип такого подхода: для определения в каком
именно сегменте хранится та или иная запись, один из ее заранее
известных параметров передается хэширующей функции, возвращающей в
качестве результата номер сегмента от 0 до (n-1), где - n общее
количество сегментов.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Композиция&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;При дальнейшем росте объемов данных и нагрузке на систему можно
несколько усложнить ее работу, реализовав сегментирование на основе
композиции из нескольких упомянутых выше признаков.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;В условиях интернет-проектов данные обычно разбиваются по принадлежности
к пользователю, автором которых он является (или просто они как-либо
взаимосвязаны), с использованием хэш-функций.&lt;/p&gt;
&lt;h3 id="dlia-chego-eto-vse-nuzhno"&gt;Для чего это все нужно?&lt;/h3&gt;
&lt;p&gt;Возможно описание получилось несколько устрашающим, но на самом деле вся
эта история с распределением данных того стоит. Для начала стоит
отметить, что такой подход позволяет оперировать огромными объемами
данными с невероятной скоростью, благодаря параллельной обработке
запросов в абсолютно разных частях системы. Количество запросов,
обрабатываемых каждым узлом одновременно невелико, что не может не
сказаться на скорости обработки каждого запроса.&lt;/p&gt;
&lt;p&gt;Использование в качестве системы хранения данных для каждого сегмента
кластера, а не просто одного сервера, может существенно повысить
надежность системы в случае программных или аппаратных сбоев. Тем более
в случае использования master-slave репликации в рамках каждого
сегмента, у системы в целом все равно не будет единственного сервера для
обработке операций записи позволит минимизировать издержки
реплицирования данных.&lt;/p&gt;
&lt;h3 id="a-kak-zhe"&gt;А как же...&lt;/h3&gt;
&lt;h4&gt;...перераспределять данные?&lt;/h4&gt;
&lt;p&gt;При необходимости по тем или иным причинам изменить количество сегментов
возникновение ситуации, когда система нагружена неравномерно, очень
вероятно. Наиболее простым решением было бы простое перераспределение
записей, но как же его реализовать?&lt;/p&gt;
&lt;p&gt;Над этой проблемой стоит задуматься сразу же при переходе к
сегментированной архитектуре. Она может сначала показаться нерешаемой
без одновременного перемещения огромных массивов данных и временной
практически полной потери производительности, но это не так. Наиболее
элегантным решением является организация для системы некоторого сервиса
определения местоположения данных, в обязанности входит не только
определение номера сегмента по заранее определенному алгоритму, но и
постепенное перемещение данных в случае необходимости. Например, в
случае появления необходимости разбить сегмент на две части в связи с
приближающимся переполнением дискового простронства, данный сервис
начнет постепенно последовательно создавать копии записей на новом или
существующем слабо загруженном сегменте. Пока копирование каждой
конкретной записи не будет завершено операции чтения перенаправляются на
исходный сегмент. Как только процесс завершился - перенаправление данных
переключается на копию, а оригинал уничтожается, после чего система
переходит к перемещению следующей записи.&lt;/p&gt;
&lt;h4&gt;...выполнить операцию, затрагивающию разные сегменты?&lt;/h4&gt;
&lt;p&gt;Когда все данные хранились в одном месте - можно было бы выполнить один
сложноструктурированный запрос и получить все нужные данные, но при
таком распределении данных это не возможно. Для достижения такого же
результата необходимо выполнение нескольких запросов к различным
сегментам и аггрегация полученных данных на программном уровне.&lt;/p&gt;
&lt;h4&gt;...реализовать все это?&lt;/h4&gt;
&lt;p&gt;Один из самых интересных вопросов, которые можно было бы задать по
данной теме. Однозначно лучшего решения для реализации этого подхода не
существует. В большинстве случаев приходится реализовывать теорию на
практике своими силами, но готовые решения все же тоже существуют:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/2b245ae9/" rel="nofollow" target="_blank" title="http://www.enterprisedb.com/community/projects/gridsql.do"&gt;GridSQL&lt;/a&gt;
    от EnterpriseDB предоставляет систему сегментирования на базе
    &lt;a href="/tag/postgresql/"&gt;PostgreSQL&lt;/a&gt; (которую, кстати, не так давно под GPL
    опубликовали);&lt;/li&gt;
&lt;li&gt;Многие реляционные системы управления базами данных, такие как MySQL
    и Oracle, имеют собственную встроенную систему разбиения данных на
    партиции;&lt;/li&gt;
&lt;li&gt;В рамках проекта &lt;a href="https://www.insight-it.ru/goto/cacbd918/" rel="nofollow" target="_blank" title="http://www.hibernate.org"&gt;Hibernate&lt;/a&gt;
    разрабатывается библиотека, инкапсулирующая сегментирование данных;&lt;/li&gt;
&lt;li&gt;В &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-livejournal/"&gt;статье об архитектуре LiveJournal&lt;/a&gt; я рассказывал о спектре opensource-решений от разработчиков этого проекта для схожих задач.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Так или иначе ни один из них не является средством из серии "установил и
все сразу заработало", они лишь могут упростить реализацию такой
системы, существенную часть работы придется проделать самостоятельно.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ещё остались вопросы по теме?&lt;/em&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 01 May 2008 20:12:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-05-01:theory/2008/segmentirovanie-bazy-dannykh/</guid><category>partitioning</category><category>shard</category><category>sharding</category><category>архитектура</category><category>Масштабируемость</category><category>партиционирование</category><category>сегментирование</category><category>сегментирование баз данных</category><category>секционирование</category><category>СУБД</category></item></channel></rss>