<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Insight IT</title><link>https://www.insight-it.ru/</link><description></description><atom:link href="https://www.insight-it.ru/tag/failovaia-sistema/feed/index.xml" rel="self"></atom:link><lastBuildDate>Sun, 18 May 2008 21:16:00 +0400</lastBuildDate><item><title>GlusterFS</title><link>https://www.insight-it.ru//storage/2008/glusterfs/</link><description>&lt;p&gt;&lt;img alt="GlusterFS Logo" class="right" src="https://www.insight-it.ru/images/glusterfs-logo.png" title="GlusterFS"/&gt;
&lt;a href="https://www.insight-it.ru/goto/12ccc1c7/" rel="nofollow" target="_blank" title="http://www.gluster.org/glusterfs.php"&gt;GlusterFS&lt;/a&gt; представляет собой
кластерную файловую систему, способную масштабироваться для хранения
далеко не одного петабайта данных. Как и многие другие кластерные
файловые системы, GlusterFS агрегирует дисковое пространство большого
количества машин в одну общую параллельную сетевую файловую систему
через Infiniband RDMA или TCP/IP соединение. Обычно в качестве
аппаратной основы для этой файловой системы используется ничем не
выдающееся недорогое серверное оборудование, в полной мере реализуя
принцип программного построения стабильности при использовании на
ненадежном оборудовании.
&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Кластерные файловые системы еще не достаточно приспособлены для
использования на крупных предприятиях: обычно процесс их развертывания и
поддержания в работающем состоянии не так уж прост. Но зато они отлично
масштабируются и достаточно дешевы, ведь для них достаточно самого
простого серверного оборудования и opensource операционных систем и
програмного обеспечения. Основной целью разработчиков
&lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; как раз и является построение кластерной
файловой системы, адаптированной для использования в рамках серьезных
компаний.&lt;/p&gt;
&lt;p&gt;Список основных ее особенностей по большей части мало чем отличается от
других кластерных файловых систем:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Состоит из клиентской и серверной частей. Клиентская часть позволяет
    монтировать файловую систему, а серверная - &lt;strong&gt;glusterfsd&lt;/strong&gt; -
    экспортировать в нее локальное дисковое пространство.&lt;/li&gt;
&lt;li&gt;Масштабируемость близка к &lt;em&gt;O(1)&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Широкий спектр возможностей за счет использования модульной
    архитектуры.&lt;/li&gt;
&lt;li&gt;Имеется возможность восстановления файлов и директорий из файловой
    системы даже без ее инициализации.&lt;/li&gt;
&lt;li&gt;Отсутствие централизованного сервера метаданных, что делает ее более
    устойчивой к потенциальным сбоям.&lt;/li&gt;
&lt;li&gt;Расширяемый интерфейс выполнения задач, с поддержкой загрузки
    модулей в зависимости от особенностей выполнения пользователями
    операций по работе с данными.&lt;/li&gt;
&lt;li&gt;Расширяющий функциональность механизм трансляторов.&lt;/li&gt;
&lt;li&gt;Поддержка &lt;em&gt;Infiniband RDMA&lt;/em&gt; и &lt;em&gt;TCP/IP&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Возможность автоматического восстановления в случае сбоев.&lt;/li&gt;
&lt;li&gt;Полностью реализована на уровне приложений, что упрощает ее
    поддержание в рабочем состоянии, портирование и дальнейшую
    разработку.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Но некоторые моменты все же заслуживают отдельного внимания:&lt;/p&gt;
&lt;h4&gt;Совместимость&lt;/h4&gt;
&lt;p&gt;Как уже упоминалось, файловая система реализована полностью на
уровне пользовательских приложений, что делает возможным ее
монтирование без каких-либо дополнительных патчей в ядре
операционной системы, единственное требование к нему: поддержка
FUSE. Серверная часть &lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; может
функционировать на любой POSIX-совместимой операционной системе и
протестирована на &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;, &lt;a href="/tag/freebsd/"&gt;FreeBSD&lt;/a&gt;,
&lt;a href="/tag/solaris/"&gt;OpenSolaris&lt;/a&gt;, в отличии от клиентской части, которая
может работать только в &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Модули&lt;/h4&gt;
&lt;p&gt;В виде модулей реализованы различные варианты выполнения
основополагающих операций: передачи данных и балансировки нагрузки в
рамках кластера. Транспортные модули обеспечивают передачу данных по
различным типам соединений:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TCP/IP&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Infiniband-verbs&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Infiniband-&lt;abbr title="Socket Direct Protocol"&gt;SDP&lt;/abbr&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Балансировка нагрузки может выполняться по следующим алгоритмам:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="Adaptive Least Usage"&gt;ALU&lt;/abbr&gt;&lt;/strong&gt; - использует
    целый ряд факторов, включающий объем свободного локального
    дискового пространства, активность операций чтения и записи,
    количество одновременно открытых файлов, скорость физического
    вращения дисков. Значимость, придаваемая каждому из показателей,
    может достаточно гибко настраиваться.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="Round Robin"&gt;RR&lt;/abbr&gt;&lt;/strong&gt; - по очереди размещает
    файлы последовательно на каждом узле, после чего начинает
    процесс заново, образуя своеобразный цикл. Этот метод эффективен
    если файлы имеют примерно одинаковый размер, а узлы кластера -
    одинаковый размер экспортированного локального дискового
    пространства.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Random&lt;/strong&gt; - распределяют файлы случайным образом.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="Non-Uniform File Access"&gt;NUFA&lt;/abbr&gt;&lt;/strong&gt; -
    приоритет отдается созданию файлов локально, а не на других
    узлах кластера.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Switch&lt;/strong&gt; - располагает файлы по определенным указанным
    особенностям имен файлов, по аналогии со &lt;strong&gt;switch(filename)&lt;/strong&gt; в
    программировании, обычно в качестве критерия распределения
    файлов имеет смысл использовать их расширение.&lt;/li&gt;
&lt;/ul&gt;
&lt;dl&gt;
&lt;dt&gt;&lt;strong&gt;Трансляторы&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Они представляют собой очень мощный механизм для расширения
возможностей GlusterFS, сама идея трансляторов бала позаимствована у
&lt;a href="https://www.insight-it.ru/goto/a8e9005c/" rel="nofollow" target="_blank" title="http://hurd.gnu.org"&gt;GNU/Hurd&lt;/a&gt; и заключается она в загрузке
бинарных библиотек (.so) в процессе работы системы в зависимости от
использованных настроек и использовании их в виде своеобразной
цепочки обработчиков при работе с файлами как на серверной, так и на
клиентской стороне. В &lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; практически все
дополнительные возможности реализованы именно виде трансляторов,
начиная от дополнений, увеличивающих производительность, заканчивая
средствами отладки. Вкратце перечислю основные из них:
+   &lt;strong&gt;&lt;abbr title="Automatic File Replication"&gt;AFR&lt;/abbr&gt;&lt;/strong&gt; - автоматическая репликация файлов.
+   &lt;strong&gt;Stripe&lt;/strong&gt; - разбивает файлы на блоки фиксированного размера.
+   &lt;strong&gt;Unify&lt;/strong&gt; - объединяет несколько узлов кластера в один большой
    виртуальный узел, один узел выделяется для обеспечения
    внутреннего namespace. Директории создаются на всех узлах,
    составляющих unify, а каждый файл - лишь на одном (если не
    используется &lt;abbr title="Automatic File Replication"&gt;AFR&lt;/abbr&gt;).
+   &lt;strong&gt;Trace&lt;/strong&gt; - предоставляют информацию для отладки в виде
    дополнительных записей в лог.
+   &lt;strong&gt;Filter&lt;/strong&gt; - фильтрация файлов на основании их имен и/или
    атрибутов.
+   &lt;strong&gt;Posix-locks&lt;/strong&gt; - обеспечивает POSIX блокировку записей
    независимую от используемой системы хранения.
+   &lt;strong&gt;Trash&lt;/strong&gt; - предоставляет функциональность сопоставимую с
    libtrash (или "корзиной" - если так понятнее).
+   &lt;strong&gt;Fixed-id&lt;/strong&gt; - обеспечивает доступ только для пользователей с
    определенными UID и GUID.
+   &lt;strong&gt;Posix&lt;/strong&gt; - соединяет GlusterFS с низлежащей локальной файловой
    системой.
+   &lt;strong&gt;rot-13&lt;/strong&gt; - транслятор обеспечивает возможность шифрования и
    дешифрования данных по примитивному одноименному алгоритму.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;Список возможностей, обеспечиваемых широким набором модулей и
транслятором, впечатляет, большинство других opensource кластерных
файловых систем не могут похвастаться подобной функциональностью
(&lt;a href="/tag/glusterfs/"&gt;GlusterFS&lt;/a&gt; выпускается под GPL). Благодаря возможности
работы через Infiniband производительность передачи данных также
достаточно высока - она может достигать десятков гигабит в секунду.
Обработка сбоев в отдельных узлах также осуществляется достаточно
эффективно, так как может быть автоматизирована. Из потенциальных
недостатков можно назвать некоторое количество редко проявляющих себя
багов в коде, а также достаточно большой размер заголовков в
используемом протоколе (несколько сотен байт). В целом эта система
вполне работоспособна и полноценно выдерживает конкуренцию со стороны
своих opensource "коллег".&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 18 May 2008 21:16:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-05-18:storage/2008/glusterfs/</guid><category>GlusterFS</category><category>GPL</category><category>Infiniband</category><category>кластер</category><category>Масштабируемость</category><category>файловая система</category></item><item><title>Файлы в космосе</title><link>https://www.insight-it.ru//storage/2008/fajjly-v-kosmose/</link><description>&lt;h4&gt;...или Kosmos Distributed File System&lt;/h4&gt;
&lt;p&gt;&lt;img alt="Kosmos Distributed File System" class="right" src="https://www.insight-it.ru/images/KFS.jpg" title="KosmosFS Logo"/&gt;
Сегодня речь пойдет об еще одной распределенной файловой системе -
&lt;a href="https://www.insight-it.ru/goto/a11ac210/" rel="nofollow" target="_blank" title="http://kosmosfs.sourceforge.net/"&gt;KosmosFS&lt;/a&gt;. У русских людей название
этого проекта определенно вызывает ассоциации с космосом, но изначально
все же свою лепту в него внес изначальный разработчик -
&lt;a href="https://www.insight-it.ru/goto/636f244d/" rel="nofollow" target="_blank" title="http://www.kosmix.com/"&gt;Kosmix&lt;/a&gt;.
&lt;!--more--&gt;
По большому счету &lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; мало чем выделяется из множества
своих конкурентов, по своей структуре она состоит из сервера метаданных
и серверов блоков, доступ к системе производится средствами клиентской
библиотеки, предоставляющей соответствующий API. Список возможностей
файловой системы также вполне стандартен:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Инкрементальная масштабируемость.&lt;/em&gt; При добавлении дополнительных
    узлов в кластер, система сама адаптируется для вовлечения их в
    полноценную работу.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Стабильный доступ.&lt;/em&gt; Реплицируемость данных (по-умолчанию в трех
    экземплярах) позволяет гарантировать доступность данных вне
    зависимости от сбоев в работе отдельных узлов.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Балансировка блоков данных.&lt;/em&gt; Периодически сервер метаданных
    перераспределяет данные с целью более оптимального использования
    дискового пространства.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Целостность данных.&lt;/em&gt; Для обеспечения целостности данных вычисляются
    и сравниваются контрольные суммы блоков данных.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Кэширование.&lt;/em&gt; Для увеличения производительности используется
    кэширования на уровне клиентской библиотеки.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Прозрачная работа с недоступными узлами.&lt;/em&gt; Клиентская библиотека
    прозрачно для приложения переключается на альтернативный сервер с
    данными, если обнаруживает что один из них недоступен.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Поддержка языков программирования:&lt;/em&gt; &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;,
    &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Скрипты.&lt;/em&gt; С системой предоставляется набор скриптов для
    развертывания, запуска и остановки узлов.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Но написать этот пост меня подтолкнул вовсе не этот список. В
комментариях к &lt;a href="https://www.insight-it.ru/storage/2008/hadoop/"&gt;одной из предыдущих моих записей&lt;/a&gt;
читатели подняли тему о целесообразности использования &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;
для реализации &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt; в частности и &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; в
целом. В качестве альтернативы был предложен &lt;a href="/tag/c/"&gt;C++&lt;/a&gt; (только на
словах конечно же), аргументируя это тем, что такая реализация была бы
эффективнее. &lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; же как раз и является той самой
альтернативой &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt;, написанной на &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;.
&lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; тесно интегрируется с &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; с помощью
его интерфейсов для файловой системы. Это позволяет Hadoop-приложениям
незаметно работать с &lt;a href="/tag/kfs/"&gt;KFS&lt;/a&gt; точно так же, как если бы на ее
месте была бы &lt;a href="/tag/hdfs/"&gt;HDFS&lt;/a&gt;. Код для интеграции с Hadoop был выпущен
в виде патча к Hadoop-JIRA-1963, а начиная с Hadoop версии 0.15 этот код
входит в стандартный дистрибутив, ровно как и детальная инструкция по
интеграции.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 30 Mar 2008 23:06:00 +0400</pubDate><guid>tag:www.insight-it.ru,2008-03-30:storage/2008/fajjly-v-kosmose/</guid><category>C++</category><category>Hadoop</category><category>KFS</category><category>Kosmos Distributed File System</category><category>кластер</category><category>файловая система</category></item><item><title>Lustre</title><link>https://www.insight-it.ru//storage/2008/lustre/</link><description>&lt;p&gt;&lt;img alt="Lustre Logo" class="right" src="https://www.insight-it.ru/images/lustre-logo.png" title="Lustre"/&gt;
&lt;a href="https://www.insight-it.ru/goto/4df3f908/" rel="nofollow" target="_blank" title="http://www.lustre.org"&gt;Lustre&lt;/a&gt; представляет собой кластерную файловую
систему, основными особенностями которой являются превосходные
надежность и масштабируемость. Производительность также более чем
высока - скорость передачи данных может достигать сотен гигабит в
секунду, а теоретический максимум доступного дискового пространства
измеряется петабайтами. Эта файловая система может использоваться как на
скромных рабочих группах из нескольких компьютеров, так и на огромных
кластерах, насчитывающих десятки тысяч машин.&lt;/p&gt;
&lt;p&gt;Помимо этого поддерживаются все возможности, который должна иметь любая
уважающая себя кластерная файловая система:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;поддержка широкого ассортимента типов высокоскоростных сетевых
    соединений;&lt;/li&gt;
&lt;li&gt;надежная система "замков" для обеспечения параллельного доступа к
    файлам;&lt;/li&gt;
&lt;li&gt;возможность автоматического самовосстановления в случае падения
    любого из узлов;&lt;/li&gt;
&lt;li&gt;распределенное управление файловыми объектами для предоставления
    масштабируемого доступа к файлам.&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;p&gt;Изначально архитектура этой файловой системы была разработана просто в
рамках исследовательского проекта Петера Браама в 1999, но он решил не
останавливаться на достигнутом и основал &lt;em&gt;Cluster File Systems, Inc.&lt;/em&gt;, в
которой уже и велась основная разработка самой файловой системы. Первый
релиз Lustre 1.0 был выпущен в 2003 году. Спустя четыре года компания
была приобретена Sun Microsystems в октябре 2007 года, но это лишь
способствовало дальнейшему развитию проекта. Программное обеспечение,
входящее в состав проекта, выпускается под лицензией GPL, что также
сыграло немаловажную роль в его жизни.&lt;/p&gt;
&lt;h3 id="arkhitektura"&gt;Архитектура&lt;/h3&gt;
&lt;p&gt;Каждый компьютер, входящий состав кластера Lustre, выполняет свою четко
определенную функцию:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="MetaData Server"&gt;MDS&lt;/abbr&gt;.&lt;/strong&gt; Сервер метаданных
    предназначен для хранения всей служебной информации о системе:
    названия файлов, директорий, прав доступа и так далее. Достаточно
    наличие одного такого сервера в системе, но для обеспечения
    надежности на случай каких-либо сбоев, обычно его дублируют.
    Возможно использование внешнего хранилища данных
    (&lt;abbr title="MetaData Target"&gt;MDT&lt;/abbr&gt;), которое может быть общим
    для двух дублирующих друг друга &lt;abbr title="MetaData Server"&gt;MDS&lt;/abbr&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;abbr title="Object Storage Server"&gt;OSS&lt;/abbr&gt;&lt;/strong&gt; Компьютеры для
    хранения самих данных. Каждый из них работает с 2-8 &lt;abbr title="Object Storage Target"&gt;OST&lt;/abbr&gt;, в их роли могут
    выступать практически любые средства хранения данных, начиная от
    просто жестких дисков или RAID массивов внутри &lt;abbr title="Object Storage Server"&gt;OSS&lt;/abbr&gt;, заканчивая
    внешними системами хранения данных enterprise-класса. Сумма
    дискового пространства всех &lt;abbr title="Object Storage Target"&gt;OST&lt;/abbr&gt; и является размером доступного
    дискового пространства всей файловой системы Lustre.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Клиент.&lt;/strong&gt; Компьютеры, непосредственно использующие файловую
    систему. Им предоставляется полный параллельный доступ, полностью
    соответствующий стандарту POSIX.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Один и тот же компьютер теоретически может совмещать в себе несколько
функций, но в большинстве случаев это нецелесообразно (за исключением
совмещения клиентов с &lt;abbr title="Object Storage Target"&gt;OST&lt;/abbr&gt; и, возможно, случаев, когда количество узлов
кластера очень мало).&lt;/p&gt;
&lt;p&gt;Возможно более наглядно вышенаписанное сможет представить схема
архитектуры системы
(&lt;a href="https://www.insight-it.ru/goto/dd98ad05/" rel="nofollow" target="_blank" title="http://manual.lustre.org/manual/LustreManual16_HTML/figures/ClusterWithLustre-4.gif"&gt;позаимствована&lt;/a&gt; с официального сайта и переведена):
&lt;img alt="Схема архитектуры файловой системы Lustre" class="responsive-img" src="https://www.insight-it.ru/images/lustre-architecture.png" title="Схема архитектуры"/&gt;&lt;/p&gt;
&lt;p&gt;Помимо этого для функционирования системы необходим еще один компонент,
по большому счету не являющийся ее частью -
&lt;abbr title="ManaGement Server"&gt;MGS&lt;/abbr&gt;. Его роль заключается в
предоставлении конфигурационной информации всем компонентам одной или
нескольким файловым системам Lustre. Он также нуждается в отдельном
хранилище данных, но чисто теоретически он может быть и совмещен с одним
из компонентов файловой системы.&lt;/p&gt;
&lt;h3 id="funktsionirovanie"&gt;Функционирование&lt;/h3&gt;
&lt;p&gt;Основным толчком для выполнения каких-либо действий в рамках всей
файловой системы обычно является запрос с одного из клиентов.
Программное обеспечение для клиентов представляет по сути интерфейс
между виртуальной файловой системой &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; и серверами
Lustre. Каждому типу серверов соответствует своя часть клиентского ПО:
&lt;abbr title="MetaData Client"&gt;MDC&lt;/abbr&gt;, &lt;abbr title="Object Storage Client"&gt;OSC&lt;/abbr&gt;, &lt;abbr title="ManaGement Client"&gt;MGC&lt;/abbr&gt;. В отличии от &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; и &lt;a href="/tag/gfs/"&gt;GFS&lt;/a&gt; файловая система Lustre должна быть примонтирована к локальной системе клиентов для полноценного их функционирования.&lt;/p&gt;
&lt;p&gt;Для осуществления коммуникации между клиентами и серверами используется
собственный API, известный как &lt;abbr title="Lustre NETworking"&gt;LNET&lt;/abbr&gt;. Он поддерживает множество
сетевых протоколов с помощью &lt;abbr title="Network Abstraction Layer"&gt;NAL&lt;/abbr&gt;.&lt;/p&gt;
&lt;p&gt;В системе отсутствуют незаменимые компоненты, это является залогом
отказоустойчивости системы. В случае возникновения каки-либо неполадок
или сбоев в работе оборудования, работу потерявших работоспособность
компонентов системы перехватят другие ее компоненты, что сделает сбой
незаметным для пользователей системы. Это достигается за счет
дублирование серверов, выполняющих одинаковые функции, а также наличие
налаженных алгоритмов действий, направленных на автоматическое
восстановление полноценного функционирования системы в случае
возникновения чрезвычайных ситуаций. Но этого конечно же не достаточно
для абсолютной надежности системы, в дополнение должна быть
предоставлена как минимум система бесперебойного питания для всех
компонентов кластера на случай проблем с электроэнергией в датацентре
(для России более чем актуально).&lt;/p&gt;
&lt;p&gt;В списке дополнительных возможностей, предоставляемых файловой системой,
можно назвать возможность выделения квот на дисковое пространство для
каждого пользователя системы, аутентификацию пользователей с помощью
механизма Kerberos, повышение физической пропускной способности сетевого
соединения путем аггрегирования физических сетевых соединений в одно
логическое виртуально сетевое соединение (достаточно интересная
возможность, способная при выполнении определенных условий существенно
повлиять на быстродействие системы). Помимо этого предоставляется целый
ряд возможностей по созданию резервных копий данных на уровне файловой
системы в целом, отдельных устройств или же файлов.&lt;/p&gt;
&lt;h3 id="zakliuchenie"&gt;Заключение&lt;/h3&gt;
&lt;p&gt;Эта файловая система нашла свое применение во множестве крупнейших
кластеров и суперкомпьютеров по всему миру, но это не мешает ей с тем же
успехом демонстрировать и на кластерах существенно меньшего масштаба.
Около половины из самых производительных суперкомпьютеров во всем мире
используют Lustre в качестве файловой системы. Помимо этого многие
компании предоставляют ее в качестве основы для Linux кластеров
(например HP StorageWorks SFS, Cray XT3, Cray XD1). Чем не показатель ее
конкурентоспособности?&lt;/p&gt;
&lt;p&gt;В качестве источников информации были использованы &lt;a href="https://www.insight-it.ru/goto/4df3f908/" rel="nofollow" target="_blank" title="http://www.lustre.org"&gt;официальный сайт проекта&lt;/a&gt; и иногда &lt;a href="https://www.insight-it.ru/goto/9328120f/" rel="nofollow" target="_blank" title="http://en.wikipedia.org/wiki/Lustre_%28file_system%29"&gt;страница английской wikipedia.org&lt;/a&gt;.
На все том же официальном сайте всегда можно найти всю необходимую
документацию, а само программное обеспечение проекта доступно на
&lt;a href="https://www.insight-it.ru/goto/7a51c11f/" rel="nofollow" target="_blank" title="http://www.sun.com/software/products/lustre/get.jsp"&gt;соответствующей странице&lt;/a&gt; сайта Sun Mictosystems.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 21 Mar 2008 21:53:00 +0300</pubDate><guid>tag:www.insight-it.ru,2008-03-21:storage/2008/lustre/</guid><category>Cluster File Systems</category><category>Linux</category><category>Lustre</category><category>Sun</category><category>кластер</category><category>Масштабируемость</category><category>файловая система</category></item></channel></rss>