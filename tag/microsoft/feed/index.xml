<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Insight IT</title><link>https://www.insight-it.ru/</link><description></description><atom:link href="https://www.insight-it.ru/tag/microsoft/feed/index.xml" rel="self"></atom:link><lastBuildDate>Sun, 12 Feb 2012 18:04:00 +0400</lastBuildDate><item><title>Как дела у Plenty of Fish</title><link>https://www.insight-it.ru//highload/2012/kak-dela-u-plenty-of-fish/</link><description>&lt;p&gt;Напомню, что &lt;a href="https://www.insight-it.ru/goto/2f6c1d33/" rel="nofollow" target="_blank" title="http://www.pof.com/"&gt;Plenty of Fish&lt;/a&gt; - один из самых
популярных в мире сайтов знакомств, созданный одним человеком и
работающий на всего нескольких серверах и коммерческих технологиях. На
&lt;strong class="trebuchet"&gt;Insight IT&lt;/strong&gt; два года назад уже была &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-plenty-of-fish/"&gt;подробная статья о технической реализации проекта&lt;/a&gt;, сегодня же я хочу поделиться свежей информацией о том, как проект развивается.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="o-proekte"&gt;О проекте&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Долгое время, около 7 лет, проект полностью разрабатывался и
    поддерживался исключительно Маркусом, его создателем&lt;/li&gt;
&lt;li&gt;Первый архитектор баз данных был нанят в июле 2011 года&lt;/li&gt;
&lt;li&gt;На сегодняшний день команда составляет около 5 сотрудников&lt;/li&gt;
&lt;li&gt;Используется стек технологий от Microsoft, а также Akamai в роли CDN&lt;/li&gt;
&lt;li&gt;Расходы на техническое обеспечение проекта составляют порядка 70
    тысяч долларов в месяц&lt;/li&gt;
&lt;li&gt;Ежегодные доходы от рекламы измеряются миллионами долларов&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika-na-noiabr-2011"&gt;Статистика на ноябрь 2011&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;6 миллиардов просмотров страниц&lt;/li&gt;
&lt;li&gt;32 миллиарда просмотров изображений, пики до 22 тыс. в секунду, по
    ночам около 7 тыс.&lt;/li&gt;
&lt;li&gt;30 миллиардов обращений к серверам мгновенного обмена сообщениями&lt;/li&gt;
&lt;li&gt;Более 6 миллионов пользователей заходят на сайт каждое воскресенье&lt;/li&gt;
&lt;li&gt;Количество веб-серверов увеличилось с 2 до 11, что обеспечивает
    практически двухкратный запас по используемым ресурсам&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="vyvody"&gt;Выводы&lt;/h2&gt;
&lt;p&gt;Чтобы проект был очень прибыльным для своих создателей, не всегда стоит
торопиться стать большой компанией. Быть маленьким тоже выгодно:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;минимальные расходы на зарплаты, офис и техническое обеспечение,&lt;/li&gt;
&lt;li&gt;востребованный продукт благодаря способности быстро подстраиваться
    под потребности рынка,&lt;/li&gt;
&lt;li&gt;отсутствие необходимости делиться существенной частью прибыли с
    инвесторами,&lt;/li&gt;
&lt;li&gt;достаточно простая архитектура и техническая реализация,&lt;/li&gt;
&lt;li&gt;отсутствие потерь времени на бюрократию и совещания.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 12 Feb 2012 18:04:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-02-12:highload/2012/kak-dela-u-plenty-of-fish/</guid><category>Akamai</category><category>Microsoft</category><category>Plenty of Fish</category><category>POF</category><category>статистика</category></item><item><title>Является ли использование продукции Microsoft причиной провала MySpace?</title><link>https://www.insight-it.ru//highload/2011/yavlyaetsya-li-ispolzovanie-produkcii-microsoft-prichinojj-provala-myspace/</link><description>&lt;p&gt;Как известно, &lt;a href="/tag/myspace/"&gt;MySpace&lt;/a&gt; появилась на рынке даже несколько
раньше, чем &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt;, с практически аналогичным
продуктом. Но при этом на сегодняшний день Facebook - общепризнанный
лидер рынка социальных сетей, а MySpace даже далеко не все слышали
название. С технической точки зрения у проектов совершенно разные
подходы: Facebook построен на opensource технологиях, типичный LAMP,
MySpace же полностью использует стек от &lt;a href="/tag/microsoft/"&gt;Microsoft&lt;/a&gt;.
Некоторое время назад в зарубежной блогосфере зародилась оживленная
дискуссия на тему: а не выбор ли закрытых технологий стал основной
причиной проигранной MySpace "гонки" социальных сетей?&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Вопрос и правда противоречивый: с одной стороны большая часть успеха
зависит от позиционирования продукта и маркетинга, с другой -
техническая неповоротливость, вызванная использованием проприетарных
продуктов, не позволяла развиваться так же быстро, как конкуренты.
Приведу ссылки на основные "очаги" дискуссии на данную тему:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/9d322b3/" rel="nofollow" target="_blank" title="http://scobleizer.com/2011/03/24/myspaces-death-spiral-due-to-bets-on-los-angeles-and-microsoft"&gt;MySpace's Death Spiral&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/cdb68f2/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2011/3/25/did-the-microsoft-stack-kill-myspace.html"&gt;Did The Microsoft Stack Kill&amp;nbsp;MySpace?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a2ceedc5/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2011/3/31/8-lessons-we-can-learn-from-the-myspace-incident-balance-vis.html"&gt;8 Lessons We Can Learn From The MySpace Incident - Balance, Vision,&amp;nbsp;Fearlessness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/48dba052/" rel="nofollow" target="_blank" title="http://www.intelligentspeculator.net/investment-talking/why-facebook-is-succeeding-where-myspace-has-failed"&gt;Why Facebook is succeeding where Myspace has failed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/36159b1d/" rel="nofollow" target="_blank" title="http://news.ycombinator.com/item?id=2369271"&gt;HackerNews&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Некоторые ключевые точки зрения:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Все интернет-компании в первую очередь технологические.&lt;/strong&gt; Даже
    если основной тематикой является развлечение, всё равно оно
    предоставляются посредством технологий.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Все неудачи в бизнесе не связаны с технологиями.&lt;/strong&gt; Если менеджмент
    не понимает ценность каждого разработчика и не может делегировать
    полномочия и ответственность - проект не станет успешным. С другой
    стороны если разработчики не знают какие вещи являются важными для
    бизнеса, так как их просто не держат в курсе - это так же ничем
    хорошим не заканчивается.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Нельзя мотивировать разрушение инновации.&lt;/strong&gt; В MySpace основным
    источником дохода являлась рекламе Google, доход с которой напрямую
    зависел от количества просмотров страниц и кликов, что в результате
    подтолкнуло MySpace добавить дополнительные не нужные страницы ко
    всем действиям пользователей, что абсолютно уничтожило все
    юзабилити.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Корпоративная разработка и веб-разработка - совершенно разные
    вещи.&lt;/strong&gt; Не смотря на тот факт, что большинство сошлось во мнении,
    что ни продукция Microsoft, ни разработчики MySpace не виноваты в
    провале проекта, тот факт, что между инструментами, подходами и
    технологиями для разработки масштабируемых интернет-проектов и
    корпоративных приложений имеется очень большой разрыв, очевиден.
    Технологии, разработанные для корпоративного рынка, не могут быть
    без серьезной настройки и доработки использованы в мире крупных
    интернет-проектов.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Нельзя позволять обратной совместимости останавливать развитие.&lt;/strong&gt;
    В MySpace этот вопрос касается в большей степени пользовательского
    интерфейса: пользователи сами могут его настраивать. В итоге при
    введении каждой новой кнопочки или технологии типа AJAX им
    приходилось задумываться: "а не сломает ли это страницу при
    настройках какого-нибудь пользователя?"&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Успех не вечен.&lt;/strong&gt; Изменения нужно вводить в эксплуатацию без
    страха ошибиться.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Не полагайтесь на единственного клиента.&lt;/strong&gt; Нужно иметь набор
    различных источников дохода, чтобы быть от них независимыми и
    продолжать развиваться даже если один из них иссяк.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Определитесь кем вы являетесь.&lt;/strong&gt; Когда битва проиграна, можно
    переключить фокус на другую нишу, где меньше конкуренции - так новый
    генеральный директор MySpace переориентировал проект с социальной
    сети на сервис по открытию для себя новой музыки. Такие решения
    могут сработать, а могут и нет - но в любом случае это лучше, чем
    просто закрыть проект.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;С моей точки зрения основной минус закрытых технологий - отсутствие
полного контроля над всем кодом проекта, если проблемы начинаются в
"закрытых" компонентах, то их исправление занимает нереальное количество
времени, так как требует взаимодействия со сторонней компанией, которая?
скорее всего? не будет торопиться решать каждую конкретную проблему и в
лучшем случае её исправление будет доступно лишь в следующем релизе
используемого продукта. Помимо этого, при использовании проприетарных
технологий у разработчиков меньше понимания того, как работают внутри
используемые технологии - никакая документация не заменит возможность
"залезть под капот". В случае же с Microsoft проект совсем связывает
себя по рукам и ногам, так как воспользовавшись монолитным стеком чаще
всего нельзя постепенно перейти на другую технологическую базу, так как
это по сути сводится к полному переписыванию кода проекта, что на
подобном уровне очень затруднительно. Эти и подобные вещи замедляют
процесс разработки, а значит и развитие проекта - этот фактор очень
важен в современном интернет-сообществе, так как, если не двигаться
вперед, конкуренты догонят и перегонят очень стремительно, как в целом и
случилось с MySpace.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;А какова твоя точка зрения на данную ситуацию? Может ли неудачное технологическое решение столь кардинально повлиять на успех бизнеса?&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 20 Apr 2011 12:37:00 +0400</pubDate><guid>tag:www.insight-it.ru,2011-04-20:highload/2011/yavlyaetsya-li-ispolzovanie-produkcii-microsoft-prichinojj-provala-myspace/</guid><category>Microsoft</category><category>MySpace</category><category>fail</category><category>провал</category></item><item><title>Архитектура Plenty of Fish</title><link>https://www.insight-it.ru//highload/2010/arkhitektura-plenty-of-fish/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/fa2360fd/" rel="nofollow" target="_blank" title="http://www.plentyoffish.com/"&gt;Plenty of Fish&lt;/a&gt; представляет собой очень
популярный сервис онлайн знакомств, насчитывающий более 45 миллионов
посетителей в месяц и 30+ миллионов просмотров страниц в сутки (что
составляет около 500-600 страниц в секунду). Но это не самая интересная
часть истории... Все это управляется единственным человеком при
использовании нескольких серверов, при этом он тратит на работу всего
пару часов в день и зарабатывает 6 миллионов долларов на рекламе от
Google. Завидуете? Я тоже :) Как же ему удалось соединить столько
влюбленных пар, используя так мало ресурсов?&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Данный пост является переводом &lt;a href="https://www.insight-it.ru/goto/e06f86e0/" rel="nofollow" target="_blank" title="http://highscalability.com/plentyoffish-architecture"&gt;англоязычной
статьи&lt;/a&gt;, автор
оригинала: Todd Hoff.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/35763c73/" rel="nofollow" target="_blank" title="http://channel9.msdn.com/ShowPost.aspx?PostID=331501#331501"&gt;Channel9 интервью с Markus Frind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/fa591b8c/" rel="nofollow" target="_blank" title="http://plentyoffish.wordpress.com/%20target="&gt;Блог Markus Frind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/ed31cb93/" rel="nofollow" target="_blank" title="http://www.readwriteweb.com/archives/plentyoffish_one_billion.php"&gt;Plentyoffish: компания одного человека может стоить 1 миллиард
долларов&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Microsoft Windows&lt;/li&gt;
&lt;li&gt;ASP.NET&lt;/li&gt;
&lt;li&gt;IIS&lt;/li&gt;
&lt;li&gt;Akamai CDN&lt;/li&gt;
&lt;li&gt;Foundry ServerIron Load Balancer&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;PlentyOfFish (POF) имеет 1.2 миллиарда просмотров страниц в месяц, в
среднем 500 тысяч уникальных авторизованных пользователей в день.
Пиковый сезон приходится на январь каждого года, когда эти цифры
возрастают на 30%.&lt;/li&gt;
&lt;li&gt;POF имеет единственного сотрудника: создатель и генеральный директор
Markus Frind.&lt;/li&gt;
&lt;li&gt;Зарабатывает до 10 миллионов долларов в год на рекламе от Google,
работает при этом только около двух часов в день.&lt;/li&gt;
&lt;li&gt;30+ миллионов просмотров страниц в день (500 - 600 страниц в секунду).&lt;/li&gt;
&lt;li&gt;1.2 миллиарда просмотров страниц и 45 миллионов посетителей в месяц.&lt;/li&gt;
&lt;li&gt;Имеет &lt;a href="https://www.insight-it.ru/goto/ca40875e/" rel="nofollow" target="_blank" title="http://ru.wikipedia.org/wiki/CTR_(%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%BD%D0%B5%D1%82)"&gt;CTR&lt;/a&gt; в 5-10 раз выше, чем Facebook.&lt;/li&gt;
&lt;li&gt;Находится в top 30 сайтов США по данным Competes Attention, top 10 в Канаде и top 30 в Великобритании.&lt;/li&gt;
&lt;li&gt;Нагрузка балансируется между двумя веб-серверами с 2 Quad Core Intel
Xeon X5355 @ 2.66Ghz, 8GB RAM (используется около 800 MB), 2 жесткими
дисками, работают под управлением Windows x64 Server 2003.&lt;/li&gt;
&lt;li&gt;3 сервера баз данных. Информация об их конфигурации не предоставляется.&lt;/li&gt;
&lt;li&gt;Приближается к 64000 одновременных соединений и 2 миллионам просмотрам
страниц в час.&lt;/li&gt;
&lt;li&gt;Интернет-канал в 1Gbps, из которых используется только 200Mbps.&lt;/li&gt;
&lt;li&gt;1 TB трафика от отдачи 171 миллионов изображений через Akamai.&lt;/li&gt;
&lt;li&gt;6TB система хранения данных для обработки миллионов полноразмерных
изображений, которые загружаются на сайт каждый месяц.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="chto-vnutri"&gt;Что внутри?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Модель монетизации заключалась в использовании рекламы от Google.
Match.com, для сравнения, получает 300 миллионов долларов в год, в
основном с платных подписок. Источник дохода POF должен измениться,
чтобы позволить ему получать больше выручки от имеющихся пользователей.
Планируется нанять больше сотрудников, в частности людей, которые будут
заниматься продажей рекламы напрямую вместо того, чтобы полностью
полагаться на AdSense.&lt;/li&gt;
&lt;li&gt;При 30 миллионах просмотрах страниц в день можно зарабатывать неплохие
деньги на рекламе, даже
если&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/b1c7d720/" rel="nofollow" target="_blank" title="http://en.wikipedia.org/wiki/Cost_per_mille"&gt;CPM&lt;/a&gt; будет всего 5-10
центов.&lt;/li&gt;
&lt;li&gt;Akamai используется для отдачи более 100 миллионов изображений в день.
Если на странице 8 изображений и каждое загружается за 100 миллисекунд -
их загрузка займет почти секунду, так что распределение изображений
целесообразно.&lt;/li&gt;
&lt;li&gt;Десятки миллионов изображений отдаются с серверов POF, но большинство из
них размером меньше 2KB и практически полностью закешированы в
оперативной памяти.&lt;/li&gt;
&lt;li&gt;Все динамично. Практически никакой статики.&lt;/li&gt;
&lt;li&gt;Все исходящие данные сжимаются с использованием Gzip, что обходится
всего 30% использованием процессорного времени. Используется много
вычислительных ресурсов, но зато существенно сокращается использование
пропускной способности интернет-канала.&lt;/li&gt;
&lt;li&gt;Кэширование ASP .NET не используется, так как данные теряют свою
актуальность практически сразу же.&lt;/li&gt;
&lt;li&gt;Встроенные компоненты ASP также не используется. Почти все написано с
чистого листа. Ничего не может быть более сложным, чем кучка простых
if-then-else и циклов. Все максимально элементарно.&lt;/li&gt;
&lt;li&gt;Балансировка нагрузки:&lt;/li&gt;
&lt;li&gt;IIS произвольно ограничивает общее количество соединений до 64000,
таким образом балансировщик нагрузки был добавлен для обработки большего
количества одновременных соединений. Вариант с добавлением второго IP
адреса и использованием round robin DNS также рассматривался, но вариант
с балансировщиком нагрузки выглядел более избыточным и позволял более
легко расширять количество серверов. Помимо этого ServerIron позволял
использовать более продвинутую функциональность, вроде блокировки ботов
и балансировку запросов по cookies, сессиям или IP-адресам
пользователей.&lt;/li&gt;
&lt;li&gt;Windows Network Load Balancing (NLB) функция не использовалась, так
как не поддерживает привязку сессий к серверам. Обходным путем было бы
хранение сессионных данных в базе данных или общей файловой системе.&lt;/li&gt;
&lt;li&gt;8-12 NLB серверов могут объединяться в кластер и может использоваться
неограниченное количество таких кластеров. Схема DNS round robin может
использоваться для распределения запросов между кластерами. Теоретически
такая архитектура могла бы позволить 70 веб-серверам обрабатывать более&amp;nbsp;300 тысяч одновременных соединений.&lt;/li&gt;
&lt;li&gt;NLB имеет опцию для отправки каждого пользователя на конкретный
сервер, таким образом не используется внешнее хранилище для сессионных
данных и если сервер выходит из строя - пользователи просто
разлогиниваются из системы. Если это состояние включает в себя например
корзину интернет-магазина или какую-то другую важную информацию, то
такой подход мог бы показаться&amp;nbsp;неприемлемым, но для сайта знакомств это
было бы не так критично.&lt;/li&gt;
&lt;li&gt;Было решено, что хранение и получение сессионных данных программными
средствами слишком дорого. Аппаратная балансировка нагрузка проще:
пользователи просто назначаются конкретным серверам и в случае сбоя
сервера назначенным ему пользователям предлагается пройти процесс
авторизации еще раз.&lt;/li&gt;
&lt;li&gt;Покупка ServerIron была дешевле и проще, чем использование NLB.
Многие крупные сайты используют их для создания пулов TCP соединений,
автоматическому определению ботов и так далее. ServerIron может делать
намного больше, чем просто балансировать нагрузку и такие функции
достаточно привлекательные за эту цену.&lt;/li&gt;
&lt;li&gt;Была большая проблема с выбором системы размещения рекламы. Многие из
них хотели несколько сотен тысяч в год и многолетний контракт.&lt;/li&gt;
&lt;li&gt;В процессе избавления от ASP.NET повторителей и использование взамен
конкатенации строк или response.write. Если у вас миллионы просмотров
страниц в день - просто напишите весь код для отображения на экране
пользователя.&lt;/li&gt;
&lt;li&gt;Большинство изначальных вложений ушло на построение SAN. Избыточность
любой ценой.&lt;/li&gt;
&lt;li&gt;Рост был за счет вирусного эффекта. Портал начал набирать популярность в
Канаде, затем о нем узнали в Великобритании и Австралии, и только потом
в США.&lt;/li&gt;
&lt;li&gt;База данных:&lt;/li&gt;
&lt;li&gt;Одна база данных является основной.&lt;/li&gt;
&lt;li&gt;Две базы данных для поиска. Поисковые запросы распределяются по их типу.&lt;/li&gt;
&lt;li&gt;Производительность наблюдается через диспетчер задач. Когда
появляются пики - ситуация рассматривается более детально. Проблемы
обычно заключались в блокировках на уровне СУБД. Собственно говоря почти
всегда это были проблемы с базами данных, очень редко они возникают на
уровне .NET. Так как POF не использует библиотеки .NET, отследить
проблемы с производительностью оказывается достаточно просто. Если бы
использовалось много уровней framework'ов, поиск мест, где скрываются
проблемы, был бы трудным и утомляющим.&lt;/li&gt;
&lt;li&gt;Если Вы делаете запрос к базе данных 20 раз при отображении одной
страницы, &amp;nbsp;Вы проиграли в любом случае, вне зависимости от того, что Вы
будете делать.&lt;/li&gt;
&lt;li&gt;Разделяйте запросы чтения и записи к базе данных. Если у вас нет
избыточного количества оперативной памяти не следование этому правилу
может заставить систему зависнуть на несколько секунд.&lt;/li&gt;
&lt;li&gt;Постарайтесь делать базы данных только для чтения.&lt;/li&gt;
&lt;li&gt;Денормализуйте данные. Если Вам приходится доставать данные из 20
разных таблиц, попробуйте сделать просто одну таблицу, где будут лежать
все нужные для чтения данные.&lt;/li&gt;
&lt;li&gt;Один день может проработать почти что угодно, но когда Ваша база
данных удвоится - использованные подход может внезапно перестать
работать.&lt;/li&gt;
&lt;li&gt;Если система делает только что-то одно, она будет делать это реально
хорошо. Только записывайте данные и все будет нормально. Только читайте
данные и все будет нормально. Делайте и то и другое - и все испортится.
База данных погрязнет в проблемах с блокировками.&lt;/li&gt;
&lt;li&gt;Если Вы полностью используете вычислительные мощности, Вы либо
делаете что-то не так, либо Ваша система на самом деле очень
оптимизирована. Если вы можете разместить всю базу в оперативной
памяти - обязательно делайте это.&lt;/li&gt;
&lt;li&gt;Процесс разработки выглядит примерно следующим образом: появляется идея,
быстро реализуется и выдается пользователям в пределах 24 часов. Отклик
от пользователей получается по слежению за тем, что они делают на сайте:
выросло количество сообщений на пользователя? среднее время сессий
выросло? Если пользователям новая фишка не пришлась по вкусу - просто
уберите её.&lt;/li&gt;
&lt;li&gt;При небольшом количестве серверов системные сбои достаточно редки и
краткосрочны. Наибольшими сложностями были проблемы с DNS, когда
некоторые интернет-провайдеры говорили, что POF больше не существует. Но
так как сайт бесплатен, пользователи нормально относятся к небольшим
периодам его недоступности. Люди часто не замечают простой сайта, так
как думают, что это какая-то проблема у них, с интернет-соединением или
еще чем-то.&lt;/li&gt;
&lt;li&gt;Переход от миллиона пользователей к 12 миллионам пользователей был
большим прыжком. Система может обслуживать и 60 миллионов пользователей
с двумя веб-серверами.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Часто смотрите на конкурентов для идей новых функциональных
возможностей.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Рассмотрите использование чего-то вроде S3, когда система начнет
требовать географической балансировки.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Вам не нужны миллионы в финансировании, размашистая инфраструктура и
целое здание сотрудников для того, чтобы создать вебсайт мирового
уровня, который обслуживает кучу пользователей и приносит неплохие
деньги. Все что нужно - всего лишь привлекательная идея, которая
понравится большому количеству идей, сайт, который становится популярным
благодаря слухам, а также опыт и видение для построения сайта, не
наступая на типичные "грабли". Вот и все, что Вам нужно :-)&lt;/li&gt;
&lt;li&gt;Необходимость - мать всех изменений.&lt;/li&gt;
&lt;li&gt;Когда вы растете быстро, но не слишком быстро, у Вас появляется шанс
расти, модифицировать и адаптироваться.&lt;/li&gt;
&lt;li&gt;Максимальное использование оперативной памяти решает массу проблем.
После этого рост возможен просто за счет использование более мощных
серверов.&lt;/li&gt;
&lt;li&gt;В начале старайтесь держать все максимально простым. Практически все
дают этот же самый совет, а Markus говорит, что все что он делает -
всего лишь очевидный здравый смысл. Но то что просто, не всегда означает
всего лишь осмысленную вещь. Создание простых вещей является результатом
многих лет практического опыта.&lt;/li&gt;
&lt;li&gt;Поддерживайте время доступа к базе данных быстрым и у Вас не будет
проблем.&lt;/li&gt;
&lt;li&gt;Одной из основных причин, по которой POF может работать с таким
небольшим количеством сотрудников и оборудования, является использование
CDN для отдачи активно используемого контента. Использование CDN может
оказаться секретным соусом для многих крупных сайтов. Markus считает,
что в top 100 не существует ни одного сайта, не использующего CDN. Без
CDN время загрузки страницы в Австралии возросло бы до 3-4 секунд только
за счет изображений.&lt;/li&gt;
&lt;li&gt;Реклама на Facebook принесла плохие результаты. Из 2000 кликов только 1
человек регистрировался. С CTR равным 0.04% Facebook выдавал 0.4 клика
на 1000 показов рекламы (CPM). При 5 центах CPM = 12.5 центов за клик,
50 центах CPM = 1.25\$ за клик. 1 доллар CPM = 2.50\$ за клик. 15\$ CPM
= 37.50\$ за клик.&lt;/li&gt;
&lt;li&gt;Это просто продавать несколько миллионов просмотров страниц с высоким
CPM, но НАМНОГО сложнее продавать миллиарды просмотров с высоким CPM,
как это делают Myspace и Facebook.&lt;/li&gt;
&lt;li&gt;Модель монетизации, основанная на рекламе, ограничивает Ваши доходы. Вам
придется переходить к платной модели чтобы повышать прибыль.
Генерировать 100 миллионов долларов в год за счет бесплатного сайта
практически невозможно - Вам потребуется слишком большой рынок.&lt;/li&gt;
&lt;li&gt;Повышение количества просмотров за счет Facebook не работает для сайтов
знакомств. Иметь посетителя на собственном сайте намного более
прибыльно. Большинство просмотров страниц на Facebook находятся за
пределами США и Вам придется делить 5 центов CPM с Facebook.&lt;/li&gt;
&lt;li&gt;Предложение пользователям при регистрации получить информацию об ипотеке
или каком-то другом продукте, может стать неплохим источником
дополнительной выручки.&lt;/li&gt;
&lt;li&gt;Вы не можете постоянно прислушиваться к отзывам пользователей. Кому-то
всегда будут нравиться новые функции, а кто-то всегда будет их
ненавидеть, но только часть из них сообщит Вам об этом. Вместо этого
лучше смотреть как новые функции влияют на то, чем люди на самом деле
занимаются, просто смотря на Ваш сайт и статистику его использования.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 18 Jan 2010 16:43:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-01-18:highload/2010/arkhitektura-plenty-of-fish/</guid><category>Akamai CDN</category><category>ASP</category><category>ASP .NET</category><category>dating</category><category>Foundry</category><category>IIS</category><category>Microsoft</category><category>online</category><category>Plenty of Fish</category><category>POF</category><category>ServerIron</category><category>Windows</category><category>Windows Server</category><category>архитектура</category><category>Архитектура Plenty of Fish</category><category>Масштабируемость</category><category>сайт знакомств</category></item><item><title>Архитектура Stack Overflow</title><link>https://www.insight-it.ru//highload/2010/arkhitektura-stack-overflow/</link><description>&lt;p&gt;&lt;img alt="Stack Overflow" class="right" src="https://www.insight-it.ru/images/stack-overflow-logo.png" title="Stack Overflow"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/33fc61d9/" rel="nofollow" target="_blank" title="https://stackoverflow.com/"&gt;Stack Overflow&lt;/a&gt; является любимым многими
программистами сайтом, где можно задать профессиональный вопрос и
получить ответы от коллег. Этот проект был написан двумя никому не
известными парнями, о которых никто никогда раньше не слышал. Хорошо, не
совсем так. Stack Overflow был создан топовыми программистами и звездами
блогосферы:&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/374a081/" rel="nofollow" target="_blank" title="http://www.codinghorror.com/blog/"&gt;Jeff Atwood&lt;/a&gt; и&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/31657700/" rel="nofollow" target="_blank" title="http://www.joelonsoftware.com/"&gt;Joel Spolsky&lt;/a&gt;. В этом отношении Stack
Overflow похож на ресторан, владельцами которого являются знаменитости.
По оценкам Joel'а около 1/3 программистов всего мира использовали этот
интернет-ресурс, так что должно быть он представляет собой что-то
достаточно полезное и интересное.&lt;/p&gt;
&lt;p&gt;Одним из ключевых моментов в истории Stack Overflow является
использование вертикального масштабирования, как достаточно
работоспособного решения достаточного большого класса проблем. Не смотря
на то, что публика на сегодняшний день больше склоняется к подходу с
использованием горизонтальным масштабирования и&amp;nbsp;не-SQL баз данных.&lt;/p&gt;
&lt;p&gt;Если Вы стремитесь к масштабу Google, у Вас нет другого выхода, как
двигаться в направлении не-SQL. Но Stack Overflow - это не Google, ровно
как и подавляющее большинство других сайтов. Когда Вы задумываетесь о
возможных вариантов дизайна Вашего проекта, попробуйте учесть и историю
Stack Overflow, она тоже имеет право на жизнь. В этот век многоядерных
машин с большим объемом оперативной памяти и невероятными темпами
развития методов параллельного программирования, вертикальное
масштабирование все еще является жизнеспособной стратегией и не должна
сразу же отбрасываться в сторону просто так как это теперь больше не
модно. Возможно в один прекрасный день мы получим лучшее из обоих миров,
но на сегодняшний момент перед нами лежит большой болезненный выбор
стратегии масштабирования, от которого определенно зависит судьба Вашего
проекта.&lt;/p&gt;
&lt;p&gt;Joel любит похвастаться тем, что они достигли производительности,
сравнимой с другими сайтами аналогичных размеров, используя в 10 раз
меньше оборудования. Он удивляется, работали над этими сайтами
по-настоящему хорошие программисты. Давайте взглянем на то, как им это
удалось, и дадим Вам возможность побыть судьей.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;&lt;em&gt;Перевод &lt;a href="https://www.insight-it.ru/goto/98b904e0/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2009/8/5/stack-overflow-architecture.html"&gt;статьи&lt;/a&gt;, автор оригинала - Todd Hoff. Возможно будет еще один пост с менее формальной информацией на ту же тему.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;16 миллионов просмотров страниц в месяц&lt;/li&gt;
&lt;li&gt;3 миллионов уникальных пользователей в месяц (для сравнения: Facebook
насчитывает около 77 миллионов уникальных пользователей в месяц)&lt;/li&gt;
&lt;li&gt;6 миллионов посещений в месяц&lt;/li&gt;
&lt;li&gt;86% трафика приходит с Google&lt;/li&gt;
&lt;li&gt;9 миллионов активных программистов во всем мире и 30% пользуются Stack
Overflow&lt;/li&gt;
&lt;li&gt;Более дешевые лицензии были получены через программу
Microsoft&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/205088ae/" rel="nofollow" target="_blank" title="http://www.microsoft.com/BizSpark"&gt;BizSpark&lt;/a&gt;. Скорее всего
они заплатили около 11000\$ за лицензии на ОС и MSSQL.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Стратегия монетизации: ненавязчивая реклама, вакансии, конференции
DevDays, достижения других смежных ниш (Server Fault, Super User),
разработка&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/631e88c2/" rel="nofollow" target="_blank" title="https://stackexchange.com/"&gt;StackExchange&lt;/a&gt; и возможно
каких-то других систем рейтингов для программистов.&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Microsoft ASP.NET MVC&lt;/li&gt;
&lt;li&gt;SQL Server 2008&lt;/li&gt;
&lt;li&gt;C#&lt;/li&gt;
&lt;li&gt;Visual Studio 2008 Team Suite&lt;/li&gt;
&lt;li&gt;jQuery&lt;/li&gt;
&lt;li&gt;LINQ to SQL&lt;/li&gt;
&lt;li&gt;Subversion&lt;/li&gt;
&lt;li&gt;Beyond Compare 3&lt;/li&gt;
&lt;li&gt;VisualSVN 1.5&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Веб уровень:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 x Lenovo ThinkServer RS110 1U&lt;/li&gt;
&lt;li&gt;4 ядра, 2.83 Ghz, 12 MB L2 cache&lt;/li&gt;
&lt;li&gt;500 GB жесткие диски, зеркалирование RAID1&lt;/li&gt;
&lt;li&gt;8 GB RAM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Уровень базы данных:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 x Lenovo ThinkServer RD120 2U&lt;/li&gt;
&lt;li&gt;8 ядер, 2.5 Ghz, 24 MB L2 cache&lt;/li&gt;
&lt;li&gt;48 GB RAM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Четвертый сервер был добавлен для запуска
&lt;a href="https://www.insight-it.ru/goto/d3829019/" rel="nofollow" target="_blank" title="https://superuser.com/"&gt;superuser.com&lt;/a&gt;. Все сервера вместе обеспечивают
работу &lt;a href="https://www.insight-it.ru/goto/33fc61d9/" rel="nofollow" target="_blank" title="https://stackoverflow.com/"&gt;Stack Overflow&lt;/a&gt;,&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/9b0a2d16/" rel="nofollow" target="_blank" title="https://serverfault.com/"&gt;Server
Fault&lt;/a&gt;, и&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/d3829019/" rel="nofollow" target="_blank" title="https://superuser.com/"&gt;Super User&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;QNAP TS-409U NAS для резервного копирования данных. Было принято решение
не использовать "облачные" решения, так как вызванные ими дополнительные
5GB трафика ежедневно были бы накладными.&lt;/li&gt;
&lt;li&gt;Сервера располагаются у&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/4f3f2e08/" rel="nofollow" target="_blank" title="http://www.peakinternet.com/"&gt;Peak Internet&lt;/a&gt;. В
основном из-за впечатляющей детализации технических ответов и разумных
расценок.&lt;/li&gt;
&lt;li&gt;Полнотекстный поиск в SQL Server активно используется для реализации
поиска по сайту и выявления повторных вопросов. Lucene .NET
рассматривается как достаточно заманчивая альтернатива.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;p&gt;Данный список является сборником уроков от Jeff и Joel, а также из
комментариев к их записям:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Если Вы комфортно себя чувствуете в деле управления серверами - не
бойтесь покупать их. Две основных проблемы с издержками аренды
оборудования:&lt;ol&gt;
&lt;li&gt;невероятные цены на дополнительную оперативную память и
жесткие диски;&lt;/li&gt;
&lt;li&gt;хостинг-провайдеры на самом деле не могут управлять
чем-либо за Вас.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Делайте одноразовые более крупные инвестиции в оборудование, чтобы
избежать быстро растущих ежемесячных издержек по аренде, которые
окажутся более высокими в долгосрочном периоде.&lt;/li&gt;
&lt;li&gt;Обновляйте сетевые драйвера. Производительность запросто может
удвоиться.&lt;/li&gt;
&lt;li&gt;Использование 48GB RAM требует обновления до MS Enterprise edition.&lt;/li&gt;
&lt;li&gt;Оперативная память невероятно дешевая. Используйте возможности по её
расширению по максимуму для получения практически бесплатной
производительности. У Dell, например, переход от 4GB памяти до 128GB
стоит всего 4378\$.&lt;/li&gt;
&lt;li&gt;Stack Overflow скопировали ключевую часть структуры базы данных у
Wikipedia. Это обернулось огромной ошибкой, для исправления которой
потребуется большой и болезненный рефакторинг базы данных. Основным
направлением изменений будет избавление от излишних операций по
объединению данных в большом количестве ключевых запросов. Это ключевой
урок, который стоит усвоить у гигантских много-терабайтных схем (вроде
Google BigTable), которые полностью избавлены от операций объединения
данных. Этот вопрос был достаточно важен для Stack Overflow, так как их
база данных практически полностью располагается в оперативной памяти и
операции join по прежнему требуют относительно много вычислительных
ресурсов.&lt;/li&gt;
&lt;li&gt;Производительность CPU оказывается на удивление важным фактором для
серверов баз данных. Переход от 1.86 GHz, к 2.5 GHz, и к 3.5 GHz
процессорам дает практически линейный прирост к времени выполнения
типичных запросов. Исключение: запросы, которые затрагивают не только
оперативную память.&lt;/li&gt;
&lt;li&gt;Когда оборудование арендуется, обычно никто не платит за дополнительную
оперативную память, если только вы не на помесячном контракте.&lt;/li&gt;
&lt;li&gt;В 90% случаев наиболее узким местом является база данных.&lt;/li&gt;
&lt;li&gt;При небольшом количестве серверов, &amp;nbsp;ключевым компонентом издержек
становится не место в стойках, электроэнергия, интернет-канал, сервера
или программное обеспечение, а СЕТЕВОЕ ОБОРУДОВАНИЕ. Вам потребуется как
минимум гигабитное соединение между уровнями веб-серверов и баз данных.
Между интернетом и веб-серверами потребуется firewall, маршрутизатор и
VPN. К моменту добавления второго веб-сервера понадобится решение для
балансировки нагрузки. Суммарная стоимость такого оборудования может
запросто вдвое превосходить стоимость пяти серверов.&lt;/li&gt;
&lt;li&gt;EC2 предназначен для горизонтального масштабирования, для того чтобы
нагрузка могла быть распределена между большим количеством машин
(достаточно хорошая идея, если Вы планируете расширяться). Еще больше
смысла в таком подходе появляется, если вы планируете масштабироваться
по необходимости (то есть добавлять и убирать машины в зависимости от
уровня нагрузки).&lt;/li&gt;
&lt;li&gt;Горизонтальное масштабирование может проходить относительно
безболезненно только при использовании open source программного
обеспечения. В противном случае вертикальное масштабирование значит
сокращение издержек, связанных с лицензиями, в ущерб стоимости
оборудования, а горизонтальное масштабирование - наоборот: экономия на
оборудовании, но требуется существенно больше лицензий на программное
обеспечение.&lt;/li&gt;
&lt;li&gt;RAID-10 отлично работает для баз данных с высокой нагрузкой операций
чтения и записи.&lt;/li&gt;
&lt;li&gt;Разделяйте работу приложений и баз данных таким образом, чтобы они могли
масштабироваться независимо друг от друга. Например, базы данных могут
масштабироваться вертикально, а сервера приложений - горизонтально.&lt;/li&gt;
&lt;li&gt;Приложения должны хранить все информацию о своем состоянии в базе данных
для обеспечения возможности роста путем простого добавления серверов
приложений в кластер.&lt;/li&gt;
&lt;li&gt;Одна из основных проблем со стратегией вертикального масштабирования -
недостаток избыточности. Кластеризация добавляет надежности, но когда
стоимость каждого сервера высока - это не так просто реализовать.&lt;/li&gt;
&lt;li&gt;Некоторые приложения могут масштабироваться линейно относительно числа
процессоров. Но зачастую будут использоваться механизмы блокировки, что
приведет к сериализации вычислений и в итоге к существенному уменьшению
эффективности приложения.&lt;/li&gt;
&lt;li&gt;С более крупными серверами, занимающими от 7U в стойке, электроэнергия и охлаждение становятся критичными вопросами. Возможно использование
чего-то среднего между 1U и 7U может облегчить Ваши взаимоотношения с
датацентром.&lt;/li&gt;
&lt;li&gt;С добавлением все новых и новых серверов баз данных издержки на лицензии SQL Server могут стать очень существенными. Если Вы начнете с
вертикального масштабирования и постепенно начнете переходить к
горизонтальному с использованием не open source продуктов, возможно это
сильно ударит по Вашему финансовому состоянию. Это справедливо, что в
этой заметке речь идет не совсем об архитектуре проекта. Мы знаем об их
серверах, об используемом наборе инструментов, об их двухуровневой
схеме, где база данных используется напрямую из кода веб-серверов. Но мы
не знаем практически ничего о самой реализации, например таких мелочей
как теги. Если Вам интересен этот вопрос, возможно Вам удастся получить
интересующую Вас информацию из &lt;a href="https://www.insight-it.ru/goto/61237733/" rel="nofollow" target="_blank" title="http://sqlserverpedia.com/wiki/Understanding_the_StackOverflow_Database_Schema"&gt;описания их схемы базы данных&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 08 Jan 2010 00:31:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-01-08:highload/2010/arkhitektura-stack-overflow/</guid><category>ASP</category><category>ASP .NET</category><category>Beyond Compare 3</category><category>C++</category><category>highload</category><category>JQuery</category><category>Lenovo</category><category>Lenovo ThinkServer</category><category>LINQ</category><category>Microsoft</category><category>MSSQL</category><category>MVC</category><category>Server Fault</category><category>SQL Server 2008</category><category>Stack Overflow</category><category>Subversion</category><category>Super User</category><category>Visual Studio 2008 Team Suite</category><category>VisualSVN</category><category>архитектура</category><category>архитектура Stack Overflow</category><category>архитектура высоконагруженных сайтов</category><category>Масштабируемость</category></item><item><title>Архитектура MySpace</title><link>https://www.insight-it.ru//highload/2009/arkhitektura-myspace/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/f989d588/" rel="nofollow" target="_blank" title="http://www.myspace.com"&gt;MySpace.com&lt;/a&gt; является одним из наиболее быстро
набирающих популярность сайтов в Интернете с 65 миллионами пользователей
и 260000 регистрациями в день. Этот сайт часто подвергается критике
из-за не достаточной производительности, хотя на самом деле MySpace
удалось избежать ряда проблем с масштабируемостью, с которыми
большинство других сайтов неизбежно сталкивались. Как же им это
удалось?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Данная статья является переводом статьи &lt;a href="https://www.insight-it.ru/goto/e9f0b809/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2009/2/12/myspace-architecture.html"&gt;MySpace
Architecture&lt;/a&gt;,
автором которой является Todd Hoff. Когда-то давно один из читателей
этого блога просил меня осветить и эту тему, тогда я так и не решился
из-за отсутствия моего личного интереса, но сейчас снова случайно
наткнулся на эту статью и подумал: а почему бы и нет?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/45549701/" rel="nofollow" target="_blank" title="http://www.infoq.com/news/2009/02/MySpace-Dan-Farino"&gt;Презентация: за сценой MySpace.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8d5c1d4d/" rel="nofollow" target="_blank" title="http://www.baselinemag.com/c/a/Projects-Networks-and-Storage/Inside-MySpacecom/"&gt;Внутри MySpace.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ASP .NET 2.0&lt;/li&gt;
&lt;li&gt;Windows&lt;/li&gt;
&lt;li&gt;IIS&lt;/li&gt;
&lt;li&gt;MSSQL Server&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="chto-vnutri"&gt;Что внутри?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;300 миллионов пользователей.&lt;/li&gt;
&lt;li&gt;Отдает 100Gbps в Интернет. 10Gbps из них является HTML контентом.&lt;/li&gt;
&lt;li&gt;4,500+ веб серверов со связкой: Windows 2003 / IIS 6.0 / ASP .NET.&lt;/li&gt;
&lt;li&gt;1,200+ кэширующих серверов, работающих на 64-bit Windows 2003. На
    каждом 16GB объектов находятся в кэше в оперативной памяти.&lt;/li&gt;
&lt;li&gt;500+ серверов баз данных, работающих на 64-bit Windows и SQL Server
    2005.&lt;/li&gt;
&lt;li&gt;MySpace обрабатывает 1.5 миллиарда просмотров страниц в день, а
    также 2.3 миллионов одновременно работающих пользователей в течении
    дня.&lt;/li&gt;
&lt;li&gt;Вехи по количеству пользователей:&lt;ul&gt;
&lt;li&gt;500 тысяч пользователей: простая архитектура перестает
справляться&lt;/li&gt;
&lt;li&gt;1 миллион пользователей: вертикальное партиционирование временно
спасает от основных болезненных вопросов с масштабированием&lt;/li&gt;
&lt;li&gt;3 миллиона пользователей: горизонтальное масштабирование
побеждает над вертикальным&lt;/li&gt;
&lt;li&gt;9 миллионов пользователей: сайт мигрирует на ASP.NET, создается
виртуализированная система хранения данных (SAN)&lt;/li&gt;
&lt;li&gt;26 миллионов пользователей: MySpace переходит на 64-битную
технологию.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;500 тысяч учетных записей было многовато для двух веб-серверов и
    одного сервера баз данных.&lt;/li&gt;
&lt;li&gt;На 1-2 миллионах учетных записей:&lt;ul&gt;
&lt;li&gt;Они использовали архитектуру базы данных, построенную на
концепции вертикального партиционирования, с отдельными базами
данных для разных частей сайта, которые использовались для
выполнения различных функций, таких как экран авторизации, профили
пользователей и блоги.&lt;/li&gt;
&lt;li&gt;Схема с вертикальным партиционированием помогала разделить
нагрузку как для операций чтения, так и для операций записи, а если
пользователям в друг оказывалась нужна новая функциональная
возможность - достаточно было просто добавить еще один сервер баз
данных для её обслуживания.&lt;/li&gt;
&lt;li&gt;MySpace переходит от использования систем хранения, подключенных
к серверам баз данных напрямую, к сетям хранения данных (SAN), при
таком подходе целый массив систем хранения объединяется вместе
специализированной сетью с высокой пропускной способностью, и
сервера баз данных также получают доступ к хранилищам через эту
сеть. Переход к SAN оказал положительное влияние как на
производительность, так и на доступность и надежность системы.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;На 3 миллионах учетных записей:&lt;ul&gt;
&lt;li&gt;Решение с вертикальным партиционированием не протянуло долго, так
как им приходилось реплицировать какую-то часть информации (например
информацию об учетных записях) по всем вертикальным частям базы
данных. С таким большим количеством операций репликации данных один
узел даже при незначительном сбое мог существенно замедлить
обновление информации во всей системе.&lt;/li&gt;
&lt;li&gt;Индивидуальные приложения вроде блогов на под-секциях сайта
достаточно быстро стали слишком большими для нормальной работы с
единственным сервером базы данных&lt;/li&gt;
&lt;li&gt;Произведена реорганизация всех ключевых данных для более логичной
организации в единственную базу данных&lt;/li&gt;
&lt;li&gt;Пользователи были разбиты на группы по миллиону в каждой и каждая
такая группа была перемещена на отдельный SQL Server&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;9&amp;ndash;17 миллионов учетных записей:&lt;ul&gt;
&lt;li&gt;Переход на ASP .NET, который требовал меньше ресурсов по
сравнению с их предыдущим вариантом архитектуры. 150 серверов,
использовавших новый код могли обработать нагрузку, для которой
раньше требовалось 246 серверов.&lt;/li&gt;
&lt;li&gt;Снова пришлось столкнуться с узким местом в системе хранения
данных. Реализация SAN решило какую-то часть старых проблем с
производительностью, но на тот момент потребности сайта начали
периодически превосходить возможности SAN по пропускной способности
операций ввода-вывода - той скорости, с которой она может читать и
писать данные на дисковые массивы.&lt;/li&gt;
&lt;li&gt;Столкнулись с лимитом производительности при размещении миллиона
учетных записей на одном сервере, ресурсы некоторых серверов начали
исчерпываться.&lt;/li&gt;
&lt;li&gt;Переход к виртуальному хранилищу, где весь SAN рассматривается
как одно большое общее место для хранения данных, без необходимости
назначать конкретные диски для хранения данных определенной части
приложения. MySpace на данный момент работает со стандартизированным
оборудованием от достаточно нового вендора SAN - 3PARdata&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Был добавлен кэширующий уровень &amp;mdash; прослойка из специализированных
    серверов, расположенных между веб-серверами и серверами данных, чья
    единственная задача была захватывать копии часто запрашиваемых
    объектов с данными в памяти и отдавать их веб-серверам для
    минимизации количества поиска данных в СУБД.&lt;/li&gt;
&lt;li&gt;26 миллионов учетных записей:&lt;ul&gt;
&lt;li&gt;Переход на 64-битные сервера с SQL Server на правах решения
проблемы с недостатком оперативной памяти. С тех пор их стандартный
сервер баз данных оснащен 64 GB RAM.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Горизонтальная федерация баз данных&lt;/strong&gt;. Базы данных
    партиционируются в зависимости от своего назначения. У них есть базы
    данных с профилями, электронными сообщениями и так далее. Каждая
    партиция основана на диапазоне пользователей. По миллиону в каждой
    базе данных. Таким образом, у них есть Profile1, Profile2 и все
    остальные базы данных вплоть до Profile300, если считать, что у них
    на данный момент зарегистрировано 300 миллионов учетных записей.&lt;/li&gt;
&lt;li&gt;Кэш ASP не используется, так как он не обеспечивает достаточного
    процента попаданий на веб серверах. Кэш, организованный как
    промежуточный слой, имеет существенно более высокое значение данного
    показателя.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Изоляция сбоев&lt;/strong&gt;. Внутри веб-сервера запросы сегментируются по
    базам данным. Разрешено использование только 7 потоков для работы с
    каждой базой данных. Таким образом, если база данных по каким-то
    причинам начинает работать медленно, только эти потоки замедлятся, в
    то время как остальные потоки будут успешно продолжать обрабатывать
    поток трафика.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="rabota-saita"&gt;Работа сайта&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Коллектор данных о производительности&lt;/strong&gt;. Централизованная система
    сбора информации о производительности через UDP. Такой подход более
    надежен, чем стандартный механизм Windows, а также позволяет любому
    клиенту подключиться и увидеть статистику.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Веб-система по просмотру дампов стеков процессов&lt;/strong&gt;. Можно просто
    сделать клик правой кнопкой мыши на проблемном сервере и увидеть
    дамп стека процессов, управляемых .NET. И это после привычки каждой
    раз удаленно подключаться к серверу, включать дебаггер и через
    полчаса получать свой ответ о том что же все таки происходит.
    Медленно, немасштабируемо и утомительно. Эта же система позволяет
    увидеть не просто стек процесса, но и предоставляет большое
    количество информации о контексте, в котором он работает.
    Обнаружение проблем намного проще при таком подходе, например можно
    легко увидеть, что база не отвечает, так как 90 ее потоков
    заблокировано.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Веб-система создания дампа heap-памяти&lt;/strong&gt;. Создает дамп всей
    выделенной памяти. Очень удобно и полезно для разработчиков.
    Сэкономьте часы на выполнение этой работы вручную.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Профайлер&lt;/strong&gt;. Прослеживает запрос от начала до конца и выводит
    подробный отчет. В нем можно увидеть URL, методы, статус, а также
    все, что поможет идентифицировать медленный запрос и его причины.
    Обнаруживает проблемы с блокировкой потоков, непредвиденными
    исключениями, другими словами все, что может оказаться интересным. В
    то же время остается очень легковесным решением. Работает на одной
    машине из каждой VIP (группа из 100 серверов) в production-среде.
    Опрашивает 1 поток каждые 10 секунд. Постоянно следит за системой в
    фоновом режиме.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Powershell&lt;/strong&gt;. Новая программная оболочка от Microsoft, которая
    работает в процессе и передаем объекты между командами вместо работы
    с текстовыми данными. MySpace разрабатывает множество так называемых
    commandlets'ов для поддержки различных операций.&lt;/li&gt;
&lt;li&gt;Разработана собственная технология асинхронной коммуникации для
    того, чтобы обойти проблемы с сетевыми проблемами Windows и работать
    с серверами как с группой. Например, она позволяет доставить файл
    .cs, скомпилировать его, запустить, и доставить результат обратно.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Развертывание&lt;/strong&gt;. Обновление кодовой базы происходит с помощью
    упомянутой выше собственной технологии. Ранее происходило до 5 таких
    обновлений в день, сейчас же они происходят лишь раз в неделю.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;С помощью стека Microsoft тоже можно делать большие веб-сайты.&lt;/li&gt;
&lt;li&gt;Стоит использовать кэширование с самого начала.&lt;/li&gt;
&lt;li&gt;Кэш является более подходящим местом для хранения временных данных,
    не требующих персистентности, например информации о пользовательских
    сессиях.&lt;/li&gt;
&lt;li&gt;Встроенные в операционные систему возможности, например по
    обнаружению DDoS-атака, могут приводить к необъяснимым сбоям.&lt;/li&gt;
&lt;li&gt;Храните свои данные в географически удаленных датацентрах для
    минимизации проблем, связанных со сбоями в электросети.&lt;/li&gt;
&lt;li&gt;Рассматривайте возможности использования виртуализированных систем
    хранения данных или кластерных файловых систем с самого начала. Это
    позволит существенно параллелизировать операции ввода-вывода, а
    также увеличивать дисковое пространство без необходимости какой-либо
    реорганизации.&lt;/li&gt;
&lt;li&gt;Разрабатывайте утилиты для работы с production окружением.
    Невозможно смоделировать все ситуации в тестовой среде.
    Масштабируемость и все различные варианты использования API не могут
    быть симулированы в процессе тестирования качества программного
    обеспечения. Обычные пользователи и хакеры обязательно найдут такие
    способы использования вашего продукта, о которых вы даже никогда и
    не подумаете в процессе тестирования, хотя конечно большая часть все
    же обнаружима в процессе QA тестирования.&lt;/li&gt;
&lt;li&gt;Когда это возможно - лучше просто использовать дополнительное
    оборудование для решения проблем. Это намного проще, чем изменять
    поведение программного обеспечения для того чтобы решать задачи
    как-то по-другому. Примером может служить добавление нового сервера
    на каждый миллион пользователей. Возможно было бы более эффективным
    изменить подход к самой работе с СУБД, но на практике все же проще и
    дешевле добавлять все новые и новые сервера. По крайней мере на
    данный момент.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 21 Dec 2009 16:15:00 +0300</pubDate><guid>tag:www.insight-it.ru,2009-12-21:highload/2009/arkhitektura-myspace/</guid><category>ASP</category><category>ASP .NET</category><category>highload</category><category>IIS</category><category>Microsoft</category><category>MSSQL</category><category>MySpace</category><category>myspace.com</category><category>online</category><category>Windows</category><category>Windows Server</category><category>архитектура</category><category>архитектура MySpace</category><category>Масштабируемость</category></item></channel></rss>