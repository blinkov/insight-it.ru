<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Insight IT</title><link>https://www.insight-it.ru/</link><description></description><atom:link href="https://www.insight-it.ru/category/highload/feed/index.xml" rel="self"></atom:link><lastBuildDate>Wed, 15 Aug 2012 22:26:00 +0400</lastBuildDate><item><title>Архитектура Pinterest</title><link>https://www.insight-it.ru//highload/2012/arkhitektura-pinterest/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/c1302f36/" rel="nofollow" target="_blank" title="http://pinterest.com"&gt;Pinterest&lt;/a&gt; - по непонятным для меня причинам
популярная в определенных кругах социальная сеть, построенная вокруг
произвольных картинок чаще всего не собственного производства. Как и
&lt;a href="https://www.insight-it.ru/highload/2012/arkhitektura-instagram/"&gt;Instagram&lt;/a&gt;
проект довольно молодой, с очень похожей историей и стеком технологий.
Тем не менее, Pinterest определенно заслуживает внимания как один из
самых быстрорастущих по посещаемости вебсайтов за всю историю.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/amazon/"&gt;Amazon&lt;/a&gt; &lt;a href="/tag/aws/"&gt;AWS&lt;/a&gt;&amp;nbsp;- хостинг и вспомогательные
    сервисы&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/nginx/"&gt;nginx&lt;/a&gt;&amp;nbsp;- вторичная балансировка нагрузки, отдача
    статики&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt;&amp;nbsp;- язык программирования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/django/"&gt;Django&lt;/a&gt;&amp;nbsp;- фреймворк&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&amp;nbsp;- основная СУБД&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;&amp;nbsp;- кэширование объектов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/redis/"&gt;Redis&lt;/a&gt;&amp;nbsp;- кэширование коллекций объектов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/solr/"&gt;Solr&lt;/a&gt;&amp;nbsp;- поиск&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt;&amp;nbsp;- анализ данных&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;3 миллиона уникальных посетителей в день&lt;/li&gt;
&lt;li&gt;18 миллионов уникальных посетителей в месяц&lt;/li&gt;
&lt;li&gt;4-я по популярности социальная сеть в США после
    &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt;,&amp;nbsp;&lt;a href="/tag/twitter/"&gt;Twitter&lt;/a&gt;&amp;nbsp;и
    &lt;a href="/tag/linkedin/"&gt;LinkedIn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Порядка 500 виртуальных машин в EC2&lt;/li&gt;
&lt;li&gt;80 миллионов объектов в S3&lt;/li&gt;
&lt;li&gt;410Тб пользовательских данных&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="razvitie"&gt;Развитие&lt;/h2&gt;
&lt;h4&gt;Март 2010&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;1 маленький виртуальный веб-сервер&lt;/li&gt;
&lt;li&gt;1 маленький виртуальный сервер MySQL&lt;/li&gt;
&lt;li&gt;Все это в Rackspace, 1 разработчик&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Январь 2011&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;1 сервер nginx для балансировки нагрузки, 4 веб-сервера&lt;/li&gt;
&lt;li&gt;2 сервера MySQL с master/slave репликацией&lt;/li&gt;
&lt;li&gt;3 сервера для отложенного выполнения задач&lt;/li&gt;
&lt;li&gt;1 сервер MongoDB&lt;/li&gt;
&lt;li&gt;Переехали на Amazon &lt;a href="/tag/ec2/"&gt;EC2&lt;/a&gt; + &lt;a href="/tag/s3/"&gt;S3&lt;/a&gt; +
    &lt;a href="/tag/cloudfront/"&gt;CloudFront&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Осень 2011&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;2 сервера nginx, 16 веб-серверов, 2 сервера для API&lt;/li&gt;
&lt;li&gt;5 функционально разделенных серверов MySQL с 9 read slave&lt;/li&gt;
&lt;li&gt;Кластер из 4 узлов Cassandra&lt;/li&gt;
&lt;li&gt;15 серверов Membase в 3 отдельных кластерах&lt;/li&gt;
&lt;li&gt;8 серверов memcached&lt;/li&gt;
&lt;li&gt;10 серверов Redis&lt;/li&gt;
&lt;li&gt;7 серверов для отложенной обработки задач&lt;/li&gt;
&lt;li&gt;4 сервера Elastic Search&lt;/li&gt;
&lt;li&gt;3 кластера MongoDB&lt;/li&gt;
&lt;li&gt;3 разработчика&lt;/li&gt;
&lt;li&gt;Если кто-то может объяснить зачем им сдался такой зоопарк, кроме как
    потестировать разные варианты, можете взять с полки пирожок.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Зима 2011-2012&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Заменили CloudFront на &lt;a href="/tag/akamai/"&gt;Akamai&lt;/a&gt; - вполне объяснимо,
    так как у Akamai намного лучше покрытие по миру, а
    качественный&amp;nbsp;&lt;a href="/tag/cdn/"&gt;CDN&lt;/a&gt; для сайта с большим количеством
    изображений - чуть ли не залог успеха.&lt;/li&gt;
&lt;li&gt;90 веб серверов и 50 серверов для API&lt;/li&gt;
&lt;li&gt;66 + 66 MySQL серверов на m1.xlarge инстансах EC2&lt;/li&gt;
&lt;li&gt;59 серверов Redis&lt;/li&gt;
&lt;li&gt;51 серверов memcached&lt;/li&gt;
&lt;li&gt;25+1 сервер для отложенной обработки задач на основе Redis&lt;/li&gt;
&lt;li&gt;Кластеризованный Solr&lt;/li&gt;
&lt;li&gt;6 разработчиков&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Весна-лето 2012&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Снова сменили CDN, на этот раз в пользу ранее неизвестного мне &lt;a href="/tag/edge-cast/"&gt;Edge
    Cast&lt;/a&gt;. Покрытие по всему миру довольно скромное,
    так что единственное логичное объяснение, которое мне приходит в
    голову - не потянули Akamai по деньгам.&lt;/li&gt;
&lt;li&gt;135 веб серверов и 75 серверов для API&lt;/li&gt;
&lt;li&gt;80 + 80 серверов MySQL&lt;/li&gt;
&lt;li&gt;110 серверов Redis&lt;/li&gt;
&lt;li&gt;60 серверов memcached&lt;/li&gt;
&lt;li&gt;60 + 2&amp;nbsp;сервера&amp;nbsp;для отложенной обработки задач на основе Redis&lt;/li&gt;
&lt;li&gt;25 разработчиков&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="vybor"&gt;Выбор&lt;/h2&gt;
&lt;h4&gt;Почему Amazon Ec2/S3?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Очень хорошая надежность, отчетность и поддержка&lt;/li&gt;
&lt;li&gt;Хорошие дополнительные сервисы: кэш, базы данных, балансировка
    нагрузки, &lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt; и т.п.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Новые виртуальные машины готовы за считанные секунды&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Почему MySQL?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Очень "зрелая", хорошо известная и любимая многими&lt;/li&gt;
&lt;li&gt;Редки катастрофичные потери данных&lt;/li&gt;
&lt;li&gt;Линейная зависимость времени отклика от частоты запросов&lt;/li&gt;
&lt;li&gt;Хорошая поддержка сторонним ПО (XtraBackup, Innotop, Maatkit)&lt;/li&gt;
&lt;li&gt;Надежное активное сообщество&lt;/li&gt;
&lt;li&gt;Отличная поддержка от Percona&lt;/li&gt;
&lt;li&gt;Бесплатна&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Почему memcached?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Очень "зрелый", отличная производительность, хорошо известный и
    любимый многими&lt;/li&gt;
&lt;li&gt;Никогда не ломается&lt;/li&gt;
&lt;li&gt;Бесплатен&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Почему Redis?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Много удобных &lt;strong&gt;структур данных&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Поддержка персистентности и репликации&lt;/li&gt;
&lt;li&gt;Также многим известен и нравится&lt;/li&gt;
&lt;li&gt;Стабильно хорошая производительность и надежность&lt;/li&gt;
&lt;li&gt;Также бесплатен&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;h4&gt;Сlustering vs Sharding&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Большую часть презентации, на основе которой написана данная статья
    (&lt;a href="https://www.insight-it.ru/goto/10005efe/" rel="nofollow" target="_blank" title="http://www.slideshare.net/eonarts/mysql-meetup-july2012scalingpinterest"&gt;ссылка&lt;/a&gt;,
    если не охота листать до секции источников информации), занимает
    раздел под названием "Clustering vs Sharding". В связи с путаницей в
    терминологии пришлось несколько раз перечитывать, чтобы понять к
    чему они клонят, сейчас попробую объяснить.&lt;/li&gt;
&lt;li&gt;Вообще есть два фундаментальных способа распределить данные между
    несколькими серверами:&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Вертикально:&lt;/strong&gt;&amp;nbsp;разные таблицы (или просто логически разные
    типы данных) разносятся на разные сервера.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Горизонтально:&lt;/strong&gt; каждая таблица разбивается на некоторое
    количество частей и эти части разносятся на разные сервера по
    определенному алгоритму.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;С первого взгляда казалось, что они пытаются вертикальное разбиение
    назвать &lt;em&gt;sharding&lt;/em&gt;, а горизонтальное - &lt;em&gt;clustering&lt;/em&gt;. Хотя вообще они
    почти синонимы и на русский я их обычно примерно одинаково перевожу.&lt;/li&gt;
&lt;li&gt;По факту же оказалось, что под словом clustering они понимают все
    программные продукты для хранения данных, которые имеют встроенную
    поддержку работы в кластере. В частности они имеют ввиду
    &lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt;, &lt;a href="/tag/membase/"&gt;Membase&lt;/a&gt;,
    &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt; и &lt;a href="/tag/riak/"&gt;Riak&lt;/a&gt;, которые прозрачно для
    пользователя горизонтально распределяют данные по кластеру.&lt;/li&gt;
&lt;li&gt;За словом &lt;em&gt;sharding&lt;/em&gt; в их терминологии стоит аналогичная схема
    собственной разработки, использующая огромное количество логических
    БД в MySQL, распределенных между меньшим количеством &lt;del&gt;физических серверов&lt;/del&gt; виртуальных машин. Именно по этому пути и пошли в
    Pinterest, плюс очень похожий подход используется в
    &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-facebook/"&gt;Facebook&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;От себя добавлю, что хоть при наличии должных ресурсов разработка
    собственной системы распределения данных и может быть
    целесообразной, в большинстве случаев на начальном этапе проще
    основываться на готовых решениях вроде перечисленных выше. К слову в
    opensource доступны и основанные на MySQL подобные решения:&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/vitess/"&gt;Vitess&lt;/a&gt; от &lt;a href="/tag/google/"&gt;Google&lt;/a&gt; /
    &lt;a href="/tag/youtube/"&gt;YouTube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/flockdb/"&gt;FlockDB&lt;/a&gt; от &lt;a href="/tag/twitter/"&gt;Twitter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;В их проекте данная подсистема развивалась следующим образом:&lt;ul&gt;
&lt;li&gt;1 БД + внешние ключи + join'ы&amp;nbsp;&amp;rarr;&lt;/li&gt;
&lt;li&gt;1 БД + денормализация + кэш&amp;nbsp;&amp;rarr;&lt;/li&gt;
&lt;li&gt;1 БД + master/slave + кэш&amp;nbsp;&amp;rarr;&lt;/li&gt;
&lt;li&gt;несколько функциональных разделенных БД + master/slave + кэш&amp;nbsp;&amp;rarr;&lt;/li&gt;
&lt;li&gt;вертикально и горизонтально разделенные БД (по
    идентификаторам) + по резервные БД (пассивный slave) + кэш&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;При использовании аналогичного решения остерегайтесь:&lt;ul&gt;
&lt;li&gt;Невозможности выполнять большинство запросов с join&lt;/li&gt;
&lt;li&gt;Отсутствия транзакций&lt;/li&gt;
&lt;li&gt;Дополнительных манипуляций для поддержания ограничений
    уникальности&lt;/li&gt;
&lt;li&gt;Необходимости тщательного планирования для изменений схемы&lt;/li&gt;
&lt;li&gt;Необходимости выполнения одного и того же запроса с последующей
    агрегацией для построения отчетов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Остальные моменты&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Кэширование многоуровневое:&lt;ul&gt;
&lt;li&gt;Коллекции объектов хранятся в списках Redis&lt;/li&gt;
&lt;li&gt;Сами объекты - в memcached&lt;/li&gt;
&lt;li&gt;На уровне SQL запросы в основном примитивны и написаны вручную,
    так что часты попадания в кэш MySQL&lt;/li&gt;
&lt;li&gt;Кэш файловой системы - само собой&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Еще пара фактов про кэширование в Pinterest:&lt;ul&gt;
&lt;li&gt;Кэш разбит также на несколько частей (шардов), для упрощения
    обслуживания и масштабирования&lt;/li&gt;
&lt;li&gt;В коде для кэширования используются Python'овские декораторы, на
    вид собственной разработки, хотя точно не уверен&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Балансировка нагрузки осуществляется в первую очередь за счет Amazon
    ELB, что позволяет легко подключать/отключать новые сервера
    посредством API.&lt;/li&gt;
&lt;li&gt;Так как большинство пользователей живут в США по ночам нагрузка
    сильно падает, что позволяет им по ночам отключать до 40%
    виртуальных машин. В пиковые часы EC2 обходится порядка 52$ в час,
    а по ночам - всего 15$.&lt;/li&gt;
&lt;li&gt;Elastic Map Reduce, основанный на Hadoop, используется для анализа
    данных и стоит всего несколько сотен долларов в месяц&lt;/li&gt;
&lt;li&gt;Текущие проблемы:&lt;ul&gt;
&lt;li&gt;Масштабирование команды&lt;/li&gt;
&lt;li&gt;Основанная на сервисах архитектура:&lt;ul&gt;
&lt;li&gt;Ограничения соединений&lt;/li&gt;
&lt;li&gt;Изоляция функционала&lt;/li&gt;
&lt;li&gt;Изоляция доступа (безопасность)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="uroki-ot-komandy-pinterest"&gt;Уроки от команды Pinterest&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;"Оно сломается. Все должно быть просто."&lt;/em&gt; - столько раз уже слышу
    это наставление, но ни разу не видел разработчиков, которые реально
    к нему прислушивались.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;"Кластеризация - страшная штука."&lt;/em&gt; - конечно страшная, большая и
    сложная. Но кому сейчас легко?&lt;/li&gt;
&lt;li&gt;&lt;em&gt;"Продолжайте получать удовольствие."&lt;/em&gt; - с этим не могу не
    согласиться, без удовольствия работать совершенно невозможно в любой
    сфере.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/10005efe/" rel="nofollow" target="_blank" title="http://www.slideshare.net/eonarts/mysql-meetup-july2012scalingpinterest"&gt;Scaling Pinterest @ MySQL Meetup&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;В презентации можно посмотреть примеры кода и SQL-запросов&lt;/li&gt;
&lt;li&gt;Если кто-то знает где можно посмотреть/послушать запись этого
    мероприятия - поделитесь ссылкой, пожалуйста&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/ac4a03d6/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2012/5/21/pinterest-architecture-update-18-million-visitors-10x-growth.html"&gt;Pinterest Architecture Update&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/db0647b0/" rel="nofollow" target="_blank" title="http://pinterest.com/about/careers/"&gt;Вакансии в Pinterest&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 15 Aug 2012 22:26:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-08-15:highload/2012/arkhitektura-pinterest/</guid><category>Akamai</category><category>Amazon</category><category>Apache Hadoop</category><category>AWS</category><category>CDN</category><category>CloudFront</category><category>django</category><category>EC2</category><category>Edge Cast</category><category>Hadoop</category><category>Memcached</category><category>MySQL</category><category>nginx</category><category>Pinterest</category><category>Python</category><category>Redis</category><category>S3</category><category>Solr</category><category>Архитектура Pinterest</category><category>архиткутера</category><category>Масштабируемость</category></item><item><title>Архитектура Instagram</title><link>https://www.insight-it.ru//highload/2012/arkhitektura-instagram/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/a8e562b3/" rel="nofollow" target="_blank" title="https://instagram.com/"&gt;Instagram&lt;/a&gt; - всего лишь &lt;a href="/tag/ios/"&gt;iOS&lt;/a&gt;, а теперь
и &lt;a href="/tag/android/"&gt;Android&lt;/a&gt;, приложение для обмена фотографиями с
друзьями. Последнее время находится на слуху благодаря новости о покупке
проекта &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt;'ом за кругленькую сумму. Недавно один
из основателей проекта, Mike Krieger, выступил на конференции с докладом
о техническом аспекте проекта, который я и хотел бы вкратце пересказать.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Начало:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 сервер слабее Macbook Pro&lt;/li&gt;
&lt;li&gt;25к регистраций в первый день&lt;/li&gt;
&lt;li&gt;2 разработчика&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Сегодня:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;40+ миллионов пользователей&lt;/li&gt;
&lt;li&gt;100+ виртуальных серверов в EC2, в том числе:&lt;/li&gt;
&lt;li&gt;Проект куплен Facebook за &lt;em&gt;1 млрд. долл&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;1 миллион регистраций за 12 часов после запуска Android-версии&lt;/li&gt;
&lt;li&gt;5 разработчиков&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="tekhnologii"&gt;Технологии&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/ubuntu/"&gt;Ubuntu&lt;/a&gt; &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; 11.04&lt;/strong&gt; - основная
операционная система&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt;&lt;/strong&gt; - основной язык программирования серверной части&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/django/"&gt;Django&lt;/a&gt;&lt;/strong&gt; - фреймворк&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/amazon/"&gt;&lt;strong&gt;Amazon&lt;/strong&gt;&lt;/a&gt;:&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/ec2/"&gt;EC2&lt;/a&gt;&lt;/strong&gt;&amp;nbsp;- хостинг&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/elb/"&gt;ELB&lt;/a&gt;&lt;/strong&gt;&amp;nbsp;- балансировка входящих HTTP-запросов&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/route53/"&gt;Route53&lt;/a&gt;&lt;/strong&gt; - DNS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/s3/"&gt;S3&lt;/a&gt;&lt;/strong&gt; - хранение фотографий&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/cloudfront/"&gt;CloudFront&lt;/a&gt;&lt;/strong&gt; - &lt;a href="/tag/cdn/"&gt;CDN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/nginx/"&gt;nginx&lt;/a&gt;&lt;/strong&gt; - второй уровень балансировки входящихHTTP-запросов&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/gunicorn/"&gt;gunicorn&lt;/a&gt;&lt;/strong&gt; - WSGI-сервер&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/haproxy/"&gt;&lt;strong&gt;HAProxy&lt;/strong&gt;&lt;/a&gt;&amp;nbsp;- балансировка нагрузки внутри системы&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/postgresql/"&gt;&lt;strong&gt;PostgreSQL&lt;/strong&gt;&lt;/a&gt; - основное хранилище данных&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/postgis/"&gt;&lt;strong&gt;postgis&lt;/strong&gt;&lt;/a&gt; - поддержка гео-запросов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/pgfouine/"&gt;&lt;strong&gt;pgfouine&lt;/strong&gt;&lt;/a&gt; - отчеты на основе логов&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/pgbouncer/"&gt;pgbouncer&lt;/a&gt;&lt;/strong&gt; - создание пула соединений&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/redis/"&gt;&lt;strong&gt;Redis&lt;/strong&gt;&lt;/a&gt; - дополнительное хранилище данных&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;&lt;strong&gt;Memcached&lt;/strong&gt;&lt;/a&gt; - кэширование&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/gearman/"&gt;&lt;strong&gt;Gearman&lt;/strong&gt;&lt;/a&gt; - очередь задач&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/solr/"&gt;&lt;strong&gt;Solr&lt;/strong&gt;&lt;/a&gt; - гео-поиск&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="/tag/munin/"&gt;munin&lt;/a&gt;&lt;/strong&gt;, &lt;a href="/tag/statsd/"&gt;&lt;strong&gt;statsd&lt;/strong&gt;&lt;/a&gt;, &lt;a href="/tag/pingdom/"&gt;&lt;strong&gt;pingdom&lt;/strong&gt;&lt;/a&gt; - мониторинг&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/fabric/"&gt;&lt;strong&gt;Fabric&lt;/strong&gt;&lt;/a&gt; - управление кластером&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/xfs/"&gt;&lt;strong&gt;xfs&lt;/strong&gt;&lt;/a&gt; - файловая система&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="filosofiia"&gt;Философия&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Простота&lt;/li&gt;
&lt;li&gt;Минимизация операционных издержек&lt;/li&gt;
&lt;li&gt;Использование подходящих инструментов&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="istoriia"&gt;История&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Забыли сделать &lt;strong&gt;favicon.ico&lt;/strong&gt; до запуска - в первый же день логи
пестрили ошибками 404&lt;/li&gt;
&lt;li&gt;Для хранения данных использовали просто &lt;strong&gt;Django &lt;a href="/tag/orm/"&gt;ORM&lt;/a&gt;&lt;/strong&gt; и
&lt;strong&gt;PostgreSQL&lt;/strong&gt; (из-за postgis)&lt;/li&gt;
&lt;li&gt;Начали с одного слабого сервера, после успешного запуска решили
переехать на &lt;strong&gt;EC2&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Довольно быстро пришлось вынести &lt;a href="/tag/subd/"&gt;СУБД&lt;/a&gt; на отдельный сервер
(виртуальный, естественно)&lt;/li&gt;
&lt;li&gt;Количество фотографий продолжало расти и расти, даже самый большой
инстанс &lt;strong&gt;EC2&lt;/strong&gt; не справлялся&lt;/li&gt;
&lt;li&gt;Решили вертикально разделить данные на несколько баз, с использованием
механизма &lt;strong&gt;routers&lt;/strong&gt; из ORM, параллельно избавившись от внешних ключей&lt;/li&gt;
&lt;li&gt;Через несколько месяцев суммарный размер базы данных перевалил за 60Гб и
перестало справляться и это решение&lt;/li&gt;
&lt;li&gt;Следующим шагом стало горизонтальное разбиение данных &lt;em&gt;(sharding)&lt;/em&gt;:&lt;/li&gt;
&lt;li&gt;Создали несколько тысяч логических баз данных.&lt;/li&gt;
&lt;li&gt;Распределили их по существенно меньшему количеству физических серверов (читай: виртуальных машин).&lt;/li&gt;
&lt;li&gt;Написали свой механизм определения где искать какую базу данных, с поддержкой миграции (вероятно тоже на основе routers).&lt;/li&gt;
&lt;li&gt;По последним данным под &lt;strong&gt;PostgreSQL&lt;/strong&gt; используется 12+12 виртуальных
машин с максимальной оперативной памятью (68.4Гб), а также сетевые диски
EBS, объединенные в программный RAID посредством mdadm. Это необходимо,
чтобы весь массив данных помещался в памяти, EBS не в состоянии
обеспечить достаточную производительность.&lt;/li&gt;
&lt;li&gt;С некоторыми задачами лучше справляется &lt;strong&gt;Redis&lt;/strong&gt;:&lt;/li&gt;
&lt;li&gt;Для каждого пользователя в Redis есть список идентификаторов новых
фотографий от других пользователей, на которых он подписан.&lt;/li&gt;
&lt;li&gt;При отображении потока новых для пользователя фотографий делается
выборка части такого списка, после чего посредством multiget достается
подробная о них информация из memcached.&lt;/li&gt;
&lt;li&gt;Пробовали возложить на него задачу хранения списков подписчиков, но в
итоге вернулись к решению на &lt;strong&gt;PostgreSQL&lt;/strong&gt; с небольшим кэшированием.&lt;/li&gt;
&lt;li&gt;В Redis также хранится информация о сессиях.&lt;/li&gt;
&lt;li&gt;Несколько фактов о Redis:&lt;ul&gt;
&lt;li&gt;Так как все находится в памяти - очень быстрые операции записи и работы с множествами.&lt;/li&gt;
&lt;li&gt;Является не заменой, а дополнением к основному хранилищу данных.&lt;/li&gt;
&lt;li&gt;Redis хорош для структур данных, которые относительно ограничены.&lt;/li&gt;
&lt;li&gt;Отлично подходит для кэширования комплексных структур данных, где нужно большее, чем просто получить значение по ключу (например - счетчики, подмножества, проверка вхождения в множества).&lt;/li&gt;
&lt;li&gt;Механизм репликации (посредством slaveof) позволяет легко
масштабировать операции чтения.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Пользователи синхронно загружают фотографии на медиа-сервер с
(опциональными) заголовком и месте на карте, все остальное происходит
асинхронно посредством очередей, например:&lt;ul&gt;
&lt;li&gt;Сохраняются гео-метки, обновляется &lt;strong&gt;Solr&lt;/strong&gt; (который впоследствии заменил postgis).&lt;/li&gt;
&lt;li&gt;Идентификатор нового фото добавляется в обсуждавшиеся выше списки для всех подписчиков автора.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Поначалу использовали &lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; + &lt;code&gt;mod_wsgi&lt;/code&gt; для запуска
&lt;strong&gt;Django&lt;/strong&gt;, впоследствии перешли к gunicorn из-за меньшего потребления
ресурсов и простоты настройки.&lt;/li&gt;
&lt;li&gt;С недавних пор начали использовать&amp;nbsp;&lt;strong&gt;Amazon ELB&lt;/strong&gt;&amp;nbsp;вместо &lt;strong&gt;DNS
round-robin&lt;/strong&gt; для первичной балансировки входяших HTTP-запросов, что
позволило:&lt;/li&gt;
&lt;li&gt;избежать необходимости дешифровки &lt;a href="/tag/ssl/"&gt;&lt;strong&gt;SSL&lt;/strong&gt;&lt;/a&gt; посредством nginx;&lt;/li&gt;
&lt;li&gt;ускорить исключение из балансировки проблемных серверов.&lt;/li&gt;
&lt;li&gt;Благодаря использованию &lt;strong&gt;xfs&lt;/strong&gt; есть возможность "замораживать" и
"размораживать" дисковые массивы при резервном копировании.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Многие проблемы с масштабируемостью - результат банальных
    человеческих ошибок.&lt;/li&gt;
&lt;li&gt;Масштабирование = замена всех деталей в машине на скорости 150 км/ч.&lt;/li&gt;
&lt;li&gt;Заранее сложно узнать как в основном будут обращаться к данным, без
    реального использования.&lt;/li&gt;
&lt;li&gt;В первую очередь попытайтесь адаптировать известные Вам технологии и
    инструменты для создания простого и понятного решения, прежде чем
    бросаться на поиски чего-то нетривиального.&lt;/li&gt;
&lt;li&gt;Дополните свое основное хранилище более гибким компонентом, вроде
    Redis.&lt;/li&gt;
&lt;li&gt;Постарайтесь не использовать два инструмента для решения одной и той
    же задачи.&lt;/li&gt;
&lt;li&gt;Оставайтесь гибкими и ловкими = напоминайте себе о том, что на самом
    деле имеет значение.&lt;/li&gt;
&lt;li&gt;Разрабатывайте решения, к которым не придется постоянно возвращаться
    из-за их сбоев.&lt;/li&gt;
&lt;li&gt;Активное юнит- и функциональное тестирование стоят потраченного на
    них времени.&lt;/li&gt;
&lt;li&gt;DRY: не делайте одну и ту же работу несколько раз.&lt;/li&gt;
&lt;li&gt;Слабая связанность посредством уведомлений или сигналов позволяет
    легко менять структуру проекта.&lt;/li&gt;
&lt;li&gt;Дисковый ввод-вывод часто оказывается узким местом, особенно на EC2.&lt;/li&gt;
&lt;li&gt;Спускаться до C нужно только при необходимости, большую часть работы
    лучше делать в Python.&lt;/li&gt;
&lt;li&gt;Короткий цикл разработки - залог быстрого развития.&lt;/li&gt;
&lt;li&gt;Частые совместные рассмотрения кода нужны, чтобы все были в курсе
    происходящего.&lt;/li&gt;
&lt;li&gt;Не изобретайте велосипед.&lt;/li&gt;
&lt;li&gt;Окружите себя с толковыми &lt;a href="https://www.insight-it.ru/consulting/"&gt;консультантами&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Культура открытости вокруг разработки.&lt;/li&gt;
&lt;li&gt;Делитесь с &lt;a href="/tag/opensource/"&gt;opensource&lt;/a&gt; сообществом.&lt;/li&gt;
&lt;li&gt;Фокусируйтесь на том, что вы делаете лучше всего.&lt;/li&gt;
&lt;li&gt;Вашим пользователям абсолютно без разницы, написали ли Вы
    собственную СУБД или нет.&lt;/li&gt;
&lt;li&gt;Не переоптимизируйте и не предполагайте заранее как сайт будет
    расти.&lt;/li&gt;
&lt;li&gt;Не рассчитывайте, что "кто-то еще присоединится к команде и
    разберется с этим".&lt;/li&gt;
&lt;li&gt;Для социальных стартапов очень мало, или даже совсем нет, нерешимых
    вопросов, связанных с масштабируемостью.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="istochnik-informatsii"&gt;Источник информации&lt;/h2&gt;
&lt;p&gt;Упоминавшаяся во вступлении неприлично длинная презентация из 185
слайдов:&lt;/p&gt;
&lt;iframe data-aspect-ratio="" data-auto-height="true" frameborder="0" height="600" id="doc_73113" scrolling="no" src="//www.scribd.com/embeds/89025069/content?start_page=1&amp;amp;view_mode=scroll" width="100%"&gt;&lt;/iframe&gt;
&lt;p&gt;На видео, к сожалению, это выступление не записывалось.&lt;/p&gt;
&lt;p&gt;Часть информации взята из &lt;a href="https://www.insight-it.ru/goto/ce2d4e38/" rel="nofollow" target="_blank" title="http://instagram-engineering.tumblr.com/"&gt;технического блога Instagram&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 13 Apr 2012 20:11:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-04-13:highload/2012/arkhitektura-instagram/</guid><category>Amazon</category><category>Android</category><category>CloudFront</category><category>django</category><category>EC2</category><category>ELB</category><category>Fabric</category><category>Facebook</category><category>gearman</category><category>gunicorn</category><category>HAProxy</category><category>Intagram</category><category>iOS</category><category>Linux</category><category>Memcached</category><category>Munin</category><category>nginx</category><category>ORM</category><category>pgbouncer</category><category>pgFouine</category><category>Pingdom</category><category>postgis</category><category>PostgreSQL</category><category>Python</category><category>Redis</category><category>Route53</category><category>S3</category><category>Solr</category><category>statsd</category><category>Ubuntu</category><category>WSGI</category><category>xfs</category><category>Архитектура Instagram</category></item><item><title>Twitter Storm</title><link>https://www.insight-it.ru//highload/2012/twitter-storm/</link><description>&lt;p&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/964382fd/" rel="nofollow" target="_blank" title="https://github.com/nathanmarz/storm/"&gt;Storm&lt;/a&gt; является распределенной
системой для выполнения вычислений в реальном времени.&lt;/em&gt; Она родилась в
рамках проекта Backtype, который специализировался на аналитике твитов и
который в июле 2011 был приобретен &lt;a href="/tag/twitter/"&gt;Twitter&lt;/a&gt;. Так же как
&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; предоставляет набор
базовых абстракций, инструментов и механизмов для пакетной обработки
данных, &lt;strong&gt;Twitter Storm&lt;/strong&gt; делает это для задачи обработки данных &lt;em&gt;в
режиме реального времени&lt;/em&gt;. Хотите узнать в чем их отличие?&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="otlichie"&gt;Отличие&lt;/h2&gt;
&lt;p&gt;Не смотря на то, что &lt;em&gt;Storm&lt;/em&gt; изначально появился на свет в процессе
неудачных попыток приспособить &lt;em&gt;Hadoop&lt;/em&gt; к задаче обработки данных в
реальном времени, сравнивать их некорректно. Никакой хак или патч не
сможет заставить Hadoop работать по-настоящему в режиме реального
времени, так как в его основе лежит фундаментально другая концепция и
набор принципов, которые актуальны лишь в контексте задачи пакетной
обработки данных. &lt;strong&gt;Storm&lt;/strong&gt; можно представить как &lt;strong&gt;"Hadoop для
вычислений в реальном времени"&lt;/strong&gt;, но по факту между ними нет практически
ничего общего, кроме изначально-распределенной природы, слегка похожей
архитектуры, работы внутри JVM и публичной доступности. Для понимания
задачи, которая стоит перед Storm, лучше взглянуть на то, как она обычно
решается.&lt;/p&gt;
&lt;p&gt;Традиционно, если перед проектом или бизнесом вставала задача обработки
какой-то информации в реальном времени, то она в итоге сводилась к
цепочке преобразований данных и распределялась по серверам, которые их
выполняют и передают результаты друг другу посредством сообщений и
очередей-посредников. При таком подходе &lt;em&gt;существенная&lt;/em&gt; часть времени
уходила на маршрутизацию сообщений, настройку и развертывание новых
промежуточных очередей и обработчиков, обеспечение отказоустойчивости и
надежности. По сути &lt;strong&gt;Storm&lt;/strong&gt; берет все вышеперечисленное на себя,
позволяя разработчикам сосредоточиться на реализации логики обработки
сообщений.&lt;/p&gt;
&lt;h2 id="osobennosti"&gt;Особенности&lt;/h2&gt;
&lt;p&gt;Итак, основные особенности Storm, вытекающие из требований к подобным
системам:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Три основных варианта использования, но ими он не ограничивается:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Обработка потоков сообщений&lt;/strong&gt; &lt;em&gt;(stream processing)&lt;/em&gt; в реальном
времени, с возможностью внесения изменений во внешние базы данных;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Постоянные вычисления&lt;/strong&gt;&amp;nbsp;&lt;em&gt;(continuous computation)&lt;/em&gt; на основе
источников данных с публикацией результатов произвольным клиентам в
реальном времени;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Распределенные удаленные вызовы&lt;/strong&gt; &lt;em&gt;(distributed RPC)&lt;/em&gt; с
выполнением комплексных вычислений параллельно во время запроса.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Масштабируемость:&lt;/strong&gt; Storm может обрабатывать огромное количество
сообщений в секунду. Для масштабирование необходимо лишь добавить
сервера в кластер и увеличить параллельность в настройках топологии. В
одном из первых приложений для Storm обрабатывался 1 миллион сообщений в
секунду на кластере из 10 серверов, при этом выполнялось несколько сотен
запросов в секунду к внешней базе данных.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Гарантия отсутствия потерь данных:&lt;/strong&gt;&amp;nbsp;в отличии от других систем
обработки сообщений в реальном времени (например
&lt;a href="https://www.insight-it.ru/goto/9cb3bfa0/" rel="nofollow" target="_blank" title="http://incubator.apache.org/s4"&gt;S4&lt;/a&gt; от Yahoo!) это свойство изначально
является частью архитектуры Storm.&amp;nbsp;Для этого используется механизм
подтверждения&amp;nbsp;&lt;em&gt;(acknowledgement)&lt;/em&gt;&amp;nbsp;успешной обработки каждого конкретного
сообщения.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Стабильность:&lt;/strong&gt;&amp;nbsp;в то время как &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; позволительны
простои по несколько часов, так как он априори не является системой
реального времени, одной из основных целей Storm является стабильная
бесперебойная работа кластера, с максимально безболезненным его
управлением.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Защита от сбоев:&lt;/strong&gt;&amp;nbsp;если что-то пошло не так во время выполнения
вычисления, Storm переназначит задачи и попробует снова. В его задачи
входит обеспечение бесконечной работы вычислений (или до момента
запланированной или ручной остановки).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Независимость от языка программирования:&lt;/strong&gt; в то время как большая
часть системы написана на &lt;a href="/tag/clojure/"&gt;Clojure&lt;/a&gt; и работает в
&lt;a href="/tag/jvm/"&gt;JVM&lt;/a&gt;, сами компоненты системы могут быть реализованы на
любом языке, что удобно для проектов, использующих в основном другие
технологии.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;У Вас уже могло сложиться общее представление, о том что собой
представляет &lt;strong&gt;Twitter Storm&lt;/strong&gt; и насколько он актуален лично для Вас или
Вашего проекта. Если интерес все еще не погас, предлагаю перейти к
концепции, предлагаемой Storm для разработки приложений под эту
платформу.&lt;/p&gt;
&lt;h2 id="kontseptsiia"&gt;Концепция&lt;/h2&gt;
&lt;p&gt;Для начала пройдемся по основным абстракциям, которые используются в
Storm:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Поток&lt;/strong&gt; &lt;em&gt;(Stream)&lt;/em&gt;: неограниченный поток сообщений, представленных в
виде кортежей (произвольных именованный список значений). При этом все
кортежи в одном потоке должны иметь одинаковую схему: элемент на каждой
позиции должен иметь один и тот же тип данных и значение.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Струя воды из крана&lt;/strong&gt;&amp;nbsp;&lt;em&gt;(Spout)&lt;/em&gt;: источник потоков, который берет их из какой-то внешней системы.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cтруя состояния&lt;/strong&gt;&amp;nbsp;&lt;em&gt;(state spout)&lt;/em&gt;: предоставляет распределенный доступ к некому общему состоянию, которое кэшируется в памяти на исполнителях и синхронно обновляется при внешних изменениях. Таким образом возможно избежать обращений к внешней базе данных при обработке каждого сообщения. В случае с Twitter этим общим состоянием является сам
социальный граф.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Молния&lt;/strong&gt; &lt;em&gt;(Bolt)&lt;/em&gt;: обрабатывает входящие потоки и создает исходящие
потоки, производя какую-либо обработку данных (по сути здесь реализуется
основная бизнес-логика). Помимо этого никто не запрещает использовать
при обработке какие угодно внешние сервисы вроде &lt;a href="/tag/subd/"&gt;СУБД&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Топология&lt;/strong&gt; &lt;em&gt;(Topology)&lt;/em&gt;: произвольная связанная сеть из "молний" и
"струй". При создании топологии можно указать:&lt;ul&gt;
&lt;li&gt;&lt;em&gt;уровень параллелизма&lt;/em&gt; для каждого компонента, что создаст
необходимое количество его потоков исполнения в кластере.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;группировку потоков&lt;/em&gt;, то есть как именно сообщения будут
распределяться между созданными потоками исполнения каждого
компонента, есть четыре основных варианта - случайно &lt;em&gt;(shuffle)&lt;/em&gt;,
каждый получит по копии &lt;em&gt;(all)&lt;/em&gt;, хэш по определенным полям сообщения
&lt;em&gt;(fields)&lt;/em&gt;, один поток получает все сообщения &amp;nbsp;&lt;em&gt;(global)&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Таким образом, для создания приложения для обработки данных в реальном
времени с использованием &lt;strong&gt;Storm&lt;/strong&gt;, необходимо:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Определить схему(ы) потока(ов) сообщений.&lt;/li&gt;
&lt;li&gt;Реализовать источник(и) сообщений, основанные на парсинге каких-то
    внешних данных (для Backtype это был Twitter firehose, поток всех
    твитов) или реакции на события (допустим действия пользователей в
    виде HTTP-запросов).&lt;/li&gt;
&lt;li&gt;Реализовать обработчик(и) сообщений, которые преобразуют входящие
    сообщения и либо создают новые потоки сообщений, либо как-то влияют
    на внешний мир, например изменяя что-то в базе данных (они
    используют &lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt; для этого).&lt;/li&gt;
&lt;li&gt;Объединить реализованные компоненты в топологию и запустить её на
    кластере.&lt;/li&gt;
&lt;li&gt;При необходимости оптимизировать систему, включив общее состояние в
    топологию.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;С точки зрения разработчика приложения большего знать и не нужно, но
самое интересное происходит как раз дальше. Что собой представляет
Storm-кластер и как с его помощью исполняется реализованное описанным
выше способом приложение?&lt;/p&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;Проект очень сильно завязан на &lt;a href="/tag/zookeeper/"&gt;Zookeeper&lt;/a&gt; для
координации работы кластера, с чем он очень неплохо справляется. Все
остальные компоненты системы системы не содержат в себе состояния, что
обеспечивает их быстрый запуск, даже после &lt;code&gt;kill -9&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Storm Cluster" class="responsive-img" src="https://www.insight-it.ru/images/storm-cluster.png" title="Storm Cluster"/&gt;&lt;/p&gt;
&lt;p&gt;В остальном все достаточно просто:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Мастер-сервер &lt;em&gt;(Nimbus)&lt;/em&gt; отвечает за распространение кода,
    распределение задач и мониторинг сбоев.&lt;/li&gt;
&lt;li&gt;На каждом сервере в кластере запускается процесс-надсмотрщик
    &lt;em&gt;(Supervisor)&lt;/em&gt;, который запускает локально потоки исполнения,
    отвечающие за выполнение назначенных ему компонентов топологий.&lt;/li&gt;
&lt;li&gt;Передача сообщений между компонентами топологий осуществляется
    напрямую, посредством &lt;a href="/tag/zeromq/"&gt;ZeroMQ&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Топологии являются &lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt;-структурами, а
    мастер-сервер - &lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt;-сервисом, что позволяет
    осуществлять регистрацию топологий и другие операции программно из
    любого языка программирования.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Присутствующий в единственном экземпляре мастер-сервер является
единственной точкой отказа лишь на первый взгляд. По факту он
используется лишь для внесение изменений в кластер и топологии, так что
его непродолжительное отсутствие не повлияет на функционирование
запущенных вычислений. А так как состояние кластера хранится в
&lt;em&gt;Zookeeper&lt;/em&gt;, то запуск мастера на другой машине в случае аппаратного
сбоя - вопрос лишь грамотно настроенного мониторинга и максимум одной
минуты.&lt;/p&gt;
&lt;p&gt;Используемый механизм подтверждений успешной обработки сообщения
&lt;em&gt;(acknowledgement)&lt;/em&gt; гарантирует, что все сообщения, попавшие в систему,
рано или поздно будут обработаны, даже при локальных сбоях оборудования.
Хотя более глобальные катаклизмы вроде "потери" стойки все же могут
нарушить функционирование системы, про работу в нескольких датацентрах
речь также не идет.&lt;/p&gt;
&lt;h2 id="plany-na-budushchee"&gt;Планы на будущее&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Использование &lt;a href="https://www.insight-it.ru/goto/6984b642/" rel="nofollow" target="_blank" title="http://incubator.apache.org/mesos"&gt;Mesos&lt;/a&gt; для
    распределения и изоляции вычислительных ресурсов.&lt;/li&gt;
&lt;li&gt;Изменение кода "на лету", сейчас для этого нужно остановить старую
    топологию и запустить новую, что может означать простой в пару
    минут.&lt;/li&gt;
&lt;li&gt;Автоматическое определения необходимого уровня параллельности и
    адаптация под изменения в интенсивности входящего потока сообщений.&lt;/li&gt;
&lt;li&gt;Еще более высокоуровневые абстракции.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;На самом деле подход, лежащий в основе Storm, не является чем-то
кардинально-новым. Помимо упоминавшегося выше S4 можно найти еще
несколько альтернатив, пускай и менее близких по идеологии. Подробнее
про эту тему можно узнать погуглив &lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/c81ea66c/" rel="nofollow" target="_blank" title="http://www.google.com/search?q=complex+event+processing"&gt;complex event processing&lt;/a&gt;&lt;/strong&gt;
или &lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/14ea9c79/" rel="nofollow" target="_blank" title="http://www.google.com/search?q=real-time+stream+processing"&gt;real-time stream processing&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Storm&lt;/strong&gt; выделяет из их числа простота, гибкость, масштабируемость и
отказоустойчивость в одном флаконе. Обеспечивает это в первую очередь
простая и понятная архитектура, основанная на (уже) проверенном временем
и многими проектами распределенном координаторе в виде &lt;strong&gt;Zookeeper&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Хоть за проектом и стоит крупный интернет-проект в лице
&lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-twitter-dva-goda-spustya/"&gt;Twitter&lt;/a&gt;,
он достаточно молод и нужно быть морально готовым к возможным сбоям и
неудачным моментам. Плюс не забывайте, что существенная часть написана
на &lt;strong&gt;Clojure&lt;/strong&gt; - для, пожалуй, большинства разработчиков изучение
исходников проекта будет капитальным "выносом мозга". Мое первое
знакомство с &lt;strong&gt;Lisp&lt;/strong&gt; &lt;em&gt;(Clojure - его диалект, работающий в
JVM)&lt;/em&gt;&amp;nbsp;надолго засело в памяти из-за обилия скобочек за каждым углом :)&lt;/li&gt;
&lt;li&gt;В любом случае из доступных &lt;a href="/tag/opensource/"&gt;opensource&lt;/a&gt; реализаций
систем для распределенных вычислений в реальном времени &lt;strong&gt;Storm&lt;/strong&gt;&amp;nbsp;на мой
взгляд является наиболее перспективным для применения в
интернет-проектах.&lt;/li&gt;
&lt;li&gt;Если Вашему проекту нужна лишь одна-две топологии и особо большого
кластера не планируется, то подобную схему достаточно не сложно
реализовать и просто посредством &lt;strong&gt;Zookeeper&lt;/strong&gt; + &lt;strong&gt;ZeroMQ&lt;/strong&gt; или
альтернативных технологий. Это избавит проект от возможных заморочек с
Clojure и другими "особенностями" Storm, ценой вероятно существенно
большей собственной кодовой базы, которую придется самостоятельно
тестировать и поддерживать. Какой путь ближе - команда каждого проекта
решает для себя сама.&lt;/li&gt;
&lt;li&gt;Помимо различных вариаций&amp;nbsp;&lt;strong&gt;веб-аналитики&lt;/strong&gt; заманчивыми применениями
подобной системы в Интернете может стать:&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;построение индекса для поисковых систем&lt;/strong&gt;, на сколько я знаю от
&lt;a href="/tag/mapreduce/"&gt;MapReduce&lt;/a&gt; здесь отказался только
&lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-google-2011/"&gt;Google&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;поведенческий таргетинг для рекламы&lt;/strong&gt; - собираем действия
пользователей и делаем на их основе выводы в реальном времени;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ведение рейтингов чего-либо в реальном времени&lt;/strong&gt; - в зависимости
от специфики проекта можно определять и показывать лучшие, самые
просматриваемые или самые комментируемые
статьи/фото/видео/музыку/товары/комментарии/что-нибудь-еще;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;предлагаем свои варианты в комментариях&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Удачи в построении приложений для вычислений в реальном времени и &lt;a href="/feed/"&gt;до встречи на страницах &lt;strong&gt;Insight IT&lt;/strong&gt;&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/6f56bb97/" rel="nofollow" target="_blank" title="http://www.infoq.com/presentations/Storm"&gt;Видео презентации проекта&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/fbfa7e01/" rel="nofollow" target="_blank" title="http://www.slideshare.net/nathanmarz/storm-distributed-and-faulttolerant-realtime-computation"&gt;Слайды с презентации проекта&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a027c9d7/" rel="nofollow" target="_blank" title="https://github.com/nathanmarz/storm"&gt;Репозиторий проекта&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 31 Mar 2012 00:08:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-03-31:highload/2012/twitter-storm/</guid><category>Clojure</category><category>JVM</category><category>opensource</category><category>real time</category><category>Storm</category><category>Thrift</category><category>Twitter</category><category>Twitter Storm</category><category>ZeroMQ</category><category>ZooKeeper</category></item><item><title>Архитектура YouTube 2012</title><link>https://www.insight-it.ru//highload/2012/arkhitektura-youtube-2012/</link><description>&lt;blockquote&gt;
&lt;p&gt;Выбирайте самое простое решение с наиболее общими гарантиями, которые
практически полезны.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;- Дао YouTube&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;YouTube&lt;/strong&gt; практически на протяжении всех 7 лет своего существования
является мировым лидером в сфере интернет-видео. С точки зрения
технической реализации проект оказался достаточно консервативным -
команда придерживается того же курса и стека технологий, с которых все
начиналось еще до приобретения проекта &lt;strong&gt;Google&lt;/strong&gt;. Но с 2008 года, когда
я написал первый обзор&amp;nbsp;&lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-youtube/"&gt;архитектуры YouTube&lt;/a&gt;, все же произошли интересные изменения, о которых я и хотел бы сегодня вкратце рассказать.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;4 млрд. просмотров страниц в день&lt;/li&gt;
&lt;li&gt;60 часов видео загружается каждую минуту&lt;/li&gt;
&lt;li&gt;350 миллионов устройств подключено к YouTube&lt;/li&gt;
&lt;li&gt;На февраль 2012 года в США по данным comScore:&lt;ul&gt;
&lt;li&gt;147,4 млн. уникальных зрителей&lt;/li&gt;
&lt;li&gt;16,7 млрд. просмотров видео (в октябре 2011 было больше 20 млрд.)&lt;/li&gt;
&lt;li&gt;Каждый зритель посмотрел в среднем 7 часов видео за месяц&lt;/li&gt;
&lt;li&gt;1.1 млрд. просмотров видео рекламы, суммарной длительностью в 10.8
млн. часов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="tekhnologii"&gt;Технологии&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; - операционная система&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; - основной HTTP-сервер&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/lighttpd/"&gt;lighttpd&lt;/a&gt; - отдача видео из YouTube CDN&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/zookeeper/"&gt;Zookeeper&lt;/a&gt; - распределенные блокировки, хранение
конфигураций&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/wiseguy/"&gt;wiseguy&lt;/a&gt; - FastCGI-прослойка между Apache и Python&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/pycurl/"&gt;pycurl&lt;/a&gt; - лучшая доступная реализация HTTP-клиента,
но в итоге все равно заменили на самописное низкоуровневое решение,
выиграв 8% в потреблении вычислительных ресурсов.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/spitfire/"&gt;spitfire&lt;/a&gt; - высокопроизводительный шаблонизатор на
основе абстрактного синтаксического дерева с регулируемым уровнем
оптимизации (как в gcc)&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/bson/"&gt;bson&lt;/a&gt; в качестве формата сериализации&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="/tag/bigtable/"&gt;BigTable&lt;/a&gt; - хранение изображений&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt; - используется просто как хранилище данных, версия
5.1.52 с InnoDB&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/vitess/"&gt;Vitess&lt;/a&gt; - система для масштабирования MySQL-кластера&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="vitess"&gt;Vitess&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Основная цель проекта - предоставление всех необходимых инструментов и
серверов для горизонтального масштабирования баз данных на основе MySQL,
с учетом потребностей современных интернет-проектов.&lt;/li&gt;
&lt;li&gt;Реализован на &lt;a href="/tag/go/"&gt;Go&lt;/a&gt; - все еще экзотическом языке
программирования, также родившемся в стенах &lt;a href="/tag/google/"&gt;Google&lt;/a&gt;.
Сравним по производительности с &lt;a href="/tag/c/"&gt;C++&lt;/a&gt; и &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, но
несколько более "выразителен".&lt;/li&gt;
&lt;li&gt;Опубликован в opensource 24 февраля 2012 года, совсем недавно, так что
&lt;a href="/tag/youtube/"&gt;YouTube&lt;/a&gt; - по-прежнему единственный пример его
использования на практике в крупном проекте.&lt;/li&gt;
&lt;li&gt;Готовые клиентские библиотеки пока только для&amp;nbsp;&lt;strong&gt;Python&lt;/strong&gt;&amp;nbsp;и&amp;nbsp;&lt;strong&gt;Go&lt;/strong&gt;, что
не удивительно, но есть и универсальные интерфейсы на основе HTTP и
просто TCP-сокетов.&lt;/li&gt;
&lt;li&gt;Основной формат данных - &lt;strong&gt;bson&lt;/strong&gt;, как и в &lt;a href="/tag/mongodb/"&gt;MongoDB&lt;/a&gt;, но
по словам разработчиков &lt;em&gt;Vitess&lt;/em&gt;&amp;ensp;&lt;a href="https://www.insight-it.ru/goto/b807d615/" rel="nofollow" target="_blank" title="http://code.google.com/p/vitess/source/browse/#hg%2Fgo%2Fbson"&gt;их
реализация&lt;/a&gt;
выполняет (де)сериализацию в 10-15 раз быстрее.&lt;/li&gt;
&lt;li&gt;Ядром проекта выступает &lt;strong&gt;Vtocc&lt;/strong&gt;, &amp;nbsp;SQL-прокси с RPC интерфейсом,
позволяющий перераспределять запросы от большого количества (более 10
тыс.) одновременно подключенных клиентов в сравнительно небольшое
количество соединений с базами данных. Пропускная способность порядка 10
тыс. запросов в секунду.&lt;/li&gt;
&lt;li&gt;Встроенные возможности &lt;strong&gt;Vtocc&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;парсер и анализатор SQL-запросов для оптимизации их выполнения;&lt;/li&gt;
&lt;li&gt;заполнение типичных запросов переменными с поддержкой кэширования
результатов;&lt;/li&gt;
&lt;li&gt;управление транзакциями и сроками их выполнения ("убивает"
затянувшиеся);&lt;/li&gt;
&lt;li&gt;для каждого пространства ключей (логической таблицы) можно указать
фактор репликации, что создаст необходимое количество второстепенных
баз данных в дополнение к мастеру;&lt;/li&gt;
&lt;li&gt;можно явно указать, что чтение необходимо произвести с мастера
(важно когда пользователь только что выполнил какое-то действие и
должен сразу же увидеть его результат);&lt;/li&gt;
&lt;li&gt;отдельные пулы соединений для выполнения операций чтения и записи;&lt;/li&gt;
&lt;li&gt;исключение "зависших" соединений из пулов;&lt;/li&gt;
&lt;li&gt;перезапуск без простоя системы;&lt;/li&gt;
&lt;li&gt;поддержка DML.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Партиционирование&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Во всех таблицах должна быть колонка с уникальным ключем, на основе
которого данные будут распределяться по кластеру.&lt;/li&gt;
&lt;li&gt;Партиционирование основано на диапазонах ключей, что позволяет держать
"карту" партиций в памяти и очень быстро определять где располагаются те
или иные данные, но обратной стороной медали является вероятное
возникновение "горячих" узлов в кластере, особенно при монотонно
увеличивающихся значениях ключей (рекомендуется использовать случайные).&lt;/li&gt;
&lt;li&gt;Поддерживаются ключи в виде натуральных чисел или произвольных бинарных
данных.&lt;/li&gt;
&lt;li&gt;При высокой нагрузке на одну партицию она может быть распределена на две
путем фильтрованной репликации; в дальнейшем планируется реализовать и
обратный процесс.&lt;/li&gt;
&lt;li&gt;Еще в планах:&lt;ul&gt;
&lt;li&gt;Поэтапное внесение изменений в схему данных без видимого простоя
системы;&lt;/li&gt;
&lt;li&gt;Поддержка работы в нескольких датацентрах с концентрацией
мастер-серверов в одном датацентре и использования остальных в
режиме только для чтения.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;YouTube&lt;/strong&gt; - еще один проект мирового масштаба, который с самого начала
использовал MySQL и оказался не в силах от него отказаться, не смотря на
трудности с горизонтальным масштабированием.&lt;/li&gt;
&lt;li&gt;По аналогичному пути пошли и другие проекты, схожие с &lt;strong&gt;Vitess&lt;/strong&gt;
надстройки над MySQL используются в &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-facebook/"&gt;Facebook&lt;/a&gt; и &lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-twitter-dva-goda-spustya/"&gt;Twitter&lt;/a&gt;:&lt;ul&gt;
&lt;li&gt;В &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt; она дополнена сильной интеграцией с
&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;&amp;nbsp;и сильно ограниченным интерфейсом, не
имеющим практически ничего общего с SQL.&amp;nbsp;Планы о публикации в
opensource, кажется, были, но я не слышал чтобы они воплотились в
жизнь. &lt;em&gt;// Уже почти дописав статью случайно заметил в коде, а потом
и мелким шрифтом в документации, что в Vitess тоже используется
memcached для кэширования из-за проблем со сборщиком мусора Go.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/twitter/"&gt;Twitter&lt;/a&gt;&amp;nbsp;по-прежнему использует свою связку
&lt;a href="https://www.insight-it.ru/goto/4fe0530b/" rel="nofollow" target="_blank" title="https://github.com/twitter/flockdb"&gt;FlockDB&lt;/a&gt; +
&lt;a href="https://www.insight-it.ru/goto/c4275cdf/" rel="nofollow" target="_blank" title="https://github.com/twitter/gizzard"&gt;Gizzard&lt;/a&gt;&amp;nbsp;на
&lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt;, которые уже пару лет публично доступны. В
отличии от Vitess она заточена на хранение информации о социальных
графах, по-этому сфера её применения как в Twitter, так и за его
пределами ограничена.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Vitess - пожалуй первая относительно успешная попытка построить
распределенную горизонтально масштабируемую СУБД на основе реляционной
базы данных, сохранив при этом SQL-интерфейс, пускай и с некоторыми
ограничениями.&lt;/li&gt;
&lt;li&gt;Выбирайте подходящее хранилище для каждого типа данных в системе - если
Vitess стал подходящим решением для структурированных данных вроде
информации о пользователях, метаданных видео и комментариев, это не
значит, что он хорошо (или плохо) справится, например, с медиа-файлами
вроде изображений и видео (для них в
&lt;a href="/tag/youtube/"&gt;YouTube&lt;/a&gt;&amp;nbsp;по-прежнему используют стек технологий
&lt;a href="/tag/google/"&gt;Google&lt;/a&gt;, подробности не публикуются).&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt; - вполне пригодный инструмент для реализации
бизнес-логики интернет-проектов, свет клином на &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; не
сошелся. Python предлагает широкий ассортимент инструментов для решения
любых типичных для интернет-проектов задач, хотя субъективно выбор
некоторых из них разработчиками YouTube мне кажется странным.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="card light-blue lighten-5"&gt;
&lt;div class="card-content"&gt;
        В комментариях предлагаю обсудить слабые и сильные стороны использования
        надстроек над реляционными базами данных, скажем по сравнению с
        использованием изначально-распределенных СУБД, таких как Riak, Cassandra
        и многих других. Может быть кто-то уже успел прикрутить к своему проекту
        Vitess или хотя бы FlockDB и готов поделиться впечатлениями?
    &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/fb446b63/" rel="nofollow" target="_blank" title="https://www.youtube.com/watch?v=G-lGCC4KKok"&gt;Mike Solomon на PyCon'12&lt;/a&gt; (один из первых
    разработчиков проекта)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/29524853/" rel="nofollow" target="_blank" title="http://code.google.com/p/vitess"&gt;О проекте Vitess&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/652a935e/" rel="nofollow" target="_blank" title="http://www.comscore.com/Press_Events/Press_Releases/2012/3/comScore_Releases_February_2012_U.S._Online_Video_Rankings"&gt;Статистика comScore на февраль '12&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 24 Mar 2012 16:50:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-03-24:highload/2012/arkhitektura-youtube-2012/</guid><category>bson</category><category>Go</category><category>Google</category><category>lighttpd</category><category>Linux</category><category>MySQL</category><category>pycurl</category><category>Python</category><category>spitfire</category><category>Vitess</category><category>wiseguy</category><category>YouTube</category><category>ZooKeeper</category></item><item><title>Архитектура Tumblr</title><link>https://www.insight-it.ru//highload/2012/arkhitektura-tumblr/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/e90ae6ea/" rel="nofollow" target="_blank" title="http://www.tumblr.com"&gt;&lt;strong&gt;Tumblr&lt;/strong&gt;&lt;/a&gt; - одна из самых популярных в мире
платформ для блоггинга, которая делает ставку на привлекательный внешний
вид, юзабилити и дружелюбное сообщество. Хоть проект и не особо на слуху
в России, цифры говорят сами за себя: 24й по посещаемости сайт в США
с&amp;nbsp;15 миллиардами просмотров страниц в месяц. Хотите познакомиться с
историей этого проекта, выросшего из простого стартапа?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="vvedenie"&gt;Введение&lt;/h2&gt;
&lt;p&gt;Как и всем успешным стартапам, Tumblr удалось преодолеть опасную пропать
между начинающим проектом и широко известной компанией. Поиск правильных
людей, эволюция инфраструктуры, поддержка старых решений, паника по
поводу значительного роста посещаемости от месяца к месяцу, при этом в
команде только 4 технических специалиста - все это заставляло
руководство Tumblr принимать тяжелые решения о том над чем стоит
работать, а над чем - нет. Сейчас же технический персонал расширился до
20 человек и у них достаточно энергии для преодоления всех текущих
проблем и разработки новых интересных технических решений.&lt;/p&gt;
&lt;p&gt;Поначалу Tumblr был вполне типичным большим &lt;a href="/tag/lamp/"&gt;LAMP&lt;/a&gt;
приложением. Сейчас же они двигаются в направлении модели распределенных
сервисов, построенных вокруг существенно менее распространенных
технологий. Основные усилия сейчас вкладываются в постепенный уход от
&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; в пользу более "правильных" и "современных" решений,
оформленных в виде сервисов. Параллельно с переходом к новым технологиям
идут изменения и в команде проекта: от небольшой группы энтузиастов к
полноценной команде разработчиков, имеющей четкую структуру и сферы
ответственности, но тем не менее жаждущей реализовывать новый функционал
и обустраивать совершенно новую инфраструктуру проекта.&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/centos/"&gt;CentOS&lt;/a&gt; на серверах, &lt;a href="/tag/mac-os-x/"&gt;Mac OS X&lt;/a&gt; для
    разработки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; - основной веб-сервер&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;, &lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt;, &lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt; - языки
    программирования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/finagle/"&gt;Finagle&lt;/a&gt;&amp;nbsp;- асинхронный RPC сервер и клиент&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;, &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt;&amp;nbsp;- СУБД&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;,&amp;nbsp;&lt;a href="/tag/redis/"&gt;Redis&lt;/a&gt;&amp;nbsp;- кэширование&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/varnish/"&gt;Varnish&lt;/a&gt;, &lt;a href="/tag/nginx/"&gt;nginx&lt;/a&gt; - отдача статики&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/haproxy/"&gt;HAProxy&lt;/a&gt; - балансировка нагрузки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/kestrel/"&gt;kestrel&lt;/a&gt;, &lt;a href="/tag/gearman/"&gt;gearman&lt;/a&gt; - очередь задач&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt; - сериализация&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/kafka/"&gt;Kafka&lt;/a&gt; - распределенная шина сообщений&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; - обработка статистики&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/zookeeper/"&gt;ZooKeeper&lt;/a&gt; - хранение конфигурации и состояний
    системы&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/git/"&gt;git&lt;/a&gt; - система контроля версий&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/jenkins/"&gt;Jenkins&lt;/a&gt; - непрерывное тестирование&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Около 500 миллионов просмотров страниц в день&lt;/li&gt;
&lt;li&gt;Более 15 миллиардов просмотров страниц в месяц&lt;/li&gt;
&lt;li&gt;Посещаемость растет примерно на 30% в месяц&lt;/li&gt;
&lt;li&gt;Пиковые нагрузки порядка 40 тысяч запросов в секунду&lt;/li&gt;
&lt;li&gt;Около 20 технических специалистов в команде&lt;/li&gt;
&lt;li&gt;Каждый день создается около 50Гб новых постов и 2.7Тб обновлений списков
последователей&lt;/li&gt;
&lt;li&gt;Более 1Тб статистики обрабатывается в &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; ежедневно&lt;/li&gt;
&lt;li&gt;Используется порядка 1000 серверов:&lt;ul&gt;
&lt;li&gt;500 веб-серверов c Apache и PHP-приложением&lt;/li&gt;
&lt;li&gt;200 серверов баз данных (существенная их часть - резервные)&lt;ul&gt;
&lt;li&gt;47 пулов&lt;/li&gt;
&lt;li&gt;30 партиций (шардов)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;30 серверов &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;25 серверов Redis&lt;/li&gt;
&lt;li&gt;15 серверов Varnish&lt;/li&gt;
&lt;li&gt;25 серверов HAProxy&lt;/li&gt;
&lt;li&gt;8 серверов nginx&lt;/li&gt;
&lt;li&gt;14 серверов для очередей задач&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="tipichnoe-ispolzovanie"&gt;Типичное использование&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tumblr&lt;/strong&gt; используется несколько по-другому, чем другие социальные
сети:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;При более чем 50 миллионах постов в день, каждый из них попадает в
среднем к нескольким сотням читателей. Это и не несколько
пользователей с миллионами читателей (например, популярные личности
в Twitter) и не миллиарды личных сообщений.&lt;/li&gt;
&lt;li&gt;Ориентированность на длинные публичные сообщения, полные интересной
информацией и картинками/видео, заставляет пользователей проводить
долгие часы каждый день за чтением Tumblr.&lt;/li&gt;
&lt;li&gt;Большинство активных пользователей подписывается на сотни других
блоггеров, что практически гарантирует много страниц нового контента
при каждом заходе на сайт. В других социальных сетях поток новых
сообщений переполнен ненужным контентом и толком не читается.&lt;/li&gt;
&lt;li&gt;Как следствие, при сложившемся количестве пользователей, средней
аудиторией каждого и высокой активностью написания постов, системе
приходится обрабатывать и доставлять огромное количество информации.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Публичные блоги называют Tumblelog'ами, они не так динамичны и легко
кэшируются.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Сложнее всего масштабировать Dashboard, страницу, где пользователи в
реальном времени читают что нового у блоггеров, на которых они
подписаны:&lt;ul&gt;
&lt;li&gt;Кэширование практически бесполезно, так как для активных
пользователей запросы редко повторяются.&lt;/li&gt;
&lt;li&gt;Информация должна отображаться в реальном времени, быть целостной и
не "задерживаться".&lt;/li&gt;
&lt;li&gt;Около 70% просмотров страниц приходится именно на Dashboard, почти
все пользователи им пользуются.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="staraia-arkhitektura"&gt;Старая архитектура&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Когда проект только начинался, Tumblr размещался в Rackspace и последние выдавали каждому блогу с собственным доменом A-запись. Когда они переросли Rackspace, они не смогли полноценно мигрировать в новый
датацентр, в том числе из-за количества пользователей. Это было в 2007
году, но у них по-прежнему часть доменов ведут на Rackspace и
перенаправляются в новый датацентр с помощью HAProxy и Varnish. Подобных
"унаследованных" проблем у проекта очень много.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;С технической точки зрения проект прошел по пути типичной эволюции
&lt;strong&gt;LAMP&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Исторически разработан на &lt;strong&gt;PHP&lt;/strong&gt;, все началось с веб-сервера,
сервера баз данных и начало потихоньку развиваться.&lt;/li&gt;
&lt;li&gt;Чтобы справляться с нагрузкой они начали использовать memcache,
затем добавили кэширование целых страниц и статических файлов, потом
поставили HAProxy перед кэшами, после чего сделали партиционирование
на уровне &lt;strong&gt;MySQL&lt;/strong&gt;, что сильно облегчило им жизнь.&lt;/li&gt;
&lt;li&gt;Они делали все, чтобы выжать максимум из каждого сервера.&lt;/li&gt;
&lt;li&gt;Было разработано два сервиса на C: генератор уникальных
идентификаторов на основе HTTP и libevent, а также
&lt;a href="https://www.insight-it.ru/goto/533d5aae/" rel="nofollow" target="_blank" title="http://engineering.tumblr.com/post/7819252942/staircar-redis-powered-notifications"&gt;Staircar&lt;/a&gt;,
использующий Redis для обеспечения уведомлений в реальном времени на
Dashboard.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dashboard использует подход
&lt;a href="https://www.insight-it.ru/goto/f4d9020c/" rel="nofollow" target="_blank" title="http://www2.parc.com/istl/projects/ia/papers/sg-sigir92/sigir92.html"&gt;"разбрасывать-собирать"&lt;/a&gt;,
так как из-за отсортировонности данных по времени традиционные схемы
партиционирования работали не очень хорошо. По их прогнозам текущая
реализация позволит им рости еще в течении полугода.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="novaia-arkhitektura"&gt;Новая архитектура&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Приоритетным направлением стали технологии, основанные на JVM, по
причине более быстрой разработки и доступности квалифицированных
кадров. Мотивация несколько спорная, особенно если учесть, что речь
идет в первую очередь о &lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt;, а не
о&amp;nbsp;&lt;a href="/tag/scala/"&gt;Java&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Основная цель - вынести все из PHP приложения в отдельные сервисы, что
сделает его лишь тонким клиентом к внутреннему API.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Почему выбор пал именно на &lt;strong&gt;Scala&lt;/strong&gt; и &lt;strong&gt;Finagle&lt;/strong&gt;?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Многие разработчики имели опыт с Ruby и PHP, так что Scala был
привлекательным (цитата, логики мало)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/d7e3c54e/" rel="nofollow" target="_blank" title="https://github.com/twitter/finagle"&gt;Finagle&lt;/a&gt; был одним из основных
факторов в пользу JVM: это библиотека, разработанная в Twitter,
которая решает большинство распределенных задач вроде маршрутизации
запросов и обнаружение/регистрацию сервисов - не пришлось
реализовывать это все с нуля.&lt;/li&gt;
&lt;li&gt;В Scala не принято использовать общие состояния, что избавляет
разработчиков от забот с потоками выполнения и блокировками.&lt;/li&gt;
&lt;li&gt;Им очень нравится Thrift в роли программного интерфейса из-за его
высокой производительности (он кроссплатформенный и к JVM никак не
относится)&lt;/li&gt;
&lt;li&gt;Нравится &lt;a href="/tag/netty/"&gt;Netty&lt;/a&gt;, но не хочется связываться с Java, еще
один аргумент в пользу Scala.&lt;/li&gt;
&lt;li&gt;Рассматривали &lt;a href="/tag/node-js/"&gt;Node.js&lt;/a&gt;, но отказались так как под
JVM проще найти разработчиков, а также из-за отсутствия стандартов,
"лучших практик" и большого количества качественно протестированного
кода.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Старые внутренние сервисы также переписываются с C + libevent на Scala + Fingle.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Был создан общий каркас для построения внутренних сервисов:&lt;ul&gt;
&lt;li&gt;Много усилий было приложено для автоматизации управления
распределенной системой.&lt;/li&gt;
&lt;li&gt;Создан аналог скаффолдинга - используется некий шаблон для создания
каждого нового сервиса.&lt;/li&gt;
&lt;li&gt;Все сервисы выглядят одинаково с точки зрения системного
администратора: получение статистики, мониторинг, запуск и остановка
реализованы одинаково для всех сервисов.&lt;/li&gt;
&lt;li&gt;Созданы простые инструменты для сборки сервисов без вникания в
детали используемых стандартных решений.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Используется 6 внутренних сервисов, над которыми работает отдельная
команд. На запуск сервиса с нуля уходит около 2-3 недель.&lt;/li&gt;
&lt;li&gt;Новые, нереляционные СУБД, такие как HBase и Redis, вводятся в
эксплуатацию, но основным хранилищем по-прежнему остается сильно
партиционированный MySQL.&lt;/li&gt;
&lt;li&gt;HBase используется для сервиса сокращенных ссылок для постов, а также
всех исторических данных и аналитики. HBase хорошо справляется с
ситуациями, где необходимы миллионы операций записи в секунду, но он не
достаточно стабилен, чтобы полностью заменить проверенное временем
решение на MySQL в критичных для бизнеса задачах.&lt;/li&gt;
&lt;li&gt;Партиционированный MySQL плохо справляется с отсортированными по времени данными, так как один из серверов всегда оказывается существенно более "горячим", чем остальными. Также сталкивались с значительными задержками в репликации из-за большого количества параллельных операций добавления данных.&lt;/li&gt;
&lt;li&gt;Используется 25 серверов Redis с 8-32 процессами на каждом, что означает
порядка 300-400 экземпляров Redis в сумме.&lt;ul&gt;
&lt;li&gt;Используется для уведомлений в реальном времени на Dashboard (о
событиях вроде "кому-то понравился Ваш пост").&lt;/li&gt;
&lt;li&gt;Высокое соотношений операций записи к операциям чтения сделало MySQL
не очень подходящим кандидатом.&lt;/li&gt;
&lt;li&gt;Уведомления не так критичны, их потеря допустима, что позволило
отключить персистентность Redis.&lt;/li&gt;
&lt;li&gt;Был создан интерфейс между Redis и отложенными задачами в Finagle.&lt;/li&gt;
&lt;li&gt;Сервис коротких ссылок также использует Redis как кэш, а HBase для
постоянного хранения.&lt;/li&gt;
&lt;li&gt;Вторичный индекс Dashboard также построен вокруг Redis.&lt;/li&gt;
&lt;li&gt;Redis также используется для хранения задач Gearman, для чего был
написан memcache proxy на основе Finale.&lt;/li&gt;
&lt;li&gt;Постепенно отказываются от memcached в пользу Redis в роли основного
кэша. Производительность у них сопоставима.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Внутренним сервисам необходим доступ к потоку всех событий в системе
(создание, редактирование и удаление постов, нравится или не нравится и
т.п.), для чего была созданна внутренняя шина сообщений &lt;em&gt;(англ.
firehose, пожарный шланг)&lt;/em&gt;:&lt;ul&gt;
&lt;li&gt;Пробовали использовать в этой роли Scribe, но так как оно по сути
свелось к пропусканию логов через grep в реальном времени - нагрузки
оно не выдержало.&lt;/li&gt;
&lt;li&gt;Текущая реализация основана на Kafka, решению аналогичной задачи от
LinkedIn на Scala.&lt;/li&gt;
&lt;li&gt;MySQL также не рассматривался из-за большой доли операций записи.&lt;/li&gt;
&lt;li&gt;Внутри сервисы используют HTTP потоки для чтения данных, хотя Thrift
интерфейс также используется.&lt;/li&gt;
&lt;li&gt;Поток сообщений хранит события за последнюю неделю с возможностью
указать момент времени с которого считывать данные при открытии
соединения.&lt;/li&gt;
&lt;li&gt;Поддерживается абстракция "группы потребителей", которая позволяет
группе клиентов вместе обрабатывать один поток данных вместе и
независимо, то есть одно и то же сообщение не попадет дважды к
клиентам из одной группы.&lt;/li&gt;
&lt;li&gt;ZooKeeper используется для периодического сохранения текущей позиции
каждого клиента в потоке.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Новая архитектура Dashboard основана на принципе ячеек или ящиков
входящих сообщений:&lt;ul&gt;
&lt;li&gt;Каждая "ячейка" отвечает за группу пользователей и читает новые события
с шины сообщений, если один из её пользователей-подопечных подписан на
автора только что опубликованного поста, то пост добавляется в "почтовый
ящик" подписанного пользователя.&lt;/li&gt;
&lt;li&gt;Когда пользователь заходит в Dashboard его запрос попадает в его ячейку,
которая возвращает ему нужную часть непрочитанных постов.&lt;/li&gt;
&lt;li&gt;Каждая ячейка состоит из трех групп серверов:&lt;ul&gt;
&lt;li&gt;HBase для постоянного хранения копий постов и почтовых ящиков;&lt;/li&gt;
&lt;li&gt;Redis для кэширование свежих данных;&lt;/li&gt;
&lt;li&gt;Сервис, читающий данные из шины и предоставляющий доступ к ящикам
посредством Thrift.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;В HBase используется две таблицы:&lt;ul&gt;
&lt;li&gt;Отсортированный &lt;strong&gt;список идентификаторов постов&lt;/strong&gt; для каждого
пользователя в ячейке, именно в том виде, как они будут отображены в
итоге.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Копии всех постов&lt;/strong&gt; по идентификаторам, что позволяет выдать все
данные для отрисовки Dashboard без обращений к серверам вне одной
ячейки.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ячейки представляют собой независимые единицы, что позволяет легко
масштабировать систему при росте числа пользователей.&lt;/li&gt;
&lt;li&gt;Платой за относительно безболезненность масштабирования является
чрезвычайная избыточность данных: при том что ежедневно создается лишь
50Гб постов, суммарный объем данных в ячейках растет на 2.7Тб в день.&lt;/li&gt;
&lt;li&gt;Альтернативой было бы использование общего кластера со всеми постами, но
тогда он бы стал единственной точкой отказа и потребовалось бы делать
дополнительные удаленные запросы. Помимо этого выигрыш по объему был бы
не велик - списки идентификаторов занимают значительно больше места, чем
сами посты.&lt;/li&gt;
&lt;li&gt;Пользователи, которые подписаны или на которых подписаны миллионы других
пользователей, обрабатываются отдельно - страницы с их постами
генерируются не заранее (как описывалось выше), а при поступлении
запроса - это позволяет не тратить впустую много ресурсов (этот подход
называется выборочная материализация,
&lt;a href="https://www.insight-it.ru/goto/ab0d48a1/" rel="nofollow" target="_blank" title="http://research.yahoo.com/pub/3203"&gt;подробнее&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Количество пользователей в одной ячейке позволяет управлять балансом
между уровнем надежности и стоимостью содержания этой подсистемы.&lt;/li&gt;
&lt;li&gt;Параллельное чтение их шины сообщений оказывает серьезную нагрузку на
сеть, в дальнейшем из ячеек можно будет составить иерархию: только часть
будет читать напрямую из шины сообщений, а остальным сообщения будут
ретранслироваться.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tumblr географически по-прежнему находится в одном датацентре (если не
считать незначительное присутствие в Rackspace), распределение по
нескольким лишь в планах.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="razvertyvanie"&gt;Развертывание&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Начиналось как несколько rsync-скриптов для распространения
    PHP-приложения. Как только машин стало больше 200 такой подход стал
    занимать слишком много времени.&lt;/li&gt;
&lt;li&gt;Следующий вариант был основан на &lt;a href="/tag/capistrano/"&gt;Capistrano&lt;/a&gt;:
    были созданы три стадии процесса развертывания (разработка,
    тестирование, боевой). Неплохо справлялся с десятками серверов, но
    на сотнях также был слишком медленным, так как основывался на SSH.&lt;/li&gt;
&lt;li&gt;Итоговый вариант основан на &lt;strong&gt;Func&lt;/strong&gt;, решении от
    &lt;a href="/tag/redhat/"&gt;RedHat&lt;/a&gt;, позволившим заменить &lt;a href="/tag/ssh/"&gt;SSH&lt;/a&gt; на
    более легковесный протокол.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="razrabotka"&gt;Разработка&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Поначалу философия была такова, что каждый мог использовать любые
технологии, которые считал уместным. Но довольно скоро пришлось
стандартизировать стек технологий, чтобы было легче нанимать и вводить в
работу новых сотрудников, а также для более оперативного решения
технических проблем.&lt;/li&gt;
&lt;li&gt;Каждый разработчик имеет одинаковую заранее настроенную рабочую станцию,
которая обновляется посредством &lt;a href="/tag/puppet/"&gt;Puppet&lt;/a&gt;:&lt;ul&gt;
&lt;li&gt;Настроена публикация изменений, тестирование и развертывание новых
версий.&lt;/li&gt;
&lt;li&gt;Разработчики используют vim и Textmate.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Новый PHP код систематически инспектируется другими разработчиками.&lt;/li&gt;
&lt;li&gt;Внутренние сервисы подвергаются непрерывному тестированию посредством
Jenkins.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="struktura-komand"&gt;Структура команд&lt;/h2&gt;
&lt;p&gt;Проект разбит на 6 команд:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Инфраструктура:&lt;/strong&gt;&amp;nbsp;все, что ниже 5 уровня по модели OSI -
    маршрутизация, TCP/IP, DNS, оборудование и.т.п.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Платформа:&lt;/strong&gt;&amp;nbsp;разработка основного приложения, партиционирование
    SQL, взаимодействие сервисов.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Надежность (SRE):&lt;/strong&gt;&amp;nbsp;сфокусирована на текущие потребности с точки
    зрения надежности и масштабируемости.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Сервисы:&lt;/strong&gt;&amp;nbsp;занимается более стратегической разработкой того, что
    понадобится через один-два месяца.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Эксплуатация:&lt;/strong&gt;&amp;nbsp;отвечает за обнаружение и реагирование на
    проблемы, плюс тонкая настройка.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="naim"&gt;Найм&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;На интервью они обычно избегают математики и головоломок, основной
    упор идет в основном именно на те вещи, которым придется заниматься
    кандидату.&lt;/li&gt;
&lt;li&gt;Основной вопрос: будет ли он успешно решать поставленные задачи?
    Цель в том, чтобы найти отличных людей, а не в том, чтобы никого не
    брать.&lt;/li&gt;
&lt;li&gt;Разработчиков обязательно просят привести пример своего кода, даже
    во время телефонных интервью.&lt;/li&gt;
&lt;li&gt;Во время интервью кандидатов не ограничивают в наборе инструментов,
    можно даже гуглить.&lt;/li&gt;
&lt;li&gt;Поиск людей с опытом в крупных проектах достаточно сложен, так как
    всего нескольких компаниях по всему миру решают подобные проблемы.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Автоматизация - ключ к успеху крупного проекта.&lt;/li&gt;
&lt;li&gt;При партиционировании MySQL может масштабироваться, но лишь при
    преобладании операций чтения.&lt;/li&gt;
&lt;li&gt;Redis с отключенной персистентностью легко может заменить memcached.&lt;/li&gt;
&lt;li&gt;Scala достойно себя проявляет в роли языка программирования для
    внутренних сервисов, во многом благодаря обширной Java-экосистеме.&lt;/li&gt;
&lt;li&gt;Внедряйте новые технологии постепенно, поначалу работать с HBase и
    Redis было очень болезненно, они были включены в основной стек
    технологий только после испытаний в некритичных сервисах и
    подпроектах, где цена ошибки не так велика.&lt;/li&gt;
&lt;li&gt;Проект должен строиться вокруг навыков его команды, а не наоборот.&lt;/li&gt;
&lt;li&gt;Нужно нанимать людей только если они вписываются в команду и в
    состоянии довести работу до результата.&lt;/li&gt;
&lt;li&gt;При выборе технологического стека одну из ключевых ролей играет
    доступность соответствующих специалистов на кадровом рынке.&lt;/li&gt;
&lt;li&gt;Читайте публикации и статьи в блогах. Ключевые аспекты архитектуры,
    включая "ячейки" и частичную материализацию были позаимствованы из
    внешних источников.&lt;/li&gt;
&lt;li&gt;Поспрашивайте своих коллег, кто-то из них мог общаться с
    специалистами из
    &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-facebook/"&gt;Facebook&lt;/a&gt;,
    &lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-twitter-dva-goda-spustya/"&gt;Twitter&lt;/a&gt;,
    &lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-google-2011/"&gt;Google&lt;/a&gt;
    или
    &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-linkedin/"&gt;LinkedIn&lt;/a&gt; -
    если нет прямого доступа, всегда можно получить нужную информацию
    через одно-два "рукопожатия".&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Статья написана на основе &lt;a href="https://www.insight-it.ru/goto/1444dc9b/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2012/2/13/tumblr-architecture-15-billion-page-views-a-month-and-harder.html"&gt;интервью&lt;/a&gt;&amp;ensp;&lt;a href="https://www.insight-it.ru/goto/d7daa0cd/" rel="nofollow" target="_blank" title="http://www.linkedin.com/in/bmatheny"&gt;Blake Matheny&lt;/a&gt;, директора по разработке платформы Tumblr.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 21 Feb 2012 16:29:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-02-21:highload/2012/arkhitektura-tumblr/</guid><category>Apache</category><category>Capistrano</category><category>CentOS</category><category>Finagle</category><category>Func</category><category>gearman</category><category>Git</category><category>Hadoop</category><category>HAProxy</category><category>HBase</category><category>jenkins</category><category>kafka</category><category>Kestrel</category><category>LAMP</category><category>Mac OS X</category><category>Memcached</category><category>MySQL</category><category>nginx</category><category>PHP</category><category>puppet</category><category>Redis</category><category>Ruby</category><category>Scala</category><category>Thrift</category><category>Tumblr</category><category>Varnish</category><category>ZooKeeper</category></item><item><title>Как дела у Plenty of Fish</title><link>https://www.insight-it.ru//highload/2012/kak-dela-u-plenty-of-fish/</link><description>&lt;p&gt;Напомню, что &lt;a href="https://www.insight-it.ru/goto/2f6c1d33/" rel="nofollow" target="_blank" title="http://www.pof.com/"&gt;Plenty of Fish&lt;/a&gt; - один из самых
популярных в мире сайтов знакомств, созданный одним человеком и
работающий на всего нескольких серверах и коммерческих технологиях. На
&lt;strong class="trebuchet"&gt;Insight IT&lt;/strong&gt; два года назад уже была &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-plenty-of-fish/"&gt;подробная статья о технической реализации проекта&lt;/a&gt;, сегодня же я хочу поделиться свежей информацией о том, как проект развивается.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="o-proekte"&gt;О проекте&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Долгое время, около 7 лет, проект полностью разрабатывался и
    поддерживался исключительно Маркусом, его создателем&lt;/li&gt;
&lt;li&gt;Первый архитектор баз данных был нанят в июле 2011 года&lt;/li&gt;
&lt;li&gt;На сегодняшний день команда составляет около 5 сотрудников&lt;/li&gt;
&lt;li&gt;Используется стек технологий от Microsoft, а также Akamai в роли CDN&lt;/li&gt;
&lt;li&gt;Расходы на техническое обеспечение проекта составляют порядка 70
    тысяч долларов в месяц&lt;/li&gt;
&lt;li&gt;Ежегодные доходы от рекламы измеряются миллионами долларов&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika-na-noiabr-2011"&gt;Статистика на ноябрь 2011&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;6 миллиардов просмотров страниц&lt;/li&gt;
&lt;li&gt;32 миллиарда просмотров изображений, пики до 22 тыс. в секунду, по
    ночам около 7 тыс.&lt;/li&gt;
&lt;li&gt;30 миллиардов обращений к серверам мгновенного обмена сообщениями&lt;/li&gt;
&lt;li&gt;Более 6 миллионов пользователей заходят на сайт каждое воскресенье&lt;/li&gt;
&lt;li&gt;Количество веб-серверов увеличилось с 2 до 11, что обеспечивает
    практически двухкратный запас по используемым ресурсам&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="vyvody"&gt;Выводы&lt;/h2&gt;
&lt;p&gt;Чтобы проект был очень прибыльным для своих создателей, не всегда стоит
торопиться стать большой компанией. Быть маленьким тоже выгодно:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;минимальные расходы на зарплаты, офис и техническое обеспечение,&lt;/li&gt;
&lt;li&gt;востребованный продукт благодаря способности быстро подстраиваться
    под потребности рынка,&lt;/li&gt;
&lt;li&gt;отсутствие необходимости делиться существенной частью прибыли с
    инвесторами,&lt;/li&gt;
&lt;li&gt;достаточно простая архитектура и техническая реализация,&lt;/li&gt;
&lt;li&gt;отсутствие потерь времени на бюрократию и совещания.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 12 Feb 2012 18:04:00 +0400</pubDate><guid>tag:www.insight-it.ru,2012-02-12:highload/2012/kak-dela-u-plenty-of-fish/</guid><category>Akamai</category><category>Microsoft</category><category>Plenty of Fish</category><category>POF</category><category>статистика</category></item><item><title>Архитектура Google 2011</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-google-2011/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-google/"&gt;Архитектура Google&lt;/a&gt;
была одной из первых статьей на &lt;strong class="trebuchet"&gt;Insight IT&lt;/strong&gt;. Именно она дала толчок развитию проекта: после её публикации посещаемость блога увеличилась в десятки раз и появились первые сотни подписчиков. Прошли годы, информация устаревает стремительно, так что пришло время взглянуть на Google еще раз, теперь уже с позиции конца 2011 года. Что мы увидим нового в архитектуре интернет-гиганта?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Общее&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Ежедневная аудитория Google составляет около 1 миллиарда
    человек&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;По данным Alexa больше половины аудитории интернета каждый
    день пользуются Google&lt;/li&gt;
&lt;li&gt;По данным IWS аудитория интернета составляет 2.1 миллиарда
    человек&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Используется более 900 тысяч серверов&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;Планируется расширение до 10 миллионов серверов в обозримом
    будущем&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/4cb67b65/" rel="nofollow" target="_blank" title="http://www.google.com/about/datacenters/locations/index.html"&gt;12 основных датацентров в США&lt;/a&gt;,
    присутствие в большом количестве точек по всему миру (более 38)&lt;/li&gt;
&lt;li&gt;Около 32 тысяч сотрудников в 76 офисах по всему миру&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Поиск&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;За последние 14 лет среднее время обработки одного поискового
    запроса уменьшилось с 3 секунд до менее 100 миллисекунд, то есть
    в 30 раз&lt;/li&gt;
&lt;li&gt;Более 40 миллиардов страниц в индексе, если приравнять каждую к
    листу А4 они бы покрыли территорию США в 5 слоев&lt;/li&gt;
&lt;li&gt;Более 1 квинтиллиона уникальных URL (10 в 18 степени); если
    распечатать их в одну строку, её длина составит 51 миллион
    километров, треть расстояния от Земли до Солнца&lt;/li&gt;
&lt;li&gt;В интернете встречается примерно 100 квинтиллионов слов, чтобы
    набрать их на клавиатуре одному человеку потребовалось бы
    примерно 5 миллионов лет&lt;/li&gt;
&lt;li&gt;Проиндексировано более 1.5 миллиардов изображений, чтобы их
    сохранить потребовалось бы 112 миллионов дискет, которые можно
    сложить в стопку высотой 391 километр&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gmail&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Активных пользователей более 170 миллионов&lt;/li&gt;
&lt;li&gt;Второй по популярности почтовый сервис в США, третий в мире (по
    данным comScore)&lt;/li&gt;
&lt;li&gt;При текущем темпе роста аудитории GMail и конкурентов, он станет
    лидером рынка через 2-3 года&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Google+&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Более 40 миллионов пользователей на октябрь 2011, при запуске в
    июне 2011&lt;/li&gt;
&lt;li&gt;25 миллионов пользователей за первый месяц&lt;/li&gt;
&lt;li&gt;70:30 примерное соотношение мужчин и женщин&lt;/li&gt;
&lt;li&gt;Себестоимость разработки больше полумиллиарда долларов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;YouTube&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Загружается более 13 миллионов часов видео в год&lt;/li&gt;
&lt;li&gt;Каждую минуту загружается 48 часов видео, что соответствует
    почти 8 годам контента или 52 тысячам полнометражных фильмов в
    день&lt;/li&gt;
&lt;li&gt;Более 700 миллиардов просмотров видео в год&lt;/li&gt;
&lt;li&gt;Месячная аудитория составляет 800 миллионов уникальных
    посетителей&lt;/li&gt;
&lt;li&gt;Несколько тысяч полнометражных фильмов в &lt;a href="https://www.insight-it.ru/goto/18cd7d94/" rel="nofollow" target="_blank" title="http://www.youtube.com/movies"&gt;YouTube Movies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Более 10% всех видео в формате HD&lt;/li&gt;
&lt;li&gt;13% просмотров (400 миллионов в день) происходит с мобильных
    устройств&lt;/li&gt;
&lt;li&gt;До сих пор работает в убыток, лишь 14% просмотров видео приносят
    выручку с рекламы&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Финансы&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;Выручка порядка 36 миллиардов долларов в год&lt;/li&gt;
&lt;li&gt;Прибыль после налогов порядка 10 миллиардов долларов в год&lt;/li&gt;
&lt;li&gt;Капитализация порядка 200 миллиардов долларов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Google&lt;/strong&gt; - огромная интернет-компания, неоспоримый лидер на рынке
поиска в Интернет и владелец большого количества продуктов, многие из
которых также добились определенного успеха в своей нише.&lt;/p&gt;
&lt;p&gt;В отличии от большинства интернет-компаний, которые занимаются лишь
одним продуктом (проектом), архитектура Google не может быть
представлена как единое конкретное техническое решение. Сегодня мы
скорее будем рассматривать общую стратегию технической реализации
интернет-проектов в Google, возможно слегка затрагивая и другие аспекты
ведения бизнеса в Интернет.&lt;/p&gt;
&lt;p&gt;Все продукты Google основываются на постоянно развивающейся программной
платформе, которая спроектирована с учетом работы на миллионах серверов,
находящихся в разных датацентрах по всему миру.&lt;/p&gt;
&lt;h3 id="oborudovanie"&gt;Оборудование&lt;/h3&gt;
&lt;p&gt;Обеспечение работы миллиона серверов и расширение их парка - одна из
ключевых статей расходов Google. Для минимизации этих издержек очень
большое внимание уделяется эффективности используемого серверного,
сетевого и инфраструктурного оборудования.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Использование энергии" class="right" src="https://www.insight-it.ru/images/dc-home-energy-use.png" title="Использование энергии"/&gt;&lt;/p&gt;
&lt;p&gt;В традиционных датацентрах потребление электричества серверами примерно
равно его потреблению остальной инфраструктурой, Google же удалось
снизить процент использования дополнительной электроэнергии до 14%.
Таким образом суммарное энергопотребление датацентром Google сравнимо с
потреблением только серверов в типичном датацентре и вдвое меньше его
общего энергопотребления. Основные концепции, которые используются для
достижения этого результата:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Точное измерение потребления электроэнергии всеми компонентами
    позволяет определить возможности по его уменьшению;&lt;/li&gt;
&lt;li&gt;В датацентрах Google тепло, что позволяет экономить на охлаждении;&lt;/li&gt;
&lt;li&gt;При проектировании датацентра уделяется внимание даже незначительным
    деталям, позволяющим сэкономить даже немного - при таком масштабе
    это окупается;&lt;/li&gt;
&lt;li&gt;Google умеет охлаждать датацентры практически без кондиционеров, с
    использованием воды и её испарения &lt;em&gt;(см. &lt;a href="https://www.insight-it.ru/goto/d1cf5d6b/" rel="nofollow" target="_blank" title="http://www.youtube.com/watch?v=VChOEvKicQQ"&gt;как это реализовано&lt;/a&gt; в Финляндии)&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В Google активно&amp;nbsp;пропагандируют максимальное использование
возобновляемой энергии. Для этого заключаются долгосрочные соглашения с
её поставщиками (на 20 и более лет), что позволяет отрасли активно
развиваться и наращивать мощности. Проекты по генерации возобновляемой
энергии, спонсируемые Google, имеют суммарную мощность более 1.7
гигаватт, что существенно больше, чем используется для работы Google.
Этой мощности хватило бы для обеспечения электричеством 350 тысяч домов.&lt;/p&gt;
&lt;p&gt;Если говорить о жизненном цикле оборудования, то используются следующие
принципы:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Уменьшение транспортировки:&lt;/strong&gt; там, где это возможно, тяжелые
    компоненты (вроде серверных стоек) закупаются у местных поставщиков,
    даже если в других местах аналогичный товар можно было бы купить
    дешевле.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Повторное использование:&lt;/strong&gt; прежде, чем покупать новое оборудование
    и материалы, рассматриваются возможности по использованию уже
    имеющихся. Этот принцип помог избежать покупки более 90 тысяч новых
    серверов.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Утилизация&lt;/strong&gt;: в тех случаях, когда повторное использование
    невозможно, оборудование полностью очищается от данных и продается
    на вторичном рынке. То, что не удается продать, разбирается на
    материалы (медь, сталь, алюминий, пластик и.т.п.) для последующей
    правильной утилизации специализированными компаниями.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Google известны за свои эксперименты и необычные решения в области
серверного оборудования и инфраструктуры. Некоторые запатентованы;
какие-то прижились, какие-то - нет. Подробно останавливаться на них не
буду, лишь вкратце о некоторых:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Резервное питание, интегрированное в блок питания
    &lt;a href="https://www.insight-it.ru/goto/ca5b8b43/" rel="nofollow" target="_blank" title="http://www.youtube.com/watch?v=xgRWURIxgbU"&gt;сервера&lt;/a&gt;, обеспеченное
    стандартными 12V батарейками;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/c7f3ff41/" rel="nofollow" target="_blank" title="http://www.datacenterknowledge.com/closer-look-googles-server-sandwich-design/"&gt;"Серверный сендвич"&lt;/a&gt;,
    где материнские платы с двух сторон окружают водяную систему
    теплоотвода в центре стойки;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8dc33e81/" rel="nofollow" target="_blank" title="http://www.youtube.com/watch?v=zRwPSFpLX8I"&gt;Датацентр из контейнеров&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В заключении этого раздела хотелось бы взглянуть правде в глаза:
&lt;strong&gt;идеального оборудования не бывает&lt;/strong&gt;. У любого современного устройства,
будь то сервер, коммутатор или маршрутизатор, есть шанс прийти в
негодность из-за производственного брака, случайного стечения
обстоятельств или других внешних факторов. Если умножить этот, казалось
бы, небольшой шанс на количество оборудования, которое используется в
Google, то окажется, что чуть ли не каждую минуту из строя выходит одно,
или несколько, устройств в системе. На оборудование полагаться нельзя,
по-этому вопрос отказоустойчивости переносится на плечи программной
платформы, которую мы сейчас и рассмотрим.&lt;/p&gt;
&lt;h3 id="platforma"&gt;Платформа&lt;/h3&gt;
&lt;p&gt;В Google очень рано столкнулись с проблемами ненадежности оборудования и
работы с огромными массивами данных. Программная платформа,
спроектированная для работы на многих недорогих серверах, позволила им
абстрагироваться от сбоев и ограничений одного сервера.&lt;/p&gt;
&lt;p&gt;Основными задачами в ранние годы была минимизация точек отказа и
обработка больших объемов слабоструктурированных данных. Решением этих
задач стали три основных слоя платформы Google, работающие один поверх
другого:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/479dfa95/" rel="nofollow" target="_blank" title="http://research.google.com/archive/gfs.html"&gt;Google File System&lt;/a&gt;:&lt;/strong&gt;
    распределенная файловая система, состоящая из сервера с метаданными
    и теоретически неограниченного количества серверов, хранящих
    произвольные данные в блоках фиксированного размера.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/f56e059a/" rel="nofollow" target="_blank" title="http://research.google.com/archive/bigtable.html"&gt;BigTable&lt;/a&gt;:&lt;/strong&gt;
    распределенная база данных, использующая для доступа к данным две
    произвольных байтовых строки-ключа (обозначающие строку и столбец) и
    дату/время (обеспечивающие версионность).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/dbd8fea6/" rel="nofollow" target="_blank" title="http://research.google.com/archive/mapreduce.html"&gt;MapReduce&lt;/a&gt;:&lt;/strong&gt;
    механизм распределенной обработки больших объемов данных,
    оперирующий парами ключ-значение для получения требуемой информации.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Такая комбинация, дополненная другими технологиями, довольно долгое
время позволяла справляться с индексацией Интернета, пока... скорость
появления информации в Интернете не начала расти огромными темпами из-за
"бума социальных сетей". Информация, добавленная в индекс даже через
полчаса, уже зачастую становилась устаревшей.&amp;nbsp;В дополнение к этому в
рамках самого Google стало появляться все больше продуктов,
предназначенных для работы в реальном времени.&lt;/p&gt;
&lt;p&gt;Спроектированные с учетом совершенно других требований Интернета
пятилетней давности компоненты, составляющие ядро платформы Google,
потребовали фундаментальной смены архитектуры индексации и поиска,
который около года назад был представлен публике&amp;nbsp;под кодовым названием
&lt;strong&gt;Google Caffeine&lt;/strong&gt;. Новые, переработанные, версии старых "слоев" также
окрестили броскими именами, но резонанса у технической публики они
вызвали намного меньше, чем новый поисковый алгоритм в SEO-индустрии.&lt;/p&gt;
&lt;h4&gt;Google Colossus&lt;/h4&gt;
&lt;p&gt;Новая архитектура GFS была спроектирована для минимизации задержек при
доступе к данным (что критично для приложений вроде GMail и YouTube), не
в ущерб основным свойствам старой версии: отказоустойчивости и
прозрачной масштабируемости.&lt;/p&gt;
&lt;p&gt;В оригинальной же реализации упор был сделан на повышение общей
пропускной способности: операции объединялись в очереди и выполнялись
разом, при таком подходе можно было прождать пару секунд еще до того,
как первая операция в очереди начнет выполняться. Помимо этого в старой
версии было большое слабое место в виде единственно мастер-сервера с
метаданными, сбой в котором грозил недоступностью всей файловой системы
в течении небольшого промежутка времени (пока другой сервер не подхватит
его функции, изначально это занимало около 5 минут, в последних версиях
порядка 10 секунд) - это также было вполне допустимо при отсутствии
требования работы в реальном времени, но для приложений, напрямую
взаимодействующих с пользователями, это было неприемлемо с точки зрения
возможных задержек.&lt;/p&gt;
&lt;p&gt;Основным нововведением в Colossus стали распределенные мастер-сервера,
что позволило избавиться не только от единственной точки отказа, но и
существенно уменьшить размер одного блока с данными (с 64 до 1
мегабайта), что в целом очень положительно сказалось на работе с
небольшими объемами данных. В качестве бонуса исчез теоретический предел
количества файлов в одной системе.&lt;/p&gt;
&lt;p&gt;Детали распределения ответственности между мастер-серверами, сценариев
реакции на сбои, а также сравнение по задержкам и пропускной
способности обоих версий, к сожалению, по-прежнему конфиденциальны. Могу
предположить, что используется вариация на тему хэш-кольца с репликацией
метаданных на ~3 мастер-серверах, с созданием дополнительной копии на
следующем по кругу сервере в случае в случае сбоев, но это лишь догадки.
Если у кого-то есть относительно официальная информация на этот счет -
буду рад увидеть в комментариях.&lt;/p&gt;
&lt;p&gt;По прогнозам Google текущий вариант реализации распределенной файловой
системы "уйдет на пенсию" в 2014 году из-за популяризации твердотельных
накопителей и существенного скачка в области вычислительных технологий
(процессоров).&lt;/p&gt;
&lt;h4&gt;Google Percolator&lt;/h4&gt;
&lt;p&gt;MapReduce отлично справлялся с задачей полной перестройки поискового
индекса, но не предусматривал небольшие изменения, затрагивающие лишь
часть страниц. Из-за потоковой, последовательной природы MapReduce для
внесения изменений в небольшую часть документов все равно пришлось бы
обновлять весь индекс, так как новые страницы непременно будут каким-то
образом связаны со старыми. Таким образом задержка между появлением
страницы в Интернете и в поисковом индексе при использовании MapReduce
была пропорциональна общему размеру индекса (а значит и Интернета,
который постоянно растет), а не размеру набора измененных документов.&lt;/p&gt;
&lt;p&gt;Ключевые архитектурные решения, лежащие в основе MapReduce, не позволяли
повлиять на эту особенность и в итоге система индексации была построена
заново с нуля, а MapReduce продолжает использоваться в других проектах
Google для аналитики и прочих задач, по прежнему не связанных с реальным
временем.&lt;/p&gt;
&lt;p&gt;Новая система получила довольно своеобразное название &lt;strong&gt;Percolator&lt;/strong&gt;,
попытки узнать что оно значит приводит к различным устройствам по
фильтрации дыма, кофеваркам и непойми чему еще. Но наиболее адекватное
объяснение мне пришло в голову, когда я прочитал его по слогам: per
col - &lt;em&gt;по колонкам&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Percolator представляет собой надстройку над BigTable, позволяющую
выполнять комплексные вычисления на основе имеющихся данных,
затрагивающие много строк и даже таблиц одновременно (в стандартном API
BigTable это не предусмотрено).&lt;/p&gt;
&lt;p&gt;Веб-документы или любые другие данные изменяются/добавляются в систему
посредством модифицированного API BigTable, а дальнейшие изменения в
остальной базе осуществляются посредством механизма&amp;nbsp;"обозревателей".
Если говорить в терминах реляционных СУБД, то обозреватели - что-то
среднее между триггерами и хранимыми процедурами. Обозреватели
представляют собой подключаемый к базе данных код (на &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;),
который исполняется в случае возникновении изменений в определенных
&lt;em&gt;колонках&lt;/em&gt; BigTable (откуда, видимо, и пошло название). Все используемые
системой метаданные также хранятся в специальных колонках BigTable. При
использовании Percolator все изменения происходят в транзакциях,
удовлетворяющих принципу ACID, каждая из которых затрагивает именно те
сервера в кластере, на которых необходимо внести изменения. Механизм
транзакций на основе BigTable разрабатывался в рамках отдельного проекта
под названием &lt;a href="https://www.insight-it.ru/storage/2011/google-megastore/"&gt;Google Megastore&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Таким образом, при добавлении нового документа (или его версии) в
поисковый индекс, вызывается цепная реакция изменений в старых
документах, скорее всего ограниченная по своей рекурсивности. Эта
система при осуществлении случайного доступа поддерживает индекс в
актуальном состоянии.&lt;/p&gt;
&lt;p&gt;В качестве бонуса в этой схеме удалось избежать еще двух недостатков
MapReduce:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Проблемы "отстающих":&lt;/strong&gt; когда один из серверов (или одна из
    конкретных подзадач) оказывался существенно медленнее остальных, что
    также значительно задерживало общее время завершения работы
    кластера.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Пиковая нагрузка:&lt;/strong&gt; MapReduce не является непрерывным процессом, а
    разделяется на работы с ограниченной целью и временем исполнения.
    Таким образом помимо необходимости ручной настройки работ и их
    типов, кластер имеет очевидные периоды простоя и пиковой нагрузки,
    что ведет к неэффективному использованию вычислительных ресурсов.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Но все это оказалось не бесплатно: при переходе на новую систему удалось
достичь той же скорости индексации, но при этом использовалось &lt;em&gt;вдвое&lt;/em&gt;
больше вычислительных ресурсов. Производительность Percolator находится
где-то между производительностью MapReduce и производительностью
традиционных СУБД. Так как Percolator является распределенной системой,
для обработки фиксированного небольшого количества данных ей приходится
использовать существенно больше ресурсов, чем традиционной СУБД; такова
цена масштабируемости. По сравнению с MapReduce также пришлось платить
дополнительными потребляемыми вычислительными ресурсами за возможность
случайного доступа с низкой задержкой.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Google Percolator Benchmark" class="left" src="https://www.insight-it.ru/images/google-percolator-benchmark.png" title="Google Percolator Benchmark"/&gt;&lt;/p&gt;
&lt;p&gt;Тем не менее, при выбранной архитектуре Google удалось достичь
практически линейного масштабирования при увеличении вычислительных
мощностей на много порядков &lt;em&gt;(см. график, основан на тесте TPC-E)&lt;/em&gt;.
Дополнительные накладные расходы, связанные с распределенной природой
решения, в некоторых случаях до 30 раз превосходят аналогичный
показатель традиционных СУБД, но у данной системы есть солидный простор
для оптимизации в этом направлении, чем Google активно и занимается.&lt;/p&gt;
&lt;h4&gt;Google Spanner&lt;/h4&gt;
&lt;p&gt;Spanner представляет собой единую систему автоматического управления
ресурсами &lt;strong&gt;всего парка серверов&lt;/strong&gt; Google.&lt;/p&gt;
&lt;p&gt;Основные особенности:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Единое пространство имен:&lt;ul&gt;
&lt;li&gt;Иерархия каталогов&lt;/li&gt;
&lt;li&gt;Независимость от физического расположения данных&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Поддержка слабой и сильной целостности данных между датацентрами&lt;/li&gt;
&lt;li&gt;Автоматизация:&lt;ul&gt;
&lt;li&gt;Перемещение и добавление реплик данных&lt;/li&gt;
&lt;li&gt;Выполнение вычислений с учетом ограничений и способов
    использования&lt;/li&gt;
&lt;li&gt;Выделение ресурсов на всех доступных серверах&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Зоны полу-автономного управления&lt;/li&gt;
&lt;li&gt;Восстановление целостности после потерь соединения между
    датацентрами&lt;/li&gt;
&lt;li&gt;Возможность указания пользователями высокоуровневых требований,
    например:&lt;ul&gt;
&lt;li&gt;99% задержек при доступе к этим данным должны быть до 50 мс&lt;/li&gt;
&lt;li&gt;Расположи эти данные на как минимум 2 жестких дисках в Европе, 2
    в США и 1 в Азии&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Интеграция не только с серверами, но и с сетевым оборудованием, а
    также системами охлаждения в датацентрах&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Проектировалась из расчета на:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1-10 миллионов серверов&lt;/li&gt;
&lt;li&gt;~10 триллионов директорий&lt;/li&gt;
&lt;li&gt;~1000 петабайт данных&lt;/li&gt;
&lt;li&gt;100-1000 датацентров по всему миру&lt;/li&gt;
&lt;li&gt;~1 миллиард клиентских машин&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Об этом проекте Google известно очень мало, официально он был
представлен публике лишь однажды в 2009 году, с тех пор лишь местами
упоминался сотрудниками без особой конкретики. Точно не известно
развернута ли эта система на сегодняшний день и если да, то в какой
части датацентров, а также каков статус реализации заявленного
функционала.&lt;/p&gt;
&lt;h4&gt;Прочие компоненты платформы&lt;/h4&gt;
&lt;p&gt;Платформа Google в конечном итоге сводится к набору сетевых сервисов и
библиотек для доступа к ним из различных языков программирования (в
основном используются&amp;nbsp;&lt;a href="/tag/c/"&gt;C/C++&lt;/a&gt;,&amp;nbsp;&lt;a href="/tag/java/"&gt;Java&lt;/a&gt;,&amp;nbsp;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt; и&amp;nbsp;&lt;a href="/tag/perl/"&gt;Perl&lt;/a&gt;). Каждый продукт, разрабатываемый Google, в большинстве случаев использует эти библиотеки для осуществления доступа к данным, выполнения комплексных вычислений и других задач, вместо стандартных механизмов, предоставляемых операционной системой, языком программирования или opensource библиотеками.&lt;/p&gt;
&lt;p&gt;Вышеизложенные проекты составляют лишь основу платформы Google, хотя она
включает в себя куда больше готовых решений и библиотек, несколько
примеров из публично доступных проектов:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/f5686a9/" rel="nofollow" target="_blank" title="http://code.google.com/webtoolkit/"&gt;GWT&lt;/a&gt; для реализации
    пользовательских интерфейсов на &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7f9cb797/" rel="nofollow" target="_blank" title="http://code.google.com/closure/"&gt;Closure&lt;/a&gt; - набор инструментов для
    работы с &lt;a href="/tag/javascript/"&gt;JavaScript&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/41604156/" rel="nofollow" target="_blank" title="http://code.google.com/apis/protocolbuffers/"&gt;Protocol Buffers&lt;/a&gt; -
    не зависящий от языка программирования и платформы формат бинарной
    сериализации структурированных данных, используется при
    взаимодействии большинства компонентов системы внутри Google;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/bb496d06/" rel="nofollow" target="_blank" title="http://code.google.com/p/leveldb/"&gt;LevelDB&lt;/a&gt; -
    высокопроизводительная встраиваемая &lt;a href="/tag/subd/"&gt;СУБД&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/44c46d8/" rel="nofollow" target="_blank" title="http://code.google.com/p/snappy/"&gt;Snappy&lt;/a&gt; - быстрая компрессия
    данных, используется при хранении данных в GFS.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi_1"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Стабильные, проработанные и повторно используемые базовые
    компоненты проекта&lt;/strong&gt; &lt;em&gt;- залог её стремительного развития, а также
    создания новых проектов на той же кодовой базе&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Если задачи и обстоятельства, с учетом которых проектировалась
    система, существенно изменились&amp;nbsp;&lt;em&gt;- не бойтесь вернуться на стадию проектирования и реализовать новое решение&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Используйте инструменты, подходящие для решения каждой конкретной
    задачи&lt;/em&gt;, а не те, которые навязывает мода или привычки участников
    команды.&lt;/li&gt;
&lt;li&gt;Даже, казалось бы, незначительные недоработки и допущения на большом
    масштабе могут вылиться в огромные потери &lt;em&gt;- уделяйте максимум
    внимания деталям при реализации проекта.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Нельзя полагаться даже на очень дорогое оборудование &lt;em&gt;- все ключевые
    сервисы должны работать минимум на двух серверах, в том числе и базы
    данных.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Распределенная платформа, общая для всех проектов, позволит новым
    разработчикам легко вливаться в работу над конкретными продуктами, с
    минимумом представления о внутренней архитектуре компонентов
    платформы.&lt;/li&gt;
&lt;li&gt;Прозрачная работа приложений в нескольких датацентрах - одна из
    самых тяжелых задач, с которыми сталкиваются интернет-компании.
    Сегодня каждая из них решает её по-своему и держит подробности в
    секрете, что сильно замедляет развитие opensource решений.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;p&gt;Не гарантирую достоверность всех нижеизложенных источников информации,
ставших основой для данной статьи, но ввиду конфиденциальности подобной
информации на большее рассчитывать не приходится.&lt;/p&gt;
&lt;p&gt;Поправки и уточнения приветствуются :)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/4431029a/" rel="nofollow" target="_blank" title="http://www.google.com/about/datacenters/index.html"&gt;Official Google Data Centers
    Site&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7437fe8a/" rel="nofollow" target="_blank" title="http://research.google.com/people/jeff/WSDM09-keynote.pdf"&gt;Challenges in Building Large-Scale Information Retrieval
    Systems&lt;/a&gt;
    (Jeff Dean, WCDMA '09)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7f1ddbff/" rel="nofollow" target="_blank" title="http://www.odbms.org/download/dean-keynote-ladis2009.pdf"&gt;Designs, Lessons and Advice from Building Large Distributed
    Systems&lt;/a&gt;
    (Jeff Dean, Ladis '09)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/6ec4bc06/" rel="nofollow" target="_blank" title="http://research.google.com/pubs/pub36726.html"&gt;Google Percolator official
    paper&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/3473b129/" rel="nofollow" target="_blank" title="http://research.google.com/pubs/pub36971.html"&gt;&lt;em&gt;Google Megastore official
    paper&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/dd8bec64/" rel="nofollow" target="_blank" title="http://www.theregister.co.uk/2010/09/24/google_percolator/"&gt;Google
    Percolator&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/82efffd8/" rel="nofollow" target="_blank" title="http://www.theregister.co.uk/2010/09/09/google_caffeine_explained/"&gt;Google Caffeine
    Explained&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/63b6ec67/" rel="nofollow" target="_blank" title="http://www.theregister.co.uk/2009/10/23/google_spanner/"&gt;Google
    Spanner&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/46520ede/" rel="nofollow" target="_blank" title="http://www.theregister.co.uk/2011/06/08/google_software_infrastructure_dubbed_obsolete_by_ex_employee/"&gt;Google Software Infrastructure Dubbed Obsolete by
    ex-Employee&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/49879386/" rel="nofollow" target="_blank" title="http://www.theregister.co.uk/2011/06/23/google_moves_off_the_google_file_system/"&gt;Google Moves Off the Google File
    System&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/745a779b/" rel="nofollow" target="_blank" title="http://www.google.co.uk/intl/en/landing/internetstats/"&gt;Google Internet
    Stats&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/d11024ba/" rel="nofollow" target="_blank" title="http://www.businessblogshub.com/2010/10/google-statistics-yes-they-are-very-big/"&gt;Google
    Statistics&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/bb33b2dd/" rel="nofollow" target="_blank" title="http://www.splashnology.com/article/google-plus-killer-facts-and-statistics-inforgaphics/2689/"&gt;Google Plus - Killer Facts and
    Statistics&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/d20404a7/" rel="nofollow" target="_blank" title="http://www.youtube.com/t/press_statistics"&gt;YouTube statistics&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/9909f6c8/" rel="nofollow" target="_blank" title="http://www.alexa.com/siteinfo/google.com"&gt;&lt;em&gt;Alexa on Google&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/b088e740/" rel="nofollow" target="_blank" title="http://www.internetworldstats.com/stats.htm"&gt;&lt;em&gt;Internet World
    Stats&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/86a009d0/" rel="nofollow" target="_blank" title="http://www.google.com/finance?fstype=bi&amp;amp;cid=694653"&gt;Google Inc.
    financials&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/2515e26d/" rel="nofollow" target="_blank" title="http://www.geekwire.com/2011/stats-hotmail-top-worldwide-gmail-posts-big-gains"&gt;Hotmail still on top worldwide; Gmail gets
    bigger&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/2428f635/" rel="nofollow" target="_blank" title="http://www.datacenterknowledge.com/archives/2011/08/01/report-google-uses-about-900000-servers/"&gt;&lt;em&gt;Google Server
    Count&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://www.insight-it.ru/goto/b6d09313/" rel="nofollow" target="_blank" title="http://www.datacenterknowledge.com/archives/2009/10/20/google-envisions-10-million-servers/"&gt;Google Envisions 10 Million
    Servers&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/bb3d77a4/" rel="nofollow" target="_blank" title="http://www.datacenterknowledge.com/archives/2008/03/27/google-data-center-faq/"&gt;&lt;em&gt;Google Data Center
    FAQ&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="bonus-tipichnyi-pervyi-god-klastera-v-google"&gt;Бонус: типичный первый год кластера в Google&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;~1/2 перегрева (большинство серверов выключаются в течении 5 минут,
    1-2 дня на восстановление)&lt;/li&gt;
&lt;li&gt;~1 отказ распределителя питания (~500-1000 резко пропадают, ~6
    часов на восстановление)&lt;/li&gt;
&lt;li&gt;~1 передвижение стойки (много передвижений, 500-100 машин, 6 часов)&lt;/li&gt;
&lt;li&gt;~1 перепрокладка сети (последовательной отключение ~5% серверов на
    протяжении 2 дней)&lt;/li&gt;
&lt;li&gt;~20 отказов стоек (40-80 машин мгновенно исчезают, 1-6 часов на
    восстановление)&lt;/li&gt;
&lt;li&gt;~5 стоек становится нестабильными (40-80 машин сталкиваются с 50%
    потерей пакетов)&lt;/li&gt;
&lt;li&gt;~8 запланированных технических работ с сетью (при четырех могут
    случаться случайные получасовые потери соединения)&lt;/li&gt;
&lt;li&gt;~12 перезагрузок маршрутизаторов (потеря DNS и внешних виртуальных
    IP на несколько минут)&lt;/li&gt;
&lt;li&gt;~3 сбоя маршрутизаторов (восстановление в течении часа)&lt;/li&gt;
&lt;li&gt;Десятки небольших 30-секундных пропаданий DNS&lt;/li&gt;
&lt;li&gt;~1000 сбоев конкретных серверов (~3 в день)&lt;/li&gt;
&lt;li&gt;Много тысяч сбоев жестких дисков, проблем с памятью, ошибок
    конфигурации и т.п.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 28 Nov 2011 01:32:00 +0400</pubDate><guid>tag:www.insight-it.ru,2011-11-28:highload/2011/arkhitektura-google-2011/</guid><category>BigTable</category><category>Caffeine</category><category>Closure</category><category>GFS</category><category>Google</category><category>GWT</category><category>highload</category><category>Percolator</category><category>Protocol Buffers</category><category>Snappy</category><category>Spanner</category><category>архитектура Google</category><category>датацентры</category><category>Масштабируемость</category><category>энергоэффективность</category></item><item><title>Является ли использование продукции Microsoft причиной провала MySpace?</title><link>https://www.insight-it.ru//highload/2011/yavlyaetsya-li-ispolzovanie-produkcii-microsoft-prichinojj-provala-myspace/</link><description>&lt;p&gt;Как известно, &lt;a href="/tag/myspace/"&gt;MySpace&lt;/a&gt; появилась на рынке даже несколько
раньше, чем &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt;, с практически аналогичным
продуктом. Но при этом на сегодняшний день Facebook - общепризнанный
лидер рынка социальных сетей, а MySpace даже далеко не все слышали
название. С технической точки зрения у проектов совершенно разные
подходы: Facebook построен на opensource технологиях, типичный LAMP,
MySpace же полностью использует стек от &lt;a href="/tag/microsoft/"&gt;Microsoft&lt;/a&gt;.
Некоторое время назад в зарубежной блогосфере зародилась оживленная
дискуссия на тему: а не выбор ли закрытых технологий стал основной
причиной проигранной MySpace "гонки" социальных сетей?&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;Вопрос и правда противоречивый: с одной стороны большая часть успеха
зависит от позиционирования продукта и маркетинга, с другой -
техническая неповоротливость, вызванная использованием проприетарных
продуктов, не позволяла развиваться так же быстро, как конкуренты.
Приведу ссылки на основные "очаги" дискуссии на данную тему:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/9d322b3/" rel="nofollow" target="_blank" title="http://scobleizer.com/2011/03/24/myspaces-death-spiral-due-to-bets-on-los-angeles-and-microsoft"&gt;MySpace's Death Spiral&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/cdb68f2/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2011/3/25/did-the-microsoft-stack-kill-myspace.html"&gt;Did The Microsoft Stack Kill&amp;nbsp;MySpace?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a2ceedc5/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2011/3/31/8-lessons-we-can-learn-from-the-myspace-incident-balance-vis.html"&gt;8 Lessons We Can Learn From The MySpace Incident - Balance, Vision,&amp;nbsp;Fearlessness&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/48dba052/" rel="nofollow" target="_blank" title="http://www.intelligentspeculator.net/investment-talking/why-facebook-is-succeeding-where-myspace-has-failed"&gt;Why Facebook is succeeding where Myspace has failed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HackerNews: &lt;a href="https://www.insight-it.ru/goto/36159b1d/" rel="nofollow" target="_blank" title="http://news.ycombinator.com/item?id=2369271"&gt;раз&lt;/a&gt; и
    &lt;a href="https://www.insight-it.ru/goto/2995e621/" rel="nofollow" target="_blank" title="http://apps.ycombinator.com/item?id=2369343"&gt;два&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Некоторые ключевые точки зрения:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Все интернет-компании в первую очередь технологические.&lt;/strong&gt; Даже
    если основной тематикой является развлечение, всё равно оно
    предоставляются посредством технологий.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Все неудачи в бизнесе не связаны с технологиями.&lt;/strong&gt; Если менеджмент
    не понимает ценность каждого разработчика и не может делегировать
    полномочия и ответственность - проект не станет успешным. С другой
    стороны если разработчики не знают какие вещи являются важными для
    бизнеса, так как их просто не держат в курсе - это так же ничем
    хорошим не заканчивается.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Нельзя мотивировать разрушение инновации.&lt;/strong&gt; В MySpace основным
    источником дохода являлась рекламе Google, доход с которой напрямую
    зависел от количества просмотров страниц и кликов, что в результате
    подтолкнуло MySpace добавить дополнительные не нужные страницы ко
    всем действиям пользователей, что абсолютно уничтожило все
    юзабилити.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Корпоративная разработка и веб-разработка - совершенно разные
    вещи.&lt;/strong&gt; Не смотря на тот факт, что большинство сошлось во мнении,
    что ни продукция Microsoft, ни разработчики MySpace не виноваты в
    провале проекта, тот факт, что между инструментами, подходами и
    технологиями для разработки масштабируемых интернет-проектов и
    корпоративных приложений имеется очень большой разрыв, очевиден.
    Технологии, разработанные для корпоративного рынка, не могут быть
    без серьезной настройки и доработки использованы в мире крупных
    интернет-проектов.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Нельзя позволять обратной совместимости останавливать развитие.&lt;/strong&gt;
    В MySpace этот вопрос касается в большей степени пользовательского
    интерфейса: пользователи сами могут его настраивать. В итоге при
    введении каждой новой кнопочки или технологии типа AJAX им
    приходилось задумываться: "а не сломает ли это страницу при
    настройках какого-нибудь пользователя?"&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Успех не вечен.&lt;/strong&gt; Изменения нужно вводить в эксплуатацию без
    страха ошибиться.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Не полагайтесь на единственного клиента.&lt;/strong&gt; Нужно иметь набор
    различных источников дохода, чтобы быть от них независимыми и
    продолжать развиваться даже если один из них иссяк.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Определитесь кем вы являетесь.&lt;/strong&gt; Когда битва проиграна, можно
    переключить фокус на другую нишу, где меньше конкуренции - так новый
    генеральный директор MySpace переориентировал проект с социальной
    сети на сервис по открытию для себя новой музыки. Такие решения
    могут сработать, а могут и нет - но в любом случае это лучше, чем
    просто закрыть проект.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;С моей точки зрения основной минус закрытых технологий - отсутствие
полного контроля над всем кодом проекта, если проблемы начинаются в
"закрытых" компонентах, то их исправление занимает нереальное количество
времени, так как требует взаимодействия со сторонней компанией, которая?
скорее всего? не будет торопиться решать каждую конкретную проблему и в
лучшем случае её исправление будет доступно лишь в следующем релизе
используемого продукта. Помимо этого, при использовании проприетарных
технологий у разработчиков меньше понимания того, как работают внутри
используемые технологии - никакая документация не заменит возможность
"залезть под капот". В случае же с Microsoft проект совсем связывает
себя по рукам и ногам, так как воспользовавшись монолитным стеком чаще
всего нельзя постепенно перейти на другую технологическую базу, так как
это по сути сводится к полному переписыванию кода проекта, что на
подобном уровне очень затруднительно. Эти и подобные вещи замедляют
процесс разработки, а значит и развитие проекта - этот фактор очень
важен в современном интернет-сообществе, так как, если не двигаться
вперед, конкуренты догонят и перегонят очень стремительно, как в целом и
случилось с MySpace.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;А какова твоя точка зрения на данную ситуацию? Может ли неудачное технологическое решение столь кардинально повлиять на успех бизнеса?&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 20 Apr 2011 12:37:00 +0400</pubDate><guid>tag:www.insight-it.ru,2011-04-20:highload/2011/yavlyaetsya-li-ispolzovanie-produkcii-microsoft-prichinojj-provala-myspace/</guid><category>Microsoft</category><category>MySpace</category><category>fail</category><category>провал</category></item><item><title>Кардинальный переворот в архитектуре поиска Twitter</title><link>https://www.insight-it.ru//highload/2011/kardinalnyjj-perevorot-v-arkhitekture-poiska-twitter/</link><description>&lt;p&gt;Не успел я опубликовать &lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-twitter-dva-goda-spustya/"&gt;обновление об архитектуре Twitter&lt;/a&gt;, как
они снова перекроили половину проекта =) На этот раз к паре
&lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt;+&lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt; активно вплелись технологии из
мира &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;. Наибольшим изменениям подверглась подсистема
поиска твитов , о которой сегодня и пойдет речь.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="novaia-arkhitektura-poiska-tvitov"&gt;Новая архитектура поиска твитов&lt;/h2&gt;
&lt;h3 id="backend"&gt;Backend&lt;/h3&gt;
&lt;p&gt;Поиск осуществляется теперь не с помощью&amp;nbsp;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;-кластера, а посредством версии &lt;a href="/tag/lucene/"&gt;Lucene&lt;/a&gt;, адаптированной для работы в реальном времени. Разработка этой подсистемы началась весной прошлого года, но полноценно использоваться она начала лишь недавно.&lt;/p&gt;
&lt;p&gt;Так как поиск в Twitter является одной из самых часто используемых поисковых систем в мире (более миллиарда поисковых запросов в день), то требования к новой
системе поиска были сопоставимо строгими:
-   Обработка более 12000 запросов в секунду
-   Индексация потока в 1000 новых твитов в секунду
-   Задержка между написанием твита и его появлением в индексе должна
    быть менее 10 секунд&lt;/p&gt;
&lt;p&gt;Lucene была взята за основу, так как на сегодняшний день это одно из
лучших решений для реализации поиска в мире opensource. Но в текущей ее
реализации она не была приспособлена к поиску в реальном времени.
Команде Twitter пришлось переписать существенную часть основных структур
в памяти, особенно списки записей. При этом внешний API Lucene остался
неизменным, что позволило использовать поисковые алгоритмы в практически
неизменном виде. Среди основных изменений в Lucene можно выделить:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;значительно улучшена производительность сбора мусора;&lt;/li&gt;
&lt;li&gt;структуры и алгоритмы более не используют блокировки;&lt;/li&gt;
&lt;li&gt;списки записей с поддержкой обхода в обратном направлении;&lt;/li&gt;
&lt;li&gt;эффективное раннее прекращение обработки запроса.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Все вышеперечисленные изменения находятся в процессе публикации обратно
в Lucene, какие-то прямо в основную ветку, какие-то в отдельную для
поиска в реальном времени.&lt;/p&gt;
&lt;p&gt;После внедрения этой системы поиск стал потреблять лишь 5% доступных ему
ресурсов, что оставило приличный запас для роста даже по меркам
невероятно быстро развивающегося Twitter. Новая подсистема индексации
способна обрабатывать в 50 раз больше твитов в секунду, чем они получали
на момент запуска, что также является очень позитивным показателем.
Помимо улучшения производительности, Lucene повысила и качество поиска,
а также открыла простор для новых улучшений в этом направлении.&lt;/p&gt;
&lt;h3 id="frontend"&gt;Frontend&lt;/h3&gt;
&lt;p&gt;Кардинальный переворот в этой части системы можно описать одной
фразой:&amp;nbsp;&lt;a href="/tag/ror/"&gt;Ruby on Rails&lt;/a&gt; заменен на Java-сервер, который они
назвали&amp;nbsp;&lt;a href="/tag/blender/"&gt;Blender&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;За неделю до развертывания Blender, количество поисков по твитам
существенно возросло из-за &lt;code&gt;#tsunami&lt;/code&gt; в Японии. Среднее время поиска
достигало 800-900мс.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Blender и #tsunami" class="left" src="https://www.insight-it.ru/images/blender-tsunami.jpg" title="Blender и #tsunami"/&gt;&lt;/p&gt;
&lt;p&gt;После введения Blender в эксплуатацию среднее время отклика 95% запросов
упало втрое: до 250мс, при этом уровень использования вычислительных
ресурсов на frontend серверах упал вдвое. Тот же поток запросов стало
возможным обрабатывать меньшим количеством серверов.&lt;/p&gt;
&lt;p&gt;Чтобы понять, откуда взялся такой прирост производительности, необходимо
показать в чем были слабые стороны старого поиска на Ruby on Rails. На
каждом frontend сервере было запущено фиксированное количество
однопоточных Rails процессов, каждый из которых занимался следующим:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;обработкой поисковых запросов&lt;/li&gt;
&lt;li&gt;синхронным обращением к серверам с индексами&lt;/li&gt;
&lt;li&gt;агрегацией и составлением результатов&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Они давно понимали, что синхронные запросы ведут к неэффективному
использованию вычислительных ресурсов. Со временем накопилось много
технически неудачных моментов, что делало все сложнее введение нового
функционала и поддержание надежности системы. Blender позволил
преодолеть эти функции следующим образом:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Создание полностью асинхронного сервиса агрегации.&lt;/strong&gt; Ни один поток
    не ждет пока осуществятся сетевые операции.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Агрегация результатов с различных сервисов:&lt;/strong&gt; индексы поиска в
    реальном времени, топа твитов и гео-информации, а также базы данных
    пользователей и твитов.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Элегантная работа с зависимостями сервисов.&lt;/strong&gt; Алгоритм обработки
    запросов автоматически обрабатывает зависимости между используемыми
    сервисами.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="chto-zhe-sobstvenno-predstavliaet-soboi-blender"&gt;Что же, собственно, представляет собой Blender?&lt;/h3&gt;
&lt;p&gt;Это HTTP и &lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt; сервер, разработанный на основе
&lt;a href="/tag/netty/"&gt;Netty&lt;/a&gt;, масштабируемой неблокирующей клиент-серверной
библиотеки на Java, позволяющей легко и быстро разрабатывать клиенты и
серверы для различных протоколов. Выбор пал именно на неё, а не на
аналоги (например Jetty или Mina) из-за более чистого API, детальной
документации и, что более важно, так как некоторые другие сервисы в
Twitter уже используют её. Интеграции с Thrift у нее не было, но этот
вопрос решился написанием простого кодека, обрабатывающего сообщения на
низком уровне.&lt;/p&gt;
&lt;p&gt;Обработка поисковых запросов представляет собой цепочку запросов к
внутренним сервисами, обработку ответов и генерацию результата.
Внутренние сервисы имеют зависимости, которые можно представить в виде
ацикличного направленного графа. После топологической сортировки графа
Blender получает последовательность выполнения запросов, которые
назначаются к выполнению в поток Netty, что в совокупности с
обработчиками событий и образует workflow обработки поисковых запросов.&lt;/p&gt;
&lt;h2 id="zakliuchenie_1"&gt;Заключение&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Blender - схема работы" class="right" src="https://www.insight-it.ru/images/blender-workflow.jpg" title="Blender - схема работы"/&gt;
Эта диаграмма демонстрирует текущую архитектуру поиска с использованием
Blender и Lucene: все входящие поисковые запросы проходят через
аппаратный балансировщик нагрузки и попадают в Blender, где они
анализируются и перераспределяются между внутренними сервисами с
использованием workflow для обработки зависимостей и генерации
результатов.&lt;/p&gt;
&lt;p&gt;На моей памяти эти нововведения в Twitter - практически единственный
случай, когда крупный успешный проект настолько кардинально поменял
основную часть стека используемых технологий. Да, они получили
существенный выигрыш в производительности не в ущерб масштабируемости,
но не поменяли же они большую часть команды разработчиков с
Ruby-программистов на Java-программистов... Понятно, что это лишь
инструменты, но довольно приличная часть людей, особенно те, кто в
возрасте, не способны резко переключиться с привычных технологий на
что-то совершенно новое. Хотя, скорее всего, в команде Twitter особо не
было разработчиков "за 40", так что для них это не было особой
проблемой.&lt;/p&gt;
&lt;h2 id="istochnik-informatsii"&gt;Источник информации&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/681764e5/" rel="nofollow" target="_blank" title="http://engineering.twitter.com/2011/04/twitter-search-is-now-3x-faster_1656.html"&gt;Twitter Search 3x Faster&lt;/a&gt; (cпасибо Сергею Гуляеву за предоставленную ссылку)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/1abfdd46/" rel="nofollow" target="_blank" title="http://engineering.twitter.com/2010/10/twitters-new-search-architecture.html"&gt;Twitter's New Search Architecture&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 15 Apr 2011 23:03:00 +0400</pubDate><guid>tag:www.insight-it.ru,2011-04-15:highload/2011/kardinalnyjj-perevorot-v-arkhitekture-poiska-twitter/</guid><category>Blender</category><category>Lucene</category><category>netty</category><category>Twitter</category></item><item><title>Google в цифрах</title><link>https://www.insight-it.ru//highload/2011/google-v-cifrakh/</link><description>&lt;p&gt;Я уже давно ищу информацию для новой версии &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-google/"&gt;статьи про Google&lt;/a&gt;, которая была первым
успешным постом на &lt;strong class="trebuchet"&gt;Insight IT&lt;/strong&gt; - скачок в посещаемости был примерно в 50 раз. Не смотря на устаревшую статистику она по-прежнему представляет собой большую практическую пользу, рекомендую прочитать или перечитать.&lt;/p&gt;
&lt;p&gt;В процессе перерывания зарубежной части интернета в поисках более свежей
информации о том, как устроен Google, наткнулся на любопытную
инфографику с цифрами, которой и решил с Вами поделиться, чтобы скрасить
ожидание нового поста :).&lt;!--more--&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Google в цифрах" class="responsive-img" src="https://www.insight-it.ru/images/google-by-the-numbers.jpg" title="Google в цифрах"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/efb4cd33/" rel="nofollow" target="_blank" title="http://www.infographicsshowcase.com/google-by-the-numbers-infographic/"&gt;Оригинал&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Не забываем подписываться на &lt;a href="/feed/"&gt;RSS&lt;/a&gt; :)&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 03 Apr 2011 21:59:00 +0400</pubDate><guid>tag:www.insight-it.ru,2011-04-03:highload/2011/google-v-cifrakh/</guid><category>Google</category><category>инфографика</category><category>статистика</category><category>цифры</category></item><item><title>Архитектура Stack Exchange Network</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-stack-exchange-network/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/8d9e30a4/" rel="nofollow" target="_blank" title="http://stackexchange.com/"&gt;Stack Exchange Network&lt;/a&gt; представляет собой
сеть из 46 сайтов вопросов-ответов на совершенно разные темы от
программирования до кулинарии. Проект вырос из известной в узких кругах
тусовки программистов &lt;a href="https://www.insight-it.ru/goto/dd7cd9bb/" rel="nofollow" target="_blank" title="http://stackoverflow.com/"&gt;Stack Overflow&lt;/a&gt;, об
архитектуре которой &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-stack-overflow/"&gt;я уже рассказывал&lt;/a&gt; чуть больше года назад. Проект активно развивается и уже появилось приличное количество новой информации, которой я и спешу с Вами поделиться.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;95 миллионов просмотров страниц в месяц&lt;/li&gt;
&lt;li&gt;800 HTTP запросов в секунду&lt;/li&gt;
&lt;li&gt;180 DNS запросов в секунду&lt;/li&gt;
&lt;li&gt;Загруженность интернет-канала в 55 Мбит/с&lt;/li&gt;
&lt;li&gt;16 миллионов уникальных пользователей в месяц&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="tekhnologii"&gt;Технологии&lt;/h2&gt;
&lt;h3 id="razrabotka"&gt;Разработка&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/c/"&gt;C#&lt;/a&gt; - основной язык программирования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/visual-studio/"&gt;Visual Studio 2010 Team Suite&lt;/a&gt; -&amp;nbsp;IDE&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/asp-net/"&gt;Microsoft ASP.NET 4.0&lt;/a&gt; - framework&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/asp-net-mvc/"&gt;ASP.NET MVC 3&lt;/a&gt; -&amp;nbsp;web Framework&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/razor/"&gt;Razor&lt;/a&gt; - генератор шаблонов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/jquery/"&gt;jQuery 1.4.2&lt;/a&gt; - JavaScript framework&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/linq-to-sql/"&gt;LINQ to SQL&lt;/a&gt; и немного чистого SQL - доступ к
    данным&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mercurial/"&gt;Mercurial&lt;/a&gt; и &lt;a href="/tag/kiln/"&gt;Kiln&lt;/a&gt; - контроль версий
    исходного кода&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/beyond-compare/"&gt;Beyond Compare 3&lt;/a&gt; - инструмент для сравнения&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="programmnoe-obespechenie"&gt;Программное обеспечение&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/886b3540/" rel="nofollow" target="_blank" title="http://stackoverflow.com/questions/177901/what-does-wisc-stack-mean"&gt;WISC&lt;/a&gt;
    стек получен условно-бесплатно с
    помощью&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/b478b941/" rel="nofollow" target="_blank" title="http://blog.stackoverflow.com/2009/03/stack-overflow-and-bizspark/"&gt;BizSpark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/windows-server/"&gt;Windows Server&lt;/a&gt;&lt;a href="/tag/windows-server-2008/"&gt;2008 R2
    x64&lt;/a&gt; - основная операционная система&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/ms-sql-server-2008/"&gt;MS SQL Server 2008 R2&lt;/a&gt; на&amp;nbsp;&lt;a href="/tag/windows-server-2008/"&gt;Windows Server
    2008 Enterprise Edition x64&lt;/a&gt; - база
    данных&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/ubuntu-server/"&gt;Ubuntu Server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/centos/"&gt;CentOS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/iis/"&gt;IIS 7.0&lt;/a&gt; - веб-сервер&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/haproxy/"&gt;HAProxy&lt;/a&gt; - балансировка нагрузки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/redis/"&gt;Redis&lt;/a&gt; - используется как распределенная система
    кэширования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/cruisecontrol-net/"&gt;CruiseControl.NET&lt;/a&gt; - сборки и
    автоматическая система развертывания кода&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/lucene/"&gt;Lucene.NET&lt;/a&gt; - полнотекстовый поиск&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/bacula/"&gt;Bacula&lt;/a&gt; - резервное копирование&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/nagios/"&gt;Nagios&lt;/a&gt; (с
    плагинами&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/d576a9fc/" rel="nofollow" target="_blank" title="http://n2rrd.diglinks.com/cgi-bin/trac.fcgi"&gt;n2rrd&lt;/a&gt; и
    &lt;a href="https://www.insight-it.ru/goto/9b00fbb3/" rel="nofollow" target="_blank" title="http://web.taranis.org/drraw/"&gt;drraw&lt;/a&gt;) для мониторинга&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/splunk/"&gt;Splunk&lt;/a&gt; - сбор и агрегация логов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/sql-monitor/"&gt;SQL Monitor&lt;/a&gt; от&amp;nbsp;Red Gate - мониторинг SQL Server&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/bind/"&gt;Bind&lt;/a&gt; -&amp;nbsp;DNS&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/dotnetopenid/"&gt;DotNetOpenId&lt;/a&gt; - реализация OpenID на .NET&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/wmd/"&gt;WMD&lt;/a&gt; - текстовый редактор&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/prettify/"&gt;Prettify&lt;/a&gt; - подсветка синтаксиса&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/markdownsharp/"&gt;MarkdownSharp&lt;/a&gt; - обработчик разметки Markdown
    на C#&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/flot/"&gt;Flot&lt;/a&gt; - построение графиков на JavaScript&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="vneshnie-servisy"&gt;Внешние сервисы&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/recaptcha/"&gt;reCAPTCHA&lt;/a&gt; - защита от спама&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/google-analytics/"&gt;Google Analytics&lt;/a&gt; - веб-аналитика&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/kiln/"&gt;Kiln&lt;/a&gt; - Mercurial хостинг&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/pingdom/"&gt;Pingdom&lt;/a&gt; - внешний мониторинг и уведомления&lt;/li&gt;
&lt;li&gt;CDN не используется, его роль выполняет&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/a5057c7b/" rel="nofollow" target="_blank" title="http://sstatic.net/"&gt;sstatic.net&lt;/a&gt;, отдельный домен для статичных файлов SEN без cookie&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="oborudovanie_1"&gt;Оборудование&lt;/h2&gt;
&lt;h3 id="datatsentry"&gt;Датацентры&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;1 стойка в Peak Internet, штат Орегон (чат и обнаружение данных)&lt;/li&gt;
&lt;li&gt;2 стойки в Peer 1, Нью-Йорк (остальная часть SEN)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="servery"&gt;Серверы&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;10 веб-серверов:&lt;ul&gt;
&lt;li&gt;Dell R610&lt;/li&gt;
&lt;li&gt;1x Intel Xeon Processor E5640 @ 2.66 GHz&lt;/li&gt;
&lt;li&gt;16 GB RAM&lt;/li&gt;
&lt;li&gt;Windows Server 2008 R2&lt;/li&gt;
&lt;li&gt;IIS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2 сервера баз данных:&lt;ul&gt;
&lt;li&gt;Dell R710&lt;/li&gt;
&lt;li&gt;2x Intel Xeon Processor X5680 @ 3.33 GHz&lt;/li&gt;
&lt;li&gt;64 GB RAM&lt;/li&gt;
&lt;li&gt;8 жестких дисков&lt;/li&gt;
&lt;li&gt;MS SQL Server 2008 R2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2 виртуальных сервера для балансировки нагрузки:&lt;ul&gt;
&lt;li&gt;1x Intel Xeon Processor E5640 @ 2.66 GHz&lt;/li&gt;
&lt;li&gt;4 GB RAM&lt;/li&gt;
&lt;li&gt;Ubuntu Server&lt;/li&gt;
&lt;li&gt;HAProxy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2 сервера для кэша:&lt;ul&gt;
&lt;li&gt;Dell R610&lt;/li&gt;
&lt;li&gt;2x Intel Xeon Processor E5640 @ 2.66 GHz&lt;/li&gt;
&lt;li&gt;16 GB RAM&lt;/li&gt;
&lt;li&gt;CentOS&lt;/li&gt;
&lt;li&gt;Redis&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1 сервер для резервного копирования:&lt;ul&gt;
&lt;li&gt;Dell R610&lt;/li&gt;
&lt;li&gt;1x Intel Xeon Processor E5640 @ 2.66 GHz&lt;/li&gt;
&lt;li&gt;32 GB RAM&lt;/li&gt;
&lt;li&gt;Linux&lt;/li&gt;
&lt;li&gt;Bacula&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;1 сервер для мониторинга, управления и сбора логов:&lt;ul&gt;
&lt;li&gt;Dell R610&lt;/li&gt;
&lt;li&gt;1x Intel Xeon Processor E5640 @ 2.66 GHz&lt;/li&gt;
&lt;li&gt;32 GB RAM&lt;/li&gt;
&lt;li&gt;Linux&lt;/li&gt;
&lt;li&gt;Nagios&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;2 сервера для виртуализации:&lt;ul&gt;
&lt;li&gt;Dell R610&lt;/li&gt;
&lt;li&gt;1x Intel Xeon Processor E5640 @ 2.66 GHz&lt;/li&gt;
&lt;li&gt;16 GB RAM&lt;/li&gt;
&lt;li&gt;VMWare ESXi&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="setevoe-oborudovanie"&gt;Сетевое оборудование&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;2 маршрутизатора на Linux&lt;/li&gt;
&lt;li&gt;5 свитчей &amp;nbsp;Dell PowerConnect&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="prochee"&gt;Прочее&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/aa5532bf/" rel="nofollow" target="_blank" title="http://www.wowwee.com/en/products/tech/telepresence/rovio/rovio"&gt;Rovio&lt;/a&gt; -
    маленький робот, позволяющий удаленным разработчиком посетить офис
    "виртуально"&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="komanda_1"&gt;Команда&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;14 разработчиков&lt;/li&gt;
&lt;li&gt;2 системных администратора&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="chto-novogo"&gt;Что нового?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;HAProxy стал использоваться вместо Windows NLB так как HAProxy
    является быстрым, нересурсоемким, бесплатным решением, которое
    работает. Полностью прозрачен для серверов, легче обслуживать по
    сравнению со старым решением, располагается на виртуальных машинах.&lt;/li&gt;
&lt;li&gt;CDN не используется, так как даже "недорогие" решения обходятся в
    очень приличную сумму по сравнению с тем трафиком, который входит в
    тарифный план хостинг-провайдера. Самое дешевой решение CDN от
    Amazon обошлось бы как минимум на тысячу долларов в месяц дороже при
    текущем уровне использования трафика.&lt;/li&gt;
&lt;li&gt;Резервное копирование на диски для быстрого восстановления и на
    кассеты для "истории".&lt;/li&gt;
&lt;li&gt;Полнотекстный поиск в SQL Server плохо интегрируется, нестабилен и
    обладает низким качеством результатов, так что они перешли на
    Lucene.&lt;/li&gt;
&lt;li&gt;Все сайты в SEN теперь работают на общей платформе: используется
    общее оборудование и программное обеспечение.&lt;/li&gt;
&lt;li&gt;Проект разделен на разные сайты для разных ниш, чтобы полностью
    изолировать группы аудитории, специализирующиеся в каждой конкретной
    области.&lt;/li&gt;
&lt;li&gt;Используется агрессивное кэширование, большинство страниц кэшируются
    в виде HTML для анонимных пользователей средствами IIS.&lt;/li&gt;
&lt;li&gt;Используется три уровня кэширования: локальный, относящийся к
    каждому сайту и глобальный.&lt;/li&gt;
&lt;li&gt;Локальный кэш доступен только для каждой пары сайт/сервер:&lt;ul&gt;
&lt;li&gt;Используется для уменьшения сетевых задержек, по сути просто
    через&amp;nbsp;HttpRuntime.Cache.&lt;/li&gt;
&lt;li&gt;Содержит такие вещи как пользовательские сессии, будущие
    обновления счетчиков просмотров страниц.&lt;/li&gt;
&lt;li&gt;Располагается полностью в оперативной памяти веб-сервера.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Кэш сайта доступен для каждого сервера, обрабатывающий запрос к
    конкретному сайту:&lt;ul&gt;
&lt;li&gt;Большинство кэшируемых данных располагаются здесь.&lt;/li&gt;
&lt;li&gt;Располагается в Redis.&lt;/li&gt;
&lt;li&gt;Redis настолько быстр, что большую часть времени доступа к кэшу
    занимает передача данных по сети.&lt;/li&gt;
&lt;li&gt;Данные сжимаются перед отправкой в Redis, так как большинство
    данных являются строками и у них есть масса свободных
    вычислительных ресурсов.&lt;/li&gt;
&lt;li&gt;Использование процессорных ресурсов на серверах с Redis
    стремится к нулю.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Глобальный кэш является общим для всех серверов и сайтов:&lt;ul&gt;
&lt;li&gt;Личные сообщения, квоты по API и несколько других по-настоящему
    глобальных вещей располагаются здесь.&lt;/li&gt;
&lt;li&gt;Также используется Redis.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Большинство данных в кэше удаляются через заданный период времени
    (обычно в районе нескольких минут) и практически никогда явно не
    удаляются.&lt;/li&gt;
&lt;li&gt;Когда требуется инвалидация кэша на уровне готовых страниц,
    используется система подписки внутри Redis для отправки сообщений в
    соответствующую часть системы кэширования.&lt;/li&gt;
&lt;li&gt;Для системы ввода-вывода они выбрали Intel X25 SSD в RAID10. RAID
    решил многие вопросы с надежностью, а SSD показывают отличную
    производительностью по сравнению с&amp;nbsp;FusionIO при существенно более
    низкой цене.&lt;/li&gt;
&lt;li&gt;Стоимость лицензий используемых продуктов Microsoft составила бы 242
    тысячи долларов. Но так как они используют программу BizSpark, им не
    пришлось платить большую часть этой суммы.&lt;/li&gt;
&lt;li&gt;Сетевые карты от Broadcom заменяются на сетевые карты от Intel на
    основных production серверах. Это решило большинство проблем с
    потерями соединений, пакетов и таблицами ARP.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8a78f426/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2011/3/3/stack-overflow-architecture-update-now-at-95-million-page-vi.html"&gt;Stack Overflow Architecture Update - Now At 95 Million Page Views
    A&amp;nbsp;Month&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/ac2efccd/" rel="nofollow" target="_blank" title="http://blog.stackoverflow.com/"&gt;Stack Overflow Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/83742213/" rel="nofollow" target="_blank" title="http://blog.serverfault.com/post/1432571770/"&gt;Stack Overflow&amp;rsquo;s New York Data
    Center&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7241f829/" rel="nofollow" target="_blank" title="http://blog.serverfault.com/post/1097492931/"&gt;Designing For Scalability of Management and Fault
    Tolerance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/955af379/" rel="nofollow" target="_blank" title="http://blog.stackoverflow.com/2011/01/stack-overflow-search-now-81-less-crappy/"&gt;Stack Overflow Search &amp;mdash; Now 81% Less
    Crappy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7ab0ab00/" rel="nofollow" target="_blank" title="http://blog.stackoverflow.com/2011/01/state-of-the-stack-2010-a-message-from-your-ceo/"&gt;State of the Stack 2010 (a message from your
    CEO)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/f4755d56/" rel="nofollow" target="_blank" title="http://blog.stackoverflow.com/2010/01/stack-overflow-network-configuration/"&gt;Stack Overflow Network
    Configuration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/d29680fc/" rel="nofollow" target="_blank" title="http://meta.stackoverflow.com/questions/69164/does-stackoverflow-use-caching-and-if-so-how"&gt;Does StackOverflow use caching and if so,
    how?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a1f157a/" rel="nofollow" target="_blank" title="http://meta.stackoverflow.com/questions/6435/how-does-stackoverflow-handle-cache-invalidation"&gt;How does StackOverflow handle cache
    invalidation?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/4812040a/" rel="nofollow" target="_blank" title="http://meta.stackoverflow.com/questions/10369/which-tools-and-technologies-build-the-stack-exchange-network"&gt;Which tools and technologies build the Stack Exchange
    Network?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/d1cfeccf/" rel="nofollow" target="_blank" title="http://meta.stackoverflow.com/questions/2765/how-does-stack-overflow-handle-spam"&gt;How does Stack Overflow handle
    spam?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/58e28ee2/" rel="nofollow" target="_blank" title="http://blog.serverfault.com/post/our-storage-decision/"&gt;Our Storage
    Decision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/6a63689c/" rel="nofollow" target="_blank" title="http://meta.stackoverflow.com/questions/4766/how-are-hot-questions-selected"&gt;How are &amp;ldquo;Hot&amp;rdquo; Questions
    Selected?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/90fef20/" rel="nofollow" target="_blank" title="http://blog.stackoverflow.com/2010/04/stack-overflow-and-dvcs/"&gt;Stack Overflow and
    DVCS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/fe105178/" rel="nofollow" target="_blank" title="http://chat.stackexchange.com/rooms/127/the-comms-room"&gt;Server Fault Chat
    Room&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Спасибо за внимание! Для оперативного получения свежей информации о
&lt;a href="https://www.insight-it.ru/highload/"&gt;высоконагруженных интернет-проектах&lt;/a&gt; рекомендую &lt;a href="/feed/"&gt;подписаться на RSS&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 31 Mar 2011 16:05:00 +0400</pubDate><guid>tag:www.insight-it.ru,2011-03-31:highload/2011/arkhitektura-stack-exchange-network/</guid><category>ASP .NET</category><category>ASP .NET MVC</category><category>Bacula</category><category>Beyond Compare 3</category><category>Bind</category><category>C++</category><category>CentOS</category><category>CruiseControl.NET</category><category>DotNetOpenId</category><category>Flot</category><category>Google Analytics</category><category>HAProxy</category><category>IIS</category><category>JQuery</category><category>Kiln</category><category>LINQ to SQL</category><category>Lucene</category><category>MarkdownSharp</category><category>Mercurial</category><category>MS SQL Server 2008</category><category>Nagios</category><category>Pingdom</category><category>Prettify</category><category>Razor</category><category>reCAPTCHA</category><category>Redis</category><category>Splunk</category><category>SQL Monitor</category><category>Ubuntu Server</category><category>Visual Studio</category><category>Windows Server</category><category>Windows Server 2008</category><category>WMD</category></item><item><title>Аналитика в реальном времени от Facebook</title><link>https://www.insight-it.ru//highload/2011/analitika-v-realnom-vremeni-ot-facebook/</link><description>&lt;p&gt;&lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt; в &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt; завоевывает все более
и более крепкие позиции, в прошлый раз я рассказывал о применении HBase
в роли системы хранения данных для их новой системы обмена сообщений.
Вторым продуктом, который теперь полноценно использует данную
технологию, является система сбора и обработки статистики в реальном
времени под названием Insights. Социальные кнопки (см. слева от поста)
стали одним из основных источников трафика для многих сайтов, новая
система аналитики позволит владельцам сайтов и страниц лучше понимать
как пользователи взаимодействуют и оптимизировать свои интернет-ресурсы,
основываясь на данных в реальном времени. Итак, 20 миллиардов событий в
день (200 тысяч в секунду) с задержкой не более 30 секунд, как же можно
этого достичь?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="tseli-servisa"&gt;Цели сервиса&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Дать пользователям надежные счетчики в реальном времени по ряду
    метрик&lt;/li&gt;
&lt;li&gt;Предоставлять анонимные данных - нельзя узнать кто конкретно были
    эти люди&lt;/li&gt;
&lt;li&gt;Продемонстрировать ценность социальных плагинов и виджетов. Что они
    дают сайту или бизнесу?&lt;/li&gt;
&lt;li&gt;Концепция воронки: сколько людей увидело плагин, сколько совершило
    действие, сколько было привлечено пользователей обратно на
    интернет-ресурс&lt;/li&gt;
&lt;li&gt;Сделать данные более оперативными: сокращение частоты обновлений с
    48 часов до 30 секунд&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="zadachi"&gt;Задачи&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Множество типов метрик, более 100: показы, лайки, просмотры и клики
    в новостной ленте, демография и.т.д.&lt;/li&gt;
&lt;li&gt;Большой поток данных: 20 миллиардов событий в день&lt;/li&gt;
&lt;li&gt;Неравномерное распределение данных: за большинство контента
    практически не голосуют, но некоторые материалы набирают просто
    невероятное количество лайков&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="neudavshiesia-popytki"&gt;Неудавшиеся попытки&lt;/h2&gt;
&lt;h2 id="mysql"&gt;MySQL&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;В каждой строке идентификатор и значение счетчика&lt;/li&gt;
&lt;li&gt;Привело к очень высокой активности в СУБД&lt;/li&gt;
&lt;li&gt;Статистика считается за каждые сутки, создание новых счетчиков в
    полночь приводило к большому скачку операций записи&lt;/li&gt;
&lt;li&gt;Пришлось пробовать искать способы решать проблему распределения
    счетчиков, пробовали учитывать временные зоны пользователей&lt;/li&gt;
&lt;li&gt;Пики операций записи неминуемо вели к блокировкам&lt;/li&gt;
&lt;li&gt;Решение оказалось не очень хорошо подходящим для данной конкретной
    задачи&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="schetchiki-v-pamiati"&gt;Счетчики в памяти&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Казалось бы: если столкнулись с проблемами ввода-вывода - надо
    перенести все в память&lt;/li&gt;
&lt;li&gt;Никаких проблем с масштабируемостью - счетчики находятся в памяти,
    обновление практически мгновенно, легко распределить по серверам&lt;/li&gt;
&lt;li&gt;Но на практике оказалось, что при таком подходе теряется точность,
    видимо из-за неатомарности операций или других последствий столь
    прямолинейной реализации, подробностей нет&lt;/li&gt;
&lt;li&gt;Погрешность даже в 1% посчитали неприемлемой и от данного варианта
    отказались&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="mapreduce"&gt;MapReduce&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Предыдущий вариант реализации был создан с помощью
    &lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt; + &lt;a href="/tag/hive/"&gt;Hive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Подход гибкий, легко справляется с большим входящим потоком
    информации и объемами данным&lt;/li&gt;
&lt;li&gt;Основной минус: даже близко не в реальном времени, слишком
    комплексная система&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="cassandra"&gt;Cassandra&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Вариант с &lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt; рассматривался, но так и не
    был реализован&lt;/li&gt;
&lt;li&gt;Причины были опубликованы достаточно сомнительные: высокие
    требования к доступности данных и производительности записи&lt;/li&gt;
&lt;li&gt;По всем данным у Cassandra нет абсолютно никаких проблем ни с одним,
    ни с другим&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="pobeditel-hbase-scribe-ptail-puma_1"&gt;Победитель: HBase + Scribe + Ptail + Puma&lt;/h1&gt;
&lt;p&gt;В целом система, на которой остановился выбор, выглядит следующим
образом:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HBase хранит все данные на распределенном кластере&lt;/li&gt;
&lt;li&gt;Используется система логов, новые данные дописываются в конец&lt;/li&gt;
&lt;li&gt;Система обрабатывает события и записывает результат в хранилище&lt;/li&gt;
&lt;li&gt;Пользовательский интерфейс забирает данные из хранилища и отображает
    пользователям&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Как обрабатывается один запрос:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Пользователь жмет на кнопку Like&lt;/li&gt;
&lt;li&gt;Браузер инициирует AJAX запрос к серверу Facebook&lt;/li&gt;
&lt;li&gt;Запрос записывается в лог в HDFS с помощью Scribe&lt;/li&gt;
&lt;li&gt;Ptail - внутренний инструмент для чтения данных из нескольких
    Scribe-логов, на выходе данные разделяются на три потока, которые
    отправляются в разные кластеры в разных датацентрах:&lt;ul&gt;
&lt;li&gt;Просмотры плагинов и виджетов&lt;/li&gt;
&lt;li&gt;Просмотры в новостной ленте&lt;/li&gt;
&lt;li&gt;Действия (плагины + новостная лента)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Puma - механизм для пакетной записи данных в HBase для снижения
    влияния "горячих" материалов:&lt;ul&gt;
&lt;li&gt;HBase может справиться с очень большим потоком операций записи,
    но популярные материалы могут заставить упереться во ввод-вывод
    даже её&lt;/li&gt;
&lt;li&gt;В среднем пакет запросов собирается в течении 1.5 секунд,
    хотелось бы больше - но из-за огромного количества URL очень
    быстро заканчивается оперативная память&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Отображение данных пользователю:&lt;ul&gt;
&lt;li&gt;Сам код для отображения написан на &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Работа с HBase осуществляется из &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Для взаимодействия по традиции используется
    &lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Система кэширования используется для ускорения отображения страниц:&lt;ul&gt;
&lt;li&gt;Чем более старые данные запрашиваются, тем реже они
    пересчитываются&lt;/li&gt;
&lt;li&gt;Многое зависит от типа запрашиваемой статистики&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MapReduce:&lt;ul&gt;
&lt;li&gt;Данные передаются для дальнейшего анализа с помощью Hive&lt;/li&gt;
&lt;li&gt;Сами логи удаляются через какой-то период времени&lt;/li&gt;
&lt;li&gt;Помимо этого старая система анализа статистики все еще в
    действии, она используется для регулярных проверок результатов
    новой системы, а также в роли запасного плана, если что-то
    пойдет не так&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="o-proekte"&gt;О проекте&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Продолжительность 5 месяцев&lt;/li&gt;
&lt;li&gt;2 с половиной разработчика самой системы&lt;/li&gt;
&lt;li&gt;2 разработчика пользовательского интерфейса&lt;/li&gt;
&lt;li&gt;Всего было задействовано около 14 человек, включая менеджмент,
    дизайнера и системных администраторов&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="napravleniia-razvitiia"&gt;Направления развития&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Списки популярных материалов: при текущем подходе их составление
    является сложной задачей&lt;/li&gt;
&lt;li&gt;Отдельные пользовательские счетчики&lt;/li&gt;
&lt;li&gt;Обобщение приложения для использования с другими сервисами, а не
    только с социальными плагинами&lt;/li&gt;
&lt;li&gt;Распределение системы на несколько датацентров&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;p&gt;У новых систем аналитики и сообщений много общего: большой поток
входящих данных для записи, HBase и требование работы в реальном
времени. Facebook делает ставку на HBase, Hadoop и HDFS, не смотря на
громоздкость системы, когда другие предпочитают Cassandra из-за её
простой схемы масштабирования, поддержку нескольких датацентров и
легкость в использовании. Какой путь окажется выигрышным - покажет
время.&lt;/p&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/c847fcc5/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2011/3/22/facebooks-new-realtime-analytics-system-hbase-to-process-20.html"&gt;Facebook's New Realtime Analytics System: HBase To Process 20 Billion Events Per Day&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/618b7573/" rel="nofollow" target="_blank" title="http://www.facebook.com/note.php?note_id=10150103900258920"&gt;Building Realtime Insights&lt;/a&gt; (&lt;a href="https://www.insight-it.ru/goto/6abcef48/" rel="nofollow" target="_blank" title="http://www.facebook.com/video/video.php?v=707216889765&amp;amp;oid=9445547199&amp;amp;comments"&gt;видео&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 24 Mar 2011 19:14:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-03-24:highload/2011/analitika-v-realnom-vremeni-ot-facebook/</guid><category>Facebook</category><category>Hadoop</category><category>HBase</category><category>Insights</category><category>Ptail</category><category>Puma</category><category>Scribe</category></item><item><title>Архитектура Одноклассников</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-odnoklassnikov/</link><description>&lt;p&gt;Сегодня представители &lt;a href="https://www.insight-it.ru/goto/2c99aef2/" rel="nofollow" target="_blank" title="http://www.odnoklassniki.ru"&gt;Одноклассников&lt;/a&gt;
рассказали о накопленном за 5 лет опыте по поддержанию высоконагруженного
проекта. Была опубликована довольно детальная информация о том, как
устроена эта социальная сеть для аудитории "постарше". Далее можно
прочитать мою версию материала, либо перейти на оригинал &lt;a href="https://www.insight-it.ru/goto/b762a864/" rel="nofollow" target="_blank" title="http://habrahabr.ru/company/odnoklassniki/blog/115881/"&gt;по сссылке&lt;/a&gt;.&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/windows/"&gt;Windows&lt;/a&gt; и &lt;a href="/tag/opensuse/"&gt;openSUSE&lt;/a&gt; - основные
    операционные системы&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/java/"&gt;Java&lt;/a&gt; - основной язык программирования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/c/"&gt;С/С++&lt;/a&gt; - для некоторых модулей&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/gwt/"&gt;GWT&lt;/a&gt; - реализация динамического веб-интерфейса&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/tomcat/"&gt;Apache Tomcat&lt;/a&gt; - сервера приложений&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/jboss/"&gt;JBoss 4&lt;/a&gt; - сервера бизнес-логики&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/lvs/"&gt;LVS&lt;/a&gt; и &lt;a href="/tag/ipvs/"&gt;IPVS&lt;/a&gt; - балансировка нагрузки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mssql/"&gt;MS SQL 2005 и 2008&lt;/a&gt; - основная СУБД&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/berkleydb/"&gt;BerkleyDB&lt;/a&gt; - дополнительная СУБД&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/lucene/"&gt;Apache Lucene&lt;/a&gt; - индексация и поиск текстовой
    информации&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;До 2.8 млн. пользователей онлайн в часы пик&lt;/li&gt;
&lt;li&gt;7,5 миллиардов запросов в день (150 000 запросов в секунду в часы
    пик)&lt;/li&gt;
&lt;li&gt;2 400 серверов и систем хранения данных, из которых 150 являются
    веб-серверами&lt;/li&gt;
&lt;li&gt;Сетевой трафик в час пик: 32 Gb/s&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="oborudovanie"&gt;Оборудование&lt;/h2&gt;
&lt;p&gt;Сервера используются двухпроцессорные с 4 ядрами, объемом памяти от 4 до
48 Гб. В зависимости от роли сервера данные хранятся либо в памяти, либо
на дисках, либо на внешних системах хранения данных.&lt;/p&gt;
&lt;p&gt;Все оборудование размещено в 3 датацентрах, объединенных в оптическое
кольцо. На данный момент на каждом из маршрутов пропускная способность
составляет 30Гбит/с. Каждый из маршрутов состоит из физически
независимых друг от друга оптоволоконных пар, которые агрегируются в
общую &amp;ldquo;трубу&amp;rdquo; на корневых маршрутизаторах.&lt;/p&gt;
&lt;p&gt;Сеть физически разделена на внутреннюю и внешнюю, разные интерфейсы
серверов подключены в разные коммутаторы и работают в разных сетях. По
внешней сети HTTP сервера, общаются с Интернетом, по внутренней сети все
сервера общаются между собой.&amp;nbsp;Топология внутренней сети &amp;ndash; звезда.
Сервера подключены в L2 коммутаторы (access switches), которые, в свою
очередь, подключены как минимум двумя гигабитными линками к aggregation
стеку маршрутизаторов. Каждый линк идет к отдельному коммутатору в
стеке. Для того, чтобы эта схема работала, используется
протокол&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/a03e0548/" rel="nofollow" target="_blank" title="http://ru.wikipedia.org/wiki/RSTP"&gt;RSTP&lt;/a&gt;. При необходимости,
подключения access коммутаторов к agregation стеку осуществляются более
чем двумя линками с использованием link aggregation портов.&amp;nbsp;Aggregation
коммутаторы подключены 10Гб линками в корневые маршрутизаторы, которые
обеспечивают как связь между датацентрами, так и связь с внешним
миром.&amp;nbsp;Используются коммутаторы и маршрутизаторы от компании Cisco.&lt;/p&gt;
&lt;p&gt;Для связи с внешним миром используются прямые подключения с несколькими
крупнейшими операторами связи, общий сетевой&amp;nbsp;трафик в часы пик доходит
до 32Гбит/с.&lt;/p&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;Архитектура проекта имеет традиционную многоуровневую структуру:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;презентационный уровень;&lt;/li&gt;
&lt;li&gt;уровень бизнес-логики;&lt;/li&gt;
&lt;li&gt;уровень кэширования;&lt;/li&gt;
&lt;li&gt;уровень баз данных;&lt;/li&gt;
&lt;li&gt;уровень инфраструктуры (логирование, конфигурация и мониторинг).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Код проекта в целом написан на Java, но есть исключения в виде модулей
для кэширования на C и C++.
Java был выбран так как он является удобным языком для разработки,
доступно множество наработок в различных сферах, библиотек и opensource
проектов.&lt;/p&gt;
&lt;h3 id="prezentatsionnyi-uroven"&gt;Презентационный уровень&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Используем собственный фреймворк, позволяющий строить композицию
    страниц на языке Jаvа, с использованием собственные GUI фабрик (для
    оформления текста, списков, таблиц и портлетов).&lt;/li&gt;
&lt;li&gt;Страницы состоят из независимых блоков (обычно портлетов), что
    позволяет обновлять информацию на них частями с помощью AJAX
    запросов.&lt;/li&gt;
&lt;li&gt;При данном подходе одновременно обеспечивается минимум перезагрузок
    страниц для пользователей с включенным JavaScript, так и полная
    работоспособность сайта для пользователей, у которых он отключен.&lt;/li&gt;
&lt;li&gt;Google Web Toolkit используется для реализации функциональные
    компонент, таких как Сообщения, Обсуждения и Оповещения, а также все
    динамических элементов (меню шорткатов, метки на фотографиях,
    сортировка фотографий,&amp;nbsp;ротация подарков и.т.д.).&amp;nbsp;В GWT используются
    UIBinder и HTMLPanel для создания интерфейсов.&lt;/li&gt;
&lt;li&gt;Кешируются все внешние ресурсы (Expires и Cache-Control заголовки).
    CSS и JavaScript файлы минимизируются и сжимаются (gzip).&lt;/li&gt;
&lt;li&gt;Для уменьшения количества HTTP запросов с браузера, все JavaScript и
    CSS файлы объединяются в один. Маленькие графические изображения
    объединяются в спрайты.&lt;/li&gt;
&lt;li&gt;При загрузке страницы скачиваются только те ресурсы, которые на
    самом деле необходимы для начала работы.&lt;/li&gt;
&lt;li&gt;Никаких универсальных CSS селекторов. Стараются не использовать
    типовые селекторы (по имени тэга), что повышает скорость отрисовки
    страниц внутри браузера.&lt;/li&gt;
&lt;li&gt;Если необходимы CSS expressions, то пишутся &amp;laquo;одноразовые&amp;raquo;. По
    возможности избегаются фильтры.&lt;/li&gt;
&lt;li&gt;Кешируется обращения к DOM дереву, а так же свойства элементов,
    приводящие к reflow. Обновляется DOM дерево в &amp;laquo;оффлайне&amp;raquo;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="uroven-biznes-logiki_1"&gt;Уровень бизнес-логики&lt;/h2&gt;
&lt;p&gt;На уровне бизнес логики располагаются около 25 типов серверов и
компонентов, общающихся между собой через удаленные интерфейсы. Каждую
секунду происходит около 3 миллионов удаленных запросов между этими
модулями.
Сервера на уровне бизнес логики разбиты на группы. Каждая группа
обрабатывает различные события. Есть механизм маршрутизации событий, то
есть любое событие или группу событий можно выделить и направить на
обработку на определенную группу серверов.&amp;nbsp;При общении серверов между
собой используется свое решение, основанное на&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/5e85a755/" rel="nofollow" target="_blank" title="http://www.jboss.org/jbossremoting"&gt;JBoss
Remoting&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="uroven-keshirovaniia"&gt;Уровень кэширования&lt;/h2&gt;
&lt;p&gt;Для кэширования данных используется самописный модуль
odnoklassniki-cache. Он предоставляет возможность хранения данных в
памяти средствами Java Unsafe. Кэшируются все данные, к которым
происходит частое обращение, например: профили пользователей, списки
участников сообществ, информация о самих сообществах, граф связей
пользователей и групп, праздники, мета информация о фотографиях и многое
другое.Для хранения больших объемов данных в памяти используется память
Java off heap memory для снятия ненужной нагрузки с сборщика
мусора.&amp;nbsp;Кеши могут использовать локальный диск для хранения данных, что
превращает их в высокопроизводительный сервер БД.&amp;nbsp;Кеш сервера, кроме
обычных операций ключ-значение, могут выполнять запросы по данным,
хранящимся в памяти, минимизируют таким образом передачу данных по сети.
Используется map-reduce для выполнения запросов и операций на кластере.
В особо сложных случаях, например для реализации запросов по социальному
графу, используется язык C. Это помогает повысить производительность.&lt;/p&gt;
&lt;p&gt;Данные распределяются между кластерами кеш серверов, а также
используется репликация партиций для обеспечения надежности.&amp;nbsp;Иногда
требования к быстродействию настолько велики, что используются локальные
короткоживущие кеши данных полученных с кеш серверов, расположенные
непосредственно в памяти серверов бизнес логики.&lt;/p&gt;
&lt;p&gt;Для примера, один сервер, кэширующий граф связей пользователей, в час
пик может обработать около 16 600 запросов в секунду. Процессоры при
этом заняты до 7%, максимальный load average за 5 минут &amp;mdash; 1.2.
Количество вершин графа - более 85 миллионов, связей 2.5 миллиарда. В
памяти граф занимает 30 GB.&lt;/p&gt;
&lt;h2 id="uroven-baz-dannykh"&gt;Уровень баз данных&lt;/h2&gt;
&lt;p&gt;Суммарный объем данных без резервирования составляет 160Тб. Используются
два решения для хранения данных: MS SQL и BerkeleyDB. Данные хранятся в
нескольких копиях, в зависимости от их типа от двух до четырех. Полное
резервное копирование всех данных осуществляется раз в сутки, плюс
каждые 15 минут делаются резервные копии новых данных. В результате
максимально возможная потеря данных составляет 15 минут.&lt;/p&gt;
&lt;p&gt;Сервера с MS SQL объединены в failover кластера, при выходе из строя
одного из серверов, находящийся в режиме ожидания сервер берет на себя
его функции. Общение с MS SQL происходит посредством JDBC драйверов.&lt;/p&gt;
&lt;p&gt;Используются как вертикальное, так и горизонтальное разбиение данных,
т.е. разные группы таблиц располагаются на разных серверах (вертикальное
партиционирование), а данные больших таблицы дополнительно
распределяются между серверами (горизонтальное партиционирование).
Встроенный в СУБД аппарат партиционирования не используется &amp;mdash; весь
процесс реализован на уровне бизнес-логики.&amp;nbsp;Распределенные транзакции не
используются &amp;mdash; всё только в пределах одного сервера. Для обеспечения
целостности, связанные данные помещаются на один сервер или, если это
невозможно, дополнительно разрабатывается логика обеспечения целостности
данных.&amp;nbsp;В запросах к БД не используются JOIN даже среди локальных таблиц
для минимизации нагрузки на CPU. Вместо этого используется
денормализация данных или JOIN происходят на уровне бизнес сервисов, что
позволяет осуществлять JOIN как с данными из баз данных, так и с данными
из кэша.&amp;nbsp;При проектировании структуры данных не используются внешние
ключи, хранимые процедуры и триггеры. Опять же для снижения потребления
вычислительных ресурсов на серверах баз данных.
SQL операторы DELETE также используются с осторожностью &amp;mdash; это самая
тяжелая операция. Данные удаляются чаще всего через маркер: запись
сначала отмечается как удаленная, а потом удаляется окончательно с
помощью фонового процесса.&amp;nbsp;Широко используются индексы, как обычные, так
и кластерные. Последние для оптимизации наиболее высокочастотных
запросов в таблицу.&lt;/p&gt;
&lt;p&gt;Используется C реализация BerkleyDB версии 4.5. Для работы с BerkleydDB
используется своя библиотека, позволяющая организовывать двухнодовые
master-slave кластера с использованием родной BDB репликация. Запись
происходит только в master, чтение происходит с обеих нод. Данные
хранятся в tmpfs, transaction логи сохраняются на дисках. Резервная
копия логов делается каждые 15 минут. Сервера одного кластера размещены
на разных лучах питания дабы не потерять обе копии одновременно. Помимо
прочего, BerkleyDB используется и в роли очереди заданий.&lt;/p&gt;
&lt;p&gt;Внутри системы используется взвешенный round robin, а также вертикальное
и горизонтальное разбиение данных как на уровне СУБД, так и на уровне
кэширования.&lt;/p&gt;
&lt;p&gt;В разработке новое решение для хранения данных, так как необходим еще
более быстрый и надежный доступ к данным.&lt;/p&gt;
&lt;h2 id="uroven-infrastruktury"&gt;Уровень инфраструктуры&lt;/h2&gt;
&lt;p&gt;Для агрегации статистики используется собственная библиотека, основанная
на log4j. Сохраняется такая информация, как количество вызовов, среднее,
максимальное и минимальное время выполнения, количество ошибок. Данные
сохраняются во временные базы, но раз в минуту данные переносятся из них
в общий склад данных (data warehouse), а временные базы очищаются. Сам
склад реализован на базе решений от Microsoft: MS SQL 2008 и сиситема
генерации отчетов Reporting Services. Он расположен на 13 серверах,
находящихся в отдельной от production среде. Некоторые из них отвечают
за статистику в реальном времени, а некоторые за ведение и
предоставление доступа к архиву. Общий объем статистических данных
составляет 13Тб.&amp;nbsp;Планируется внедрение многомерного анализа статистики
на основе OLAP.&lt;/p&gt;
&lt;p&gt;Управление сервисами происходит через самописную централизованную
систему конфигурации. Через веб-интерфейс доступно изменение
расположения портлетов, конфигурации кластеров, изменение логики
сервисов и прочее. Вся конфигурация сохраняется в базе данных. Каждый из
серверов периодически проверяет, есть ли обновления для приложений,
которые на нем запущены, и, если есть, применяет их.&lt;/p&gt;
&lt;p&gt;Мониторинг логически разделен на две части:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Мониторинг сервисов и компонентов&lt;/li&gt;
&lt;li&gt;Мониторинг ресурсов, оборудования и сети&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Система мониторинга сервисов также самописная и основывается на
оперативных данных с упомянутого выше склада. Мониторинг ресурсов и
здоровья оборудования же онован на Zabbix, а статистика по
использованию ресурсов серверов и сети накапливаетя в Cacti.&amp;nbsp;Для
предпринятия мер по устранению чрезвычайных ситуаций работают дежурные,
которые следят за всеми основными параметрами.&amp;nbsp;Оповещения о наиболее
критичных аномалиях приходят по смс, остальные оповещения отсылаются по
емейлу.&lt;/p&gt;
&lt;h2 id="komanda"&gt;Команда&lt;/h2&gt;
&lt;p&gt;Над проектом работают около 70 технических специалистов:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;40 разработчиков;&lt;/li&gt;
&lt;li&gt;20 системных администраторов и инженеров;&lt;/li&gt;
&lt;li&gt;8 тестеров.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Все разработчики разделены на небольшие команды до 3х человек. Каждая из
команд работает автономно и разрабатывает либо какой-то новый сервис,
либо работает над улучшением существующих. В каждой команде есть
технический лидер или архитектор, который ответственен за архитектуру
сервиса, выбор технологий и подходов. На разных этапах к команде могут
примыкать дизайнеры, тестеры и системные администраторы.&lt;/p&gt;
&lt;p&gt;Разработка ведется итерациями в несколько недель. Как пример жизненного
цикла разработки можно привести 3х недельный цикл:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;определение архитектуры;&lt;/li&gt;
&lt;li&gt;разработка, тестирование на компьютерах разработчиков;&lt;/li&gt;
&lt;li&gt;тестирование на pre-production среде, релиз на production среду.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Практически весь новый функционал делается &amp;laquo;отключаемым&amp;raquo;, типичный
процесс запуска новой функциональной возможности:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Функционал разрабатывается и попадает в production релиз;&lt;/li&gt;
&lt;li&gt;Через централизованную систему конфигурации функционал включается
    для небольшой части пользователей;&lt;/li&gt;
&lt;li&gt;Анализируется статистика активности пользователей, нагрузка на
    инфраструктуру;&lt;/li&gt;
&lt;li&gt;Если предыдущий этап прошел успешно, функционал включается
    постепенно для все большей аудитории;&lt;/li&gt;
&lt;li&gt;Если в процессе запуска собранная статистика выглядет
    неудовлетворительно, либо непозволительно вырастает нагрузка на
    инфраструктуру, то функционал отключается, анализируются причины,
    исправляются ошибки, происходит оптимизация и все повторяется с
    начала.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;В отличии от остальных популярных социальных сетей в Одноклассниках
    используются технологии, рассчитанные в первую очередь на
    корпоративный рынок, начиная от обоих СУБД и заканчивая
    операционными системами.&lt;/li&gt;
&lt;li&gt;Во многом этот факт обуславливает комплексный подход к генерации
    пользовательского интерфейса, не слишком высокую производительность
    и многие другие особенности этой социальной сети.&lt;/li&gt;
&lt;li&gt;Использование "тяжелых" технологий с самого начала оставило
    Одноклассники с большим количеством доставшегося по наследству от
    ранних версий устаревшего кода и купленных давно лицензий на
    проприетарный софт, которые выступают в роли оков, от которых
    довольно сложно избавиться.&lt;/li&gt;
&lt;li&gt;Возможно эти факторы и являются одними из основных препятствий на
    пути к завоеванию большей доли рынка и быстрому развитию платформы
    как в функциональном, так и техническом плане.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 22 Mar 2011 00:17:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-03-22:highload/2011/arkhitektura-odnoklassnikov/</guid><category>BerkleyDB</category><category>C. GWT</category><category>IPVS</category><category>Java</category><category>Jboss</category><category>Lucene</category><category>LVS</category><category>MSSQL</category><category>openSUSE</category><category>Tomcat</category><category>Windows</category><category>Архитектура Одноклассников</category><category>Масштабируемость</category><category>Одноклассники</category></item><item><title>Архитектура Dropbox</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-dropbox/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/4f16cf21/" rel="nofollow" target="_blank" title="http://db.tt/4TDAr1L"&gt;Dropbox&lt;/a&gt; - это самый простой способ...
&lt;img alt="Dropbox" class="right" src="https://www.insight-it.ru/images/dropbox_logo.png" title="Dropbox Logo"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Хранить файлы в безопасном месте&lt;/li&gt;
&lt;li&gt;Делиться файлами с другими людьми&lt;/li&gt;
&lt;li&gt;Постоянно иметь к ним доступ вне зависимости от своего месторасположения&lt;/li&gt;
&lt;/ul&gt;
&lt;!--more--&gt;
&lt;h2 id="vzryvnoi-rost"&gt;Взрывной рост&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;1 миллион файлов сохраняются в Dropbox каждые 15 минут (по
    презентации это больше, чем твитов в Twitter за тот же период
    времени, но это &lt;a href="https://www.insight-it.ru/highload/2011/arkhitektura-twitter-dva-goda-spustya/"&gt;несколько преувеличено&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Одно из самых скачиваемых приложений, уступает лишь Skype&lt;/li&gt;
&lt;li&gt;Важная часть жизни многих пользователей: "не могу жить без этого"&lt;/li&gt;
&lt;li&gt;Рост обеспечен "сарафанным радио", практически без рекламы&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Dropbox" class="responsive-img" src="https://www.insight-it.ru/images/dropbox.jpeg" title="Dropbox"/&gt;&lt;/p&gt;
&lt;h2 id="komanda-v-nachale-proekta"&gt;Команда в начале проекта&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Все хорошие друзья&lt;/li&gt;
&lt;li&gt;Самые умные, голодные и страстные, которых они знали :)&lt;/li&gt;
&lt;li&gt;Для каждого это была первая реальная работа, очень ограниченный опыт
    в данной индустрии&lt;/li&gt;
&lt;li&gt;Эти качества хорошо сочетаются с итеративной методологией разработки
    на &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="rannie-dostizheniia"&gt;Ранние достижения&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Reverse engineering приложения Finder для Mac OSX 10.4/5 для
    отображения иконок статуса синхронизации ("в процессе", "все
    готово")&lt;/li&gt;
&lt;li&gt;Сложная инфраструктура HTTP нотификаций для избежания регулярного
    опроса серверов на каждом клиенте, основанная на
    &lt;a href="/tag/twisted/"&gt;Twisted&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Общая кодовая база, работающая на всех основных операционных
    системах: Windows, Mac OS, Linux (на основе PyObjC, wxPython,
    ctypes, py2exe, py2app, PyWin32)&lt;/li&gt;
&lt;li&gt;Горизонтально масштабируемое хранилище для информации о файлах на
    основе &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Собственная инфраструктура для аналитики в реальном времени&lt;/li&gt;
&lt;li&gt;Собственный механизм выделения памяти для Python, позволивший
    сократить её потребление на 90%&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="sfery-primeneniia-python"&gt;Сферы применения Python&lt;/h2&gt;
&lt;p&gt;По сути он используется во всех частях проекта:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;логика backend синхронизации;&lt;/li&gt;
&lt;li&gt;клиенты для основных ОС (Windows, Mac, Linux);&lt;/li&gt;
&lt;li&gt;контроллер основного сайта;&lt;/li&gt;
&lt;li&gt;обработка API запросов;&lt;/li&gt;
&lt;li&gt;аналитика.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Исключения из-за ограничений по доступной оперативной памяти:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Android (мог бы быть Jython)&lt;/li&gt;
&lt;li&gt;iPhone (мог бы быть Cpython)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="pochemu-imenno-python"&gt;Почему именно Python?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Легок в изучении и понимании:&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;новые люди легко втягиваются в процесс;&lt;/li&gt;
&lt;li&gt;позволяет людям переключаться с проекта на проект.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Легок в написании:&lt;/strong&gt; важно для быстрой реализации функционала и
    выпуска новых версий.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Легок в изменении:&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;нет необходимости в перекомпиляции;&lt;/li&gt;
&lt;li&gt;высокая скорость итерации;&lt;/li&gt;
&lt;li&gt;динамическая типизация:&lt;ul&gt;
&lt;li&gt;рефакторинг очень прост;&lt;/li&gt;
&lt;li&gt;уменьшение прямых зависимостей модулей;&lt;/li&gt;
&lt;li&gt;динамические инструменты.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Позволяет обеспечивать качество&lt;/strong&gt; путем создания более-менее
    работающей версии продукта и доведения её до качественного уровня
    путем быстрых итераций.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="ne-bez-trudnostei"&gt;Не без трудностей&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Аргх!!! Python потребляет слишком много оперативной памяти и о-о-очень
медленный!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;На серверной стороне можно вообще не заморачиваться и купить больше
оборудования - всегда помогает. Но для клиентской части такой подход не
прокатит, если только Вы не собираетесь купить новые компьютеры и
телефоны всем своим клиентам.&lt;/p&gt;
&lt;p&gt;Как с этим бороться?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Убедитесь, что все длинные внутренние циклы выполняются в C (может
    сэкономить до 44% процессорного времени)&lt;/li&gt;
&lt;li&gt;С оптимизацией потребления вычислительных ресурсов все просто:&lt;ul&gt;
&lt;li&gt;есть много готовых решений для профайлинга;&lt;/li&gt;
&lt;li&gt;проблемный код чаще всего не раскидан по всей кодовой базе.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;С оптимизацией потребляемой памяти все сложнее:&lt;ul&gt;
&lt;li&gt;нет готовых решений для профайлинга память (одновременно в
    Python и C);&lt;/li&gt;
&lt;li&gt;много потенциальных причин для повышенного потребления памяти:&lt;ul&gt;
&lt;li&gt;утечки в Python и C;&lt;/li&gt;
&lt;li&gt;фрагментация памяти;&lt;/li&gt;
&lt;li&gt;неэффективное её использование.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Как исправить? Однозначного решения нет - чаще всего приходится
    перерывать всю кодовую базу в поисках неоптимальных мест и
    источников утечек.&lt;/li&gt;
&lt;li&gt;Почему память фрагментируется?&lt;ul&gt;
&lt;li&gt;Большинство объектов расположены в heap памяти (большой
    последовательный кусок, выделенный приложению);&lt;/li&gt;
&lt;li&gt;Много маленьких объектов вперемешку с большими;&lt;/li&gt;
&lt;li&gt;Много временных объектов вперемешку с постоянными;&lt;/li&gt;
&lt;li&gt;В CPython нет сборщика мусора, позволяющего собрать объекты
    компактно в одной части heap'а.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Решением проблемы с фрагментацией стал собственный аллокатор
    памяти (механизм её выделения под определенные типы объектов):&lt;ul&gt;
&lt;li&gt;В Dropbox обнаружили, что большая часть heap-памяти
    захламляется контейнерами с метаданными файлов, которые
    синхронизируется;&lt;/li&gt;
&lt;li&gt;Было решено вынести их куда-нибудь еще;&lt;/li&gt;
&lt;li&gt;CPython позволяет управлять процессом выделения памяти для
    типов данных расширений;&lt;/li&gt;
&lt;li&gt;Делается это с помощью простых структур на C;&lt;/li&gt;
&lt;li&gt;"Not Rocket Science, just C code";&lt;/li&gt;
&lt;li&gt;Выделяются области памяти по 4Мб, состоящие из заголовка и
    буферов фиксированной длины.&lt;/li&gt;
&lt;li&gt;Что насчет внутренней фрагментации?&lt;ul&gt;
&lt;li&gt;по идее же даже в одном буфере останутся данные, весь
    блок нельзя будет освободить...&lt;/li&gt;
&lt;li&gt;потенциально этот факт может привести к еще большему
    расходу оперативной памяти;&lt;/li&gt;
&lt;li&gt;но это оказывается не так, если учесть, что все объекты
    являются временными, то есть живут не долго!&lt;/li&gt;
&lt;li&gt;в результате Dropbox редко использует более 100Мб памяти
    для больших синхронизаций (раньше эта цифра могла
    достигать 1.5Гб)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Но писать C расширение для каждой структуры данных, которую
    может понадобиться вынести в отдельный аллокатор - дело
    неблагодарное:&amp;nbsp;в результате эта история закончилась
    изменением &lt;code&gt;type_new()&lt;/code&gt; в &lt;code&gt;Objects/typeobject.c&lt;/code&gt; для того,
    чтобы можно было указывать &lt;code&gt;__use_region_allocator__&lt;/code&gt;
    = True** для тех классов, которым требуется такой механизм
    выделения памяти.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Python стал ключом к технической реализации обоих сторон Dropbox:
    серверной и клиентской.&lt;/li&gt;
&lt;li&gt;Залогом успеха является максимальная простота с пользовательской
    точки зрения: "положил файл в папку и он становится доступен
    отовсюду"&lt;/li&gt;
&lt;li&gt;Dropbox не брали на себя реализацию стороннего функционала, вроде
    собственно создания хранилища файлов - используется &lt;a href="https://www.insight-it.ru/goto/d0869408/" rel="nofollow" target="_blank" title="http://aws.amazon.com/s3/"&gt;Amazon S3&lt;/a&gt;, что позволило им запуститься очень
    быстро и стремительно.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Лучше решать одну задачу, но качественно, чем много, но так себе!&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/afbbb2b7/" rel="nofollow" target="_blank" title="http://pycon.blip.tv/file/4878722/"&gt;Видео&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/f70a4522/" rel="nofollow" target="_blank" title="https://www.dropbox.com/s/mwxsyxtu9qieboo/pycon%20talk%20minus%20video.pptx"&gt;Презентация&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 18 Mar 2011 18:57:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-03-18:highload/2011/arkhitektura-dropbox/</guid><category>ctypes</category><category>Dropbox</category><category>MySQL</category><category>py2app</category><category>py2exe</category><category>PyObjC</category><category>PyWin32</category><category>Twisted</category><category>wxPython</category><category>Архитектура Dropbox</category><category>Масштабируемость</category></item><item><title>Архитектура Twitter. Два года спустя.</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-twitter-dva-goda-spustya/</link><description>&lt;p&gt;В далеком 2008м я уже публиковал статью про &lt;a href="https://www.insight-it.ru/highload/2008/arkhitektura-twitter/"&gt;архитектуру Twitter&lt;/a&gt;, но время летит
стремительно и она уже абсолютно устарела. За это время аудитория
Twitter росла просто фантастическими темпами и многое поменялось и с
технической точки зрения. Интересно что новенького у одного из самых
популярных социальных интернет-проектов?&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;3 год, 2 месяца и 1 день потребовалось Twitter, чтобы набрать 1
    миллиард твитов&lt;/li&gt;
&lt;li&gt;На сегодняшний день, чтобы отправить миллиард твитов пользователям
    нужна всего одна неделя&lt;/li&gt;
&lt;li&gt;752% рост аудитории за 2008 год&lt;/li&gt;
&lt;li&gt;1358% рост аудитории за 2009 год&amp;nbsp;(без учета API, по данным comScore)&lt;/li&gt;
&lt;li&gt;175 миллионов зарегистрированных пользователей на сентябрь 2010 года&lt;/li&gt;
&lt;li&gt;460 тысяч регистраций пользователей в день&lt;/li&gt;
&lt;li&gt;9й сайт в мире по популярности (по данным Alexa, год назад был на 12
    месте)&lt;/li&gt;
&lt;li&gt;50 миллионов твитов в день год назад, 140 миллионов твитов в день
    месяц назад, 177 миллионов твитов в день на 11 марта 2011г.&lt;/li&gt;
&lt;li&gt;Рекорд по количеству твитов за секунду 6939, установлен через минуту
    после того, как Новый Год 2011 наступил в Японии&lt;/li&gt;
&lt;li&gt;600 миллионов поисков в день&lt;/li&gt;
&lt;li&gt;Лишь 25% трафика приходится на веб сайт, остальное идет через API&lt;/li&gt;
&lt;li&gt;Росто числа мобильных пользователей за последний год 182%&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;6 миллиардов&lt;/strong&gt; запросов к API в день, около 70 тысяч в секунду&lt;/li&gt;
&lt;li&gt;8, 29, 130, 350, 400 - это количество сотрудников Twitter на январь
    2008, январь 2009, январь 2010, январь и март 2011, соответственно&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Самая свежая &lt;a href="https://www.insight-it.ru/goto/682783c0/" rel="nofollow" target="_blank" title="http://blog.twitter.com/2011/03/numbers.html"&gt;статистика про Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; + &lt;code&gt;mod_proxy&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/unicorn/"&gt;Unicorn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt; +&amp;nbsp;&lt;a href="/tag/ror/"&gt;Ruby on Rails&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/flock/"&gt;Flock&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/kestrel/"&gt;Kestrel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/cassandra/"&gt;Cassandra&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/scribe/"&gt;Scribe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/hadoop/"&gt;Hadoop&lt;/a&gt;, &lt;a href="/tag/hbase/"&gt;HBase&lt;/a&gt; и &lt;a href="/tag/pig/"&gt;Pig&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Сравните с аналогичным разделом предыдущей статьи о Twitter - увидите
много новых лиц, подробнее ниже.&lt;/p&gt;
&lt;h2 id="oborudovanie"&gt;Оборудование&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Сервера расположены в NTT America&lt;/li&gt;
&lt;li&gt;Никаких облаков и виртуализации, существующие решения страдают
    слишком высокими задержками&lt;/li&gt;
&lt;li&gt;Более тысячи серверов&lt;/li&gt;
&lt;li&gt;Планируется переезд в собственный датацентр&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="chto-takoe-tvit"&gt;Что такое твит?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Сообщение длиной до 140 символов + метаданные&lt;/li&gt;
&lt;li&gt;Типичные запросы:&lt;ul&gt;
&lt;li&gt;по идентификатору&lt;/li&gt;
&lt;li&gt;по автору&lt;/li&gt;
&lt;li&gt;по @упоминаниям пользователей&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Процесс обработки запроса в Twitter" class="responsive-img" src="https://www.insight-it.ru/images/twitter-request-flow.jpeg" title="Процесс обработки запроса в Twitter"/&gt;&lt;/p&gt;
&lt;h3 id="unicorn"&gt;Unicorn&lt;/h3&gt;
&lt;p&gt;Сервер приложений для Rails:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Развертывание новых версий кода &lt;strong&gt;без простоя&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;На 30% меньше расход вычислительных ресурсов и оперативной памяти,
    по сравнению с другими решениями&lt;/li&gt;
&lt;li&gt;Перешли с &lt;code&gt;mod_proxy_balancer&lt;/code&gt; на &lt;code&gt;mod_proxy_pass&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="rails"&gt;Rails&lt;/h3&gt;
&lt;p&gt;Используется в основном для генерации страниц, работа за сценой
реализована на чистом Ruby или Scala.&lt;/p&gt;
&lt;p&gt;Столкнулись со следующими проблемами:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Проблемы с кэшированием, особенно по части инвалидации&lt;/li&gt;
&lt;li&gt;ActiveRecord генерирует не самые удачные SQL-запросы, что замедляло
    время отклика&lt;/li&gt;
&lt;li&gt;Высокие задержки в очереди и при репликации&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="memcached"&gt;memcached&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;memcached не идеален. Twitter начал сталкиваться с Segmentation
    Fault в нем очень рано.&lt;/li&gt;
&lt;li&gt;Большинство стратегий кэширования основываются на длинных TTL
    (более минуты).&lt;/li&gt;
&lt;li&gt;Вытеснение данных делает его непригодным для важных конфигурационных
    данных (например флагов "темного режима", о котором пойдет речь
    ниже).&lt;/li&gt;
&lt;li&gt;Разбивается на несколько пулов для улучшения производительности и
    снижения риска вытеснения.&lt;/li&gt;
&lt;li&gt;Оптимизированная библиотека для доступа к memcached из Ruby на
    основе libmemcached + FNV hash, вместо чистого Ruby и md5.&lt;/li&gt;
&lt;li&gt;Twitter является одним их наиболее активных проектов, участвующих в
    разработке libmemcached.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="mysql"&gt;MySQL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Разбиение больших объемов данных является тяжелой задачей.&lt;/li&gt;
&lt;li&gt;Задержки в репликации и вытеснение данных из кэша является причиной
    нарушения целостности данных с точки зрения конечного пользователя.&lt;/li&gt;
&lt;li&gt;Блокировки создают борьбу за ресурсы для популярных данных.&lt;/li&gt;
&lt;li&gt;Репликация однопоточна и происходит недостаточно быстро.&lt;/li&gt;
&lt;li&gt;Данные социальных сетей плохо подходят для реляционных СУБД:&lt;ul&gt;
&lt;li&gt;NxN отношения, социальный граф и обход деревьев - не самые
    подходящие задачи для таких баз данных&lt;/li&gt;
&lt;li&gt;Проблемы с дисковой подсистемой (выбор файловой системы,
    noatime, алгоритм планирования)&lt;/li&gt;
&lt;li&gt;ACID практически не требуется&lt;/li&gt;
&lt;li&gt;Для очередей также практически непригодны&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Twitter сталкивался с большими проблемами касательно таблиц
    пользователей и их статусов&lt;/li&gt;
&lt;li&gt;Читать данные с мастера при Master/Slave репликации = медленная
    смерть&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="flockdb"&gt;FlockDB&lt;/h3&gt;
&lt;p&gt;Масштабируемое хранилище для данных социального графа:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Разбиение данных через Gizzard&lt;/li&gt;
&lt;li&gt;Множество серверов MySQL в качестве низлежащей системы хранения&lt;/li&gt;
&lt;li&gt;В Twitter содержит 13 миллиардов ребер графа и обеспечивает 20 тысяч
    операций записи и 100 тысяч операций чтения в секунду&lt;/li&gt;
&lt;li&gt;Грани хранятся и индексируются в обоих направлениях&lt;/li&gt;
&lt;li&gt;Поддерживает распределенный подсчет количества строк&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/4fe0530b/" rel="nofollow" target="_blank" title="https://github.com/twitter/flockdb"&gt;Open source!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Среднее время на выполнение операций:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Подсчет количества строк: 1мс&lt;/li&gt;
&lt;li&gt;Временные запросы: 2мс&lt;/li&gt;
&lt;li&gt;Запись: 1мс для журнала, 16мс для надежной записи&lt;/li&gt;
&lt;li&gt;Обход дерева: 100 граней/мс&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Подробнее про эволюцию систем хранения данных в Twitter &lt;a href="https://www.insight-it.ru/goto/32077a90/" rel="nofollow" target="_blank" title="http://www.slideshare.net/nkallen/q-con-3770885"&gt;в презентации
Nick Kallen&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="cassandra"&gt;Cassandra&lt;/h3&gt;
&lt;p&gt;Распределенная система хранения данных, ориентированная на работу в
реальном времени:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Изначально разработана в &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Очень высокая производительность на запись&lt;/li&gt;
&lt;li&gt;Из слабых сторон: высокая задержка при случайном доступе&lt;/li&gt;
&lt;li&gt;Децентрализованная, способна переносить сбои оборудования&lt;/li&gt;
&lt;li&gt;Гибкая схема данных&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Планируется полный переход на нее по
    следующему алгоритму:&lt;/del&gt;&lt;ul&gt;
&lt;li&gt;&lt;del&gt;Все твиты пишутся и в Cassandra
    и в MySQL&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Динамически часть операций
    чтения переводится на Cassandra&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Анализируется реакция системы,
    что сломалось&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Полностью отключаем чтение из
    Cassandra, чиним неисправности&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;del&gt;Начинаем сначала&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/e83e4e8e/" rel="nofollow" target="_blank" title="http://engineering.twitter.com/2010/07/cassandra-at-twitter-today.html"&gt;Обновление:&lt;/a&gt;&lt;/strong&gt; стратегия по поводу использования Cassandra изменилась, попытки
    использовать её в роли основного хранилища для твитов прекратились,
    но она продолжает использоваться для аналитики и географической
    информации.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Подробнее почему Twitter пришел к решению использовать Cassandra можно
прочитать &lt;a href="https://www.insight-it.ru/goto/ffc31d1/" rel="nofollow" target="_blank" title="http://www.slideshare.net/ryansking/scaling-twitter-with-cassandra"&gt;в отдельной презентации&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Помимо всего прочего Cassandra&amp;nbsp;&lt;del&gt;планируется использовать&lt;/del&gt; используется для аналитики в реальном времени.&lt;/p&gt;
&lt;h3 id="scribe"&gt;Scribe&lt;/h3&gt;
&lt;p&gt;Пользователи Twitter генерируют огромное количество данных, около 15-25
Гб в минуту, более 12 Тб в день, и эта цифра удваивается несколько раз
в год.&lt;/p&gt;
&lt;p&gt;Изначально для сбора логов использовали &lt;code&gt;syslog-ng&lt;/code&gt;, но он очень быстро
перестал справляться с нагрузкой.&lt;/p&gt;
&lt;p&gt;Решение нашлось очень просто: &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt; столкнулся с
аналогичной проблемой и разработал проект Scribe, который был
опубликован в opensource.&lt;/p&gt;
&lt;p&gt;По сути это фреймворк для сбора и агрегации логов, основанный на
&lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt;. Вы пишете текст для логов и указываете
категорию, остальное он берет на себя.&lt;/p&gt;
&lt;p&gt;Работает локально, надежен даже в случае потери сетевого соединения,
каждый узел знает только на какой сервер передавать логи, что позволяет
создавать масштабируемую иерархию для сбора логов.&lt;/p&gt;
&lt;p&gt;Поддерживаются различные системы для записи в данным, &amp;nbsp;в том числе
обычные файлы и HDFS (о ней ниже).&lt;/p&gt;
&lt;p&gt;Этот продукт полностью решил проблему Twitter со сбором логов,
используется около 30 различных категорий. В процессе использования была
создана и опубликована масса доработок. Активно сотрудничают с командой
Facebook в развитии проекта.&lt;/p&gt;
&lt;h3 id="hadoop"&gt;Hadoop&lt;/h3&gt;
&lt;p&gt;Как Вы обычно сохраняете 12Тб новых данных, поступающих каждый день?&lt;/p&gt;
&lt;p&gt;Если считать, что средняя скорость записи современного жесткого диска
составляет 80Мбайт в секунду, запись 12Тб данных заняла бы почти 48
часов.&lt;/p&gt;
&lt;p&gt;На одном даже очень большом сервере данную задачу не решить, логичным
решением задачи стало использование кластера для хранения и анализа
таких объемов данных.&lt;/p&gt;
&lt;p&gt;Использование кластерной файловой системы добавляет сложности, но
позволяет меньше заботиться о деталях.&lt;/p&gt;
&lt;p&gt;Hadoop Distributed File System (HDFS) предоставляет возможность
автоматической репликации и помогает справляться со сбоями оборудования.&lt;/p&gt;
&lt;p&gt;MapReduce framework позволяет обрабатывать огромные объемы данных,
анализируя пары ключ-значение.&lt;/p&gt;
&lt;p&gt;Типичные вычислительные задачи, которые решаются с помощью Hadoop в
Twitter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Вычисление связей дружбы в социальном графе (&lt;code&gt;grep&lt;/code&gt; и &lt;code&gt;awk&lt;/code&gt; не
    справились бы, self join в MySQL на таблицах с миллиардами строк -
    тоже)&lt;/li&gt;
&lt;li&gt;Подсчет статистики (количество пользователей и твитов, например
    подсчет количества твитов занимает 5 минут при 12 миллиардах
    записей)&lt;/li&gt;
&lt;li&gt;Подсчет PageRank между пользователями для вычисления репутации.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;В твиттер используется бесплатный дистрибутив от Cloudera, версия Hadoop
0.20.1, данные храняться &lt;a href="https://www.insight-it.ru/goto/1ac5bba3/" rel="nofollow" target="_blank" title="https://github.com/kevinweil/hadoop-lzo"&gt;в сжатом по алгоритму LZO виде&lt;/a&gt;, библиотеки для работы с
данными опубликованы под названием
&lt;a href="https://www.insight-it.ru/goto/a1b5430e/" rel="nofollow" target="_blank" title="https://github.com/kevinweil/elephant-bird"&gt;elephant-bird&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="pig"&gt;Pig&lt;/h3&gt;
&lt;p&gt;Для того чтобы анализировать данные с помощью MapReduce обычно
необходимо разрабатывать код на Java, что далеко не все умеют делать, да
и трудоемко это.&lt;/p&gt;
&lt;p&gt;Pig представляет собой высокоуровневый язык, позволяющий
трансформировать огромные наборы данных шаг за шагом.&lt;/p&gt;
&lt;p&gt;Немного напоминает SQL, но намного проще. Это позволяет писать в 20 раз
меньше кода, чем при анализе данных с помощью обычных MapReduce работ.
Большая часть работы по анализу данных в Twitter осуществляется с
помощью Pig.&lt;/p&gt;
&lt;h3 id="dannye"&gt;Данные&lt;/h3&gt;
&lt;p&gt;Полу-структурированные данные:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;логи Apache, RoR, MySQL, A/B тестирования, процесса регистрации&lt;/li&gt;
&lt;li&gt;поисковые запросы&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Структурированные данные:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Твиты&lt;/li&gt;
&lt;li&gt;Пользователи&lt;/li&gt;
&lt;li&gt;Блок-листы&lt;/li&gt;
&lt;li&gt;Номера телефонов&lt;/li&gt;
&lt;li&gt;Любимые твиты&lt;/li&gt;
&lt;li&gt;Сохраненные поиски&lt;/li&gt;
&lt;li&gt;Ретвиты&lt;/li&gt;
&lt;li&gt;Авторизации&lt;/li&gt;
&lt;li&gt;Подписки&lt;/li&gt;
&lt;li&gt;Сторонние клиенты&lt;/li&gt;
&lt;li&gt;География&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Запутанные данные:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Социальный граф&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Что же они делают с этим всем?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Подсчет математического ожидания, минимума, максимума и дисперсии
    следующих показателей:&lt;ul&gt;
&lt;li&gt;Количество запросов за сутки&lt;/li&gt;
&lt;li&gt;Средняя задержка, 95% задержка&lt;/li&gt;
&lt;li&gt;Распределение кодов HTTP-ответов (по часам)&lt;/li&gt;
&lt;li&gt;Количество поисков осуществляется каждый день&lt;/li&gt;
&lt;li&gt;Количество уникальных запросов и пользователей&lt;/li&gt;
&lt;li&gt;Географическое распределение запросов и пользователей&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Подсчет вероятности, ковариации, влияния:&lt;ul&gt;
&lt;li&gt;Как отличается использование через мобильные устройства?&lt;/li&gt;
&lt;li&gt;Как влияет использование клиентов сторонних разработчиков?&lt;/li&gt;
&lt;li&gt;Когортный анализ&lt;/li&gt;
&lt;li&gt;Проблемы с сайтом (киты и роботы, подробнее ниже)&lt;/li&gt;
&lt;li&gt;Какие функциональные возможности цепляют пользователей?&lt;/li&gt;
&lt;li&gt;Какие функциональные возможности чаще используются популярными
    пользователями?&lt;/li&gt;
&lt;li&gt;Корректировка и предложение поисковых запросов&lt;/li&gt;
&lt;li&gt;A/B тестирование&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Предсказания, анализ графов, естественные языки:&lt;ul&gt;
&lt;li&gt;Анализ пользователей по их твитам, твитов, на которые они
    подписаны, твитам их фоловеров&lt;/li&gt;
&lt;li&gt;Какая структура графа ведет к успешным популярным сетям&lt;/li&gt;
&lt;li&gt;Пользовательская репутация&lt;/li&gt;
&lt;li&gt;Анализ эмоциональной окраски&lt;/li&gt;
&lt;li&gt;Какие особенности заставляют людей ретвитнуть твит?&lt;/li&gt;
&lt;li&gt;Что влияет на глубину дерева ретвитов ?&lt;/li&gt;
&lt;li&gt;Долгосрочное обнаружение дубликатов&lt;/li&gt;
&lt;li&gt;Машинное обучение&lt;/li&gt;
&lt;li&gt;Обнаружения языка&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Подробнее про обработку данных &lt;a href="https://www.insight-it.ru/goto/3d4649ef/" rel="nofollow" target="_blank" title="http://www.slideshare.net/kevinweil/nosql-at-twitter-nosql-eu-2010"&gt;в презентации Kevin Weil&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="hbase"&gt;HBase&lt;/h3&gt;
&lt;p&gt;Twitter начинают строить настоящие сервисы на основе Hadoop, например
поиск людей:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HBase используется как изменяемая прослойка над HDFS&lt;/li&gt;
&lt;li&gt;Данные экспортируются из HBase c помощью периодической MapReduce
    работы:&lt;ul&gt;
&lt;li&gt;На этапе Map используются также данные из FlockDB и нескольких
    внутренних сервисов&lt;/li&gt;
&lt;li&gt;Собственная схема разбиения данных&lt;/li&gt;
&lt;li&gt;Данные подтягиваются через высокопроизводительный, горизонтально
    масштабируемый сервис на Scala (&lt;a href="https://www.insight-it.ru/goto/917f8c95/" rel="nofollow" target="_blank" title="http://www.slideshare.net/al3x/building-distributed-systems-in-scala"&gt;подробнее о построении распределенных сервисов на Scala&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;На основе HBase разрабатываются и другие продукты внутри Twitter.&lt;/p&gt;
&lt;p&gt;Основными её достоинствами являются гибкость и легкая интеграция с
Hadoop и Pig.&lt;/p&gt;
&lt;p&gt;По сравнению с Cassandra:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"Их происхождение объясняет их сильные и слабые стороны"&lt;/li&gt;
&lt;li&gt;HBase построен на основе системы по пакетной обработке данных,
    высокие задержки, работает далеко не в реальном времени&lt;/li&gt;
&lt;li&gt;Cassandra построена с нуля для работы с низкими задержками&lt;/li&gt;
&lt;li&gt;HBase легко использовать при анализе данных как источник или место
    сохранения результатов, Cassandra для этого подходит меньше, но они
    работают над этим&lt;/li&gt;
&lt;li&gt;HBase на данный момент единственную точку отказа в виде мастер-узла&lt;/li&gt;
&lt;li&gt;В твиттере HBase используется для аналитики, анализа и создания
    наборов данных, а Cassandra - для онлайн систем&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="loony"&gt;Loony&lt;/h3&gt;
&lt;p&gt;Централизованная система управления оборудованием.&lt;/p&gt;
&lt;p&gt;Реализована с использованием:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/django/"&gt;Django&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8927633f/" rel="nofollow" target="_blank" title="http://www.lag.net/paramiko"&gt;Paraminko&lt;/a&gt; (реализация протокола SSH
    на Python, разработана и опубликована в opensource в Twitter)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Интегрирована с LDAP, анализирует входящую почту от датацентра и
автоматически вносит изменения в базу.&lt;/p&gt;
&lt;h3 id="murder"&gt;Murder&lt;/h3&gt;
&lt;p&gt;Система развертывания кода и ПО, основанная на протоколе BitTorrent.&lt;/p&gt;
&lt;p&gt;Благодаря своей P2P природе позволяет обновить более тысячи серверов за
30-60 секунд.&lt;/p&gt;
&lt;h3 id="kestrel"&gt;Kestrel&lt;/h3&gt;
&lt;p&gt;Распределенная очередь, работающая по протоколу memcache:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;set&lt;/code&gt; - поставить в очередь&lt;/li&gt;
&lt;li&gt;&lt;code&gt;get&lt;/code&gt; - взять из очереди&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Особенности:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Отсутствие строгого порядка выполнения заданий&lt;/li&gt;
&lt;li&gt;Отсутствие общего состояния между серверами&lt;/li&gt;
&lt;li&gt;Разработана на &lt;a href="/tag/scala/"&gt;Scala&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="daemony"&gt;Daemon'ы&lt;/h3&gt;
&lt;p&gt;Каждый твит обрабатывается с помощью daemon'ов.&lt;/p&gt;
&lt;p&gt;В unicorn обрабатываются только HTTP запросы, вся работа за сценой
реализована в виде отдельных daemon'ов.&lt;/p&gt;
&lt;p&gt;Раньше использовалось много разных демонов, по одному на каждую задачу
(Rails), но перешли к меньшему их количеству, способному решать
несколько задач одновременно.&lt;/p&gt;
&lt;h3 id="kak-oni-spravliaiutsia-s-takimi-tempami-rosta"&gt;Как они справляются с такими темпами роста?&lt;/h3&gt;
&lt;p&gt;Рецепт прост, но эффективен, подходит практически для любого
интернет-проекта:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;обнаружить самое слабое место в системе;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;принять меры по его устранению;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;перейти к следующему самому слабому месту.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;На словах звучит и правда примитивно, но на практике нужно предпринять
ряд мер, чтобы такой подход был бы реализуем:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Автоматический сбор метрик (причем в агрегированном виде)&lt;/li&gt;
&lt;li&gt;Построение графиков (RRD, Ganglia)&lt;/li&gt;
&lt;li&gt;Сбор и анализ&amp;nbsp;логов&lt;/li&gt;
&lt;li&gt;Все данные должны получаться с минимальной задержкой, как можно
    более близко к реальному времени&lt;/li&gt;
&lt;li&gt;Анализ:&lt;ul&gt;
&lt;li&gt;Из данных необходимо получать &lt;em&gt;информацию&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Следить за динамикой показателей: стало лучше или хуже?&lt;/li&gt;
&lt;li&gt;Особенно при развертывании новых версий кода&lt;/li&gt;
&lt;li&gt;Планирование использования ресурсов намного проще, чем решение
    экстренных ситуаций, когда они на исходу&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Примерами агрегированных метрик в Twitter являются "киты" и "роботы",
вернее их количество в единицу времени.&lt;/p&gt;
&lt;h5&gt;Что такое "робот"?&lt;/h5&gt;
&lt;p&gt;&lt;img alt="Twitter Робот" class="responsive-img" src="https://www.insight-it.ru/images/twitter-bot.jpg" title="Twitter Робот"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ошибка внутри Rails (HTTP 500)&lt;/li&gt;
&lt;li&gt;Непойманное исключение&lt;/li&gt;
&lt;li&gt;Проблема в коде или нулевой результат&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;Что такое "кит"?&lt;/h5&gt;
&lt;p&gt;&lt;img alt="Twitter Кит" class="responsive-img" src="https://www.insight-it.ru/images/twitter-whale.jpg" title="Twitter Кит"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HTTP ошибка 502 или 503&lt;/li&gt;
&lt;li&gt;В твиттер используется фиксированный таймаут в 5 секунд (лучше
    кому-то показать ошибку, чем захлебнуться в запросах)&lt;/li&gt;
&lt;li&gt;Убитый слишком длинный запрос к базе данных (mkill)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Значительное превышение нормального количества китов или роботов в
минуту является поводом для беспокойством.&lt;/p&gt;
&lt;p&gt;Реализован этот механизм простым bash-скриптом, который просматривает
агрегированные логи за последние 60 секунд, подсчитывает количество
китов/роботов и рассылает уведомления, если значение оказалось выше
порогового значения. Подробнее про работу команды оперативного
реагирования &lt;a href="https://www.insight-it.ru/goto/30562be/" rel="nofollow" target="_blank" title="http://www.slideshare.net/netik/billions-of-hits-scaling-twitter"&gt;в презентации John Adams&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="temnyi-rezhim"&gt;"Темный режим"&lt;/h3&gt;
&lt;p&gt;Для экстренных ситуаций в Twitter предусмотрен так называемый "темный
режим", который представляет собой набор механизмов для отключения
тяжелых по вычислительным ресурсам или вводу-выводу функциональных
частей сайта. Что-то вроде стоп-крана для сайта.&lt;/p&gt;
&lt;p&gt;Имеется около 60 выключателей, в том числе и полный режим "только для
чтения".&lt;/p&gt;
&lt;p&gt;Все изменения в настройках этого режима фиксируются в логах и сообщаются
руководству, чтобы никто не баловался.&lt;/p&gt;
&lt;h2 id="podvodim-itogi_1"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Не бросайте систему на самотек, начинайте собирать метрики и их
    визуализировать как можно раньше&lt;/li&gt;
&lt;li&gt;Заранее планируйте рост требуемых ресурсов и свои действия в случае
    экстренных ситуаций&lt;/li&gt;
&lt;li&gt;Кэшируйте по максимуму все, что возможно&lt;/li&gt;
&lt;li&gt;Все инженерные решения не вечны, ни одно из решений не идеально, но
    многие будут нормально работать в течение какого-то периода времени&lt;/li&gt;
&lt;li&gt;Заранее начинайте задумываться о плане масштабирования&lt;/li&gt;
&lt;li&gt;Не полагайтесь полностью на memcached и базу данных - они могут Вас
    подвести в самый неподходящий момент&lt;/li&gt;
&lt;li&gt;Все данные для запросов в реальном времени должны находиться в
    памяти, диски в основном для записи&lt;/li&gt;
&lt;li&gt;Убивайте медленные запросы (mkill) прежде, чем они убьют всю систему&lt;/li&gt;
&lt;li&gt;Некоторые задачи могут решаться путем предварительного подсчета и
    анализа, но далеко не все&lt;/li&gt;
&lt;li&gt;Приближайте вычисления к данным по возможности&lt;/li&gt;
&lt;li&gt;Используйте не mongrel, а unicorn для RoR&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Спасибо за внимание, &lt;a href="/feed/"&gt;жду Вас снова&lt;/a&gt;! Буду рад, если Вы
&lt;a href="https://www.insight-it.ru/goto/26b8fa1/" rel="nofollow" target="_blank" title="http://twitter.com/blinkov"&gt;подпишитесь на меня в Twitter&lt;/a&gt;, с
удовольствием пообщаюсь со всеми читателями :)&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sat, 05 Mar 2011 20:47:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-03-05:highload/2011/arkhitektura-twitter-dva-goda-spustya/</guid><category>Apache</category><category>Cassandra</category><category>featured</category><category>Flock</category><category>FlockDB</category><category>Hadoop</category><category>HBase</category><category>Kestrel</category><category>Memcached</category><category>MySQL</category><category>Pig</category><category>Ruby</category><category>Ruby on Rails</category><category>Scala</category><category>Scribe</category><category>Twitter</category><category>Unicorn</category><category>архитектура Twitter</category><category>интернет-проекты</category><category>Масштабируемость</category><category>социальная сеть</category></item><item><title>Архитектура DISQUS</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-disqus/</link><description>&lt;p&gt;&lt;img alt="DISQUS" class="left" src="https://www.insight-it.ru/images/disqus.jpg" title="DISQUS"/&gt;
&lt;a href="https://www.insight-it.ru/goto/a754581e/" rel="nofollow" target="_blank" title="https://disqus.com"&gt;DISQUS&lt;/a&gt; - самая популярная система
комментирования и одновременно самое большое в мире Django-приложение.
Она установлена более чем на полумиллионе сайтов и блогов, в том числе и
очень крупных, таких как Engadget, CNN, MTV, IGN. Основной особенностью
в её реализации является тот факт, что DISQUS не является тем сайтом,
который хотят увидеть пользователи, он лишь предоставляет механизмы
комментирования, авторизации и интеграции с социальными сетями. Пики
нагрузки возникают одновременно c появлением какой-то шумихи в
Интернете, что достаточно непредсказуемо. Как же им удается справляться
с этой ситуацией?&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; - операционная система&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/python/"&gt;Python&lt;/a&gt; - язык программирования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/django/"&gt;Django&lt;/a&gt; - основной framework&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache 2.2&lt;/a&gt; +&amp;nbsp;&lt;a href="/tag/wsgi/"&gt;mod_wsgi&lt;/a&gt; - веб-сервер&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/postgresql/"&gt;PostgreSQL&lt;/a&gt; - СУБД&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; - кэширование&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/haproxy/"&gt;HAProxy&lt;/a&gt; - балансировка нагрузки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/slony/"&gt;Slony&lt;/a&gt; - репликация данных&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/heartbeat/"&gt;heartbeat&lt;/a&gt; - обеспечение
    доступности&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;До 17 тысяч запросов в секунду&lt;/li&gt;
&lt;li&gt;500 000 сайтов&lt;/li&gt;
&lt;li&gt;15 миллионов зарегистрированных пользователей&lt;/li&gt;
&lt;li&gt;75 миллионов комментариев&lt;/li&gt;
&lt;li&gt;250 миллионов посетителей (на август 2010г.)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="osnovnye-trudnosti"&gt;Основные трудности&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Непредсказуемость нагрузки (основными причинами шумихи в Интернете
    являются катастрофы и выходки знаменитостей)&lt;/li&gt;
&lt;li&gt;Обсуждения никогда не теряют актуальность (нельзя держать в кэше все
    дискуссии с 2008 года)&lt;/li&gt;
&lt;li&gt;Нельзя угадать на каком сайте из тысяч возникнет пик трафика&lt;/li&gt;
&lt;li&gt;Персональные настройки, динамическое разбиение на страницы и
    сортировки снижают эффективность кэширования&lt;/li&gt;
&lt;li&gt;Высокая доступность (из-за разнообразия сайтов и их аудитории сложно
    запланировать технические работы)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Архитектура DISQUS" class="responsive-img" src="https://www.insight-it.ru/images/disqus_architecture.jpeg" title="Архитектура DISQUS"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Оборудование&lt;/strong&gt;, в сумме около 100 серверов:&lt;ul&gt;
&lt;li&gt;30% веб-серверов (Apache + &lt;code&gt;mod_wsgi&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;10% серверов баз данных (PostgreSQL)&lt;/li&gt;
&lt;li&gt;25% кэш-серверов (memcached)&lt;/li&gt;
&lt;li&gt;20% балансировка нагрузки и обеспечение доступности (HAProxy +
    heartbeat)&lt;/li&gt;
&lt;li&gt;15% прочие сервера (Python скрипты)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Балансировка нагрузки&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;HAProxy:&lt;ul&gt;
&lt;li&gt;Высокая производительность&lt;/li&gt;
&lt;li&gt;Интеллектуальная проверка доступности&lt;/li&gt;
&lt;li&gt;Неплохая статистика&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Репликация&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Используется Slony-I&lt;/li&gt;
&lt;li&gt;Основана на триггерах&lt;/li&gt;
&lt;li&gt;Master/Slave для обеспечения большего объема операций чтения&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Высокая доступность&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;heartbeat&lt;/li&gt;
&lt;li&gt;Пассивная копия мастер баз данных на случай сбоя основной&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Партиционирование&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Реализовано на уровне кода&lt;/li&gt;
&lt;li&gt;Простая реализация, быстрые положительные результаты&lt;/li&gt;
&lt;li&gt;Два метода разделения данных:&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Вертикальное:&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;Создание нескольких таблиц с меньшим количеством колонок
    вместо одной (она же нормализация)&lt;/li&gt;
&lt;li&gt;Позволяет разделять базы данных&lt;/li&gt;
&lt;li&gt;Данные объединяются в коде (медленнее, чем на уровне
    СУБД, но не намного)&lt;/li&gt;
&lt;li&gt;Бартер производительности на масштабируемость&lt;/li&gt;
&lt;li&gt;Более эффективное кэшировние&lt;/li&gt;
&lt;li&gt;Механизм роутеров в Django позволяет достаточно легко
    реализовать данный функционал&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Горизонтальное:&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;Некоторые сайты имеют очень большие массивы данных&lt;/li&gt;
&lt;li&gt;Партнеры требуют повышенного уровня доступности&lt;/li&gt;
&lt;li&gt;Помогает снижать загрузку по записи на мастер базе
    данных&lt;/li&gt;
&lt;li&gt;В основном используется все же вертикальное
    партиционирование&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Производительность базы данных&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Особое внимание уделяется тому, чтобы индексы помещались в
    оперативную память&lt;/li&gt;
&lt;li&gt;Логирование медленных запросов (автоматизировано с помощью
    syslog-ng + pgFouine + cron)&lt;/li&gt;
&lt;li&gt;Использование пулов соединений (Django не умеет этого,
    используется pgbouncer, позволяет экономить на ресурсоемких
    операциях установления и прекращения соединений)&lt;/li&gt;
&lt;li&gt;Оптимизация QuerySet'ов:&lt;ul&gt;
&lt;li&gt;Не используется чистый SQL&lt;/li&gt;
&lt;li&gt;Встроенный кэш позволяет выделять части выборки&lt;/li&gt;
&lt;li&gt;Но это не всегда нужно, они убрали этот кэш&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Атомарные операции:&lt;ul&gt;
&lt;li&gt;Поддерживают консистентность данных&lt;/li&gt;
&lt;li&gt;Использование update(), так как save() не является
    thread-safe&lt;/li&gt;
&lt;li&gt;Отлично работают для таких вещей, как счетчики&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Транзакции:&lt;ul&gt;
&lt;li&gt;TransactionMiddleware поначалу использовалось, но со
    временем стало обузой&lt;/li&gt;
&lt;li&gt;В &lt;code&gt;postgrrsql_psycopg2&lt;/code&gt; есть опция autocommit:&lt;ul&gt;
&lt;li&gt;Это означает что каждый запрос выполняется в отдельной
    транзакции&lt;/li&gt;
&lt;li&gt;Обработка каждого пользовательского HTTP-запроса не
    начинает новую транзакцию&lt;/li&gt;
&lt;li&gt;Но все же транзакции из нескольких операций записи в
    СУБД нужны (сохранение нескольких объектов одновременно
    и полный откат в случае ошибки)&lt;/li&gt;
&lt;li&gt;В итоге все HTTP-запросы по-умолчанию начинаются в
    режиме autocommit, но в случае необходимости
    переключаются в транзакционный режим&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Отложенные сигналы&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Постановка в очередь низкоприоритетных задач (даже если они не
    длинные по времени)&lt;/li&gt;
&lt;li&gt;Асинхронные сигналы очень удобны для разработчика (но не так,
    как настоящие сигналы)&lt;/li&gt;
&lt;li&gt;Модели отправляются в очередь в сериализованном виде&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Кэширование&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Используется memcached&lt;/li&gt;
&lt;li&gt;Новый pylibmcна основе libmemcached в качестве клиента (проекты
    django-pylibmc и django-newcache)&lt;/li&gt;
&lt;li&gt;Настраиваемые алгоритмы поведения клиента&lt;/li&gt;
&lt;li&gt;Используется &lt;code&gt;_auto_reject_hosts&lt;/code&gt; и &lt;code&gt;_retry_timeout&lt;/code&gt; для
    предотвращения повторных подключений к вышедшим из строя
    кэш-серверам&lt;/li&gt;
&lt;li&gt;Алгоритм размещения ключей: консистентное хэширование на основе
    libketama&lt;/li&gt;
&lt;li&gt;Существует проблема, когда одно очень часто используемое
    значение в кэше инвалидируется:&lt;ul&gt;
&lt;li&gt;Множество клиентов одновременно пытаются получить новое
    значение из СУБД одновременно&lt;/li&gt;
&lt;li&gt;В большинстве случаев правильным решением было бы вернуть
    большинству устаревшие данные и позволить одному клиенту
    обновить кэш&lt;/li&gt;
&lt;li&gt;django-newcache и MintCache умеют это делать&lt;/li&gt;
&lt;li&gt;Заполнение кэша новым значением вместо удаления при
    инвалидации также помогает избежать этой проблемы&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Мониторинг&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Информация о производительности запросов к БД, внешних вызовов и
    рендеринге шаблонов записывается через собственный middleware&lt;/li&gt;
&lt;li&gt;Сбор и отображение с помощью Ganglia&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Отключение функционала&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Необходим способ быстро отключить новый функционал, если
    оказывается, что он работает не так, как планировалось&lt;/li&gt;
&lt;li&gt;Система должна срабатывать мгновенно, по всем серверам, без
    записи на диск&lt;/li&gt;
&lt;li&gt;Позволяет запускать новые возможности постепенно, лишь для части
    аудитории&lt;/li&gt;
&lt;li&gt;Позволяет постоянно использовать основную ветку кода&lt;/li&gt;
&lt;li&gt;Аналогичная система используется и в &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Масштабирование команды разработчиков&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Небольшая команда&lt;/li&gt;
&lt;li&gt;Месячная аудитория / количество разработчиков = 40 миллионов&lt;/li&gt;
&lt;li&gt;Это означает:&lt;ul&gt;
&lt;li&gt;Автоматическое тестирование&lt;/li&gt;
&lt;li&gt;И максимально простой процесс разработки&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Новый сотрудник может начать работать уже через несколько минут,
    нужно лишь:&lt;ul&gt;
&lt;li&gt;Установить и настроить PostgreSQL&lt;/li&gt;
&lt;li&gt;Скачать исходный код из &lt;a href="/tag/git/"&gt;git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;С помощью pip и virtualenv установить зависимости&lt;/li&gt;
&lt;li&gt;Изменить настройки в settings.py&lt;/li&gt;
&lt;li&gt;Выполнить автоматическое создание структуры данных
    средствами Django&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Непрерывное тестирование&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Ежедневное развертывание с помощью &lt;a href="/tag/fabric/"&gt;Fabric&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/hudson/"&gt;Hudson&lt;/a&gt; обеспечивает регулярно осуществляет и
    тестирует сборки&lt;/li&gt;
&lt;li&gt;Интегрирован &lt;a href="/tag/selenium/"&gt;Selenium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Быстрое тестирование с помощью &lt;a href="/tag/pyflakes/"&gt;Pyflakes&lt;/a&gt; и
    post-commit hooks&lt;/li&gt;
&lt;li&gt;70 тысяч строк Python кода, 73% покрытие тестами, прогон всех
    тестов занимает 20 минут&lt;/li&gt;
&lt;li&gt;Собственная система исполнения тестов с поддержкой XML,
    Selenium, подсчета количества запросов, тестирования
    Master/Slave базы данных и интеграцией с очередью&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Отслеживание проблем и задач&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Переключились с Trac на Redmine (из-за поддержки под-задач)&lt;/li&gt;
&lt;li&gt;Отправка исключений на e-mail - плохая идея&lt;/li&gt;
&lt;li&gt;Раньше использовали django-db-log, но теперь опубликовали свою
    систему сбора ошибок и логов под названием
    &lt;a href="https://www.insight-it.ru/goto/2e33ac0/" rel="nofollow" target="_blank" title="https://github.com/dcramer/django-sentry"&gt;Sentry&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="delaem-vyvody"&gt;Делаем выводы&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Язык программирования, каким бы он ни был, не является проблемой&lt;/li&gt;
&lt;li&gt;Django в целом очень хорош (но приходится все же использовать набор
    собственных патчей)&lt;/li&gt;
&lt;li&gt;Даже при использовании низкопроизводительного framework можно
    построить масштабируемую систему&lt;/li&gt;
&lt;li&gt;Вертикальное партиционирование позволяет пожертвовать
    производительностью в пользу масштабируемости&lt;/li&gt;
&lt;li&gt;Даже небольшой командой разработчиков можно добиться высоких
    результатов, если не пренебрегать автоматизацией тестирования&lt;/li&gt;
&lt;li&gt;Большое значение имеет возможность вовремя отслеживать и оперативно
    реагировать на сбои&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="istochnik-informatsii"&gt;Источник информации&lt;/h2&gt;
&lt;p&gt;Данная статья написана на основе выступления Jason Yan и David Cramer на
DjangoConf 2010. В презентации можно найти примеры кода, ссылки на
упоминаемые проекты и дополнительные материалы:&lt;/p&gt;
&lt;div class="video-container no-controls"&gt;
&lt;iframe allowfullscreen="" frameborder="0" height="355" marginheight="0" marginwidth="0" scrolling="no" src="//www.slideshare.net/slideshow/embed_code/key/21F2PzBmYATx2Y" width="425"&gt; &lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Другие статьи по масштабируемости высоконагруженных систем можно
почитать &lt;a href="https://www.insight-it.ru/highload/"&gt;в соответствующем разделе&lt;/a&gt;, а вовремя узнавать о
новых - &lt;a href="/feed/"&gt;подписавшись на RSS&lt;/a&gt;. Вчера, кстати, прикрутил DISQUS к
Insight IT, приглашаю постоянных читателей и всех остальных
потестировать :)&lt;/em&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 02 Mar 2011 03:37:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-03-02:highload/2011/arkhitektura-disqus/</guid><category>Apache</category><category>DISQUS</category><category>django</category><category>Fabric</category><category>Ganglia</category><category>Git</category><category>HAProxy</category><category>heartbeat</category><category>Hudson</category><category>Linux</category><category>Memcached</category><category>pgbouncer</category><category>pgFouine</category><category>PostgreSQL</category><category>Pyflakes</category><category>Python</category><category>Selenium</category><category>Slony</category><category>syslog-ng</category><category>WSGI</category><category>Архитектура DISQUS</category><category>Масштабируемость</category></item><item><title>Facebook за 20 минут</title><link>https://www.insight-it.ru//highload/2011/facebook-za-20-minut/</link><description>&lt;p&gt;Facebook&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/ddb55695/" rel="nofollow" target="_blank" title="http://www.facebook.com/notes/democracy-uk-on-facebook/a-snapshot-of-facebook-in-2010/172769082761603"&gt;поделились новыми цифрами&lt;/a&gt; в конце прошлого года.&lt;/p&gt;
&lt;h2 id="chto-obychno-proiskhodit-za-20-minut-na-facebook"&gt;Что обычно происходит за 20 минут на Facebook?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Люди делятся миллионом ссылок&lt;/li&gt;
&lt;li&gt;Отмечают друзей на 1323 тысячах фотографий&lt;/li&gt;
&lt;li&gt;Приглашают 1 484 000 знакомых на мероприятиях&lt;/li&gt;
&lt;li&gt;Отправляют 1 587 000 сообщений на стену&lt;/li&gt;
&lt;li&gt;Пишут 1 851 000 новых статусов&lt;/li&gt;
&lt;li&gt;2 миллиона пар людей становятся друзьями&lt;/li&gt;
&lt;li&gt;Загружается 2.7 миллиона фотографий&lt;/li&gt;
&lt;li&gt;Появляется 10.2 миллиона комментариев 10,208,000&lt;/li&gt;
&lt;li&gt;Отправляется 4 632 000 личных сообщения&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="za-2010-god"&gt;За 2010 год:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;43 869 800 изменили свой статус на "не женат / не замужем"&lt;/li&gt;
&lt;li&gt;3 025 791 изменили свой статус на "все сложно"&lt;/li&gt;
&lt;li&gt;28 460 516 изменили свой статус на "есть парень / девушка"&lt;/li&gt;
&lt;li&gt;5 974 574 изменили свой статус на "помолвлен"&lt;/li&gt;
&lt;li&gt;36 774 801 изменили свой статус на "женат /замужем"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Хочется узнать больше? &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-facebook/"&gt;Прочитайте статью про архитектуру Facebook&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Sun, 20 Feb 2011 07:37:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-02-20:highload/2011/facebook-za-20-minut/</guid><category>Facebook</category><category>статистика</category></item><item><title>Архитектура Mollom</title><link>https://www.insight-it.ru//highload/2011/arkhitektura-mollom/</link><description>&lt;p&gt;&lt;img alt="Mollom" class="right" src="https://www.insight-it.ru/images/mollom-logo.jpg" title="Mollom"/&gt;
&lt;a href="https://www.insight-it.ru/goto/aa3c2fd2/" rel="nofollow" target="_blank" title="http://mollom.com/"&gt;Mollom&lt;/a&gt; - это прибыльный SaaS сервис по фильтрации различных форм спама из
контента, сгенерированного пользователями: комментариев, постов на
форумах и блогах, опросов, контактных и регистрационных форм.
Определение спама основано не только на контенте, но и репутации и
прошлой активности разместившего его пользователя. Алгоритм машинного
обучения Mollom выполняет роль цифрового модератора 24х7 для более 40
тысяч сайтов, в том числе и очень крупных компаний.&lt;/p&gt;
&lt;p&gt;С того момента, как Mollom запустили систему анализа цифрового контента,
они выявили более 373 миллионов спам сообщений, обнаружив в процессе что
впечатляющие 90% всех прошедших через них сообщений оказались спамом.
Весь этот поток спама в 100 сообщений в секунду обрабатывается всего
двумя географически распределенными серверами. На каждом из них работает
сервер Java-приложений и Cassandra. Так мало ресурсов требуется лишь
из-за того, что они создали очень эффективную систему машинного
обучения. Разве не круто? Так как же они это делают?&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Обслуживаются 40000 активных веб-сайтов, многие их которых
    принадлежат крупным клиентам, таким как Adobe, Sony BMG, Warner
    Brothers, Fox News и The Economist. Много крупных брендов, с
    крупными сайтами, масса комментариев.&lt;/li&gt;
&lt;li&gt;Обнаруживают пол-миллиона спам-сообщений ежедневно.&lt;/li&gt;
&lt;li&gt;Обрабатывается около 100 запросов к API в секунду.&lt;/li&gt;
&lt;li&gt;Проверка сообщения на спам занимает очень мало времени, обычно около
    30-50 миллисекунд, 95% запросов укладывается в 250 миллисекунд,
    когда самые медленные обрабатываются пол секунды.&lt;/li&gt;
&lt;li&gt;Эффективность определения спама составляет 99.95%. Это означает, что
    из 10000 спам-сообщений Mollom пропустит только 5.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/82e0a09b/" rel="nofollow" target="_blank" title="http://mollom.com/blog/netlog-using-mollom"&gt;Netlog&lt;/a&gt;, европейская
    социальная сеть, имеет отдельный Mollom-сервер в своем датацентре.
    Netlog проверяют на спам около 4 миллионов сообщений каждый день на
    классификаторах, специально натренированных на их данных.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/java/"&gt;Java&lt;/a&gt; - исторически сложилось, что Mollom был с самого
    начала был разработан на Java.&lt;/li&gt;
&lt;li&gt;Два сервера обслуживают основную часть клиентов:&lt;ul&gt;
&lt;li&gt;Один сервер на восточном побережье США, другой - на западном&lt;/li&gt;
&lt;li&gt;В случае сбоя один сервер может полностью подменить другой&lt;/li&gt;
&lt;li&gt;Конфигурация обоих: Intel Xeon Quad core, 2.8GHz, 16GB RAM, 4
    диска по 300 GB, RAID 10.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/ad66f2bf/" rel="nofollow" target="_blank" title="http://www.softlayer.com/"&gt;SoftLayer&lt;/a&gt; - хостинг-провайдер.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/baf11ec8/" rel="nofollow" target="_blank" title="http://cassandra.apache.org/"&gt;Cassandra&lt;/a&gt; - NoSQL база данных,
    выбранная из-за высокой производительности на запись и способности
    работать на серверах, располагающихся в разных датацентрах (была
    разработана в&amp;nbsp;&lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-facebook/"&gt;Facebook&lt;/a&gt;,
    но там практически не используется).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/dce103e6/" rel="nofollow" target="_blank" title="http://www.mysql.com/"&gt;MySQL&lt;/a&gt; - Java Persistence API используется
    для обычных наборов данных, когда Cassandra используется для больших
    объемов данных.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/2b056aff/" rel="nofollow" target="_blank" title="http://en.wikipedia.org/wiki/GlassFish"&gt;Glassfish&lt;/a&gt; - open source
    сервер приложений для платформы Java EE. Они выбрали именно
    Glassfish за его возможности корпоративного уровня, такие как
    репликация и обработка сбоев.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/cc84f430/" rel="nofollow" target="_blank" title="http://hudson-ci.org/"&gt;Hudson&lt;/a&gt; - предоставляет непрерывное
    тестирование и развертывание кода серверной части на всех
    используемых машинах.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/f94e85c8/" rel="nofollow" target="_blank" title="http://munin-monitoring.org/"&gt;Munin&lt;/a&gt; - измерение и построение
    графиков, касающихся здоровья серверов.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/cfd5a6c7/" rel="nofollow" target="_blank" title="http://www.pingdom.com/"&gt;Pingdom&lt;/a&gt; - внешний мониторинг.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/acf8f59d/" rel="nofollow" target="_blank" title="http://www.zendesk.com/"&gt;Zendesk&lt;/a&gt; - используется для оказания
    поддержки клиентам.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/17f204ac/" rel="nofollow" target="_blank" title="http://drupal.org/"&gt;Drupal&lt;/a&gt; - используется для основного сайта со
    специализированным модулем интернет-магазина.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a81d9bb9/" rel="nofollow" target="_blank" title="http://unfuddle.com/"&gt;Unfuddle&lt;/a&gt; - хостинг Subversion для
    взаимодействия удаленной команды разработчиков.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="kak-eto-rabotaet"&gt;Как это работает?&lt;/h2&gt;
&lt;p&gt;Процесс выглядит следующим образом:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Когда пользователь отправляет комментарий на сайт, происходит запрос
    к API Mollom.&lt;/li&gt;
&lt;li&gt;Контент анализируется, если он оказывается спамом, то сайту
    сообщается, что необходимо его заблокировать, если же алгоритм не
    уверен на 100% - сайту советуют показать CAPTCHA, которую сервис
    также предоставляет.&lt;/li&gt;
&lt;li&gt;После того, как CAPTCHA будет успешно заполнена, контент
    принимается. В большинстве случаев пользователи не будут ее видеть и
    контент будет приниматься сразу же.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Обнаружение спама является сложным балансом между отказом нормальному
контенту и принятию спама.&lt;/p&gt;
&lt;h2 id="biznes-model"&gt;Бизнес-модель&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Основным залогом популярности Mollom является бесплатная возможность
    попробовать сервис, ограничение составляет 100 нормальных (не спам)
    сообщений в день. Небольшие сайты могут никогда и не достичь этого
    ограничения.&lt;/li&gt;
&lt;li&gt;Далее есть два тарифа: 1 евро в день и 3600 евро с возможностями
    вполне соответствующими этим суммам&lt;/li&gt;
&lt;li&gt;Сайты, использующие бесплатный тариф, вовсе не зря тратят ресурсы
    системы, как кажется на первый взгляд, а являются жизненно-важным
    источником данных для тренировки системы. Без этих данных алгоритмы
    были бы существенно менее точны.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Разработчики Mollom уделяют максимум внимания времени отклика,
    эффективности кода и использования серверных ресурсов.&lt;/li&gt;
&lt;li&gt;Физически каждый сервер может справиться со всеми запросами, два
    сервера нужны для избежания перерывов в работе системы. Когда оба
    сервера в строю - работа распределяется между ними, когда один
    падает - второй перехватывает его запросы.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mollom прошел через несколько этапов развития:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Изначально маленькая команда из двух человек работала вечерами
    над основными алгоритмами, классификаторами и реальными
    бизнес-задачами, которые они пытались решить. Для построения
    инфраструктуры серверной части они использовали свои реализации
    базовых механизмов по управлению ресурсами, соединениями и
    потоками. В итоге они обнаружили, что тратят слишком много
    времени на эти вещи. После этого они переключились на Glassfish,
    что позволило им намного меньше беспокоится об управлении
    памятью, REST-запросах, парсинге XML и поддержании пула
    соединений с базой данных.&lt;/li&gt;
&lt;li&gt;В прошлом основной проблемой была пропускная способность
    дисковой подсистемы. Они должны хранить информацию о репутации
    всех IP-адресов и URL по всему Интернету, что привело к
    массивному набору данных с большим количеством случайных
    обращений.&lt;/li&gt;
&lt;li&gt;Поначалу они использовали MySQL на недорогой виртуальной машине,
    что в итоге не смогло масштабироваться.&lt;/li&gt;
&lt;li&gt;Они перенесли данные на твердотельные жесткие диски (SSD) и
    стали все хранить в файлах. Этот шаг решил проблемы с записью,
    но возникли новые проблемы:&lt;ol&gt;
&lt;li&gt;Это правда дорого.&lt;/li&gt;
&lt;li&gt;Очень чувствительно к типу используемой файловой системы&lt;/li&gt;
&lt;li&gt;Запись стала происходить быстрой, но итерация по большим
    наборам данным (что они делали довольно часто для очистки
    данных и обучения классификаторов) по-прежнему была очень
    медленным процессом.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;В итоге они отказались от твердотельных накопителей и стали
    использовать Cassandra.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cassandra сейчас используется для обработки интенсивного потока
    запросов на запись и в роли кэша:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Работает на RAID10, что хорошо подходит для высоких смешанных
    нагрузок на запись/чтение.&lt;/li&gt;
&lt;li&gt;Cassandra оптимизирована для записи, а в Mollom запись как раз
    происходит намного чаще, чем чтение.&lt;/li&gt;
&lt;li&gt;Разработана для распределенной работы как внутри датацентра, так
    и между датацентрами.&lt;/li&gt;
&lt;li&gt;Обратной стороной медали является отсутствие стандартного NoSQL
    интерфейса, что усложняет реализацию приложений.&lt;/li&gt;
&lt;li&gt;Механизм кэширования строк в Cassandra позволяет им не
    использовать отдельную систему для кэширования, что существенно
    упростило код приложения.&lt;/li&gt;
&lt;li&gt;Cassandra имеет функцию удаления устаревшей информации после
    определенного периода времени. В Европе существуют строгие
    законы о приватности личных данных, согласно которым они должны
    храниться не более определенного срока (штаб-квартира Mollom
    находится в Бельгии). В этом плане эта функция очень удобна.
    Эта функция опять же избавляет от необходимости реализовывать
    данный функционал вручную.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Типичный путь одного комментария внутри системы:&lt;ul&gt;
&lt;li&gt;Балансировка нагрузки между серверами лежит на клиентской
    библиотеке, в роли типичного клиента может выступать сайт на
    Drupal, осуществляющий запрос к API через XML-RPC или REST.&lt;/li&gt;
&lt;li&gt;Запросы обрабатываются сервером приложений Glassfish и проходят
    стандартный процесс обработки с помощью сервлетов и специфичных
    классов.&lt;/li&gt;
&lt;li&gt;Платящие клиенты обслуживаются в первую очередь, что приводит к
    тому, что клиенты на бесплатном тарифе могут ожидать результата
    несколько дольше.&lt;/li&gt;
&lt;li&gt;Запрос анализируется и оценка вероятности спама возвращается
    пользователю. Помимо этого отдельная часть кода Mollom отвечает
    за генерацию, выдачу и проверку CAPTCHA.&lt;/li&gt;
&lt;li&gt;Классификаторы полностью располагаются в оперативной памяти.
    Небольшой кусок контента разбивается на тысячи и тысячи
    крошечных частей, которые могут быть идентифицированы как спам.
    Такие классификаторы хранят в памяти до нескольких миллионов
    признаков, характерных для спама. Анализ должен выполняться
    очень быстро, так что никаких других вариантов кроме
    расположения всех требуемых данных в оперативной памяти просто
    не было.&lt;/li&gt;
&lt;li&gt;В Cassandra хранятся очки репутации, частоты, URL и IP-адреса.&lt;/li&gt;
&lt;li&gt;Струтуры данных в памяти не реплицируются напрямую. Они
    записываются в Cassandra, которая и передает их на второй
    сервер. Промежуток времени, когда данные не консистентны, очень
    невелик, так что это не сказывается негативно на алгоритмах.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Балансировка нагрузки с помощью клиента:&lt;ul&gt;
&lt;li&gt;Mollom использует такой подход к балансировке, так как стартап
    не может себе позволить дорогой железный балансировщик нагрузки.
    Если учесть, что им нужна балансировка между датацентрами,
    решение от любого из вендоров было бы комплексным и дорогим.&lt;/li&gt;
&lt;li&gt;У каждого клиента есть индивидуальный список серверов, которыми
    он может воспользоваться. Этот список изменяется через API.&lt;/li&gt;
&lt;li&gt;Каждый клиент может использовать разный список, платящим
    клиентам могут предоставлять отдельные сервера для уменьшения
    задержек.&lt;/li&gt;
&lt;li&gt;Если сервер упал - клиент пытается подключиться к следующему
    серверу в списке.&lt;/li&gt;
&lt;li&gt;С другой стороны такой подход усложняет разработку неофициальных
    клиентов: авторам проекта приходится тесно работать с
    разработчиками сторонних клиентов для обеспечения правильной
    реализации в них балансировки нагрузки.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Машинное обучение:&lt;ul&gt;
&lt;li&gt;Mollom - это набор самообучающихся систем. Отдельные
    CAPTCHA-решения, не учитывают ни пользовательское поведение, ни
    источник контента, заставляя каждого пользователя вводить
    проверочный код при каждом сообщении. В случае с Mollom это
    происходит только когда система анализа контента не уверена в
    конкретном решении.&lt;/li&gt;
&lt;li&gt;Средняя длина сообщения - 500 символов, обычно оно разбивается
    на 3000 характеристик. Принадлежность контента к спаму
    определяется путем оценки репутации IP адреса или Open ID,
    пользовательского идентификатора, эмоциональной окраски, языка,
    профанации, проверки на наличие специфичных слов и фраз, также
    учитывается качество написания текста и многие другие факторы.
    Все эти данные основываются на классификаторах. Некоторые из них
    статистические по природе, так что обучение происходит
    автоматически. Другие же основываются на правилах для того,
    чтобы быть уверенными, что они никогда не могут быть настроены
    неверно. Комбинация результатов всех тестов после нормализации и
    образует финальный рейтинг принадлежности к спаму каждого
    конкретного сообщения.&lt;/li&gt;
&lt;li&gt;Классификаторы и внутренние метрики обучаются с каждым новым
    сообщением и обновляются в реальном времени.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Glassfish берет на себя планировку нагрузки, учитывая многоядерность
    системы:&lt;ul&gt;
&lt;li&gt;Ключ к дизайну системы в многопроцессорном окружении заключается
    в максимальном параллелизации работы при минимальном простое
    из-за блокировок.&lt;/li&gt;
&lt;li&gt;Они используют 16 thread'ов на сервер.&lt;/li&gt;
&lt;li&gt;Большинство запросов обрабатываются сессионными объектами (Java
    Bean), не имеющими состояний. Они хорошо подходят для управления
    параллельными запросами.&lt;/li&gt;
&lt;li&gt;Они держат пул из нескольких сессионных объектов, но определение
    их количества делегируется Glassfish. В пиковую нагрузку это
    число увеличивается для более эффективной обработки запросов,
    порой оно достигает 32.&lt;/li&gt;
&lt;li&gt;Все классификаторы реализованы как раз как такие объекты,
    повторно использующиеся различными thread'ами.&lt;/li&gt;
&lt;li&gt;У каждого объекта есть свое клиентское соединение с Cassandra,
    чтобы гарантировать отсутствие блокировок.&lt;/li&gt;
&lt;li&gt;Когда пользователь не отвечает на CAPTCHA сессия очищается и
    Mollom узнает что это скорее всего был спам.&lt;/li&gt;
&lt;li&gt;На каждом сервере запущено по одной копии каждого
    классификатора.&lt;/li&gt;
&lt;li&gt;В момент очистки сессии происходит небольшая блокировка, когда
    происходит обновление классификаторов.&lt;/li&gt;
&lt;li&gt;Обновленные классификаторы записываются в Cassandra каждые пол
    часа.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Интеграция приложений:&lt;ul&gt;
&lt;li&gt;Mollom использует открытый API, который может быть интегрирован
    в любую систему.&lt;/li&gt;
&lt;li&gt;Библиотеки: Java, PHP, Ruby и другие.&lt;/li&gt;
&lt;li&gt;Готовые модули: Drupal, Joomla, Wordpress и прочие системы
    управления контентом.&lt;/li&gt;
&lt;li&gt;Решения от сторонних разработчиков, основанные на примерах кода
    от &amp;nbsp;Mollom.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Для мониторинга здоровья серверов они используют Munin:&lt;ul&gt;
&lt;li&gt;Каков размер heap памяти после сбора мусора?&lt;/li&gt;
&lt;li&gt;Каково количество доступных соединений?&lt;/li&gt;
&lt;li&gt;Каково количество thread'ов в пуле?&lt;/li&gt;
&lt;li&gt;Оценка времени блокировок в каждом thread'е.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Если взглянуть в целом на архитектуру Mollom, можно увидеть, что они
    стараются построить систему, способную прозрачно работать в
    нескольких датацентрах, чтобы позволить горизонтально расширить
    систему, когда они перерастут текущую двухсерверную конфигурацию:&lt;ul&gt;
&lt;li&gt;Балансировка нагрузки на клиенте позволяет выбирать оптимальный
    сервер и справляться со сбоями одного из них.&lt;/li&gt;
&lt;li&gt;Кластеризация Glassfish облегчает добавление/удаление новых
    машин и позволяет перехватывать запросы, когда один из серверов
    выходит из строя.&lt;/li&gt;
&lt;li&gt;Cassandra используется для управления данными между серверами в
    нескольких датацентрах.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Инсталляция Mollom в Netlog обладает некоторыми интересными
    характеристиками. Она обрабатывает больше сообщений, чем основные
    сервера Mollom, но распределение спама в ней совершенно другое, так
    как люди в ней общаются в рамках социальной сети. Внутри Netlog лишь
    10% сообщений является спамом, когда в суровом мире информационных
    порталов распределение обратно. Интересным следствием является тот
    факт, что обработка нормальных сообщений требует меньше
    вычислительных ресурсов, так что на аналогичном оборудовании удается
    обрабатывать больший поток сообщений.&lt;/li&gt;
&lt;li&gt;Изначально они думали о виртуализированных серверах, в частности об
    Amazon EC2, но в итоге обнаружилось, что наиболее узким местом
    являются операции ввода-вывода - низкая производительность дисковой
    подсистемы в виртуальных машинах создавали реальные проблемы, так
    что они решили воспользоваться вертикальным масштабированием и
    переехали на более дорогие физические машины с большим объемом
    дискового пространства:&lt;ul&gt;
&lt;li&gt;На удивление они не упираются в вычислительные ресурсы: лишь два
    ядра из 8 занимаются вычислениями, когда остальные же работают
    над операциями ввода-вывода.&lt;/li&gt;
&lt;li&gt;Трафик Mollom практически постоянен, так что физические сервера
    более эффективны с финансовой точки зрения. Они рассматривают
    Amazon лишь как запасной вариант для обработки непредвиденных
    пиков нагрузки.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Процесс разработки:&lt;ul&gt;
&lt;li&gt;Команда распределена: трое в Бельгии, остальные в Техасе,
    Бостоне и Германии.&lt;/li&gt;
&lt;li&gt;Scrum используется в процессе разработки и они довольны этой
    методологией. Scrum-собрание проходит через Skype в два часа дня
    по Бельгии.&lt;/li&gt;
&lt;li&gt;Разработчики работают локально и отправляют код на Unfuddle.&lt;/li&gt;
&lt;li&gt;Hudson используется для непрерывного интеграционного
    тестирования.&amp;nbsp;Hudson позволил облегчить миграцию, так как перед
    развертыванием все тесты должны быть пройдены. Они не теряли
    лишнего времени на проблемах, обнаруженных уже в развернутом
    приложении.&lt;/li&gt;
&lt;li&gt;Они активно используют автоматическое тестирование: юнит-тесты,
    системные тесты, тесты Drupal.&lt;/li&gt;
&lt;li&gt;Развертывание по-прежнему делается вручную для минимизации риска
    простоя (что правда спорный момент).&lt;/li&gt;
&lt;li&gt;Для обнаружения утечек памяти они используют анализ дампов
    оперативной памяти. Анализ дампа сервера с 16Гб памяти - дело
    непростое, практически невозможное на обычном компьютере, так
    что они арендуют большую виртуальную машину на Amazon для
    проведения анализа. Весь процесс занимает всего около 2 часов.
    Они сравнивают два дампа: через 10 и 20 часов после запуска
    сервера. Если обнаруживаются значительные отличия, то скорее
    всего дело в утечке памяти.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="puti-razvitiia"&gt;Пути развития&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Mollom API основано на XML-RPC, REST-интерфейс находится на стадии
    тестирования для облегчения интеграции других сервисов.&lt;/li&gt;
&lt;li&gt;Они мигрировали на Cassandra, чтобы облегчить процесс
    горизонтального масштабирования, когда нагрузка достигнет
    соответствующего уровня.&lt;/li&gt;
&lt;li&gt;Скоро будут выпущены корпоративные возможности, которые позволят
    работать с сотнями сайтов как с единым целым. Появится возможность
    легко модерировать несколько сайтов одновременно по эмоциональной
    окраске сообщений, рейтингу спама или удалить все сообщения с
    определенного IP-адреса.&lt;/li&gt;
&lt;li&gt;Они думали над участием в бизнесе потоковых данных вроде Twitter, но
    они сильно ограничены европейскими более строгими требованиями по
    приватности.&lt;/li&gt;
&lt;li&gt;Планируются эксперименты по использованию Glassfish для балансировки
    нагрузки в рамках каждого датацентра.&lt;/li&gt;
&lt;li&gt;Если нагрузка увеличится десятикратно им придется добавить больше
    серверов в Cassandra. Дисковый ввод-вывод является узким местом.
    Дополнительные сервера приложения понадобятся только если нагрузка
    вырастет более, чем на порядок.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mollom очень серьезно относится к
    разработке&amp;nbsp;высокопроизводительной системы.&lt;/strong&gt; Они гордятся тем, что
    Mollom очень эффективно использует вычислительные и финансовые
    ресурсы. Множество запросов может обрабатываться одним сервером с
    низкой задержкой, что очень радует как клиентов, так и владельцев
    проекта, так как издержки очень низки. Этот вопрос был выбран
    приоритетным с самого начала и они выбрали подходящие технологии для
    реализации своих целей. Это позволило им вкладывать средства в
    маркетинг, построить базу клиентов и создавать новые продукты на
    основе Mollom.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Машинное обучение требует много исходных данных&amp;nbsp;для успешного
    обнаружение спама.&lt;/strong&gt; Для сбора этих данных предлагает бесплатные
    услуги. Крупные клиенты обеспечивают доход и получают выгоду от
    данных, полученных от более мелких клиентов. Эта модель очень хорошо
    себя проявила в машинном обучении, за которым как известно будущее.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Старайтесь избавиться от проблем, не связанных напрямую с
    продуктом.&lt;/strong&gt; Большие системы требуют серьезных усилий на разработку
    инфраструктуры. Можно убить все время на построение&amp;nbsp;инфраструктуры,
    вместо создания по-настоящему ценного продукта (классификаторов,
    системы репутации, клиентских библиотек). Mollom постоянно пытались
    максимально избавляться от лишних проблем, именно по-этому они
    выбрали Cassandra и Glassfish.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Будьте осторожны с клиентским кодом.&lt;/strong&gt; Выполнение кода на
    клиентской части привлекательно тем, что он тратит чужие ресурсы, а
    не серверные. Проблемы начинаются когда сторонние библиотеки
    разрабатываются некачественно, что заставляет систему в целом
    работать плохо. Плотно работайте с разработчиками клиентских
    библиотек для повышения качества их продукции.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Отдавайте приоритет платящим клиентам.&lt;/strong&gt; Платящие клиенты получают
    более высокое качество услуг, обрабатываются вне очереди, получают
    меньше задержек и получают доступ к запасному серверу когда основной
    дал сбой. Этого вполне достаточно, чтобы подтолкнуть клиентов
    платить.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Уменьшайте объем кода, позволяя используемым сторонним продуктам
    брать на себя грязную работу&lt;/strong&gt;.&amp;nbsp;Поначалу код Mollom был существенно
    большим по объему, чем сейчас. Использование Cassandra и Glassfish
    позволило убрать массу кода, связанного с кэшированием,
    кластеризацией, репликацией и обработкой сбоев. Упрощайте систему со
    временем.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Минимизируйте блокировки.&lt;/strong&gt; Mollom потратили много времени на
    устранение блокировок внутри Glassfish, так как это начинало
    становиться узким местом. Минимизируйте простой от блокировок для
    достижения полного параллелизма.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="istochniki-informatsii-i-dopolnitelnye-materialy"&gt;Источники информации и дополнительные материалы&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/796d6b53/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2011/2/8/mollom-architecture-killing-over-373-million-spams-at-100-re.html"&gt;Mollom Architecture - Killing Over 373 Million Spams At 100 Request
    Per
    Second&lt;/a&gt;
    (основной источник информации)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/a695ef0e/" rel="nofollow" target="_blank" title="http://mollom.com/files/mollom-technical-whitepaper.pdf"&gt;Mollom Technical
    Whitepaper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/23d06e29/" rel="nofollow" target="_blank" title="http://blogs.sun.com/glassfishpodcast/entry/episode_072_mollom_com_s"&gt;Episode #072 - Mollom.com's GlassFish backend with Dries and
    Johan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/c1eeadac/" rel="nofollow" target="_blank" title="http://buytaert.net/mollom-gets-a-new-backend"&gt;Mollom gets a new
    backend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/fd87538b/" rel="nofollow" target="_blank" title="http://blogs.lodgon.com/johan/"&gt;Fighting spam with Mollom on
    Glassfish&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/9c3dc3c9/" rel="nofollow" target="_blank" title="http://mollom.com/api"&gt;Mollom API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Если Вам понравилась данная статья, можете ознакомиться с другими
&lt;a href="https://www.insight-it.ru/highload/"&gt;материалами по архитектуре высоконагруженных систем&lt;/a&gt; и &lt;a href="/feed/"&gt;подписаться на RSS&lt;/a&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Tue, 15 Feb 2011 19:19:00 +0300</pubDate><guid>tag:www.insight-it.ru,2011-02-15:highload/2011/arkhitektura-mollom/</guid><category>Cassandra</category><category>Drupal</category><category>Glassfish</category><category>Hudson</category><category>Intel</category><category>Java</category><category>Mollom</category><category>Munin</category><category>MySQL</category><category>Pingdom</category><category>saas</category><category>SoftLayer</category><category>Unfuddle</category><category>Xeon</category><category>Zendesk</category><category>Архитектура Mollom</category></item><item><title>Архитектура Вконтакте</title><link>https://www.insight-it.ru//highload/2010/arkhitektura-vkontakte/</link><description>&lt;p&gt;&lt;img alt="Логотип Вконтакте" class="left" src="https://www.insight-it.ru/images/vkontakte-logo.png" title="Логотип Вконтакте"/&gt;
Самая популярная социальная сеть в рунете пролила немного света на то,
как же она работает. Представители проекта в лице Павла Дурова и Олега
Илларионова на конференции &lt;a href="https://www.insight-it.ru/event/2010/highload-2010/"&gt;HighLoad++&lt;/a&gt; ответили на шквал вопросов по совершенно разным аспектам работы
&lt;a href="https://www.insight-it.ru/goto/a14c17dd/" rel="nofollow" target="_blank" title="http://www.vkontakte.ru"&gt;Вконтакте&lt;/a&gt;, в том числе и техническим. Спешу
поделиться своим взглядом на архитектуру проекта по результатам данного
выступления.&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/debian/"&gt;Debian&lt;/a&gt; &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; - основная операционная
    система&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/nginx/"&gt;nginx&lt;/a&gt; - балансировка нагрузки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; + &lt;a href="/tag/xcache/"&gt;XCache&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; + &lt;a href="/tag/mod_php/"&gt;mod_php&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Собственная СУБД на &lt;a href="/tag/c/"&gt;C&lt;/a&gt;, созданная "лучшими умами" России&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/node-js/"&gt;node.js&lt;/a&gt; - прослойка для реализации XMPP, живет за
    &lt;a href="/tag/haproxy/"&gt;HAProxy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Изображения отдаются просто с файловой системы &lt;a href="/tag/xfs/"&gt;xfs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/ffmpeg/"&gt;ffmpeg&lt;/a&gt; - конвертирование видео&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;95 миллионов учетных записей&lt;/li&gt;
&lt;li&gt;40 миллионов активных пользователей во всем мире (сопоставимо с
    аудиторией интернета в России)&lt;/li&gt;
&lt;li&gt;11 миллиардов запросов в день&lt;/li&gt;
&lt;li&gt;200 миллионов личных сообщений в день&lt;/li&gt;
&lt;li&gt;Видеопоток достигает 160Гбит/с&lt;/li&gt;
&lt;li&gt;Более 10 тысяч серверов, из которых только 32 - фронтенды на
    &lt;a href="/tag/nginx/"&gt;nginx&lt;/a&gt; (количество серверов с
    &lt;a href="/tag/apache/"&gt;Apache&lt;/a&gt; неизвестно)&lt;/li&gt;
&lt;li&gt;30-40 разработчиков, 2 дизайнера, 5 системных администраторов, много
    людей в датацентрах&lt;/li&gt;
&lt;li&gt;Каждый день выходит из строя около 10 жестких дисков&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;h3 id="obshchie-printsipy"&gt;Общие принципы&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cервера многофункциональны и используются одновременно в нескольких
    ролях:&lt;ul&gt;
&lt;li&gt;Перебрасывание полуавтоматическое&lt;/li&gt;
&lt;li&gt;Требуется перезапускать daemon'ы&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Генерация страниц с новостями (микроблоги) происходит очень похожим
    образом с &lt;a href="/tag/facebook/"&gt;Facebook&lt;/a&gt; (см. &lt;a href="https://www.insight-it.ru/highload/2010/arkhitektura-facebook/"&gt;Архитектура&amp;nbsp;Facebook&lt;/a&gt;), основное
    отличие - использование собственной СУБД вместо MySQL&lt;/li&gt;
&lt;li&gt;При балансировке нагрузки используются:&lt;ul&gt;
&lt;li&gt;Взвешенный round robin внутри системы&lt;/li&gt;
&lt;li&gt;Разные сервера для разных типов запросов&lt;/li&gt;
&lt;li&gt;Балансировка на уровне ДНС на 32 IP-адреса&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Большая часть внутреннего софта написано самостоятельно, в том
    числе:&lt;ul&gt;
&lt;li&gt;Собственная СУБД (см. ниже)&lt;/li&gt;
&lt;li&gt;Мониторинг с уведомлением по СМС (Павел сам помогал верстать
    интерфейс :) )&lt;/li&gt;
&lt;li&gt;Автоматическая система тестирования кода&lt;/li&gt;
&lt;li&gt;Анализаторы статистики и логов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Мощные сервера:&lt;ul&gt;
&lt;li&gt;8-ядерные процессоры Intel (по два на сервер, видимо)&lt;/li&gt;
&lt;li&gt;64Гб оперативной памяти&lt;/li&gt;
&lt;li&gt;8 жестких дисков (соответственно скорее всего корпуса 2-3U)&lt;/li&gt;
&lt;li&gt;RAID не используется&lt;/li&gt;
&lt;li&gt;Не брендированные&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Вычислительные мощности серверов используются менее, чем на 20%&lt;/li&gt;
&lt;li&gt;Сейчас проект расположен в 4 датацентрах в Санкт-Петербурге и
    Москве, причем:&lt;ul&gt;
&lt;li&gt;Вся основная база данных располагается в одном датацентре в
    Санкт-Петербурге&lt;/li&gt;
&lt;li&gt;В Московских датацентрах только аудио и видео&lt;/li&gt;
&lt;li&gt;В планах сделать репликацию базы данных в другой датацентр в
    ленинградской области&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/cdn/"&gt;CDN&lt;/a&gt; на данный момент не используется, но в планах есть&lt;/li&gt;
&lt;li&gt;Резервное копирование данных происходит ежедневно и инкрементально&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="volshebnaia-baza-dannykh-na-c"&gt;Волшебная база данных на &lt;a href="/tag/c/"&gt;C&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Этому продукту, пожалуй, уделялось максимум внимания аудитории, но при
этом почти никаких подробностей о том, что он собственно говоря собой
представляет, так и не было обнародовано. Известно, что:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Разработана "лучшими умами" России, победителями олимпиад и
    конкурсов топкодер; озвучили даже имена этих "героев" Вконтакте
    (писал на слух и возможно не всех успел, так что извиняйте):&lt;ul&gt;
&lt;li&gt;Андрей Лопатин&lt;/li&gt;
&lt;li&gt;Николай Дуров&lt;/li&gt;
&lt;li&gt;Арсений Смирнов&lt;/li&gt;
&lt;li&gt;Алексей Левин&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Используется в огромном количестве сервисов:&lt;ul&gt;
&lt;li&gt;Личные сообщения&lt;/li&gt;
&lt;li&gt;Сообщения на стенах&lt;/li&gt;
&lt;li&gt;Статусы&lt;/li&gt;
&lt;li&gt;Поиск&lt;/li&gt;
&lt;li&gt;Приватность&lt;/li&gt;
&lt;li&gt;Списки друзей&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Нереляционная модель данных&lt;/li&gt;
&lt;li&gt;Большинство операций осуществляется в оперативной памяти&lt;/li&gt;
&lt;li&gt;Интерфейс доступа представляет собой расширенный протокол
    &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;, специальным образом составленные ключи
    возвращают результаты сложных запросов (чаще всего специфичных для
    конкретного сервиса)&lt;/li&gt;
&lt;li&gt;Хотели бы сделать из данной системы универсальную СУБД и
    опубликовать под GPL, но пока не получается из-за высокой степени
    интеграции с остальными сервисами&lt;/li&gt;
&lt;li&gt;Кластеризация осуществляется легко&lt;/li&gt;
&lt;li&gt;Есть репликация&lt;/li&gt;
&lt;li&gt;Если честно, я так и не понял зачем им &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt; с такой
    штукой - возможно просто как legacy живет со старых времен&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="audio-i-video"&gt;Аудио и видео&lt;/h3&gt;
&lt;p&gt;Эти подпроекты являются побочными для социальной сети, на них особо не
фокусируются. В основном это связанно с тем, что они редко коррелируют с
основной целью использования социальной сети - &lt;em&gt;общением&lt;/em&gt;, а также
создают большое количество проблем: видео траффик - основная статья
расходов проекта, плюс всем известные проблемы с нелегальным контентом и
претензиями правообладателей. Медиа-файлы банятся по хэшу при удалении
по просьбе правообладателей, но это неэффективно и планируется
усовершенствовать этот механизм.&lt;/p&gt;
&lt;p&gt;1000-1500 серверов используется для перекодирования видео, на них же оно
и хранится.&lt;/p&gt;
&lt;h3 id="xmpp"&gt;XMPP&lt;/h3&gt;
&lt;p&gt;Как известно, некоторое время назад появилась возможность общаться на
Вконтакте через протокол Jabber (он же XMPP). Протокол совершенно
открытый и существует масса opensource реализаций.&lt;/p&gt;
&lt;p&gt;По ряду причин, среди которых проблемы с интеграцией с остальными
сервисами, было решено за месяц создать собственный сервер,
представляющий собой прослойку между внутренними сервисами Вконтакте и
реализацией XMPP протокола. Основные особенности этого сервиса:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Реализован на &lt;a href="/tag/node-js/"&gt;node.js&lt;/a&gt; (выбор обусловлен тем, что
    JavaScript знают практически все разработчики проекта, а также
    хороший набор инструментов для реализации задачи)&lt;/li&gt;
&lt;li&gt;Работа с большими контакт-листами - у многих пользователей
    количество друзей на Вконтакте измеряется сотнями и тысячами&lt;/li&gt;
&lt;li&gt;Высокая активность смены статусов - люди появляются и исчезают из
    онлайна чаще, чем в других аналогичных ситуациях&lt;/li&gt;
&lt;li&gt;Аватарки передаются в &lt;code&gt;base64&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Тесная интеграция с внутренней системой обмена личными сообщениями
    Вконтакте&lt;/li&gt;
&lt;li&gt;60-80 тысяч человек онлайн, в пике - 150 тысяч&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/haproxy/"&gt;HAProxy&lt;/a&gt; обрабатывает входящие соединения и
    используется для балансировки нагрузки и развертывания новых версий&lt;/li&gt;
&lt;li&gt;Данные хранятся в &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt; (думали о MongoDB, но
    передумали)&lt;/li&gt;
&lt;li&gt;Сервис работает на 5 серверах разной конфигурации, на каждом из них
    работает код на&lt;a href="/tag/node-js/"&gt;node.js&lt;/a&gt; (по 4 процесса на сервер), а
    на трех самых мощных - еще и &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;В &lt;a href="/tag/node-js/"&gt;node.js&lt;/a&gt; большие проблемы с использованием
    &lt;a href="/tag/openssl/"&gt;OpenSSL&lt;/a&gt;, а также течет память&lt;/li&gt;
&lt;li&gt;Группы друзей в XMPP не связаны с группами друзей на сайте - сделано
    по просьбе пользователей, которые не хотели чтобы их друзья из-за
    плеча видели в какой группе они находятся&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="integratsiia-so-vneshnimi-resursami"&gt;Интеграция со внешними ресурсами&lt;/h3&gt;
&lt;p&gt;Во Вконтакте считают данное направление очень перспективным и
осуществляют массу связанной с этим работы. Основные предпринятые шаги:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Максимальная кроссбраузерность для виджетов на основе библиотек
    easyXDM и fastXDM&lt;/li&gt;
&lt;li&gt;Кросс-постинг статусов в &lt;a href="/tag/twitter/"&gt;Twitter&lt;/a&gt;, реализованный с
    помощью очередей запросов&lt;/li&gt;
&lt;li&gt;Кнопка "поделиться с друзьями", поддерживающая openGraph теги и
    автоматически подбирающая подходящую иллюстрацию (путем сравнивание
    содержимых тега &lt;code&gt;&amp;lt;title&amp;gt;&lt;/code&gt; и атрибутов alt у изображений, чуть ли не
    побуквенно)&lt;/li&gt;
&lt;li&gt;Возможность загрузки видео через сторонние видео-хостинги (YouTube,
    RuTube, Vimeo, и.т.д.), открыты к интеграции с другими&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="interesnye-fakty-ne-po-teme_1"&gt;Интересные факты не по теме&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Процесс разработки близок к Agile, с недельными итерациями&lt;/li&gt;
&lt;li&gt;Ядро операционной системы модифицированно (на предмет работы с
    памятью), есть своя пакетная база для &lt;a href="/tag/debian/"&gt;Debian&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Фотографии загружаются на два жестких диска одного сервера
    одновременно, после чего создается резервная копия на другом сервере&lt;/li&gt;
&lt;li&gt;Есть много доработок над &lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt;, в.т.ч. для
    более стабильного и длительного размещения объектов в памяти; есть
    даже persistent версия&lt;/li&gt;
&lt;li&gt;Фотографии не удаляются для минимизации фрагментации&lt;/li&gt;
&lt;li&gt;Решения о развитии проекта принимают Павел Дуров и Андрей Рогозов,
    ответственность за сервисы - на них и на реализовавшем его
    разработчике&lt;/li&gt;
&lt;li&gt;Павел Дуров откладывал деньги на хостинг с 1 курса :)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;p&gt;В целом Вконтакте развивается в сторону увеличения скорости
распространения информацию внутри сети. Приоритеты поменялись в этом
направлении достаточно недавно, этим обусловлено, например, перенос
выхода почтового сервиса Вконтакте, о котором очень активно говорили
когда появилась возможность забивать себе текстовые URL вроде
&lt;code&gt;vkontakte.ru/ivan.blinkov&lt;/code&gt;. Сейчас этот подпроект имеет низкий приоритет
и ждет своего часа, когда они смогут предложить что-то более удобное и
быстрое, чем Gmail.&lt;/p&gt;
&lt;p&gt;Завеса тайны насчет технической реализации Вконтакте была немного
развеяна, но много моментов все же остались секретом. Возможно в будущем
появится более детальная информация о собственной СУБД Вконтакте,
которая как оказалось является ключом к решению всех самых сложных
моментов в масштабируемости системы.&lt;/p&gt;
&lt;p&gt;Как я уже упоминал этот пост написан почти на память, на основе
небольшого конспекта "круглого стола Вконтакте", так что хочется сразу
извиниться за возможные неточности и недопонимания. Я лишь
структурировал хаотичную кучу ответов на вопросы. Буду рад уточнениям и
дополнениям.&lt;/p&gt;
&lt;p&gt;Если хотите быть в курсе новых веяний в сфере масштабируемости
высоконагруженных интернет-проектов - по традиции рекомендую
&lt;a href="/feed/"&gt;подписаться на RSS&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 28 Oct 2010 21:12:00 +0400</pubDate><guid>tag:www.insight-it.ru,2010-10-28:highload/2010/arkhitektura-vkontakte/</guid><category>Apache</category><category>C++</category><category>Debian</category><category>featured</category><category>ffmpeg</category><category>HAProxy</category><category>highload</category><category>Linux</category><category>Memcached</category><category>mod_php</category><category>MySQL</category><category>nginx</category><category>node.js</category><category>openssl</category><category>PHP</category><category>XCache</category><category>xfs</category><category>Архитектура Вконтакте</category><category>Вконтакте</category></item><item><title>Архитектура Facebook</title><link>https://www.insight-it.ru//highload/2010/arkhitektura-facebook/</link><description>&lt;p&gt;&lt;img alt="Facebook Logo" class="left" src="https://www.insight-it.ru/images/facebook_logo.jpg" title="Facebook Logo"/&gt;
На сегодняшний день &lt;a href="https://www.insight-it.ru/goto/fbae133c/" rel="nofollow" target="_blank" title="https://www.facebook.com"&gt;Facebook&lt;/a&gt; является пожалуй
самым обсуждаемым интернет-проектом во всем мире. Не смотря на довольно
низкий уровень проникновения Facebook в России, темпы захвата аудитории
этим проектом мягко говоря поражают. Как же им удается управляться с
таким огромным социальным графом и удовлетворять потребности в общении
невероятно большого количества людей по всему миру?&lt;/p&gt;
&lt;!--more--&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt; - операционная система&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/php/"&gt;PHP&lt;/a&gt; с &lt;a href="/tag/hiphop/"&gt;HipHop&lt;/a&gt; - код на PHP компилируется в
    C++&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;memcached&lt;/a&gt; - агрессивное кэширование объектов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt; - используется как хранилище пар ключ-значение,
    никаких join'ов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/thrift/"&gt;Thrift&lt;/a&gt; - интерфейс взаимодействия между сервисами,
    написанными на разных языках программирования&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/scribe/"&gt;Scribe&lt;/a&gt; - универсальная система сбора и агрегации
    данных с рабочих серверов&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Более 500 миллионов активных пользователей (месячная аудитория)&lt;/li&gt;
&lt;li&gt;Более миллиарда социальных связей&lt;/li&gt;
&lt;li&gt;Более 200 миллиардов просмотров страниц в месяц&lt;/li&gt;
&lt;li&gt;Более 4 триллионов действий попадает в новостные ленты каждый день&lt;/li&gt;
&lt;li&gt;Более 150 миллионов обращений к кэшу в секунду; 2 триллиона объектов
    в кэше&lt;/li&gt;
&lt;li&gt;Более 8 миллиардов минут провели пользователи на Facebook'е
    ежедневно&lt;/li&gt;
&lt;li&gt;Более 3 миллиардов фотографий загружается каждый месяц, до 1.2
    миллиона фотографий в секунду&lt;/li&gt;
&lt;li&gt;20 миллиардов фотографий в 4 разрешениях = 80 миллиардов фотографий,
    их бы хватило чтобы покрыть поверхность земли в 10 слоев; это
    больше, чем на всех других фото-ресурсах в месте взятых&lt;/li&gt;
&lt;li&gt;О более чем 5 миллиардах единиц контента рассказывается друзьям
    еженедельно&lt;/li&gt;
&lt;li&gt;Более миллиарда сообщений в чате каждый день&lt;/li&gt;
&lt;li&gt;Более ста миллионов поисковых запросов в день&lt;/li&gt;
&lt;li&gt;Более 250 приложений и 80 тысяч сторонних ресурсов на платформе
    Facebook Connect&lt;/li&gt;
&lt;li&gt;Более 400 тысяч разработчиков сторонних приложений&lt;/li&gt;
&lt;li&gt;Менее 500 разработчиков и системных администраторов в штате&lt;/li&gt;
&lt;li&gt;Более миллиона активных пользователей на одного инженера&lt;/li&gt;
&lt;li&gt;Десятки тысяч серверов, десятки гигабит трафика&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;h3 id="obshchie-printsipy"&gt;Общие принципы&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Балансировщик нагрузки выбирает веб-сервер для обработки запроса&lt;/li&gt;
&lt;li&gt;PHP-код в веб-сервере подготавливает HTML, пользуясь данными из
    различных источников:&lt;ul&gt;
&lt;li&gt;MySQL&lt;/li&gt;
&lt;li&gt;memcached&lt;/li&gt;
&lt;li&gt;Специализированные сервисы&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Если взглянуть с другой стороны, то получим трехуровневую
    архитектуру:&lt;ul&gt;
&lt;li&gt;Вер-приложение&lt;/li&gt;
&lt;li&gt;Распределенный индекс&lt;/li&gt;
&lt;li&gt;Постоянное хранилище&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Использование открытых технологий там, где это возможно&lt;/li&gt;
&lt;li&gt;Поиск возможностей оптимизации используемых продуктов&lt;/li&gt;
&lt;li&gt;Философия Unix:&lt;ul&gt;
&lt;li&gt;Старайтесь делать каждый компонент системы простым и
    производительным&lt;/li&gt;
&lt;li&gt;Комбинируйте компоненты для решения задач&lt;/li&gt;
&lt;li&gt;Концентрируйте внимание на хорошо обозначенных точках
    взаимодействия&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Все усилия направлены на масштабируемость&lt;/li&gt;
&lt;li&gt;Попытки минимизации количества точек отказа&lt;/li&gt;
&lt;li&gt;Простота, простота, простота!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="php"&gt;PHP&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Почему PHP?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Во многом "так исторически сложилось"&lt;/li&gt;
&lt;li&gt;Хорошо подходит для веб-разработки&lt;/li&gt;
&lt;li&gt;Легок в изучении: небольшой набор выражений и языковых конструкций&lt;/li&gt;
&lt;li&gt;Легок в написании: нестрогая типизация и универсальный "массив"&lt;/li&gt;
&lt;li&gt;Легок в чтении: синтаксис похож на C++ и Java&lt;/li&gt;
&lt;li&gt;Прост в дебаггинге: нет необходимости в перекомпиляции&lt;/li&gt;
&lt;li&gt;Большой ассортимент библиотек, актуальных для веб-проектов&lt;/li&gt;
&lt;li&gt;Подходит для процесса разработки с короткими итерациями&lt;/li&gt;
&lt;li&gt;Активное сообщество разработчиков по всему миру&lt;/li&gt;
&lt;li&gt;Динамическая типизация, интерпретируемый язык для скриптов&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Как оказалось на самом деле?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Высокий расход оперативной памяти и вычислительных ресурсов&lt;/li&gt;
&lt;li&gt;Сложно работать, когда объем исходного кода очень велик: слабая
    типизация и ограниченные возможности для статичного анализа и
    оптимизации кода&lt;/li&gt;
&lt;li&gt;Не особо оптимизирован для использования в крупных проектах&lt;/li&gt;
&lt;li&gt;Линейный рост издержек при подключении файлов с исходным кодом&lt;/li&gt;
&lt;li&gt;Механизм разработки расширений не очень удобен&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Доработки:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Оптимизация байт-кода&lt;/li&gt;
&lt;li&gt;Улучшения в APC (ленивая загрузка, оптимизация блокировок,
    "подогрев" кэша)&lt;/li&gt;
&lt;li&gt;Свои расширения (клиент memcache, формат сериализации, логи,
    статистика, мониторинг, механизм асинхронной обработки событий)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://www.insight-it.ru/goto/169aa287/" rel="nofollow" target="_blank" title="http://github.com/facebook/hiphop-php"&gt;HipHop&lt;/a&gt;&lt;/strong&gt; - трансформатор
    исходных кодов:&lt;ul&gt;
&lt;li&gt;Разработчики пишут на PHP, который конвертируется в
    оптимизированный C++&lt;/li&gt;
&lt;li&gt;Статический анализ, определение типов данных, генерация кода,
    и.т.д.&lt;/li&gt;
&lt;li&gt;Облегчает разработку расширений&lt;/li&gt;
&lt;li&gt;Существенно сокращает расходы оперативной памяти и
    вычислительных ресурсов&lt;/li&gt;
&lt;li&gt;У команды из трех программистов ушло полтора года на разработку,
    переписаны большая часть интерпретатора и многие расширения
    языка&lt;/li&gt;
&lt;li&gt;Опубликован под opensource лицензией в начале года, нет
    необходимости проходить этот же путь с нуля&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="mysql"&gt;MySQL&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Как используется MySQL?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Используется как хранилище пар ключ-значение&lt;/li&gt;
&lt;li&gt;Большое количество логических узлов распределено между физическими
    машинами&lt;/li&gt;
&lt;li&gt;Балансировка нагрузке на уровне физических серверов&lt;/li&gt;
&lt;li&gt;Репликация для распределения операций чтения &lt;strong&gt;не&lt;/strong&gt; используется&lt;/li&gt;
&lt;li&gt;Большинство запросов касаются самой свежей информации: оптимизация
    таблиц для доступа к новым данным, архивация старых записей&lt;/li&gt;
&lt;li&gt;В целом быстро и надежно&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Как оказалось на самом деле?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Логическая миграция данных &lt;em&gt;очень&lt;/em&gt; сложна&lt;/li&gt;
&lt;li&gt;Создавать большое количество логических баз данных и
    перераспределять их между физическими узлами, балансируя таким
    образом нагрузку, намного удобнее&lt;/li&gt;
&lt;li&gt;Никаких join'ов на рабочих серверах баз данных&lt;/li&gt;
&lt;li&gt;Намного проще наращивать вычислительные мощности на веб-серверах,
    чем на серверах баз данных&lt;/li&gt;
&lt;li&gt;Схемы, основанные на структуре данных, делают программистов
    счастливыми и создают большую головную боль администраторам&lt;/li&gt;
&lt;li&gt;Никогда не храните не-статичные данные в централизованное базе
    данных&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Доработки:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Практически никаких модификаций исходного кода MySQL&lt;/li&gt;
&lt;li&gt;Своя схема партиционирования с глобально-уникальными
    идентификаторами&lt;/li&gt;
&lt;li&gt;Своя схема архивирования, основанная на частоте доступа к данным
    относительно каждого пользователя&lt;/li&gt;
&lt;li&gt;Расширенный движок запросов для репликации между датацентрами и
    поддержания консистенции кеша&lt;/li&gt;
&lt;li&gt;Библиотеки для доступа к данным на основе графа:&lt;ul&gt;
&lt;li&gt;Объекты (вершины графа) с ограниченными типами данных (целое
    число, строка ограниченно длины, текст)&lt;/li&gt;
&lt;li&gt;Реплицированные связи (ребра графа)&lt;/li&gt;
&lt;li&gt;Аналоги распределенных внешних ключей (foreign keys)&lt;/li&gt;
&lt;li&gt;Большинство данных распределено случайно&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="memcache"&gt;Memcache&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Как используется memcached?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Высокопроизводительная распределенная хэш-таблица&lt;/li&gt;
&lt;li&gt;Содержит "горячие" данные из MySQL&lt;/li&gt;
&lt;li&gt;Снижает нагрузку на уровень баз данных&lt;/li&gt;
&lt;li&gt;Основная форма кэширования&lt;/li&gt;
&lt;li&gt;Используется более 25TB памяти на нескольких тысячах серверов&lt;/li&gt;
&lt;li&gt;Среднее время отклика менее 250 микро-секунд&lt;/li&gt;
&lt;li&gt;Кэшируются сериализованные структуры данных PHP&lt;/li&gt;
&lt;li&gt;Отсутствие автоматического механизма проверки консистенции данных
    между memcached и MySQL - приходится делать это на уровне
    программного кода&lt;/li&gt;
&lt;li&gt;Множество multi-get запросов для получения данных на другом конце
    ребер графа&lt;/li&gt;
&lt;li&gt;Ограниченная модель данных, неэффективен для маленьких объектов&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Доработки:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Порт на 64-битную архитектуру&lt;/li&gt;
&lt;li&gt;Более эффективная сериализация&lt;/li&gt;
&lt;li&gt;Многопоточность&lt;/li&gt;
&lt;li&gt;Улучшенный протокол&lt;/li&gt;
&lt;li&gt;Компрессия&lt;/li&gt;
&lt;li&gt;Проксирование запросов&lt;/li&gt;
&lt;li&gt;Доступ к memcache через UDP:&lt;ul&gt;
&lt;li&gt;уменьшает расход памяти благодаря отсутствию тысяч буферов TCP
    соединений&lt;/li&gt;
&lt;li&gt;управление ходом исполнения приложение (оптимизация для
    multi-get)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Статистика о работе потоков по запросу - уменьшает блокировки&lt;/li&gt;
&lt;li&gt;Ряд изменений в ядре Linux для оптимизации работы memcache:&lt;ul&gt;
&lt;li&gt;распределение управления сетевыми прерывания по всем ядрам&lt;/li&gt;
&lt;li&gt;оппортунистический опрос сетевых интерфейсов&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;После вышеперечисленных модификаций memcached способен выполнять до
    250 тысяч операций в секунду, по сравнению со стандартными 30-40
    тысячами без данных изменений&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="thrift"&gt;Thrift&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Что это?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Легкий механизм построения приложений с использованием нескольких
    языков программирования&lt;/li&gt;
&lt;li&gt;Высокая цель: предоставить механизм прозрачного взаимодействия между
    языками программирования.&lt;/li&gt;
&lt;li&gt;Предоставляет язык описания интерфейсов, статический генератор кода&lt;/li&gt;
&lt;li&gt;Поддерживаемые языки: &lt;a href="/tag/c/"&gt;C++&lt;/a&gt;, &lt;a href="/tag/php/"&gt;PHP&lt;/a&gt;,
    &lt;a href="/tag/python/"&gt;Python&lt;/a&gt;, &lt;a href="/tag/java/"&gt;Java&lt;/a&gt;, &lt;a href="/tag/ruby/"&gt;Ruby&lt;/a&gt;,
    &lt;a href="/tag/erlang/"&gt;Erlang&lt;/a&gt;, &lt;a href="/tag/perl/"&gt;Perl&lt;/a&gt;, &lt;a href="/tag/haskell/"&gt;Haskell&lt;/a&gt; и
    многие другие&lt;/li&gt;
&lt;li&gt;Транспорты: простой интерфейс для ввода-вывода (сокеты, файлы,
    буферы в памяти)&lt;/li&gt;
&lt;li&gt;Протоколы: стандарты сериализации (бинарный, JSON)&lt;/li&gt;
&lt;li&gt;Серверы: неблокирующие, асинхронные, как однопоточные, так и
    многопоточные&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Почему именно Thrift?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Альтернативные технологии: SOAP, CORBA, COM, Pillar, Protocol
    Buffers - но у всех есть свои существенные недостатки, что вынудило
    Facebook создать свою технологию&lt;/li&gt;
&lt;li&gt;Он быстрый, очень быстрый&lt;/li&gt;
&lt;li&gt;Меньше рабочего времени тратится каждым разработчиком на сетевые
    интерфейсы и протоколы&lt;/li&gt;
&lt;li&gt;Разделение труда: работа над высокопроизводительными серверами
    ведется отдельно от работы над приложениями&lt;/li&gt;
&lt;li&gt;Общий инструментарий, знакомый всем разработчикам&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="scribe"&gt;Scribe&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Что это?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Масштабированный распределенный механизм ведения логов&lt;/li&gt;
&lt;li&gt;Перемещает данные с серверов в центральный репозиторий&lt;/li&gt;
&lt;li&gt;Широкая сфера применения:&lt;ul&gt;
&lt;li&gt;Логи поисковых запросов&lt;/li&gt;
&lt;li&gt;Публикации в новостных лентах&lt;/li&gt;
&lt;li&gt;Данные по A/B тестированиям&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Более надежен, чем традиционные системы логгирования, но
    недостаточно надежен для транзакций баз данных&lt;/li&gt;
&lt;li&gt;Простая модель данных&lt;/li&gt;
&lt;li&gt;Построен на основе Thrift&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="khranenie-fotografii"&gt;Хранение фотографий&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Сначала сделали это просто:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Загрузка на сервер: приложение принимает изображение, создает
    миниатюры в нужных разрешениях, сохраняет в NFS&lt;/li&gt;
&lt;li&gt;Загрузка с сервера: изображения отдаются из NFS через HTTP&lt;/li&gt;
&lt;li&gt;NFS построена на коммерческих продуктах&lt;/li&gt;
&lt;li&gt;Это было необходимо, чтобы сначала проверить, что продукт
    востребован пользователями и они правда будут активно загружать
    фотографии&lt;/li&gt;
&lt;li&gt;На самом деле оказалось, что:&lt;ul&gt;
&lt;li&gt;Файловые системы непригодны для работы с большим количеством
    небольших файлов&lt;/li&gt;
&lt;li&gt;Метаданные не помещаются в оперативную память, что приводит к
    дополнительным обращениям к дисковой подсистеме&lt;/li&gt;
&lt;li&gt;Ограничивающим фактором является ввод-вывод, а не плотность
    хранения&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Потом начали оптимизировать:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Кэширование более часто используемых миниатюр изображений в памяти
    на оригинальных серверах для масштабируемости, надежности и
    производительности&lt;/li&gt;
&lt;li&gt;Распределение их по &lt;a href="/tag/cdn/"&gt;CDN&lt;/a&gt; для уменьшения сетевых задержек&lt;/li&gt;
&lt;li&gt;Возможно сделать еще лучше:&lt;ul&gt;
&lt;li&gt;Хранение изображений в больших бинарных файлах (blob)&lt;/li&gt;
&lt;li&gt;Сервис, отвечающий за фотографии имеет информацию о том, в каком
    файле и с каким отступом от начала расположена каждая фотография
    (по ее идентификатору)&lt;/li&gt;
&lt;li&gt;Этот сервис в Facebook называется Haystack и он оказался в 10
    раз эффективнее "простого" подхода и в 3 раза эффективнее
    "оптимизированного"&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="drugie-servisy"&gt;Другие сервисы&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SMC&lt;/strong&gt;: консоль управления сервисами - централизованная
    конфигурация, определение на какой физической машине работает
    логический сервис&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ODS&lt;/strong&gt;:&amp;nbsp;инструмент для визуализации изменений любых статистических
    данных, имеющихся в системе; удобен для мониторинга и оповещений&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gatekeeper:&lt;/strong&gt; разделение развертывания и запуска, A/B
    тестирования, таргетированный запуск, постепенный запуск&lt;/li&gt;
&lt;li&gt;И еще около 50 других сервисов...&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="kak-eto-rabotaet-vse-vmeste_1"&gt;Как это работает все вместе?&lt;/h2&gt;
&lt;h3 id="novye-albomy-druzei"&gt;Новые альбомы друзей&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Facebook Screenshot" class="responsive-img" src="https://www.insight-it.ru/images/facebook_screenshot.jpg" title="Facebook"/&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Получаем профиль по идентификатору пользователя (скорее всего из
    кэша, но потенциально возможно обращение к базе данных)&lt;/li&gt;
&lt;li&gt;Получаем список друзей (опять же на основе идентификатора
    пользователя из кэша или из базы данных в случае промаха)&lt;/li&gt;
&lt;li&gt;Параллельно запрашиваем идентификаторы последних 10 альбомов для
    каждого из друзей (multi-get, каждый промах мимо кэша индивидуально
    вытаскивается из MySQL)&lt;/li&gt;
&lt;li&gt;Параллельно получаем данные о всех альбомах (на основе
    идентификаторов альбомов из предыдущего шага)&lt;/li&gt;
&lt;li&gt;Все данные получены, выполняем логику отрисовки конкретной страницы
    на PHP&lt;/li&gt;
&lt;li&gt;Отправляем HTML в браузер, пользователь счастлив :)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="novostnaia-lenta"&gt;Новостная лента&lt;/h3&gt;
&lt;p&gt;&lt;img alt="News Feed Screenshot" class="responsive-img" src="https://www.insight-it.ru/images/facebook_screenshot_2.jpg" title="News Feed"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="News Feed Scheme" class="responsive-img" src="https://www.insight-it.ru/images/facebook_screenshot_3.jpg" title="News Feed"/&gt;&lt;/p&gt;
&lt;h3 id="poisk"&gt;Поиск&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Search Screenshot" class="responsive-img" src="https://www.insight-it.ru/images/facebook_screenshot_4.jpg" title="Search"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Search Scheme" class="responsive-img" src="https://www.insight-it.ru/images/facebook_screenshot_5.jpg" title="Search"/&gt;&lt;/p&gt;
&lt;h2 id="podvodim-itogi_1"&gt;Подводим итоги&lt;/h2&gt;
&lt;p&gt;LAMP не идеален&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PHP+MySQL+Memcache решает большинство задач, но не может решить
    совсем все:&lt;ul&gt;
&lt;li&gt;PHP не может хранить состояния&lt;/li&gt;
&lt;li&gt;PHP не самый производительный язык&lt;/li&gt;
&lt;li&gt;Все данные находятся удаленно&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Facebook разрабатывает собственные внутренние сервисы, чтобы:&lt;ul&gt;
&lt;li&gt;Располагать исполняемый код ближе к данным&lt;/li&gt;
&lt;li&gt;Скомпилированное окружение более эффективно&lt;/li&gt;
&lt;li&gt;Некоторая функциональность присутствует только в других языках
    программирования&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Философия сервисов:&lt;ul&gt;
&lt;li&gt;Создание сервисов только при необходимости (минимизация издержек
    по развертке, поддержке и ведению отдельной кодовой базы;
    потенциальная дополнительная точка сбоя)&lt;/li&gt;
&lt;li&gt;Создание общего набора инструментов для создания сервисов
    (Thrift, Scribe, ODS, средства мониторинга и уведомлений)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Использование правильных языка программирования, библиотек и
    инструментов для решения задачи&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Возвращение инноваций общественности - важный аспект разработки в
    Facebook:&lt;ul&gt;
&lt;li&gt;Опубликованные свои проекты:&lt;ul&gt;
&lt;li&gt;Thrift&lt;/li&gt;
&lt;li&gt;Scribe&lt;/li&gt;
&lt;li&gt;Tornado&lt;/li&gt;
&lt;li&gt;Cassandra&lt;/li&gt;
&lt;li&gt;Varnish&lt;/li&gt;
&lt;li&gt;Hive&lt;/li&gt;
&lt;li&gt;xhprof&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Доработки популярных решений:&lt;ul&gt;
&lt;li&gt;PHP&lt;/li&gt;
&lt;li&gt;MySQL&lt;/li&gt;
&lt;li&gt;memcached&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Информация о взаимодействии Facebook с opensource-сообществом,
    этих и других проектах расположена на &lt;a href="https://www.insight-it.ru/goto/535d8e6b/" rel="nofollow" target="_blank" title="http://developers.facebook.com/opensource/"&gt;странице, посвященной
    opensource&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Ключевые моменты культуры разработки в Facebook:&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Двигайся быстро&lt;/strong&gt; и не бойся ломать некоторые вещи&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Большое влияние&lt;/strong&gt; маленьких команд&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Будь откровенным&lt;/strong&gt; и инновационным&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;p&gt;Данная статья не является переводом готовой статьи, в качестве
источников информации послужили записи выступлений сотрудников Facebook
на конференциях:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/4c672c94/" rel="nofollow" target="_blank" title="http://www.infoq.com/presentations/Facebook-Software-Stack"&gt;Facebook Architecture: Science and the Social Graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/2ccd1899/" rel="nofollow" target="_blank" title="http://www.infoq.com/presentations/Facebook-Moving-Fast-at-Scale"&gt;Facebook: Moving Fast at Scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/3867625a/" rel="nofollow" target="_blank" title="http://www.infoq.com/presentations/Scale-at-Facebook"&gt;Scale at Facebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Очень рекомендую посмотреть материалы в оригинале, так как естественно я
осветил в статье далеко не все, да и неточности какие-либо неисключены.
Помимо этого возможно многим будет интересно мероприятие &lt;a href="https://www.insight-it.ru/goto/ff11ad2b/" rel="nofollow" target="_blank" title="http://styleru.timepad.ru/event/3571"&gt;"Facebook: how we scaled to 500 000 000 users "&lt;/a&gt;,
где Robert Johnson выступает 22 октября в Москве. Еще он числится в
списке докладчиков &lt;a href="https://www.insight-it.ru/goto/727c9436/" rel="nofollow" target="_blank" title="http://www.highload.ru"&gt;Highload++&lt;/a&gt; с аналогичным
выступлением. Дополнительную информацию можно почерпнуть в &lt;a href="https://www.insight-it.ru/goto/f38bc794/" rel="nofollow" target="_blank" title="http://facebook.com/eblog"&gt;блоге инженеров Facebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPD:&lt;/strong&gt; Обновил некоторые моменты после посещения вышеупомянутого
выступления Роберта.&lt;/p&gt;
&lt;p&gt;И по традиции напоминаю, что так как я пишу довольно редко - читать мой
блог намного удобнее по &lt;a href="/feed/"&gt;RSS&lt;/a&gt;. Спасибо за внимание :)&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Wed, 20 Oct 2010 13:02:00 +0400</pubDate><guid>tag:www.insight-it.ru,2010-10-20:highload/2010/arkhitektura-facebook/</guid><category>CDN</category><category>Facebook</category><category>featured</category><category>HipHop</category><category>Linux</category><category>Memcached</category><category>MySQL</category><category>ODS</category><category>PHP</category><category>Scribe</category><category>Thrift</category><category>Архитектура Facebook</category></item><item><title>Архитектура Plenty of Fish</title><link>https://www.insight-it.ru//highload/2010/arkhitektura-plenty-of-fish/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/fa2360fd/" rel="nofollow" target="_blank" title="http://www.plentyoffish.com/"&gt;Plenty of Fish&lt;/a&gt; представляет собой очень
популярный сервис онлайн знакомств, насчитывающий более 45 миллионов
посетителей в месяц и 30+ миллионов просмотров страниц в сутки (что
составляет около 500-600 страниц в секунду). Но это не самая интересная
часть истории... Все это управляется единственным человеком при
использовании нескольких серверов, при этом он тратит на работу всего
пару часов в день и зарабатывает 6 миллионов долларов на рекламе от
Google. Завидуете? Я тоже :) Как же ему удалось соединить столько
влюбленных пар, используя так мало ресурсов?&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Данный пост является переводом &lt;a href="https://www.insight-it.ru/goto/e06f86e0/" rel="nofollow" target="_blank" title="http://highscalability.com/plentyoffish-architecture"&gt;англоязычной
статьи&lt;/a&gt;, автор
оригинала: Todd Hoff.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/35763c73/" rel="nofollow" target="_blank" title="http://channel9.msdn.com/ShowPost.aspx?PostID=331501#331501"&gt;Channel9 интервью с Markus Frind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/fa591b8c/" rel="nofollow" target="_blank" title="http://plentyoffish.wordpress.com/%20target="&gt;Блог Markus Frind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/ed31cb93/" rel="nofollow" target="_blank" title="http://www.readwriteweb.com/archives/plentyoffish_one_billion.php"&gt;Plentyoffish: компания одного человека может стоить 1 миллиард
долларов&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Microsoft Windows&lt;/li&gt;
&lt;li&gt;ASP.NET&lt;/li&gt;
&lt;li&gt;IIS&lt;/li&gt;
&lt;li&gt;Akamai CDN&lt;/li&gt;
&lt;li&gt;Foundry ServerIron Load Balancer&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;PlentyOfFish (POF) имеет 1.2 миллиарда просмотров страниц в месяц, в
среднем 500 тысяч уникальных авторизованных пользователей в день.
Пиковый сезон приходится на январь каждого года, когда эти цифры
возрастают на 30%.&lt;/li&gt;
&lt;li&gt;POF имеет единственного сотрудника: создатель и генеральный директор
Markus Frind.&lt;/li&gt;
&lt;li&gt;Зарабатывает до 10 миллионов долларов в год на рекламе от Google,
работает при этом только около двух часов в день.&lt;/li&gt;
&lt;li&gt;30+ миллионов просмотров страниц в день (500 - 600 страниц в секунду).&lt;/li&gt;
&lt;li&gt;1.2 миллиарда просмотров страниц и 45 миллионов посетителей в месяц.&lt;/li&gt;
&lt;li&gt;Имеет &lt;a href="https://www.insight-it.ru/goto/ca40875e/" rel="nofollow" target="_blank" title="http://ru.wikipedia.org/wiki/CTR_(%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%BD%D0%B5%D1%82)"&gt;CTR&lt;/a&gt; в 5-10 раз выше, чем Facebook.&lt;/li&gt;
&lt;li&gt;Находится в top 30 сайтов США по данным Competes Attention, top 10 в Канаде и top 30 в Великобритании.&lt;/li&gt;
&lt;li&gt;Нагрузка балансируется между двумя веб-серверами с 2 Quad Core Intel
Xeon X5355 @ 2.66Ghz, 8GB RAM (используется около 800 MB), 2 жесткими
дисками, работают под управлением Windows x64 Server 2003.&lt;/li&gt;
&lt;li&gt;3 сервера баз данных. Информация об их конфигурации не предоставляется.&lt;/li&gt;
&lt;li&gt;Приближается к 64000 одновременных соединений и 2 миллионам просмотрам
страниц в час.&lt;/li&gt;
&lt;li&gt;Интернет-канал в 1Gbps, из которых используется только 200Mbps.&lt;/li&gt;
&lt;li&gt;1 TB трафика от отдачи 171 миллионов изображений через Akamai.&lt;/li&gt;
&lt;li&gt;6TB система хранения данных для обработки миллионов полноразмерных
изображений, которые загружаются на сайт каждый месяц.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="chto-vnutri"&gt;Что внутри?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Модель монетизации заключалась в использовании рекламы от Google.
Match.com, для сравнения, получает 300 миллионов долларов в год, в
основном с платных подписок. Источник дохода POF должен измениться,
чтобы позволить ему получать больше выручки от имеющихся пользователей.
Планируется нанять больше сотрудников, в частности людей, которые будут
заниматься продажей рекламы напрямую вместо того, чтобы полностью
полагаться на AdSense.&lt;/li&gt;
&lt;li&gt;При 30 миллионах просмотрах страниц в день можно зарабатывать неплохие
деньги на рекламе, даже
если&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/b1c7d720/" rel="nofollow" target="_blank" title="http://en.wikipedia.org/wiki/Cost_per_mille"&gt;CPM&lt;/a&gt; будет всего 5-10
центов.&lt;/li&gt;
&lt;li&gt;Akamai используется для отдачи более 100 миллионов изображений в день.
Если на странице 8 изображений и каждое загружается за 100 миллисекунд -
их загрузка займет почти секунду, так что распределение изображений
целесообразно.&lt;/li&gt;
&lt;li&gt;Десятки миллионов изображений отдаются с серверов POF, но большинство из
них размером меньше 2KB и практически полностью закешированы в
оперативной памяти.&lt;/li&gt;
&lt;li&gt;Все динамично. Практически никакой статики.&lt;/li&gt;
&lt;li&gt;Все исходящие данные сжимаются с использованием Gzip, что обходится
всего 30% использованием процессорного времени. Используется много
вычислительных ресурсов, но зато существенно сокращается использование
пропускной способности интернет-канала.&lt;/li&gt;
&lt;li&gt;Кэширование ASP .NET не используется, так как данные теряют свою
актуальность практически сразу же.&lt;/li&gt;
&lt;li&gt;Встроенные компоненты ASP также не используется. Почти все написано с
чистого листа. Ничего не может быть более сложным, чем кучка простых
if-then-else и циклов. Все максимально элементарно.&lt;/li&gt;
&lt;li&gt;Балансировка нагрузки:&lt;/li&gt;
&lt;li&gt;IIS произвольно ограничивает общее количество соединений до 64000,
таким образом балансировщик нагрузки был добавлен для обработки большего
количества одновременных соединений. Вариант с добавлением второго IP
адреса и использованием round robin DNS также рассматривался, но вариант
с балансировщиком нагрузки выглядел более избыточным и позволял более
легко расширять количество серверов. Помимо этого ServerIron позволял
использовать более продвинутую функциональность, вроде блокировки ботов
и балансировку запросов по cookies, сессиям или IP-адресам
пользователей.&lt;/li&gt;
&lt;li&gt;Windows Network Load Balancing (NLB) функция не использовалась, так
как не поддерживает привязку сессий к серверам. Обходным путем было бы
хранение сессионных данных в базе данных или общей файловой системе.&lt;/li&gt;
&lt;li&gt;8-12 NLB серверов могут объединяться в кластер и может использоваться
неограниченное количество таких кластеров. Схема DNS round robin может
использоваться для распределения запросов между кластерами. Теоретически
такая архитектура могла бы позволить 70 веб-серверам обрабатывать более&amp;nbsp;300 тысяч одновременных соединений.&lt;/li&gt;
&lt;li&gt;NLB имеет опцию для отправки каждого пользователя на конкретный
сервер, таким образом не используется внешнее хранилище для сессионных
данных и если сервер выходит из строя - пользователи просто
разлогиниваются из системы. Если это состояние включает в себя например
корзину интернет-магазина или какую-то другую важную информацию, то
такой подход мог бы показаться&amp;nbsp;неприемлемым, но для сайта знакомств это
было бы не так критично.&lt;/li&gt;
&lt;li&gt;Было решено, что хранение и получение сессионных данных программными
средствами слишком дорого. Аппаратная балансировка нагрузка проще:
пользователи просто назначаются конкретным серверам и в случае сбоя
сервера назначенным ему пользователям предлагается пройти процесс
авторизации еще раз.&lt;/li&gt;
&lt;li&gt;Покупка ServerIron была дешевле и проще, чем использование NLB.
Многие крупные сайты используют их для создания пулов TCP соединений,
автоматическому определению ботов и так далее. ServerIron может делать
намного больше, чем просто балансировать нагрузку и такие функции
достаточно привлекательные за эту цену.&lt;/li&gt;
&lt;li&gt;Была большая проблема с выбором системы размещения рекламы. Многие из
них хотели несколько сотен тысяч в год и многолетний контракт.&lt;/li&gt;
&lt;li&gt;В процессе избавления от ASP.NET повторителей и использование взамен
конкатенации строк или response.write. Если у вас миллионы просмотров
страниц в день - просто напишите весь код для отображения на экране
пользователя.&lt;/li&gt;
&lt;li&gt;Большинство изначальных вложений ушло на построение SAN. Избыточность
любой ценой.&lt;/li&gt;
&lt;li&gt;Рост был за счет вирусного эффекта. Портал начал набирать популярность в
Канаде, затем о нем узнали в Великобритании и Австралии, и только потом
в США.&lt;/li&gt;
&lt;li&gt;База данных:&lt;/li&gt;
&lt;li&gt;Одна база данных является основной.&lt;/li&gt;
&lt;li&gt;Две базы данных для поиска. Поисковые запросы распределяются по их типу.&lt;/li&gt;
&lt;li&gt;Производительность наблюдается через диспетчер задач. Когда
появляются пики - ситуация рассматривается более детально. Проблемы
обычно заключались в блокировках на уровне СУБД. Собственно говоря почти
всегда это были проблемы с базами данных, очень редко они возникают на
уровне .NET. Так как POF не использует библиотеки .NET, отследить
проблемы с производительностью оказывается достаточно просто. Если бы
использовалось много уровней framework'ов, поиск мест, где скрываются
проблемы, был бы трудным и утомляющим.&lt;/li&gt;
&lt;li&gt;Если Вы делаете запрос к базе данных 20 раз при отображении одной
страницы, &amp;nbsp;Вы проиграли в любом случае, вне зависимости от того, что Вы
будете делать.&lt;/li&gt;
&lt;li&gt;Разделяйте запросы чтения и записи к базе данных. Если у вас нет
избыточного количества оперативной памяти не следование этому правилу
может заставить систему зависнуть на несколько секунд.&lt;/li&gt;
&lt;li&gt;Постарайтесь делать базы данных только для чтения.&lt;/li&gt;
&lt;li&gt;Денормализуйте данные. Если Вам приходится доставать данные из 20
разных таблиц, попробуйте сделать просто одну таблицу, где будут лежать
все нужные для чтения данные.&lt;/li&gt;
&lt;li&gt;Один день может проработать почти что угодно, но когда Ваша база
данных удвоится - использованные подход может внезапно перестать
работать.&lt;/li&gt;
&lt;li&gt;Если система делает только что-то одно, она будет делать это реально
хорошо. Только записывайте данные и все будет нормально. Только читайте
данные и все будет нормально. Делайте и то и другое - и все испортится.
База данных погрязнет в проблемах с блокировками.&lt;/li&gt;
&lt;li&gt;Если Вы полностью используете вычислительные мощности, Вы либо
делаете что-то не так, либо Ваша система на самом деле очень
оптимизирована. Если вы можете разместить всю базу в оперативной
памяти - обязательно делайте это.&lt;/li&gt;
&lt;li&gt;Процесс разработки выглядит примерно следующим образом: появляется идея,
быстро реализуется и выдается пользователям в пределах 24 часов. Отклик
от пользователей получается по слежению за тем, что они делают на сайте:
выросло количество сообщений на пользователя? среднее время сессий
выросло? Если пользователям новая фишка не пришлась по вкусу - просто
уберите её.&lt;/li&gt;
&lt;li&gt;При небольшом количестве серверов системные сбои достаточно редки и
краткосрочны. Наибольшими сложностями были проблемы с DNS, когда
некоторые интернет-провайдеры говорили, что POF больше не существует. Но
так как сайт бесплатен, пользователи нормально относятся к небольшим
периодам его недоступности. Люди часто не замечают простой сайта, так
как думают, что это какая-то проблема у них, с интернет-соединением или
еще чем-то.&lt;/li&gt;
&lt;li&gt;Переход от миллиона пользователей к 12 миллионам пользователей был
большим прыжком. Система может обслуживать и 60 миллионов пользователей
с двумя веб-серверами.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Часто смотрите на конкурентов для идей новых функциональных
возможностей.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Рассмотрите использование чего-то вроде S3, когда система начнет
требовать географической балансировки.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Вам не нужны миллионы в финансировании, размашистая инфраструктура и
целое здание сотрудников для того, чтобы создать вебсайт мирового
уровня, который обслуживает кучу пользователей и приносит неплохие
деньги. Все что нужно - всего лишь привлекательная идея, которая
понравится большому количеству идей, сайт, который становится популярным
благодаря слухам, а также опыт и видение для построения сайта, не
наступая на типичные "грабли". Вот и все, что Вам нужно :-)&lt;/li&gt;
&lt;li&gt;Необходимость - мать всех изменений.&lt;/li&gt;
&lt;li&gt;Когда вы растете быстро, но не слишком быстро, у Вас появляется шанс
расти, модифицировать и адаптироваться.&lt;/li&gt;
&lt;li&gt;Максимальное использование оперативной памяти решает массу проблем.
После этого рост возможен просто за счет использование более мощных
серверов.&lt;/li&gt;
&lt;li&gt;В начале старайтесь держать все максимально простым. Практически все
дают этот же самый совет, а Markus говорит, что все что он делает -
всего лишь очевидный здравый смысл. Но то что просто, не всегда означает
всего лишь осмысленную вещь. Создание простых вещей является результатом
многих лет практического опыта.&lt;/li&gt;
&lt;li&gt;Поддерживайте время доступа к базе данных быстрым и у Вас не будет
проблем.&lt;/li&gt;
&lt;li&gt;Одной из основных причин, по которой POF может работать с таким
небольшим количеством сотрудников и оборудования, является использование
CDN для отдачи активно используемого контента. Использование CDN может
оказаться секретным соусом для многих крупных сайтов. Markus считает,
что в top 100 не существует ни одного сайта, не использующего CDN. Без
CDN время загрузки страницы в Австралии возросло бы до 3-4 секунд только
за счет изображений.&lt;/li&gt;
&lt;li&gt;Реклама на Facebook принесла плохие результаты. Из 2000 кликов только 1
человек регистрировался. С CTR равным 0.04% Facebook выдавал 0.4 клика
на 1000 показов рекламы (CPM). При 5 центах CPM = 12.5 центов за клик,
50 центах CPM = 1.25\$ за клик. 1 доллар CPM = 2.50\$ за клик. 15\$ CPM
= 37.50\$ за клик.&lt;/li&gt;
&lt;li&gt;Это просто продавать несколько миллионов просмотров страниц с высоким
CPM, но НАМНОГО сложнее продавать миллиарды просмотров с высоким CPM,
как это делают Myspace и Facebook.&lt;/li&gt;
&lt;li&gt;Модель монетизации, основанная на рекламе, ограничивает Ваши доходы. Вам
придется переходить к платной модели чтобы повышать прибыль.
Генерировать 100 миллионов долларов в год за счет бесплатного сайта
практически невозможно - Вам потребуется слишком большой рынок.&lt;/li&gt;
&lt;li&gt;Повышение количества просмотров за счет Facebook не работает для сайтов
знакомств. Иметь посетителя на собственном сайте намного более
прибыльно. Большинство просмотров страниц на Facebook находятся за
пределами США и Вам придется делить 5 центов CPM с Facebook.&lt;/li&gt;
&lt;li&gt;Предложение пользователям при регистрации получить информацию об ипотеке
или каком-то другом продукте, может стать неплохим источником
дополнительной выручки.&lt;/li&gt;
&lt;li&gt;Вы не можете постоянно прислушиваться к отзывам пользователей. Кому-то
всегда будут нравиться новые функции, а кто-то всегда будет их
ненавидеть, но только часть из них сообщит Вам об этом. Вместо этого
лучше смотреть как новые функции влияют на то, чем люди на самом деле
занимаются, просто смотря на Ваш сайт и статистику его использования.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 18 Jan 2010 16:43:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-01-18:highload/2010/arkhitektura-plenty-of-fish/</guid><category>Akamai CDN</category><category>ASP</category><category>ASP .NET</category><category>dating</category><category>Foundry</category><category>IIS</category><category>Microsoft</category><category>online</category><category>Plenty of Fish</category><category>POF</category><category>ServerIron</category><category>Windows</category><category>Windows Server</category><category>архитектура</category><category>Архитектура Plenty of Fish</category><category>Масштабируемость</category><category>сайт знакомств</category></item><item><title>Aladdin от Baidu</title><link>https://www.insight-it.ru//highload/2010/aladdin-ot-baidu/</link><description>&lt;p&gt;Наверняка все прекрасно знают о лидерах интернет-поиска в российской
части интернета: про Google, Яндекс или Рамблер сказано уже не мало
слов, все много раз о них читали, пользовались, обсуждали - ведь уже
прошло больше 10 лет с момента создания каждой из этих поисковых систем
и, как следствие, их конкуренции на просторах рунета. Намного меньше же
внимания на российских информационных сайтах уделяется национальным
проектам других стран, а ведь среди них тоже есть заслуживающие внимания
экземпляры, об одном из них я бы и хотел сегодня поведать.
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="istochniki-dannykh"&gt;Источники данных&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/7e247449/" rel="nofollow" target="_blank" title="http://tech.sina.com.cn/i/2009-12-16/14423683386.shtml"&gt;Baidu Aladdin Technology Guashudila&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/9d98f3c7/" rel="nofollow" target="_blank" title="http://tech.sina.com.cn/i/2009-08-18/16063362415.shtml"&gt;Rachel Liao, лекция директора по архитектуре Baidu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/d1f8deb1/" rel="nofollow" target="_blank" title="http://news.xinhuanet.com/it/2006-04/06/content_4390847.htm"&gt;Baidu Chief Architect: алгоритмы на службе разработчиков Baidu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/cc13f208/" rel="nofollow" target="_blank" title="http://baike.baidu.com/view/2086291.htm"&gt;Aladdin Plans&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Если кто-то достаточно любопытен, чтобы нажать на приведенные ссылки -
они все на китайском, так что статья написана на основе перевода Google
Translate со всеми вытекающими последствиями. Даже за название "Aladdin"
не ручаюсь, его тоже он придумал :)&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="o-kompanii-baidu"&gt;О компании Baidu&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/50a1d1a7/" rel="nofollow" target="_blank" title="http://www.baidu.com"&gt;Baidu.com&lt;/a&gt; является лидером китайского рынка
интернет-поиска, объем которого достаточно значителен. На данный момент
Китай насчитывает около 340-360 миллионов интернет-пользователей, что
превышает общую численность населения США. Не трудно представить с каким
трафиком приходится сталкиваться крупнейшей китайской поисковой системе.&lt;/p&gt;
&lt;p&gt;Чтобы не быть голословным, еще немного цифр о Baidu:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;100 миллионов поисковых запросов в день&lt;/li&gt;
&lt;li&gt;Более миллиарда проиндексированных страниц&lt;/li&gt;
&lt;li&gt;300-400 миллионов проиндексированных сайтов&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Уже на сегодняшний день размеры китайской части интернета производят
впечатление и с каждым днем она расширяется все больше. Как следствие,
на рынке образуются все новые и новые возможности для создания сервисов,
удовлетворяющих потребности китайских пользователей Интернет.
Компания&amp;nbsp;&lt;strong&gt;Baidu Inc.&lt;/strong&gt; пристально наблюдает за развитием ситуации и
обнаружила огромную потребность среди сервис-провайдеров в удобной
платформе для создания и предоставления пользователям новых сервисов.
Baidu считает создание платформы для использования их технологии
сторонними разработчиками и сервис-провайдерами очень важным
направлением развития на пути к повышению качества пользовательского
опыта в целом. Эти наблюдения стали толчком к рождению в рамках Baidu
новой технологии под названием&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/e0a5512c/" rel="nofollow" target="_blank" title="http://open.baidu.com/"&gt;Aladdin&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Как крупнейшей китайской поисковой системе, Baidu приходится быть чем-то
большим, чем просто инструментом для поиска, это позволяет удовлетворять
потребности потенциальных клиентов наиболее гармоничным и целесообразным
образом. Помимо неустанной погони за технологическими инновациями, Baidu
предпочитает придерживаться политики "потребности клиентов важнее
всего".&lt;/p&gt;
&lt;h2 id="aladdin"&gt;Aladdin&lt;/h2&gt;
&lt;p&gt;Согласно официальному сайту Baidu, эта технология представляет собой
открытую поисковую платформу, позволяющую сторонним разработчикам
использовать технологию Baidu в своих приложениях и сервисах. Владельцы
интернет-проектов и разработчики могут предоставить Baidu данные в уже
структурированном виде для того, чтобы создать еще более мощные и
функционально-насыщенные приложения, позволяя интернет-сайтам получать
еще более значимый трафик, а пользователям - еще больше облегчить
использование сайтов и поиск в сети Интернет.&lt;/p&gt;
&lt;p&gt;В декабре 2008 года Baidu объявили о высокоприоритетной программе под
кодовым названием&amp;nbsp;&lt;em&gt;"Aladdin"&lt;/em&gt;, основной идеей была попытка расширить
текущие рамки веб-поиска, по большей части за счет включения так
называемого "глубинного интернета" в поисковую базу, проведения более
глубокого анализа контента. Помимо этого упоминались возможность
интеграции и управляемой обработки информации, направленных на
минимизацию издержек поиска и времени обработки запроса при повышение
общего качества поисковых результатов. В том же заявлении Baidu также
описали их общую позицию по данному направлению: платформа Aladdin
является надстройкой над текущей поисковой системой Baidu, позволяющей
дополнение и расширение функциональных возможностей.&lt;/p&gt;
&lt;p&gt;Согласно исследованиям Baidu, только 75% пользователей поисковых систем
в конечном итоге удовлетворяют свои информационные потребности. В
процессе анализа причин данного факта было выявлено, что в большом
количестве случаев искомая информация находится на ресурсах по каким-то
причинам находящимся вне доступа поисковых систем (начиная от
технических ограничений, отсутствия внешних ссылок на ресурс и
заканчивая искусственными&amp;nbsp;барьерами вроде REP или принудительной
авторизации).&lt;/p&gt;
&lt;p&gt;Перед разработчиками Aladdin встают две основные проблемы с точки зрения
технической реализации: "как определить пользовательские потребности" и
"как сортировать". Конечно же они очень тесно связаны между собой, это
хорошо демонстрирует пример с поисковым запросом "полное солнечное
затмение": до затмения пользователи хотят когда оно будет и откуда лучше
смотреть, а во время и после него намного актуальнее будет увидеть
видео-запись или прямую трансляцию, а также прочитать и поделиться
комментариями. Самым простым методом решения данного класса задач
является статистический анализ - Aladdin выделяет два основных фактора,
используемых для сортировки результатом в соответствии с потребностями
пользователей: "удовлетворенность потребностей" и "уровень отклика на
спрос". Конечно же оценочные характеристики спроса и потребностей не
означают сам спрос, то есть возможны и более сложные ситуации, когда за
пользовательским запросом стоит целый комплекс более простых
потребностей.&lt;/p&gt;
&lt;p&gt;Алгоритмы, используемые в Aladdin для решения упомянутых проблем,
основаны на машинном обучении, анализе поведения пользователей, а также
обратной связи от использования технологии на практике. Конечная цель
данной платформы заключается в построении целой интеллектуальной
экосистемы, &amp;nbsp;которая станет новым шагом в развитии компании Baidu и
китайской части интернета в целом.&lt;/p&gt;
&lt;h3 id="vozmozhnosti-platformy"&gt;Возможности платформы&lt;/h3&gt;
&lt;p&gt;С технической точки зрения Aladdin от Baidu представляет собой открытый
API к поисковой технологии Baidu, позволяющий добавлять свои данные в
структурированном виде в поисковый индекс, отмечать релевантные ключевые
слова, методы отображения информации и пометки данных гео-метками.&lt;/p&gt;
&lt;p&gt;Одним из важнейших направлений развития поисковых систем является
повышение "интеллектуальности" поиска, Baidu уделяет внимание не только
обнаружению более ценной информации в глубинах Интернета, но и
предоставлению более удобных, точных и сообразительных поисковых
сервисов.&lt;/p&gt;
&lt;p&gt;На сегодняшний день, технология Aladdin была интегрирована в ряд
приложений, позволив тем самым реализовать на страницах с результатами
поиска&amp;nbsp;множество интересных возможностей: прямой звонок клиенту для
обсуждения каких-то товаров или услуг, интеграция с почтовым сервисом,
прослушивание музыки с использованием встроенного flash-плеера и многие
другие.&lt;/p&gt;
&lt;p&gt;После обязательной процедуры подачи и рассмотрения заявки пользователям
платформы Aladdin предоставляются следующие возможности:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Добавление данных в индекс в структурированном виде&lt;/li&gt;
&lt;li&gt;Указание ключевых слов для более точного прямого воздействия на
    целевую аудиторию&lt;/li&gt;
&lt;li&gt;Управление сортировкой и отображением информационного контента&lt;/li&gt;
&lt;li&gt;Управление стилем и внешним видом имеющихся ресурсов, причем не
    только текстовых&lt;/li&gt;
&lt;li&gt;Выбор частоты обновления информации для синхронизации данных&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;На первый взгляд все эти рассуждения и заявления о функциональных
возможностях кажутся абсурдными, даже отчасти ироничными. Ну кому может
понадобиться вручную управлять результатами поиска, добавлять и
структурировать данные, возиться с сортировкой и внешним видом?&lt;/p&gt;
&lt;h3 id="vzgliad-s-drugoi-storony"&gt;Взгляд с другой стороны&lt;/h3&gt;
&lt;p&gt;Да, вся платформа Aladdin по своей задумке очень искуственна:
практически все делается вручную, но по сути это лишь процесс
интеграции, а не работа с самим контентом. Для большинства других
поисковых систем такой подход неприемлем: где найти столько людей, чтобы
управлять огромными массивами данных вручную? Наоборот все поисковые
системы стремятся по максимуму все автоматизировать и борятся с
искуственным вмешательством в поисковый индекс (т.н. SEO), но... если
вспомнить, что Baidu работает в Китае - вся затея начинает обретать
здравый смысл. Как сама компания Baidu, так и большинство их
потенциальных партнеров, клиентов и пользователей находится в примерно
одинаковой ситуации: большое количество дешевой рабочей силы,
относительно низкий уровень образования и профессиональной подготовки, а
также прочие национальные особенности. В их ситуации не выгодно идти по
пути Google и делать&amp;nbsp;&lt;em&gt;основной&lt;/em&gt; акцент на построении полностью
автоматизированных систем анализа контента, добавления дополнительного
материала к поисковым результатам и самим делать различные
дополнительные приложения и сервисы. Намного выгоднее пойти по
собственному пути, более адаптированному к ситуации в Китае, большое
количество трудолюбивых людей позволяет строить сервисы коллективно, с
привлечением партнеров, клиентов и заинтересованных лиц. Да, во многом
вручную, за счет интеграции совершенно различных систем и сервисов, но
зато более качественно и продуманно. В этом-то и заключается вся магия
Китая.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 14 Jan 2010 00:01:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-01-14:highload/2010/aladdin-ot-baidu/</guid><category>Aladdin</category><category>Baidu.com</category><category>online</category><category>Масштабируемость</category><category>поиск</category><category>поисковые системы</category></item><item><title>Архитектура Stack Overflow</title><link>https://www.insight-it.ru//highload/2010/arkhitektura-stack-overflow/</link><description>&lt;p&gt;&lt;img alt="Stack Overflow" class="right" src="https://www.insight-it.ru/images/stack-overflow-logo.png" title="Stack Overflow"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/33fc61d9/" rel="nofollow" target="_blank" title="https://stackoverflow.com/"&gt;Stack Overflow&lt;/a&gt; является любимым многими
программистами сайтом, где можно задать профессиональный вопрос и
получить ответы от коллег. Этот проект был написан двумя никому не
известными парнями, о которых никто никогда раньше не слышал. Хорошо, не
совсем так. Stack Overflow был создан топовыми программистами и звездами
блогосферы:&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/374a081/" rel="nofollow" target="_blank" title="http://www.codinghorror.com/blog/"&gt;Jeff Atwood&lt;/a&gt; и&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/31657700/" rel="nofollow" target="_blank" title="http://www.joelonsoftware.com/"&gt;Joel Spolsky&lt;/a&gt;. В этом отношении Stack
Overflow похож на ресторан, владельцами которого являются знаменитости.
По оценкам Joel'а около 1/3 программистов всего мира использовали этот
интернет-ресурс, так что должно быть он представляет собой что-то
достаточно полезное и интересное.&lt;/p&gt;
&lt;p&gt;Одним из ключевых моментов в истории Stack Overflow является
использование вертикального масштабирования, как достаточно
работоспособного решения достаточного большого класса проблем. Не смотря
на то, что публика на сегодняшний день больше склоняется к подходу с
использованием горизонтальным масштабирования и&amp;nbsp;не-SQL баз данных.&lt;/p&gt;
&lt;p&gt;Если Вы стремитесь к масштабу Google, у Вас нет другого выхода, как
двигаться в направлении не-SQL. Но Stack Overflow - это не Google, ровно
как и подавляющее большинство других сайтов. Когда Вы задумываетесь о
возможных вариантов дизайна Вашего проекта, попробуйте учесть и историю
Stack Overflow, она тоже имеет право на жизнь. В этот век многоядерных
машин с большим объемом оперативной памяти и невероятными темпами
развития методов параллельного программирования, вертикальное
масштабирование все еще является жизнеспособной стратегией и не должна
сразу же отбрасываться в сторону просто так как это теперь больше не
модно. Возможно в один прекрасный день мы получим лучшее из обоих миров,
но на сегодняшний момент перед нами лежит большой болезненный выбор
стратегии масштабирования, от которого определенно зависит судьба Вашего
проекта.&lt;/p&gt;
&lt;p&gt;Joel любит похвастаться тем, что они достигли производительности,
сравнимой с другими сайтами аналогичных размеров, используя в 10 раз
меньше оборудования. Он удивляется, работали над этими сайтами
по-настоящему хорошие программисты. Давайте взглянем на то, как им это
удалось, и дадим Вам возможность побыть судьей.&lt;/p&gt;
&lt;!--more--&gt;
&lt;p&gt;&lt;em&gt;Перевод &lt;a href="https://www.insight-it.ru/goto/98b904e0/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2009/8/5/stack-overflow-architecture.html"&gt;статьи&lt;/a&gt;, автор оригинала - Todd Hoff. Возможно будет еще один пост с менее формальной информацией на ту же тему.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;16 миллионов просмотров страниц в месяц&lt;/li&gt;
&lt;li&gt;3 миллионов уникальных пользователей в месяц (для сравнения: Facebook
насчитывает около 77 миллионов уникальных пользователей в месяц)&lt;/li&gt;
&lt;li&gt;6 миллионов посещений в месяц&lt;/li&gt;
&lt;li&gt;86% трафика приходит с Google&lt;/li&gt;
&lt;li&gt;9 миллионов активных программистов во всем мире и 30% пользуются Stack
Overflow&lt;/li&gt;
&lt;li&gt;Более дешевые лицензии были получены через программу
Microsoft&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/205088ae/" rel="nofollow" target="_blank" title="http://www.microsoft.com/BizSpark"&gt;BizSpark&lt;/a&gt;. Скорее всего
они заплатили около 11000\$ за лицензии на ОС и MSSQL.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Стратегия монетизации: ненавязчивая реклама, вакансии, конференции
DevDays, достижения других смежных ниш (Server Fault, Super User),
разработка&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/631e88c2/" rel="nofollow" target="_blank" title="https://stackexchange.com/"&gt;StackExchange&lt;/a&gt; и возможно
каких-то других систем рейтингов для программистов.&lt;/p&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Microsoft ASP.NET MVC&lt;/li&gt;
&lt;li&gt;SQL Server 2008&lt;/li&gt;
&lt;li&gt;C#&lt;/li&gt;
&lt;li&gt;Visual Studio 2008 Team Suite&lt;/li&gt;
&lt;li&gt;jQuery&lt;/li&gt;
&lt;li&gt;LINQ to SQL&lt;/li&gt;
&lt;li&gt;Subversion&lt;/li&gt;
&lt;li&gt;Beyond Compare 3&lt;/li&gt;
&lt;li&gt;VisualSVN 1.5&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Веб уровень:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 x Lenovo ThinkServer RS110 1U&lt;/li&gt;
&lt;li&gt;4 ядра, 2.83 Ghz, 12 MB L2 cache&lt;/li&gt;
&lt;li&gt;500 GB жесткие диски, зеркалирование RAID1&lt;/li&gt;
&lt;li&gt;8 GB RAM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Уровень базы данных:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 x Lenovo ThinkServer RD120 2U&lt;/li&gt;
&lt;li&gt;8 ядер, 2.5 Ghz, 24 MB L2 cache&lt;/li&gt;
&lt;li&gt;48 GB RAM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Четвертый сервер был добавлен для запуска
&lt;a href="https://www.insight-it.ru/goto/d3829019/" rel="nofollow" target="_blank" title="https://superuser.com/"&gt;superuser.com&lt;/a&gt;. Все сервера вместе обеспечивают
работу &lt;a href="https://www.insight-it.ru/goto/33fc61d9/" rel="nofollow" target="_blank" title="https://stackoverflow.com/"&gt;Stack Overflow&lt;/a&gt;,&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/9b0a2d16/" rel="nofollow" target="_blank" title="https://serverfault.com/"&gt;Server
Fault&lt;/a&gt;, и&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/d3829019/" rel="nofollow" target="_blank" title="https://superuser.com/"&gt;Super User&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;QNAP TS-409U NAS для резервного копирования данных. Было принято решение
не использовать "облачные" решения, так как вызванные ими дополнительные
5GB трафика ежедневно были бы накладными.&lt;/li&gt;
&lt;li&gt;Сервера располагаются у&amp;nbsp;&lt;a href="https://www.insight-it.ru/goto/4f3f2e08/" rel="nofollow" target="_blank" title="http://www.peakinternet.com/"&gt;Peak Internet&lt;/a&gt;. В
основном из-за впечатляющей детализации технических ответов и разумных
расценок.&lt;/li&gt;
&lt;li&gt;Полнотекстный поиск в SQL Server активно используется для реализации
поиска по сайту и выявления повторных вопросов. Lucene .NET
рассматривается как достаточно заманчивая альтернатива.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;p&gt;Данный список является сборником уроков от Jeff и Joel, а также из
комментариев к их записям:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Если Вы комфортно себя чувствуете в деле управления серверами - не
бойтесь покупать их. Две основных проблемы с издержками аренды
оборудования:&lt;ol&gt;
&lt;li&gt;невероятные цены на дополнительную оперативную память и
жесткие диски;&lt;/li&gt;
&lt;li&gt;хостинг-провайдеры на самом деле не могут управлять
чем-либо за Вас.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Делайте одноразовые более крупные инвестиции в оборудование, чтобы
избежать быстро растущих ежемесячных издержек по аренде, которые
окажутся более высокими в долгосрочном периоде.&lt;/li&gt;
&lt;li&gt;Обновляйте сетевые драйвера. Производительность запросто может
удвоиться.&lt;/li&gt;
&lt;li&gt;Использование 48GB RAM требует обновления до MS Enterprise edition.&lt;/li&gt;
&lt;li&gt;Оперативная память невероятно дешевая. Используйте возможности по её
расширению по максимуму для получения практически бесплатной
производительности. У Dell, например, переход от 4GB памяти до 128GB
стоит всего 4378\$.&lt;/li&gt;
&lt;li&gt;Stack Overflow скопировали ключевую часть структуры базы данных у
Wikipedia. Это обернулось огромной ошибкой, для исправления которой
потребуется большой и болезненный рефакторинг базы данных. Основным
направлением изменений будет избавление от излишних операций по
объединению данных в большом количестве ключевых запросов. Это ключевой
урок, который стоит усвоить у гигантских много-терабайтных схем (вроде
Google BigTable), которые полностью избавлены от операций объединения
данных. Этот вопрос был достаточно важен для Stack Overflow, так как их
база данных практически полностью располагается в оперативной памяти и
операции join по прежнему требуют относительно много вычислительных
ресурсов.&lt;/li&gt;
&lt;li&gt;Производительность CPU оказывается на удивление важным фактором для
серверов баз данных. Переход от 1.86 GHz, к 2.5 GHz, и к 3.5 GHz
процессорам дает практически линейный прирост к времени выполнения
типичных запросов. Исключение: запросы, которые затрагивают не только
оперативную память.&lt;/li&gt;
&lt;li&gt;Когда оборудование арендуется, обычно никто не платит за дополнительную
оперативную память, если только вы не на помесячном контракте.&lt;/li&gt;
&lt;li&gt;В 90% случаев наиболее узким местом является база данных.&lt;/li&gt;
&lt;li&gt;При небольшом количестве серверов, &amp;nbsp;ключевым компонентом издержек
становится не место в стойках, электроэнергия, интернет-канал, сервера
или программное обеспечение, а СЕТЕВОЕ ОБОРУДОВАНИЕ. Вам потребуется как
минимум гигабитное соединение между уровнями веб-серверов и баз данных.
Между интернетом и веб-серверами потребуется firewall, маршрутизатор и
VPN. К моменту добавления второго веб-сервера понадобится решение для
балансировки нагрузки. Суммарная стоимость такого оборудования может
запросто вдвое превосходить стоимость пяти серверов.&lt;/li&gt;
&lt;li&gt;EC2 предназначен для горизонтального масштабирования, для того чтобы
нагрузка могла быть распределена между большим количеством машин
(достаточно хорошая идея, если Вы планируете расширяться). Еще больше
смысла в таком подходе появляется, если вы планируете масштабироваться
по необходимости (то есть добавлять и убирать машины в зависимости от
уровня нагрузки).&lt;/li&gt;
&lt;li&gt;Горизонтальное масштабирование может проходить относительно
безболезненно только при использовании open source программного
обеспечения. В противном случае вертикальное масштабирование значит
сокращение издержек, связанных с лицензиями, в ущерб стоимости
оборудования, а горизонтальное масштабирование - наоборот: экономия на
оборудовании, но требуется существенно больше лицензий на программное
обеспечение.&lt;/li&gt;
&lt;li&gt;RAID-10 отлично работает для баз данных с высокой нагрузкой операций
чтения и записи.&lt;/li&gt;
&lt;li&gt;Разделяйте работу приложений и баз данных таким образом, чтобы они могли
масштабироваться независимо друг от друга. Например, базы данных могут
масштабироваться вертикально, а сервера приложений - горизонтально.&lt;/li&gt;
&lt;li&gt;Приложения должны хранить все информацию о своем состоянии в базе данных
для обеспечения возможности роста путем простого добавления серверов
приложений в кластер.&lt;/li&gt;
&lt;li&gt;Одна из основных проблем со стратегией вертикального масштабирования -
недостаток избыточности. Кластеризация добавляет надежности, но когда
стоимость каждого сервера высока - это не так просто реализовать.&lt;/li&gt;
&lt;li&gt;Некоторые приложения могут масштабироваться линейно относительно числа
процессоров. Но зачастую будут использоваться механизмы блокировки, что
приведет к сериализации вычислений и в итоге к существенному уменьшению
эффективности приложения.&lt;/li&gt;
&lt;li&gt;С более крупными серверами, занимающими от 7U в стойке, электроэнергия и охлаждение становятся критичными вопросами. Возможно использование
чего-то среднего между 1U и 7U может облегчить Ваши взаимоотношения с
датацентром.&lt;/li&gt;
&lt;li&gt;С добавлением все новых и новых серверов баз данных издержки на лицензии SQL Server могут стать очень существенными. Если Вы начнете с
вертикального масштабирования и постепенно начнете переходить к
горизонтальному с использованием не open source продуктов, возможно это
сильно ударит по Вашему финансовому состоянию. Это справедливо, что в
этой заметке речь идет не совсем об архитектуре проекта. Мы знаем об их
серверах, об используемом наборе инструментов, об их двухуровневой
схеме, где база данных используется напрямую из кода веб-серверов. Но мы
не знаем практически ничего о самой реализации, например таких мелочей
как теги. Если Вам интересен этот вопрос, возможно Вам удастся получить
интересующую Вас информацию из &lt;a href="https://www.insight-it.ru/goto/61237733/" rel="nofollow" target="_blank" title="http://sqlserverpedia.com/wiki/Understanding_the_StackOverflow_Database_Schema"&gt;описания их схемы базы данных&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Fri, 08 Jan 2010 00:31:00 +0300</pubDate><guid>tag:www.insight-it.ru,2010-01-08:highload/2010/arkhitektura-stack-overflow/</guid><category>ASP</category><category>ASP .NET</category><category>Beyond Compare 3</category><category>C++</category><category>highload</category><category>JQuery</category><category>Lenovo</category><category>Lenovo ThinkServer</category><category>LINQ</category><category>Microsoft</category><category>MSSQL</category><category>MVC</category><category>Server Fault</category><category>SQL Server 2008</category><category>Stack Overflow</category><category>Subversion</category><category>Super User</category><category>Visual Studio 2008 Team Suite</category><category>VisualSVN</category><category>архитектура</category><category>архитектура Stack Overflow</category><category>архитектура высоконагруженных сайтов</category><category>Масштабируемость</category></item><item><title>Архитектура MySpace</title><link>https://www.insight-it.ru//highload/2009/arkhitektura-myspace/</link><description>&lt;p&gt;&lt;a href="https://www.insight-it.ru/goto/f989d588/" rel="nofollow" target="_blank" title="http://www.myspace.com"&gt;MySpace.com&lt;/a&gt; является одним из наиболее быстро
набирающих популярность сайтов в Интернете с 65 миллионами пользователей
и 260000 регистрациями в день. Этот сайт часто подвергается критике
из-за не достаточной производительности, хотя на самом деле MySpace
удалось избежать ряда проблем с масштабируемостью, с которыми
большинство других сайтов неизбежно сталкивались. Как же им это
удалось?
&lt;!--more--&gt;&lt;/p&gt;
&lt;h2 id="istochniki-informatsii"&gt;Источники информации&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Данная статья является переводом статьи &lt;a href="https://www.insight-it.ru/goto/e9f0b809/" rel="nofollow" target="_blank" title="http://highscalability.com/blog/2009/2/12/myspace-architecture.html"&gt;MySpace
Architecture&lt;/a&gt;,
автором которой является Todd Hoff. Когда-то давно один из читателей
этого блога просил меня осветить и эту тему, тогда я так и не решился
из-за отсутствия моего личного интереса, но сейчас снова случайно
наткнулся на эту статью и подумал: а почему бы и нет?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/45549701/" rel="nofollow" target="_blank" title="http://www.infoq.com/news/2009/02/MySpace-Dan-Farino"&gt;Презентация: за сценой MySpace.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.insight-it.ru/goto/8d5c1d4d/" rel="nofollow" target="_blank" title="http://www.baselinemag.com/c/a/Projects-Networks-and-Storage/Inside-MySpacecom/"&gt;Внутри MySpace.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="platforma"&gt;Платформа&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ASP .NET 2.0&lt;/li&gt;
&lt;li&gt;Windows&lt;/li&gt;
&lt;li&gt;IIS&lt;/li&gt;
&lt;li&gt;MSSQL Server&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="chto-vnutri"&gt;Что внутри?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;300 миллионов пользователей.&lt;/li&gt;
&lt;li&gt;Отдает 100Gbps в Интернет. 10Gbps из них является HTML контентом.&lt;/li&gt;
&lt;li&gt;4,500+ веб серверов со связкой: Windows 2003 / IIS 6.0 / ASP .NET.&lt;/li&gt;
&lt;li&gt;1,200+ кэширующих серверов, работающих на 64-bit Windows 2003. На
    каждом 16GB объектов находятся в кэше в оперативной памяти.&lt;/li&gt;
&lt;li&gt;500+ серверов баз данных, работающих на 64-bit Windows и SQL Server
    2005.&lt;/li&gt;
&lt;li&gt;MySpace обрабатывает 1.5 миллиарда просмотров страниц в день, а
    также 2.3 миллионов одновременно работающих пользователей в течении
    дня.&lt;/li&gt;
&lt;li&gt;Вехи по количеству пользователей:&lt;ul&gt;
&lt;li&gt;500 тысяч пользователей: простая архитектура перестает
справляться&lt;/li&gt;
&lt;li&gt;1 миллион пользователей: вертикальное партиционирование временно
спасает от основных болезненных вопросов с масштабированием&lt;/li&gt;
&lt;li&gt;3 миллиона пользователей: горизонтальное масштабирование
побеждает над вертикальным&lt;/li&gt;
&lt;li&gt;9 миллионов пользователей: сайт мигрирует на ASP.NET, создается
виртуализированная система хранения данных (SAN)&lt;/li&gt;
&lt;li&gt;26 миллионов пользователей: MySpace переходит на 64-битную
технологию.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;500 тысяч учетных записей было многовато для двух веб-серверов и
    одного сервера баз данных.&lt;/li&gt;
&lt;li&gt;На 1-2 миллионах учетных записей:&lt;ul&gt;
&lt;li&gt;Они использовали архитектуру базы данных, построенную на
концепции вертикального партиционирования, с отдельными базами
данных для разных частей сайта, которые использовались для
выполнения различных функций, таких как экран авторизации, профили
пользователей и блоги.&lt;/li&gt;
&lt;li&gt;Схема с вертикальным партиционированием помогала разделить
нагрузку как для операций чтения, так и для операций записи, а если
пользователям в друг оказывалась нужна новая функциональная
возможность - достаточно было просто добавить еще один сервер баз
данных для её обслуживания.&lt;/li&gt;
&lt;li&gt;MySpace переходит от использования систем хранения, подключенных
к серверам баз данных напрямую, к сетям хранения данных (SAN), при
таком подходе целый массив систем хранения объединяется вместе
специализированной сетью с высокой пропускной способностью, и
сервера баз данных также получают доступ к хранилищам через эту
сеть. Переход к SAN оказал положительное влияние как на
производительность, так и на доступность и надежность системы.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;На 3 миллионах учетных записей:&lt;ul&gt;
&lt;li&gt;Решение с вертикальным партиционированием не протянуло долго, так
как им приходилось реплицировать какую-то часть информации (например
информацию об учетных записях) по всем вертикальным частям базы
данных. С таким большим количеством операций репликации данных один
узел даже при незначительном сбое мог существенно замедлить
обновление информации во всей системе.&lt;/li&gt;
&lt;li&gt;Индивидуальные приложения вроде блогов на под-секциях сайта
достаточно быстро стали слишком большими для нормальной работы с
единственным сервером базы данных&lt;/li&gt;
&lt;li&gt;Произведена реорганизация всех ключевых данных для более логичной
организации в единственную базу данных&lt;/li&gt;
&lt;li&gt;Пользователи были разбиты на группы по миллиону в каждой и каждая
такая группа была перемещена на отдельный SQL Server&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;9&amp;ndash;17 миллионов учетных записей:&lt;ul&gt;
&lt;li&gt;Переход на ASP .NET, который требовал меньше ресурсов по
сравнению с их предыдущим вариантом архитектуры. 150 серверов,
использовавших новый код могли обработать нагрузку, для которой
раньше требовалось 246 серверов.&lt;/li&gt;
&lt;li&gt;Снова пришлось столкнуться с узким местом в системе хранения
данных. Реализация SAN решило какую-то часть старых проблем с
производительностью, но на тот момент потребности сайта начали
периодически превосходить возможности SAN по пропускной способности
операций ввода-вывода - той скорости, с которой она может читать и
писать данные на дисковые массивы.&lt;/li&gt;
&lt;li&gt;Столкнулись с лимитом производительности при размещении миллиона
учетных записей на одном сервере, ресурсы некоторых серверов начали
исчерпываться.&lt;/li&gt;
&lt;li&gt;Переход к виртуальному хранилищу, где весь SAN рассматривается
как одно большое общее место для хранения данных, без необходимости
назначать конкретные диски для хранения данных определенной части
приложения. MySpace на данный момент работает со стандартизированным
оборудованием от достаточно нового вендора SAN - 3PARdata&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Был добавлен кэширующий уровень &amp;mdash; прослойка из специализированных
    серверов, расположенных между веб-серверами и серверами данных, чья
    единственная задача была захватывать копии часто запрашиваемых
    объектов с данными в памяти и отдавать их веб-серверам для
    минимизации количества поиска данных в СУБД.&lt;/li&gt;
&lt;li&gt;26 миллионов учетных записей:&lt;ul&gt;
&lt;li&gt;Переход на 64-битные сервера с SQL Server на правах решения
проблемы с недостатком оперативной памяти. С тех пор их стандартный
сервер баз данных оснащен 64 GB RAM.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Горизонтальная федерация баз данных&lt;/strong&gt;. Базы данных
    партиционируются в зависимости от своего назначения. У них есть базы
    данных с профилями, электронными сообщениями и так далее. Каждая
    партиция основана на диапазоне пользователей. По миллиону в каждой
    базе данных. Таким образом, у них есть Profile1, Profile2 и все
    остальные базы данных вплоть до Profile300, если считать, что у них
    на данный момент зарегистрировано 300 миллионов учетных записей.&lt;/li&gt;
&lt;li&gt;Кэш ASP не используется, так как он не обеспечивает достаточного
    процента попаданий на веб серверах. Кэш, организованный как
    промежуточный слой, имеет существенно более высокое значение данного
    показателя.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Изоляция сбоев&lt;/strong&gt;. Внутри веб-сервера запросы сегментируются по
    базам данным. Разрешено использование только 7 потоков для работы с
    каждой базой данных. Таким образом, если база данных по каким-то
    причинам начинает работать медленно, только эти потоки замедлятся, в
    то время как остальные потоки будут успешно продолжать обрабатывать
    поток трафика.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="rabota-saita"&gt;Работа сайта&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Коллектор данных о производительности&lt;/strong&gt;. Централизованная система
    сбора информации о производительности через UDP. Такой подход более
    надежен, чем стандартный механизм Windows, а также позволяет любому
    клиенту подключиться и увидеть статистику.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Веб-система по просмотру дампов стеков процессов&lt;/strong&gt;. Можно просто
    сделать клик правой кнопкой мыши на проблемном сервере и увидеть
    дамп стека процессов, управляемых .NET. И это после привычки каждой
    раз удаленно подключаться к серверу, включать дебаггер и через
    полчаса получать свой ответ о том что же все таки происходит.
    Медленно, немасштабируемо и утомительно. Эта же система позволяет
    увидеть не просто стек процесса, но и предоставляет большое
    количество информации о контексте, в котором он работает.
    Обнаружение проблем намного проще при таком подходе, например можно
    легко увидеть, что база не отвечает, так как 90 ее потоков
    заблокировано.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Веб-система создания дампа heap-памяти&lt;/strong&gt;. Создает дамп всей
    выделенной памяти. Очень удобно и полезно для разработчиков.
    Сэкономьте часы на выполнение этой работы вручную.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Профайлер&lt;/strong&gt;. Прослеживает запрос от начала до конца и выводит
    подробный отчет. В нем можно увидеть URL, методы, статус, а также
    все, что поможет идентифицировать медленный запрос и его причины.
    Обнаруживает проблемы с блокировкой потоков, непредвиденными
    исключениями, другими словами все, что может оказаться интересным. В
    то же время остается очень легковесным решением. Работает на одной
    машине из каждой VIP (группа из 100 серверов) в production-среде.
    Опрашивает 1 поток каждые 10 секунд. Постоянно следит за системой в
    фоновом режиме.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Powershell&lt;/strong&gt;. Новая программная оболочка от Microsoft, которая
    работает в процессе и передаем объекты между командами вместо работы
    с текстовыми данными. MySpace разрабатывает множество так называемых
    commandlets'ов для поддержки различных операций.&lt;/li&gt;
&lt;li&gt;Разработана собственная технология асинхронной коммуникации для
    того, чтобы обойти проблемы с сетевыми проблемами Windows и работать
    с серверами как с группой. Например, она позволяет доставить файл
    .cs, скомпилировать его, запустить, и доставить результат обратно.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Развертывание&lt;/strong&gt;. Обновление кодовой базы происходит с помощью
    упомянутой выше собственной технологии. Ранее происходило до 5 таких
    обновлений в день, сейчас же они происходят лишь раз в неделю.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;С помощью стека Microsoft тоже можно делать большие веб-сайты.&lt;/li&gt;
&lt;li&gt;Стоит использовать кэширование с самого начала.&lt;/li&gt;
&lt;li&gt;Кэш является более подходящим местом для хранения временных данных,
    не требующих персистентности, например информации о пользовательских
    сессиях.&lt;/li&gt;
&lt;li&gt;Встроенные в операционные систему возможности, например по
    обнаружению DDoS-атака, могут приводить к необъяснимым сбоям.&lt;/li&gt;
&lt;li&gt;Храните свои данные в географически удаленных датацентрах для
    минимизации проблем, связанных со сбоями в электросети.&lt;/li&gt;
&lt;li&gt;Рассматривайте возможности использования виртуализированных систем
    хранения данных или кластерных файловых систем с самого начала. Это
    позволит существенно параллелизировать операции ввода-вывода, а
    также увеличивать дисковое пространство без необходимости какой-либо
    реорганизации.&lt;/li&gt;
&lt;li&gt;Разрабатывайте утилиты для работы с production окружением.
    Невозможно смоделировать все ситуации в тестовой среде.
    Масштабируемость и все различные варианты использования API не могут
    быть симулированы в процессе тестирования качества программного
    обеспечения. Обычные пользователи и хакеры обязательно найдут такие
    способы использования вашего продукта, о которых вы даже никогда и
    не подумаете в процессе тестирования, хотя конечно большая часть все
    же обнаружима в процессе QA тестирования.&lt;/li&gt;
&lt;li&gt;Когда это возможно - лучше просто использовать дополнительное
    оборудование для решения проблем. Это намного проще, чем изменять
    поведение программного обеспечения для того чтобы решать задачи
    как-то по-другому. Примером может служить добавление нового сервера
    на каждый миллион пользователей. Возможно было бы более эффективным
    изменить подход к самой работе с СУБД, но на практике все же проще и
    дешевле добавлять все новые и новые сервера. По крайней мере на
    данный момент.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Mon, 21 Dec 2009 16:15:00 +0300</pubDate><guid>tag:www.insight-it.ru,2009-12-21:highload/2009/arkhitektura-myspace/</guid><category>ASP</category><category>ASP .NET</category><category>highload</category><category>IIS</category><category>Microsoft</category><category>MSSQL</category><category>MySpace</category><category>myspace.com</category><category>online</category><category>Windows</category><category>Windows Server</category><category>архитектура</category><category>архитектура MySpace</category><category>Масштабируемость</category></item><item><title>Как проект Ravelry дорос до 10 миллионов запросов с помощью Rails</title><link>https://www.insight-it.ru//highload/2009/kak-proekt-ravelry-doros-do-10-millionov-zaprosov-s-pomoshhyu-rails/</link><description>&lt;p&gt;Данная статься основана на замечательном интервью, взятом Tim Bray у
Casey Forbes, создателя &lt;a href="https://www.insight-it.ru/goto/ce0996b1/" rel="nofollow" target="_blank" title="http://www.ravelry.com/"&gt;Ravelry&lt;/a&gt;, сайта на
Ruby on Rails, поддерживаемое сообществом вязальщиц и специалистов по
вышивке крючком численностью более 400000 человек.&lt;/p&gt;
&lt;p&gt;Casey и его небольшой команде удалось реализовать массу великолепных
идей на Ravelry. Этот сайт очень сфокусирован на своей тематике и
представляет собой большую информационную ценность для заинтересованных
лиц. Все пользователи Ravelry просто обожают этот сайт, этот факт
очевиден по их комментариям полным энтузиазма и невероятно быстрому
освоению Ravelry.&lt;/p&gt;
&lt;p&gt;Десять лет назад сайт масштаба Ravelry потребовал бы далеко не один
миллион долларов для поддержания своего функционирования. Сегодня же
Casey является единственным разработчиком Ravelry, а поддержанием
работоспособности системы занимается всего несколько человек.
Изначальный процесс разработки занял у Casey 4 месяца работы по ночам и
выходным. Если Вы взглянете на список технологий, используемых в
Ravelry, Вам станет видно, что проект построен практически полностью на
свободном и бесплатном программном обеспечении, которые просто было
собрано вместе в единую полноценную систему. В сегодняшней экосистеме
существует множество возможностей для того чтобы делать новые вещи
просто комбинируя существующие качественные приложения, языки
программирования, системы хранения, а также услуги по размещению и
предоставлению доступа к веб-приложениям и данным.&lt;/p&gt;
&lt;p&gt;Сейчас Casey и еще несколько сотрудников живут за счет Ravelry. Не это
ли является мечтой любого предприятия малого бизнеса? Хотите узнать как
и Вы могли бы достичь подобных успехов?
&lt;!--more--&gt;
&lt;em&gt;Данный текст является переводом статьи &lt;a href="https://www.insight-it.ru/goto/24572014/" rel="nofollow" target="_blank" title="http://highscalability.com/how-ravelry-scales-10-million-requests-using-rails"&gt;How Ravelry Scales to 10 Million Requests Using Rails&lt;/a&gt;,
автор оригинала - &lt;a href="https://www.insight-it.ru/goto/f3f1b405/" rel="nofollow" target="_blank" title="http://highscalability.com/user/todd-hoff"&gt;Todd Hoff&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="statistika"&gt;Статистика&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;10 миллионов запросов ежедневно обрабатывается &lt;a href="/tag/rails/"&gt;Rails&lt;/a&gt; (AJAX + RSS + API)&lt;/li&gt;
&lt;li&gt;3.6 миллиона просмотров страниц ежедневно&lt;/li&gt;
&lt;li&gt;430,000 зарегистрированных пользователей. 70,000 активно пользуются
    сайтом ежедневно. 900 новых пользователей регистрируется ежедневно.&lt;/li&gt;
&lt;li&gt;2.3 миллиона проектов по вязанию, 50000 новых сообщений на форуме
    ежедневно, всего 19 миллионов сообщений на форуме, 13 миллионов
    сообщений, 8 миллионов фотографий (большая часть размещена на
    &lt;a href="/tag/flickr/"&gt;Flickr&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Проект начинался на небольшом VPS, но потребности в ресурсах очень
    быстро вышли за его возможности.&lt;/li&gt;
&lt;li&gt;Монетизация: рекламодатели + магазин соответствующей продукции +
    продажа узоров&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="platform"&gt;Platform&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/tag/ruby-on-rails/"&gt;Ruby on Rails&lt;/a&gt; (1.8.6, Ruby GC патчи)&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/percona/"&gt;Percona&lt;/a&gt; сборка &lt;a href="/tag/mysql/"&gt;MySQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/gentoo/"&gt;Gentoo&lt;/a&gt; &lt;a href="/tag/linux/"&gt;Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Servers: Silicon Mechanics (не арендуемые, в их собственности)&lt;/li&gt;
&lt;li&gt;Хостинг: Colocation от Hosted Solutions&lt;/li&gt;
&lt;li&gt;Интернет-канал: Cogent (очень дешево)&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/capistrano/"&gt;Capistrano&lt;/a&gt; для развертывания&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/nginx/"&gt;Nginx&lt;/a&gt; существенно более быстрый и менее требовательный к оперативной памяти по сравнению с Apache&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/xen/"&gt;Xen&lt;/a&gt; для виртуализации&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/haproxy/"&gt;HAproxy&lt;/a&gt; для балансировки нагрузки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/munin/"&gt;Munin&lt;/a&gt; для мониторинга&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/tokyo-cabinet/"&gt;Tokyo Cabinet&lt;/a&gt; / &lt;a href="/tag/tokyo-tyrant/"&gt;Tokyo Tyrant&lt;/a&gt; для кеширования больших объектов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/nagios/"&gt;Nagios&lt;/a&gt; для предупреждений&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/hoptoad/"&gt;HopToad&lt;/a&gt; для уведомлений об исключительных ситуациях.&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/newrelic/"&gt;NewRelic&lt;/a&gt; для тонкой настройки&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/syslog-ng/"&gt;Syslog-ng&lt;/a&gt; для агрегации логов&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/s3/"&gt;S3&lt;/a&gt; для хранения данных&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/cloudfront/"&gt;Cloudfront&lt;/a&gt; в роли CDN&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/sphinx/"&gt;Sphinx&lt;/a&gt; для текстового поиска&lt;/li&gt;
&lt;li&gt;&lt;a href="/tag/memcached/"&gt;Memcached&lt;/a&gt; для кеширования маленьких объектов&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="arkhitektura"&gt;Архитектура&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;7 серверов (Gentoo Linux). Средствами виртуализации (Xen) создано 13
    виртуальных серверов:&lt;ul&gt;
&lt;li&gt;Для обработки пользовательских запросов используются Nginx и
Haproxy. Запросы проходят следущую цепочку: &lt;code&gt;nginx -&amp;gt; haproxy -&amp;gt; apache + mod_passenger&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Один небольшой сервер для резервного копирования данных.&lt;/li&gt;
&lt;li&gt;Один небольшой вспомогательный сервер для некритичных процессов
и тестирования новых версий.&lt;/li&gt;
&lt;li&gt;2 сервера с 32 GB оперативной памяти для master+slave баз
данных, а также поисковой системы Sphinx.&lt;/li&gt;
&lt;li&gt;3 сервера приложений, состоящих из 6 Apache Passenger и
запущенных экземпляров Ruby, каждый ограничен 20-ю потоками.
Суммарно 6 четырехядерных процессоров и 40 GB оперативной памяти.
Часть оперативной памяти большую часть времени простаивает.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;5 терабайт данных располагается в Amazon S3. Cloudfront используется
    как CDN.&lt;/li&gt;
&lt;li&gt;Tokyo Cabinet/Tyrant используется вместо memcached в некоторых
    местах для кеширования более крупных объектов, в частности уже
    размеченного текста в HTML.&lt;/li&gt;
&lt;li&gt;HAproxy и Capistrano используются для вывода новых версий сайта без
    негативного влияния на производительность и работу пользователей.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="podvodim-itogi"&gt;Подводим итоги&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Позвольте своим пользователям работать над Вашим сайтом за Вас&lt;/strong&gt;.
    Проводите итерации и развивайтесь. Начните с чего-то, что просто
    работает, и позвольте людям начать пользоваться продуктом, развивать
    проект совместно с пользователями намного проще. Не торопясь
    развивайте бета-версию своего проекта. Также медленно приглашайте
    новых людей. Старайтесь ежедневно обсуждать с пользователями что бы
    они хотели увидеть нового в проекте. Разрешите им оказывать помощь в
    развитии проекта и результат станет существенно более
    обнадеживающим, утешительным, интуитивно-понятным и эффективным.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Позвольте пользователям спонсировать Ваш проект&lt;/strong&gt;. Ravelry
    частично был создан за счет его пользователей, которые пожертвовали
    в пользу проекта более 71 тысячи долларов. Эти средства были
    переданы проекту просто как дар, а не в обмен на акции. Не
    недооценивайте значимость капитала компании. Ravelry потребовалось 6
    месяцев непрерывной работы и экономии на издержках, связанных с
    серверным оборудованием и каналами связи, чтобы наконец-то начать
    получать прибыль, и полученные от пользователей средства оказались
    основным фактором, позволившим проекту пережить этот тяжелый период.
    Залогом их успеха является поддержание интереса и искры в глазах
    своих пользователей, подталкивание пользователей к оказанию помощи и
    поддержки проекту. Для этого требуется любовь к своему делу и
    самоотдача.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Станьте центром выбранной ниши&lt;/strong&gt;. Найдите нишу на рынке с
    недостаточным предложением. Не стремитесь к массовым рынкам. Совсем
    не обязательно делать что-то для многих миллионов людей. Миллионы
    скорее всего просто зевнут от скуки и в скором времени о Вас
    забудут. Лучше создайте что-нибудь очень полезное для небольшой
    заинтересованной группы лиц и их страсть к их интересам перейдет и к
    Вам.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Успех не обязательно должен быть связан с масштабностью проекта, намного большее значение имеет стабильная и качественная реализация&lt;/strong&gt; &amp;copy; Jeff Putz.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Основная проблема в базе данных&lt;/strong&gt;. Практически вся работа,
    относящаяся к масштабируемости/настройке/производительности, так или
    иначе связана с базой данных. Например, изменение схемы данных для
    больших таблиц в MySQL всегда связано с рядом проблем, особенно если
    простой сервиса неприемлем. Еще один аргумент в пользу баз данных,
    не имеющих схем данных.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Продолжайте получать удовольствие&lt;/strong&gt;. Casey перешел на Ruby on
    Rails так как ему хотелось снова заняться программированием с
    энтузиазмом. Этот факт стал одним из основных факторов, которые
    помогли сделать проект успешным.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Придумывайте новые вещи, которые будут приводить в восторг Ваших
    пользователей&lt;/strong&gt;. Воспользуйтесь магией, людям это нравится. Это тоже
    один из принципов данного проекта. Например по этой
    &lt;a href="https://www.insight-it.ru/goto/e231d34/" rel="nofollow" target="_blank" title="http://www.tbray.org/ongoing/When/200x/2009/09/02/Ravelry#c1252474782.65559"&gt;ссылке&lt;/a&gt;, можно почитать об использовании очень инновационных подходов к управлению форумами.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ruby &amp;mdash; это круто&lt;/strong&gt;. Он представляет собой интересный язык
    программирования, позволивший Ravelry быстро пройти стадию
    изначальной разработки и выпускать новые версии дважды в день в
    период бета-тестирования.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Получайте большую прибыль за счет минимизации издержек&lt;/strong&gt;. У
    Ravelry есть свой магазин с соответствующей тематике продукцией,
    оптовые счета, принтеры и реализующая компания. Это позволяет им
    поддерживать издержки на низком уровне, таким образом их прибыль не
    уходит сторонним компаниям вроде CafePress.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Наиболее сложный переход заключается в переходе от одного сервера к нескольким&lt;/strong&gt;. В этом процессе все меняется и становится более
    сложным и комплексным. Всегда имейте этот переход ввиду, когда
    планируете архитектуру веб-приложения.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;В сегодняшней экосистеме имеется возможность делать массу различных вещей даже обладая минимумом ресурсов&lt;/strong&gt;. Для создания
    комплексного сайта вроде Ravelry больше не нужно много людей или
    финансов. Взгляните на список различных программ, используемых в
    Ravelry, а также на небольшое количество людей, работающих над
    поддержанием работы проекта.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Некоторые люди могут жаловаться, что здесь нет практически никаких
подробностей о том, как же все таки работает Ravelry. Сайты таких
размеров не должны иметь развернутого описания мистического процесса его
масштабирования, такие проекты могут быть построены просто из составных
частей, с умом собранных вместе. И это очень здорово.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Иван Блинков</dc:creator><pubDate>Thu, 24 Sep 2009 11:31:00 +0400</pubDate><guid>tag:www.insight-it.ru,2009-09-24:highload/2009/kak-proekt-ravelry-doros-do-10-millionov-zaprosov-s-pomoshhyu-rails/</guid><category>Ravelry</category><category>Ruby</category><category>Rails</category><category>Ruby on Rails</category><category>Percona</category><category>MySQL</category><category>Gentoo</category><category>Linux</category><category>Capistrano</category><category>nginx</category><category>HAProxy</category><category>Munin</category><category>Tokyo Cabinet</category><category>Tokyo Tyrant</category><category>Xen</category><category>Nagios</category><category>HopToad</category><category>NewRelic</category><category>syslog-ng</category><category>Cloudfront</category><category>S3</category><category>Sphinx</category><category>memcached</category></item></channel></rss>